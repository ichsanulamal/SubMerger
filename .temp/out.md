## Part 01-Module 01-Lesson 01_Welcome to the Nanodegree Program

### 01. M1L1 01 Welcome V1-W2R32yXgwcg.en

Hi, I'm Liz.Welcome to the AI for trading nano degree program.And I'm Eddie. Financial markets have always been complex and in the past few decades,the level of complexity has grown with advancesin modern technology and the growth of available data.But there's so much to process.From stock prices, earnings reports,and news articles, to social media,the list is seemingly endless.Is it even possible to make sense of it all?This is where quantitative financial analysts,or quants play an important role.But what does a quant?And what do quants do?In the next video,you'll hear from Jonathan.A quant who has held senior roles in several major financial institutions,and who has been instrumental in buildingthe nano degree program that you are now a part of.

### 02. Jonathan Larkin - What Is A Quant-G22oM0qv0Hs.en

In his book, The Quants,Scott Patterson really hit the nail on the head.He described Quants as those searching for the truth.The truth being the universal underlying hidden dynamicsin the markets and that has always captivated me.Quants build computational models of the world.Specifically, that could be about financial instruments or markets,and Quants apply the scientific method to finance.You'll find Quants working across the entire financial industry these days.Most people associate the job of a Quant withvery well known quantitative hedge funds andit is certainly true that Quants work in that type of organization,but Quants also work in commercial banks, proprietary trading firms,asset management firms, data vendor firms,and they also work across functions.You'll find Quants working in many aspects of a investment organization.You'll find Quants in the research function.Those are Quants looking for predictive signals anddata that can be applied downstream to a portfolio.You'll find Quants working in risk management trying topredict the risk of a particular portfolio or the risk firm wide.You'll find Quants working in portfolio construction,combining the signals and models produced by other Quants.You'll also find Quants working in data vendors,data vendors who are preparing unstructured data,then to sell to the financial industry.One of the reasons that I'm so fascinated withthis field is that there really is no typical day.That being said, the work product of a Quant is to create a model.That could be a model of financial time series or it couldbe the model of a portfolio or firm-wide risk,or it could be a model to reduceunstructured data to structured form for further use in the investment process.One thing that makes this field fascinating is that things are always changing.There's new developments in markets.There are new datasets.There are new computational techniques.You always need to keep learning and youalways need to continually up your game in order to compete.People who have achieved substantial success in other scientific disciplinesare often humbled when they come to quantitative finance.The reason for this is that signals in financial data are very, very faint,and you need to combine not just a scientific approach,but you need to combine that with understanding of markets,intuition of markets, and domain knowledge about markets.For example, take Isaac Newton who experienced significant stock trading lossesin his personal account and is quoted assaying "I can predict the movements of the heavens,but I cannot predict the madness of people."

### 02. M1L1 02 Interview W Jonathan V1-AeranuDRL7k.en

You're probably really excited to get started,but maybe you have a few questions,like what's a quant?And what does a quant do?We were really lucky when building this nanodegree program.We got to work with some of the leading players in the quant world.To tell you a little more about that world,we have Jonathan Larkin,a strategist in quantitative equities.He's held senior positions inquantitative equity trading at several of the leading financial institutions.

### 03. M1L1 Introducing The Instructors 1 V4-l5gG7r-BWYc.en

Hi, I'm Miriam.Every nanodegree program created at Udacitycomes to life through the efforts of many, many hard-working people.It's my very great pleasure to introduce you to the team ofinstructors and industry experts who will guide you on your learning journey.First step, your Udacity instructors.Jonathan is an expert in quantitative investment strategies,he has a ton of experience relevant to what you'll learn in this program.He was Global Head of Equities at Millennium Management andco-head of America's Equity Derivatives Trading at JPMorgan.Cindy was a quantitative analyst at Bank of America,Merrill Lynch, Morgan Stanley,and Ping An securities.Cindy oversees this nanodegree program as the curriculum lead.Brok has designed several coding projects that students build inUdacity's deep learning self-driving car and artificial intelligence nanodegree programs,and now this one as well.Liz completed her PhD in Physics and experimental neuroscience,she was also a data science instructor at the data incubator.Eddie rect at BlackRock,Thompson writers and Morgan Stanley and hasdegrees in financial engineering and computer science.Miriam, that's me, I'm the senior learning strategist here at Udacity.That I felt courses in mathematics physics and programming here.I've also worked on our community student experience and motivational design,and you'll be hearing from me throughout this program.Arpan has taught computer science courses for Georgia Tech and also for youUdacity's deep learning and natural language processing nanodegree programs.We also worked with a team of industry experts who helped to put togetherthe course materials that will introduce you tothe exciting and fast-moving world of quant training.Kendall has worked as a quant trainer athedge funds such as Citadel and Millennium partners.He also worked as a quantitative researcher at JPMorgan.He holds degrees in electrical engineering and financial mathematics.Justin has held a variety of roles in industry.He was an Investment Strategist atBlackRock Scientific Active Equity Group and most recently,he was a quant research analyst at MUFG Highmark capital.Harry developed algorithmic trading software atMorgan Stanley and was also a portfolio manager at apogee fund management.He is also the Chief Technology Officer at Carlisle blue wave.Murat has a PhD in statistics and has spenthis entire career in systematic electronic trading.He's currently a quant researcher atredex training and has worked for JP Morgan and Citadel.All of us are so excited to support you as you learn and grow,and now I hand you over to Liz and Eddie to talk about the program overview.

### 04. M1L1 05 Program Overview V1-Ci0j_UwLlQQ.en

Now let's take a deep dive into the program,and talk about what you will learn in the next few months.First of all, to get the most out of this program,you'll ideally have Intermediate Python skills including Numpy and Pandas,and prior exposure to statistics,linear algebra, and calculus.We've designed this program to be accessible to anyone who is passionateabout using advanced data analytics to make sense of financial data.With that in mind, we will review certain bath encoding concepts as they areused or point you to other Udacity resources to refresh your background knowledge.The Nadir degree program has two terms.Each term contains four projects and will take three months to complete.Prior to each project,there's a set of lessons, quizzes,and exercises that will introduce you to a topic and prepare you for the project.You will then practice what you've learned by completing the project.Note that much of your learning will come from working on the project,trying different things and even goingbeyond what is required as you explore what's possible.We highly encourage you to take the time,to make each project your own uniquereflection of what you have learned and what you are capable of.Students who successfully complete term one are eligible to enroll in term two.At the end of term two,you will embark on a Capstone Project which will give you the opportunity to useeverything you've learned to design and evaluate quantitative trading strategies.In term one, you will learn about the Fundamentals of Quantitative Finance.You will apply quantitative methods that are practiced inthe financial industry including regression,optimization, and trading signal generation.You will work on projects that reflectthe actual quant workflow as it's done in financial firms,with real data sets.Term one is designed to prepare you for term two,which applies more advanced methods to the quant workflow.In term two, you'll use machine learning algorithms to generate trading signals.You'll use natural language processing to analyzefinancial statements and apply recurrent neural networks to analyze news data.You'll get hands on practice with backtesting,which is a realistic simulation designed to evaluate the effectiveness of a strategy.Finally, you will use advanced techniques tocombine several trading signals together to optimize a portfolio.We hope that sounds exciting to you.We can't wait to see you in the classroom.

### 06. M1L1 MV 01 Intro In The First Five V1-magg5AVJRVA.en

Hey there. I'm Miriam and I work on student motivation and learning science at Udacity.I'll pop up at different moments throughout your needed degree experience,to give you context for why we've designed your learning experience as we have.The goal of the entire Eudacity team is to support you in meeting your goals,whether you're here to expand your understanding of topics you're interested in,or unlock a whole new stage of your career.To help you grow in the ways that you want to,we're going to provide you with productive challenges that will stretch your brain,as well as powerful support from your community.We'll guide you in learning by doing, through your projects,through helping out your fellow students,and through questions we asked you in the classroom tolet you play around with your new knowledge and skills.Actively engaging your brain in these ways will help youdevelop deep, flexible, long-lasting learning.We can't wait to see where you go and what you do with all that you gain here.

### 06. M1L1 MV 04 Study Groups V1-vmjk1EKR6mM.en

Your study group is a small circle of students inyour degree program who will stick together throughout your whole experience.You'll find some mentors there too.Your study group is the place to go to talk about goal setting,work through problems with others in real time,and just get to know some really cool, supportive people.Research shows that teaching others allows you to applyyour learning in new ways that powerfully strengthen your mastery of material,and your study group will give you plenty of opportunities for that.

### 06. M1L1 MV 04a Knowledge V1-lX_is8cq0Bg.en

In today's students like you have access toa technical community driven Q and A platform called Knowledge,where we curate the most useful content to help you grow your skills and expertise.You can search for, ask,and answer questions in Knowledge,helping to expand your learning while simultaneouslycontributing to communities collected mastery.Knowledge is the place to go for efficient help with content related issues.Whether you're stuck wrestling with a tricky problem or want to get some practiceapplying your learning to issues others have faced, Knowledge can help.

### 06. M1L1 MV 04b Project Reviews V1-KJbx9f9VKJE.en

Projects and project reviews area super important part of your learning experience at Udacity.Completion of projects enables you to graduate from the nanodegree program,but more importantly your projects alsoact as substantial building blocks for your portfolio of work.Once you've finished each project, you'll submit it to us,and we'll send it back to you with feedback from one of our mentors.You can submit each project as many times as you needto until it meets the specifications of the rubric.This process of getting feedback from experts inyour community and making changes in accordance with that,will really strengthen your learning.It will also help you produce super high quality work.It gives you practice at the kind of iterative processthat happens in the tech world all day every day.

### 08. Career Services-cuKecPpZ7PM.en

Hi, am Kathleen, I lead the career team at Udacity.We support students in their job search.The careers team knows what employers are looking for in a job candidate and wewant to work with you to market yourself as the best person for the job,whether it's a new role or a promotion at your current company.You need to be proactive in your career development to be job ready at graduation.For this reason, career services are available throughout your Nanodegree experience.It's important to remember,while you're here to learn the technical skills for a job,it's equally important to know how to communicatethose skills to an employer through an application,your online profiles, and in interviews.Let's meet the Careers team will be with you along the way.Hi I'm Rachel. Along with a network of career mentors,our team is here to help you from when you beginto look for jobs until you accept your job offer.Career support is present at different stages throughout your Nanodegree and weencourage you to go through all of them regardless of where you are in your job search.Udacity career services include guides, lessons,and personalized feedback on topics like talking to recruiters,interviewing, and optimizing your online profiles.I'm Trinh. In order to graduate career ready,you should continuously work on your career development.Like with all learning, beginning something earliermeans you're more prepared when it's time to apply that learning.When your dream job pops up,you want to be ready for it.Hi, I'm Kirsten, and I head up employer relations at Udacity.Once you've completed the career curriculum,you'll be ready to apply to jobs and talk with employers.Udacity partners with industry leading companies,who are looking to hire skilled and passionate people, like you.Having hiring partners means you'll havedirect and preferential exposure at these companies.This will help give you a leg up in your job search,as you explore your career options and develop a career path you need to you,your passion, your location,and other career goals.After you complete your first core Nanodegree projects,you'll then be prompted to begin your career development with us.We'll be there with you from then on as youexplore your career options and pursue your dream role.We look forward to working with you.

### 08. Jonathan Larkin Careers-QhHNPxM_Ku4.en

My name is Jonathan Larkin,and I've worked in quantitative finance now forabout 20 years at Investment Banks Hedge Funds and Fintech startups,and I'm very excited to be advising Udacity on this program.I wanted to become a quant trader,because I was genuinely very,very fascinated with the field.It's an incredibly interdisciplinary field, to be successful,you have to draw uponpure technical skills in areas like computer science and mathematics,but you also have to understand markets,understand psychology, and it needs to be able tobring that together in a commercial result.I really enjoy working with complex dynamic systems,and I've always enjoyed that invarious endeavors and there's nothing more complex than financial markets.There's always something to learn.If I look back on my career spanning over 20 years,I've seen things change in an extraordinary way.When I first started in the field,there was no algorithmic trading,now everything is completely automated.You'll find quants working across the entire financial industry these days.Most people associate the job of a quant with very well known quantitative hedge funds,and it is certainly true that quants work in that type of organization,but quants also work in commercial banks, proprietary trading firms,asset management firms, data vendor firms,and they also work across functions.You'll find quants working in many aspects of an investment organization.You'll find quants in the research function,those are quants looking for predictive signals anddata that can be applied downstream to a portfolio,you'll find quants working in risk management trying topredict the risk of a particular portfolio or the risk firm wide.You'll find quants working inportfolio construction combining the signals and models produced by other quants,you'll also find quants working in data vendors,and data vendors who are preparingunstructured data than to sell to the financial industry.The curriculum in this program is specifically designedto be very very practical and very applicable to the quant trading process,both in a classical sense,but also in the analysis of new data and then theapplication of machine learning and artificial intelligence techniques.As such, students coming out of this program shouldbe very attractive candidates to go into quant-trading rules.The traditional way to become a quant is to get a stem based degree at a top university.Go to your Career Center and they'll introduce you to a host of financial firms,you interview, you get an offer,you join an analyst program andthen you get training and mentorship and you take it from there.Quant firms have traditionally hired from say the top 20 universities.The problem with that is say for circumstantial reasons,the top talent in the world may not be at those 20 universities.Given that quant firms are in a war for talent,and their success is dependent onnew innovative and diverse ideas coming from their employee base,it really behooves them to cast the widest possible net.The Udacity nanodegree, I'm confident will bea strong feeder for this next generation of quant.

### 09. MV 05 Time Management V1-22PdQNlhCt8.en

The single biggest contributor tostudent success in nanodegree programs is time management.We at Udacity aim to create a learning experience that will motivate you tostick with your learning and help you get excited to engage with content all the time.But, realistically, many students havea lot of other things going on in their lives like jobs,families, or other big commitments.Because of that, we provided you witha timeline in your program to help you pace yourself.This will allow you to make substantial progress,and will also act as a guide to how you're doing.You can think of the suggested deadlines as learning milestones to shoot for.Each one represents a tremendous amount of effort and growth toward your goals.If you're not able to meet a deadline all is not lost.The most important thing is to stick to a rhythm of learning that you cansustain and to dive right back intoyour nanodegree program as soon as you can if you have to take a break.To figure nanodegree learning into your life effectively,I recommend a two-pronged approach.Tip number one, continue to block outtime on your calendar each week for your nanodegree program.The most successful Udacity studentswork on their content and projects multiple times a week,not just in one big block.Having at least three substantial chunks of learning times sprinkled throughoutyour weekly schedule will help you createa sustainable routine and will help your learning stick much better.Tip number two, slot in smaller chunks of learning whenever you have a few minutes free.Using Udacity's mobile apps is a great way to turn your train ride,time waiting in a line, or random free moment into a productive step toward your goals.Every little bit counts.Following these best practices will give your brainfrequent chances to digest new material,which will help you develop strong skills and deep knowledge.Before you know it, jumping into new challenges andkeeping up with your timeline will feel much more natural.

### img

## Part 01-Module 01-Lesson 02_Get Help from Peers and Mentors

### img

## Part 01-Module 01-Lesson 03_Get Help with Your Account

### img

## Part 01-Module 01-Lesson 04_Stock Prices

### 01. M1L2 01 Stocks V6-23sv5ey0ySs.en

You've probably heard of stocks and shares before.In this lesson, we're going to discuss stocks and stock prices.We'll also clarify the meaning of some of the words you'll hear when discussing them.Shares of stock represent fractional ownership in a company.If you own one share of a company stock and the stock has beensplit into eight shares in total or eight shares outstanding,you own one-eighth of the company.These days however, companies split their stock into millions or even billions of shares.Facebook for example, has issued just under three billion shares,while Alphabet, the company that owns Google,has issued about 700 million shares.But what does it really mean to own part of a company as a shareholder?Can you take the company car out for a spin? Not usually.There are actually two types of stock,common and preferred stock.Shareholders who own common stock usually get a portion ofthe company's profits in the form of dividends if the company distributes them.A dividend is a payment made by a corporation to its investors,usually as a distribution of profits.Shareholders also get the right to vote onmatters like who should be on the company's board of directors.If the company is liquidated due to bankruptcy,they also get a portion ofthe remaining assets after all other stakeholders are compensated.Generally, ownership of shares in publicly traded companies,companies whose stock is owned and traded by the general public,entails what's called limited liability.In part, limited liability means that shareholderscannot be made to pay the company's debts if it goes bankrupt.Shareholders who own preferred stock get a slightly different deal,they're promised a fixed amount of income each year and getpaid before owners of common stock get paid dividends,but they usually do not have voting rights.But of course, one of the main benefits of owning stock isthe potential opportunity to profit from selling it after it's value has increased.These increases in value are called capital gains.

### 02. M1L2 01 Stock  Pt II V1-SGb54HLbk1g.en

Let's take a moment to clarify the meaning ofsome words you might hear when discussing stock.A security is a financial instrument that has some type of monetary value.Some securities that you might have heard of are stocks, bonds, and options.Securities can be classified into three broad types;debt securities, derivative securities, and equity securities.Debt securities represent money that is owed andmust be repaid like government or corporate bonds,or certificates of deposit.Debt securities are also called fixed-incomesecurities because they promise a stream of income overtime in the form of interest either ata fixed rate or at a rate determined by a specific formula.Derivative securities such as options or futures contracts areso-named because their values depend on the prices of other assets.For example, an option is a contract which gives the buyer the right but notthe obligation to buy or sellan underlying asset at a specified price on a specified date.An example of an option contract would be employee stock options.As part of employee incentive programs,employers frequently offer employees the option to buycompanies stock at a lower price called the strike price,and later sell it on the open market to make a profit.On the other hand, a futures contract obligates the buyer to buy orthe seller to sell an asset at a predetermined price at a specified time in the future.For example, say, a dairy farmer concurrently sell milk for $2 a gallon.Next month, he plans to sell 200 gallons of milk,but he's concerned that fluctuations in the market price for milk will erode his profits.So he enters into a futures contract with an investor.By doing so, he is obligated to sell 200 gallons of milk on the agreed date,but he's guaranteed to receive $400 regardless of the market price of milk at that time.This type of agreement would protect him from the risk ofcatastrophic loss if the price of milk tanks,but will also limit his ability to profit if the price of milk rises.Then there's that last category, equity securities.Let's break that phrase down.Equity is the value of an owned asset minusthe amount of all debts or liabilities on that asset.So if you own a $15,000 car,but you owe a $5,000 loan against it,the car represents $10,000 of equity.The word equity can have somewhat different meanings in different contexts.But in general, you can think of equity as the net value of something owned.Stocks are called equity securities because they represent ownership in a firm.One more thing, a security representingownership interest in a private company is called private equity.

### 02. M1L2 02 Stock Prices V7-l_PilXVuh8I.en

So say, you want a chance to reap the potential financial rewards of investment in stock.Where do you start? Well, usually,you find a brokerage.A brokerage company functions as a middleman connecting buyers and sellers.However, they usually charge fees to complete transactions.It is also sometimes possible to buy stock directly from companies.In this case, you don't have to use or pay commissions to a broker,but you have less control over the price and timing of your purchase.You can't buy or sell at a specific market price or at a specific time.Typically, the company buys or sells shares forthe plan at set times and at an average market price,but how the shares are purchased depends on the company and the details of the plan.Once you've bought some stock,you'll be very interested in the patterns of changes inits price because in order to make money from your investment,you want to sell it when its price is higher than when you bought it.A great deal of effort has gone into predicting when the price of a stock will go up.In order to follow your stock,you'll look up its unique symbol or ticker.Alphabets unique symbol is GOOG,Apples ticker is AAPL.This is a price graph of Alphabet stock from 2005 until September 2017.While the graph seems to progress upwards,it isn't a steady or linear climb.The history of a stock's price is important andpotentially an indicator of how the stock will do in the future.As such, price histories have been studied byinvestors and traders since stocks were first traded.But when was that?How did all this get started in the first place?Some people trace the history of stock back tothe Roman Republic when the state contracted out many ofits services to private companies which issuedshares to individuals to help with government services.In the early 1600s,the Dutch East India Company became the first company inhistory to issue bonds and shares of stock to the general public.The company established the Amsterdam Stock Exchange in1602 as a forum for trade of securities.The Amsterdam Stock Exchange pictured here couldbe called the first incarnation of a modern stock market.Patterns in market activity have been studied since marketsearliest days and some patterns have been observed to repeat.One of the most interesting market phenomena is the market bubble.Bubbles occur when market participants drive stock pricesabove their value in relation to some system evaluation.There are notable similarities betweenthe tulip bulb bubble which took place for centuries ago and saw the value ofa single tulip bulb grow to nearly 10 times the annual wage ofa skilled worker and the.com bubble which occurred in the early 2000s.The similarities in the graphs are fascinating and academics frombehavioral economists to theoretical mathematicians havedevised captivating theories as to why bubbles might happen.One of the earliest works Extraordinary Popular Delusions and the Madness ofCrowds was published by Scottish journalists Charles Mackay way back in 1841.If you're interested in reading more about market bubbles or understandingthe forces driving markets which are not explained by standard quantitative models,I'd suggest the work of Robert Shiller and Nassim Taleb.

## Part 01-Module 01-Lesson 05_Market Mechanics

### 01. M1L3 01 Intro V4-LE-4Xf8lzHk.en

This lesson will introduce you to the mechanics behind modern-day stock markets.But not only that,you also build a simple market simulator that matches and executes orders,and sets prices just like a real market.This will give you a better idea of all the activities that keep a market running.These activities generate a lot of data.Market data, which may hold important clues that youcan use to decide when to buy or sell stocks.As you get introduced to the various dimensions of data available to us,try to build an intuition for why they might be important,what signals they might carry.Finally, you will learn to deal with data duringthe closing and opening of global markets.If you pay close attention,there might be some training opportunities there too.Are you ready? Let's get on with it.

### 02. M1L3 02 Farmers Market V3-i_itXOdetCc.en

This is Betty. She sells the best fruit.Let's ask her how she decides what price to sell at.Good morning.Morning. So, if you don't mind me asking,I was wondering how do you choose the prices for your oranges.Sometimes I see they're selling at two bucks a pound,some days they're four bucks a pound, what's going on?Well, I obviously try to cover my costs and then keeps some profit.That gives me a starting point.Sometimes people will buy it at that price whichmakes me think I could get a bit more money for them.So, I bump up the price a little bit.If I don't see anyone buying,I might lower the price later in the day.Does that always work or do people come back and bargain with you as well?Yes, all the time.They'll often want to pay a lower price and then I tell them what price I can sell at,and we come to a middle ground.Some people are smart. They'll ask around whatprices others are selling it and then pick the lowest price.Eventually, we`re all forced to sell at that lowest price.I see. So, the prices of oranges are set by actual transactions,like the maximum price a buyer is willing to buyat and the minimum price the seller is willing to sell at?You got it,

### 03. M1L3 03 Trading Stocks V3-GHoRtfUrUMc.en

Stock markets function in a very similar way.When a company first begins trading publicly it typically picksa value for its stock based on certain company metrics like revenue,profits, assets et cetera.But from that point on,it's stock prices almost entirely driven by how much demand exists for its stock.But how do you quantify demand?Think about it. Demand is nothing but how many people want to own some stock,or how much money they're willing to pay for it.In market speak you want to seehow many orders people are placing for it and at what price.Modern markets keep an electronic record of all orders that have been submitted.People who want to buy a stock can specify howmuch or the limit that they're willing to pay for it.People who want to sell can specify the least amount.Again the limit they're willing to sell it for.Whenever a suitable match is found between a buyer and seller,a trade is executed.It's important to understand how orders are matched.Let say I submit a limit buy order for two shares of Netflix at $180 each.Think of this as an offer or commitment to buy.It is also known as placing a bid where $180 is my bid price.This order gets added to the market's queue whereall the different buyer orders for Netflix are listed sorted by price.Someone else submits a limit sell order for four shares at $179.90.This is a commitment to sell.$179.90 is their asking price.Since that is less than the top most buy order,the one with the highest bid price a transaction cannot take place.If there are enough stocks to fulfill that order thenthe next bid could also be fulfilled with that same seller.In this case the top to buy orders areexecuted and three out of the four shares offered by the seller are sold.The remaining one share can be used to partially fulfilled my buy order,and the rest of it can be satisfied by the next seller who offers a suitable price.Say someone else offers to sell three shares at $179.80.This order rises to the top of the sell queue due to its low price.Since this is less than my bid price my remaining one share can come from the seller.So, in the end,with my limit order of my two Netflix shares at $180 I got one share at $179.90,and one at $179.80 for an average price of $179.85 per share.This sounds a lot like an auction doesn't it?In fact, multiple auctions going on simultaneously orchestrated bysome authority that ensures every buyer andseller gets the best available price for the stock they want to trade.This authority is the stock exchange.One problem with this model is that trading can take a lot of effort ifeveryone has to participate in an auction each time they want to buy or sell a stock.What if you wanted to purchase some right away,how do you make sure you're getting the best price?Well, if you're at a farmers' market you might go around asking every vendor,but that's very inefficient.Fortunately, since stock market transactions are carriedout electronically we have better ways of dealing with this.The stock exchange keeps track of the last price at which a stock was traded.This is publicly posted as the current price of the stock,and if you wish to buy or sell shares aroundthat price you should be able to do that immediately.This is known as placing a market order.But it does introduce one complication,someone has to be willing to sell or buy that stock on the other end at all times.That's where a market maker comes into the picture.A market maker is a financial firm.Usually a brokerage that continuously offers tobuy and sell stocks at publicly advertised prices.For example, the New York Stock Exchange has designated market makers such asCitadel securities and J Stryker and Cole who areobligated to make large volumes of shares available for trading.You might think why would someone want to do that?What if a market maker buys a bunch of sharesand then the price starts falling. That's right.They do take a risk by continuously buying and selling.But they are usually compensated for that risk bycommissions and fees that they earn on those traits.They also maintain a small difference between the bid and ask prices for each stock.Say you want to sell some Microsoft shares at market price,a market maker is bidding $74 per stock which happens to be the best bid price available.So the market executes your order with that market maker.Later, another investor comes in and wants to buy some Microsoft shares at market price.The same market maker maybe asking for a slightly higher price to sell say $74.10.Assuming that's the best price available they just made $0.10 on each share,that might not seem much.But over the course of a day,a market maker typically trades thousands or millions of shares.That easily adds up.The difference between these prices is known as the Bid-Ask Spread.This is a very important concept to understand intrading and comes into play in other situations as well.

### 04. M1L3 04 Liquidity V4-KNVQeH6Y_YA.en

Market makers play an important role in keeping a stock market running smoothly.Imagine what would happen without market-makers?It would be harder to buy or sell stocks at a consistent price.As people start selling a stock,its price would start falling and vice versa.Formally, we say that a market maker provides liquidity.Liquidity is the property of a financial asset like a stock,to be bought or sold without causing sharp changes in its price.For instance, Apple stock sees a lot of day-to-day activity.So, we say that it is liquid.If you have a large number of Apple shares and you want to sell them,there's a good chance you will get a consistent and predictable price for them.On the other hand, stocks that are relativelydifficult to buy or sell are said to be illiquid.Penny stocks, for example,are typically very thinly traded.Consequently, buying or selling them may be difficult.Liquidity can also vary from market to market.Oh yes, the same stock can be bought or sold in different markets.HSBC is one such stock.It's listed in both New York and Hong Kong.Let's look at how its price changed during a month of trading.Each market maintains its own book of orders sothe same stock can trade at different prices in different markets.The difference is usually small, as you can see,and any big changes permeate across markets pretty quickly.That being said, it's possible to profit frommarket inefficiencies by simultaneouslypurchasing and selling an asset in different markets,thereby exploiting the differences in price.This idea is known as arbitrage.

### 05. M1L3 08 Tick Data V4-2O0eSKmI6YQ.en

Stock exchanges publish a stream of data that includes each individual trade.This is known as tick data.Ticks are an intuitive way to gauge the health of a stock or even an entire market.Are investors happy?Are they making money?Are prices generally rising or falling, ect.?For example, you can compare each stock's latest tick pricewith its previous price to see if it is going up or down.You can then aggregate this information by comparingthe number of stocks on an up-tick with the number of stocks on a down-tick.These ticks also form the basis for all market data that is available for analysis.Tick data can provide further insight into howa particular stock is behaving and can help you take better intraday decisions,but you're talking about a massive volume of data.Have you ever wondered how busy stock markets are?The New York Stock Exchange carries out billions of trades every day.That's billions with a B.If you want to keep track ofall these transactions and use all that data to make trading decisions,that might slow you down.Even if you want to use historical tick data to buildstatistical models or train a learning algorithm,it might be too much data to process.Consider that over $42 trillion worth of stocks and shares were traded in the US in 2016.A single year.That's more than double the GDP of the entire country.Fortunately, market data can be summarized in a way that drastically reducesthe volume of information to be processed andyet retains most of the important characteristics.

### 06. M1L3 09 Open High Low Close V4-FgNY4YgVWFk.en

Investors like to focus on pieces of data that are most important.One way to do that, is to take the tick dataand bucket it into regular intervals of time.It could be minutes, hours, days, months, extra.Then summarize all the trades that happened duringeach interval using measures that are relevant to you.Let's take a closer look at one of these intervals.Let's say it's a minute.The most common set of measures used in practice are Open High Low Close or OHLC.Open is the stock price at the beginning of the period,high and low capture its range of movement,and close is where it ends.These measures are often visualized using OHLC bars,which show these four measures for each time interval in a compact form.This gives you a high level summary that might betterrepresent price movements than looking at lower prices.Here's an OHLC chart,that shows a month of price data bucketed into single days.You can spot days where the intraday prices jumped beyond opening and closing prices,indicating a lot of volatility.If you try to plot all the individual takes over this period,it would be hard to identify these patterns.Sometimes these measures are used for a very specific purposes.The daily closing price is the one that is coded most often.It is used by casual traders and investors who are interested in long-term gains.It is also used for accounting purposes.The opening price, is where you would expect the first trade of the day to take place.There might be a gap from last day's closing price,because of pre-market trading or trading in other markets.Some trading strategies try to utilize this difference.High-low, the high and low prices capture the movement of the stock.These peaks are easier to remember for humans compared toindividual trades or the distribution of prices during that period.

### 08. M1L3 10 Volume V3-DFp7kp0xRCo.en

Besides the current price of a particular stock.Another metric that traders liked to keep an eye on is the numberof shares that are being traded over a period of time known as volume.Often shown along the bottom of an OHLC chart.The sum of unit price times volume gives youa more accurate measure of the total amount of money moving around.Volume is also important because it can affect how sharply its price might rise or fall.Let's take a look at an example.Acme Inc is trading at $100 per share.Several investors speculate that Acme is going to launcha new product and expecting the new product to increase Acme's value significantly,they place large orders to buy a total of 100,000 shares.The action of buying the shares increases the apparent demand for the stockand consequently the price jumps to $120 per share.That is, someone actually buys the shares at $120.Why does it jump? Because there are more people whowant to buy the stock than those who want to sell.So, the sellers can charge a premium and you bet they do.Let's assume that over the next few days there aren'tany announcements and no significant new activity in Acme's stock.Some shareholders want to pull out their moneyand invest in another opportunity they hear about.So the price begins to fall gradually.Noticing this trend other investors decide to sell the majority of shares they bought.So that they can make a profit while the price is still high.As expected, this results in a sharp drop inAcme's stock price it comes down to $90 per share.If you're monitoring the price and volume of Acme's stock,you might have noticed the relationship between volume of transactions and price changes.In general, large volumes of buy orders tend to sharply increasethe stock price and large volumes of sell orders make it fall.This is another signal that you can use to decide when to trade a given stock.It is also something to keep in mind if you'rea major investor and you trade in large volumes.Some hedge funds for instance will spread their transactions over multiple days,so that their own actions don't end up affecting the price adversely.Note that the volume of transactions also varies throughout any trading day.This is something to be aware of if you want to use volume as a trading signal.Stocks that are of active interest willtypically see a lot of training at the beginning of the day.Since that is when traders get to act on,all the new information gathered and analyzed since the previous day's market close.This initial activity known as price discovery,helps buyers and sellers converge on a mutually acceptable market price for each stock.Then the volume falls to a lower level as typical daily trading activity resumes.Finally, towards the end of the trading day,activity tends to increase a little resulting in a higher volume.This can be due to several reasons including day traders who want to close outany open positions and funds that typically update their holdings at the end of the day.In fact, some traders who have decided to buy-sell particular stocks,may not want to risk waiting for the next day.Who knows what events might take place in the meantime and how they might affect prices.Volume carries a lot of information about a stock and is an often overlooked metric.High volume trade, is more likely to affect the price of a stock than a low volume trade.Therefore, volume is an important factor to consider within your trading strategy.

### 09. M1L3 12 Gaps In Market Data V3-jMT3VbUGiZI.en

Stock markets typically remain open for a finite number of hours every day,say 9:30 AM to 4:00 PM.This is when the majority of the transactions take place.Why you ask?Well, traders and exchange operators are humans just like you and me.They need to go back to their families,sleep, and live a life.Okay, there is another reason.With the growing popularity of algorithmic trading,a large portion of transactions today are being carried out by automated systems.They can be much more responsive to market conditions than humans,and result in a sharp increase or decrease instock prices over a very short period of time.If left unchecked, this can result in things spiraling out of control.Closing market operations at a regular time every day providesan artificial barrier that can limit the damage such events can cause.This also give stock market regulators some time toanalyze the situation and implement control measures.Now, some exchanges do allow for pre-market and post-market trading.Typically 4:00 AM to 9:30 AM for pre-market,and 4:00 PM to 8:00 PM post-market.However, not many traders participate in the session.So, trading volume is typically low.Either way, the takeaway from this is thatstock markets close operations at a certain time every day,and they typically remain closed over weekends and holidays.As a result, when you look at stock data,you'll notice these gaps or discontinuities during periods when the markets are closed.Depending on your analysis,this may or may not have an impact on the conclusions you draw.For instance, if you treat market data as a simple sequence of observations and ignorethe timestamps as if the market's just smoothlycontinue from one end of day to the next opening bell,then these gaps are not as significant.But, if you compute a metric that involves time like the number of transactions per hour,then ignoring these gaps can give you inaccurate results.The discontinuities can produce spikes or unusual flats in calculated values,or just outright wrong information.So, you need to be careful when dealing with market data.

### 10. M1L3 14 Markets In Different Timezones V3-wmmEpPM-HVs.en

Another aspect worth considering is that most major stock markets areactual physical buildings located in a city somewhere on the planet,and because different cities around the globe are at different time zones,the actual opening and closing times also vary.On any given date,the Hong Kong Stock Exchange opens at 09:30 AM local time.Seven hours later, the London Stock Exchange opens at 09:30 AM their local time.And the New York Stock Exchange opens five hours after that.This produces some additional complications and opportunities for traders.For example, consider the stocks that are listed on multiple global exchanges.The price behavior of stock onthe Hong Kong exchange is likely to have an effect on how it performs in London.With a full seven hours between them,you have plenty of time to decide how you want to trade that day on the London Exchange.Imagine the kinds of strategies you can devise.If HSBC rises in Hong Kong,you can buy HSBC in London the moment the market opens,knowing that it is likely to rise there too.Of course, you have to be careful about how you execute such strategies.Remember that if you are trying to take advantage of markets in different time zones,chances are high that other traders are too.

### 11. M1L3 15 Outro V2-XVvfToYCsmo.en

Now that you know how stock markets work,how trades are executed and how prices are set.We'll move on and take a closer look at the data you need to pay attention to.Market data forms the basis for quantitative analysis and trading.In addition, there are several other sources ofinformation that you also need to take into account,such as corporate actions,data derived from fundamental information et cetera.All of these affect stock prices.So you need to factor them in when designing your quantitative trading pipeline.

## Part 01-Module 01-Lesson 06_Data Processing

### 01. M1L4 01 Stock Data V2-sN0_IqmMGGA.en

Let's use the knowledge you took from Marketplace Mechanics,and dive deep into the data.The data is the most important thing to quantitative analysis.Without good data, you don't have good predictions.That is sometimes referred to as the garbage in, garbage out principle.That is the quality of your data directly impacts the quality of your predictions,no matter what model you use.In this lesson, we'll talk about a few common data setsthat you'll use and the difficulty with working with that data.

### 02. M1L4 02 Market Data V5-9aEp374GsgQ.en

Let's start with market data.This is any data that you receive from the market typically generated from trades.This is perhaps the most important form of data relevant to quantitative analysis.Market data is temporal in nature,it's a series of trading events that happen in a moment of time,each moment is called a tick.A ticker contains the time of the event,information about which stock or ticker symbol is traded trade data and quote data.The trade data is the price and the amount of the transaction.The quote data is the price and size of the bid and ask.A bid is request to buy stock at a price for a set amount of shares.A ask is the other side of the trade it's the request tosell stock at a price for a set amount of shares.In large marketplaces the number of trades can easily hit 200 trades per second.This is a massive amount of data when using historical tech data.Analyzing individual texts may not be visible with this much data.So, we bucket these texts into equally space time integrals such as minutes or days.For each bucket, we can computeopen high low close prices and total volume of transactions.This sort of minute or day level data is much easier to work with.You have the ability to ignorethe timestamps and treat the data as equally spaced sequences.In some cases, you may still need to use the timestamps.But we'll save that for another lesson.In the next video,you will learn about another type of finance data called corporate actions.

### 03. M1L4 04 Corporate Actions V5-S60WArbQK7k.en

The next set of data we'll talk about is related toevents a company can take that affects its shareholders.This data is called Corporate Actions.For this lesson, we'll describe only two of the many corporate actions.Stock splits and dividends.Let's start with stock splits.On June 2nd, 1998,you could buy a share of Amazon for about $85.On this date, Amazon decided to perform a two to one stock split.Every Amazon shareholder, had their shares doubled.This does not mean all of their money in Amazon doubled.When a stock is split into two,its price drops by half.This makes sure that the total market capitalizationof a stock has not changed by a split.Market capitalization is the dollar value of a company's outstanding shares.This is calculated by multiplyingthe stock price by the total number of shares outstanding.In this case, there are twice as many outstanding shares.To keep the market capitalization the same,the stock price has to drop by half.Why would you ever want to split a stock?One reason could be to make the stock more liquid.Less expense of stocks are believed to be more liquid.Current shareholders and potential investors can now buy and sell and more granularity.This helps maintain a healthy volume of transactions.Amazon later went on to have two more stock splits.A three-to-one split on January 5th, 1999.A two-to-one split on September 2nd of the same year.The price dropped to roughly one-third and one-half respectively.I say roughly, because there is still some trading during the day.The closing price ends up being a little different from exactly one-third or one-half,typically, a little higher due to the renewed interest in the stock.If you look at the graph of the stock prices at that time,you'll notice that the large drops are due to the stock splits.Looking at this graph makes it look like the company dropped in value.Using these prices, a computer would read the data the same way.However, this is not true.The value of the company has not changed from the split.How do you correct for this?One common approach is to normalize the prices to reduce the sudden changes.For instance, you could double the stock price after each two-to-one split,triple after each three-to-one split, and so on.The trouble with that is,your current normalized price will bevery different from what the stock is actually trading at.Instead, we'll do the opposite.We'll have the price before each two-to-one split,turn it into thirds before any three-to-one split, and so on.Now, the newest adjusted price matches the price currently on the market.Stock prices normalized in this manner is called adjusted price.This is typically provided in most historical data sets.Needless to say, you must keep in mind that this is a distortion of reality.In June of 1988,before the first split,Amazon was not trading at seven dollars.It was trading around $85.It is the repeated adjustments to historical prices that have reduced 85 to $7.With this in mind,let's learn about the next corporate action, dividend.

### 03. M1L4 04b Dividends V2-OVZw9tci55w.en

The next corporate action we'll talk about is dividend, specifically cash dividends.Dividends are when companies sharesome fraction of their profits with their shareholders.Let's take a look at Qualcomm which pays out dividends pretty much every quarter.On May, 21st of 2017,they paid out $0.57 per share.However, dividends are given to everyone that holdshares in the stock at the time of the payout.A shareholder must have bought the share before the ex-dividend date.An ex-dividend date is the cutoff point to receive the future dividends.Let's take a look at an ex-dividend date for AIG.AIG had announced on February 8th,2018 that they will be giving out dividends in the future.The date the dividends will be given out is March 29th,2018, the ex-dividend date is March 14th, 2018.Whoever held the stock before the state will receive the dividend.With the basics of dividends out of the way,let's go over adjusting the prices to dividends.We'll start with why we adjust the prices based on dividends.Imagine that you own one share of company A and one share of company B,both are worth $50.The next day is an ex-dividend date of $1 in dividends for company A.That day, company A closes at $49.50.Company B does not have ex-dividend date,but also closes at $49.50.If you only looked at the prices,you might think you lost $0.50 on both stocks.In reality, you made $0.50 on company A and lost $0.50 on company B.Just like the stock splits,we'll normalize the prices to reflect this.To get the normalised prices,we first need to calculate the adjusted price factor.The formula for this is 1+D over S. D is the dividend,S is the stock price at ex-dividend date.To normalize the price,you would divide the historical price bythe adjusted price factor up until the day before the ex-dividend date.With the knowledge you've just learned,let's dive into technical indicators.

### 04. M1L4 06 Technical Indicators V6-jo740Kq3YN4.en

So, you have your stock prices adjusted for corporate actions like splits and dividends.How do you use this information to perform trading?When you buy, when you sell,or even which stocks do you buy or sell.You can take these decisions based on signalsthat can be derived from historical price data.The first step in this process is to computestatistical measures that are called indicators.You can think of the raw price of a stock as the most basic kind of indicator.Here are the stock prices for Facebook over a one-year period.The latest price is about $115.Is that good? Should we buy some Facebook shares?Well, it's not clear.The price seems to be jumping around a lot andwe don't have a sense of where we should expect to be.If we had that,then we could check if their current price is significantly higheror lower than the expected price and make a decision based on that.So, what is the expected price of a stock?Is it the average price from when it began trading?That seems a little too extreme.Stocks can grow in orders of magnitude over the years.Most current prices will be well over the average.What might be relevant is the recent average price of the stock.Maybe over the past week or month,we can extend this idea throughout the history ofthe stock and compute the average of a fixed length window of time.It's like moving the window one unit ata time and taking the average price within that window.This is known as the simple moving average or rolling mean.We could devise a trading strategy that looks forlarge deviations from the moving average,and generate trading signals based on that.For instance, if a stock falls too far below its average,then we should buy it,or if it rises too far above,then we should sell.But, how much is too much.Using a constant number or a threshold does not seem like a good idea.Different stocks trade at different price levels.We need a measure that is tied to the price of the stock,maybe some fraction of the stock price.But again, what fraction? We don't know.Some stocks jump around a lot.Some are more stable.A better idea might be to computethe threshold from the jumpiness or volatility of the stock.How about standard deviation?In fact, we can reuse the windowing idea and computestandard deviation over the same fixed length window across time.These lines are called Bollinger bands.All these peaks sticking above the upper bandare signaling that the stock is trading at a higher price than normal.The dips below the lower band are signaling abnormally low price.One problem that you might notice is that there's too many such points.We can reduce them by increasing the width of the bands.That is, by considering a wider range of variation to be normal.A common threshold is to choose two standard deviations above and below.Now, we have far fewer outliers.But the question remains what do we do with these outlying points?Sometimes we get short burst of outliers.So, should we buy or sell each of these points?If you think about it,when the price falls below the lower band,you don't really know if it's going to keep falling or is it going to rise back up.Maybe it's wiser to focus on these inflection points.When the price is below the lower band and starts to cross back inside towards the mean,that should be a good time to buy.The price is still fairly low and on a rise.On the other side,we can sell when the price crosses down the upper band to the inside.This gives us a series of buy and sell signals.At this point, you must be wondering,how much money would I make from following these signals.Well, that opens up a whole can of worms.We'll have to decide how much money you invest to account forthe brokerage charges and for a few other things.Let's just kick this can down the road andwe will look at again when we talk about backtesting.For now, think about the indicators we just computed.Simple moving average andstandard deviation and the trading signals we derived from them.This was a fairly simple approach.See if you can compute other indicators andbetter signals from the price and volume data.

### 05. M1L4 08 Missing Values V5-XaMaVFUIc_I.en

Up until now, you've been treating stock prices as a continuous time series.For instance, the end of day data for stock includes a row for every day or does it?Here's how Facebook traded over the month of May in 2017.As you can see, there's these clumps of five samples andthen a gap and then another clump and so on.Take a look at the calendar for that month each clump offive dates in the plot corresponds to a workweek.These gaps are simply weekends when the market was closed.Two months later, it's July and we see a similar pattern,but here at the beginning,there seems to be a date missing,its fourth of July a holiday in the United States.So, gaps in the data can result from weekends,holidays, and other reasons the market might be closed.You might be thinking why is this important?After all, if we forget about these missing days,the data is still continuous in terms of trading days.Well, that's true if you treat the price data assimple sequence and ignore the timestamps,then you don't need to worry about the gaps.Say, you're computing daily returns.Take the price on each day and subtract it from the price on the previous day.Well, previous trading day that is.For more robust approach to trading,you may not want to ignore the missing days.Even if the market is closed,other events can occur that might influence stock prices when the market reopens.For example, company announcements, news articles,geopolitical events, natural disasters,anything and everything can affect the stock price.The more time between two trading days,the bigger the window for things to happen.So, you could try to normalize returns by dividingthe actual number of days between any two samples.This may work in certain applications,but can reduce the genuine large differences,or you could just use that information about these irregular gapsbetween samples when trying to make trading decisions.Okay, let's recap, weekends,holidays, and other events can cause market to be closed on certain dates.These dates may be missing in stock market data,you can choose to ignore these gaps,normalize for them, or identify and deal with them as needed.Another kind of gap you should be aware of is the time between the marketcloses for the day and when it reopens the next day.Markets often allow some additional trading during the pre and post market sessions.Few traders participate in these sessions,so the volume is low,but these transactions can still affect stock prices.Moreover, when a stock is listed on multiple exchanges around the globe,its price may be actually changing around the clock on another exchange.When a particular market opens for trading,the price of that stock can be different fromthe closing price on that market from the previous day.Again, depending on how you are using the price information,you may not need to worry about these differences,but they may give you an additional clue that you can use for trading.A more significant case of missing values isproduced by a major corporate action like listings and mergers.For instance, say you're analyzing stock data from the year 2000 to 2016,Google only IPOed in 2004,there is no existence of the stock prior to that.So, what do you do?If you absolutely need a value to work with,you can backfill Google's opening price from its IPO date tothe beginning of your analysis period using the same price for open,high, low, and close.Since no trading actually happened on these days,you can set volume to zero.But, this may not be necessary,and it can be misleading.So instead, you can maintain a list of valid ticker symbols that formthe universe of stocks you're considering and this list can change from day to day.A more bizarre case happens when a company is delisted from exchange.Perhaps because they went bankrupt or got bought out entirely by private investors.Dell went private in 2013 by buying back all its public shares.No record of Dell stock exists from that point onwards.If you held a share of Dell stock on that day it went private,it's not like you'd lost that investment,you would have been paid by Dell for that share.So, it would be wrong to assume that the price dropped to zero.One way to mitigate this is to fill the last known price onthe stock forward till the end of your analysis period,or if you're simulating trades over that time period,you can force sell the stock on that date andremove Dell from the stock universe going forward.How you ultimately deal with this misleading values willdepend on exactly what you're trying to do with the data,but completely ignoring them,might not be the right choice.

### 08. M1L4 11 Survivor Bias V2-39MeCCw5ndM.en

The average return from experiment A is indeed higher than from B. Why is that?It's due to a phenomenon known as survivor bias or survivorship bias.If you only picked from the stocks that have survived until today,that already filtered out all the other stocks that failed during that period.Another way to think about experiment A,is that you essentially traveled forward in time from 2005 to the present day.You know then which companies have survived,went back and bought from among those stocks.Of course you are going to do well,but what determines you?Experiment B is more honest.You buy stocks based on information available in 2005.No time travel mumbo jumbo and as expected some ofthese stocks will fail giving you a lower return on average.This is very critical when you're testing the efficiency ofa trading signal or strategy using historical prices.If you allow survivor bias to creep into your analysis,you will get results that are overlyoptimistic and probably won't work in the real world.

### 11. M1L4 13 Exchange Traded Funds V4-Zx7v5GCfpvI.en

Let's take a moment to think about what a trading algorithms goal should be.The obvious part is making money i.e,we should try to generate as much positive returns from trading as possiblebut the stock market is inherently very unpredictable as we have already seen.One approach is to buy stocks that have been showingconsistent growth and hold onto them for long periods of time.This usually generates some returns but nothingspectacular unless you happen to pick stocks that grow spectacularly.But, how you tell ahead of time?What if they don't do as well or even fall?This inherent risk in the market is very real.It is as much part of the game as the actual stock prices.To mitigate this risk,you should maintain a fairly broad portfolio stocks instead of investing a handful.How you choose these stocks can affect how muchyour returns are affected by market behavior.For instance, you could buy a collection of different technology stocks,so the value of your portfolio will go up.If there's general growth across the sector,you would be somewhat immune to the chance of individual companies failing badlyor you can bind stocks from different sectors to create a more diverse portfolio.It will be less sensitive toany particular sector but is also not likely to go through the roof.This is because in order to generate large returns,all these sectors will need to perform well together.It is much more likely for only some stocks or sectors to perform strongly.Which means the average performance of all the other sectors will reduce any such effect.So, how do you go about picking your basket of stocks?You could perform complex statistical analysis to find collections that are likely togenerate good returns but also reduce the overall risk.If you're just investing your personal money,this might be too much work.Not everyone who wants to invest they havethe knowledge and skills to conduct a proper analysis.For this reason, many banks andother financial institutions offer investment funds which are managed by professionals.Mutual funds are one such option which pulling money frommultiple investors and then buy shares on their behalf.You can choose funds according to your investment goals.Some are designed to reduce risk,yield a lower expected rate of return while othersare configured to give a higher rate of return at increased risk.Some funds track the performance of specific sectors such as infrastructure,technology, communications, et cetera while others may be tied to specific indices.In addition to combining multiple stocks,some funds are traded on stock exchanges themselves.That is, in order to invest money in these funds,you buy their shares on the market.Hence, they are known as Exchange Traded Funds or ETFs.They are very popular investments for stock market investors as it tend to producesome growth as long as the sector or index they're tracking does well.In addition to mitigating risk,they are also much more economically compared to investing in many stocks individually,because you typically have to paybrokerage and other transaction fees on them separately.A popular ETF is Standard &amp; Poor's 500 orS&amp;P 500 which trades under the ticker symbol SPY.The S&amp;P 500 include 500 stocks withthe large market capitalization that trade on the New York Stock Exchange or Nasdaq,selected from diverse sectors.The composition of an ETF,the stocks and their proportions can vary over time.This gives rise to another source ofinformation that can be useful in making trading decisions.ETF compositional data.For example, let's say you want to diversify your investment to reduce market risk.You do that by analyzing the individual performance or correlations between stocks.This generates a portfolio that is well-balanced and not too correlated.If you include ETFs like SPY in your portfolio,then there will be some correlation betweenthe ETFs and the individual company stocks that they are made up of.Now, instead of trying to compute these correlations from historic data,wouldn't it be better if you had access to the exact proportion of stocks used.That's precisely what an ETF compositional data provides.Using this information, you can obtain a much more accurate measure of how correlatedthe stocks in a portfolio are and balance them out to further mitigate risks.

### 13. M1L4 16 Alternate Data V2-DFwu2ysGY8c.en

Think about all the sources of information we have considered so far,market data, corporate actions,fundamental information, compositional data.This is the standard information that everyone uses,it's small compared to the amount of data that can affect the market.There is useful information all around you to help you predict the market,information like news articles that have the ability to shape investor opinion,social media posts that can convey sentiment towards companies,satellite images used to estimate crop yield,consumer data that can predict sales and revenue long before official announcements.You may not see the world through the eyes of a clock right now,but once it clicks,we hope you'll have a new appreciation for how the world works.Until then, enjoy the ride.

### 14. MV 06 Our Goal Is To Help You Meet Your Goals V1--pSppDzJRu8.en

Albert Einstein reportedly said,"It's not that I'm so smart,it's just that I stay with problems longer."Your drive to stick with things and see themthrough in a large part will determine your success,not just with this Nanodegree program,but in your job, and in your life.Our goal at Udacity is to ensure that you meet your goals.,so we're here to help you learn like Einsteindid to tackle challenges and persevere through them.Our students come from all walks of life,and their goals are as varied and different as they are.But whatever your personal goals are, we have content,resources, and support mechanisms to make sure that you reach them.If you're stuck, or even if you just fancy saying hello,remember that your community is here for you.Students, mentors, and Udacity staff in your network are all cheering you on,and want to help you learn as much as you can

## Part 01-Module 01-Lesson 07_Stock Returns

### 01. M1L5 01 Intro V2-mE8OOxkgzy8.en

So far, you've seen some stock market data andlearned about some of the market processes that generate these data,so you're beginning to see what these long streams of numbers actually mean.As is true of all types of data,stock data represent measurements of things inthe real world that have certain properties,and people often take particular steps or make certain assumptions on analyzing them.In this lesson, we're going to talk about some of the important propertiesof stock data that lead people to analyze them in certain ways.

### 01. M1L5 02 Returns V6-PngIo6G73Z8.en

Say we're looking at a time series of prices for a stock.The ups and downs are interesting if we'revaguely curious about how the company is doing.But more likely than not,we're looking at the price series because we own some ofthe stock or we've invested money in the stock on someone else's behalf.What we care about is how our investment has increased or decreased in value.So, how do we measure that increase or decrease?There are actually several metrics we might use to quantify changes in price over time.One is the simple difference in price.P_t minus P_t minus one.This might represent the difference between the price of a stockthis month and it's price one month ago for example.Another is the percentage return or raw return.This is the difference divided by the starting price,P_t minus P_t minus one divided by P_t minus one.Imagine you bought a $1,000 of stock one month ago and sold it this month for $1050.Your return on your initial investment of$50 is five percent of your original investment.If you buy $5,000 of stock this month and sell it for $5,300 next month,you might want to compare the success of this new investment to the previous one.Instead of comparing $300 to $50,you can calculate the return and compare six percent tofive percent to see that as a proportion of your original investment.You did slightly better with the second investment.That's the advantage of using returns.Prices are normalized and thus similar in scale.This is actually a requirement for many statistical and machine learning techniques.

### 03. M1L5 03 Log Returns V5-62fZN1QnGjc.en

Quantitative analysts frequently work with a quantityrelated to but slightly different from the raw return,the natural logarithm of return.Remember how the return was defined asP sub t minus P sub t minus one divided by P sub t minus 1.Well, the log return is defined slightly differently,as the natural logarithm of P sub t divided by P sub t minus one.To see how to convert between these two quantities,note that if we add one to P sub t minus P sub t minus one divided by P sub t minus one,we can convert one to P sub t minus one,divided by P sub t minus one,and the expression simplifies to P sub t divided by P sub t minus one.So, if we denote the log return as capital R,and the raw return as little r,then capital R equals the log of little r plusone and little r equals e to the capital R minus one.Why the quantitative analysts use log returns?Well, it turns out that log returns have a number of appealing properties.Instead of listing them all now,we're going to highlight them as they arise while wetalk about the remaining content in this lesson.For one thing, if the value of R is small,or much less than one,then the natural log of one plus r approximately equalsr. Since returns are typically small percentages,the values of log returns are typically close to the values of returns.To understand this intuitively,take a look at the graph of the natural log of x plus one,it goes through the point 00,and it has a slope of one there,so at 00 it's tangent to the line y equals x.If we were to zoom into a tiny neighborhood around x equals 0,the two lines practically overlie each other,so the natural log of x plus one approximately equals X.That's all that equation is saying.

### 05. M1L5 06 Distribution Of Stock Prices Part 2 V1-cGoXGiO1DYk.en

Let's talk about how a series of daily price values arises.It has to do with our earlier discussion of compounding.Let's say, a stock starts at P sub zero,and each day the price changes by some small percent, the return.We saw earlier how the price at time T could be writtenas a product of all of these one plus little r sub i terms.Remember, one plus little r equals P sub t divided by P sub t minus one.If we take the log of both sides of this equation,we get log of P sub t on one side anda sum of log of one plus little r sub i terms on the other side.Here, for convenience, I'm going to move log of P sub zero back to the left-hand side.Remember, P sub zero is a constant,it's not dependent on time.Now, we're going to use the central limit theorem.You might remember the central limit theorem from a class on probability,but if you don't I'll just explain it briefly here.It says that, the sum of random variables thathave the same distribution and are not dependent on eachother approaches a normal distribution andthe limit that the number of random variables and the sum goes to infinity.So, let's make the assumption that the returns on each day are independent,but come from the same underlying distribution.This seems reasonable certainly more reasonable for returns and for prices.After all, the price of a stock on one dayalmost certainly depends on its price the day before.So, if we make that assumption,we can see that the right hand side,the wholesome is a random variable that follows a normal distribution.So, we have that log of P sub t minus log of P sub zero is distributed normally.But if P sub 0 is a constant,then log of P sub t is also distributed normally,because a normally distributed random variable minusthe constant is still normally distributed.Now, I'm going to tell you about a new distribution called the log-normal distribution.Guess what, it looks kind of like that right-skewed distribution we talked about earlier.What you need to know about the log normal distribution is that if Y isdistributed normally then e to the y is distributed log normally.I'll say that again, if Y is distributed normally,then e to the y is distributed log normally.So, if log of P sub t is distributed normally,then P sub t which equals e to the log of P sub t is distributed log normally.In fact, prices are frequently assumed to arise from a log normal distribution.This seems reasonable from just eyeballing the distributionbecause we can see that it doesn't go below zero and it's skewed to the right.Those properties match our actual price distribution well.However, when working with real data,the assumption that prices are distributed log normally,may or may not be a good one.These distributions can be convenient models,but don't confuse them with actual distributions of stock prices or returns.

### img

## Part 01-Module 01-Lesson 08_Momentum Trading

### 01. M1L6 01 Designing A Trading Strategy V4-O7c6bPXBUsU.en

A trading strategy is a set of steps andrules that help you decide what stocks to buy or sell,when to perform these trades and how much money to invest in them.Designing a successful training strategy that reliably generates profits foryou while minimizing the risk of losing money is indeed a complex task,but it starts with looking for signals that mighthold a clue about the future performance of stocks.You might be wondering,where do we find such signals?Is there a book that I can read? Well, yes.There are some ideas that have led to good strategies,but if an idea becomes popular,chances are, everyone will try to exploit it.In order to have a competitive advantage over other traders,you should try and come up with your own ideas or variations.Your idea could be something simple.For example, retail stocks rise duringthe holiday shopping season or something more complicated,like oil stocks that deviate fromthe oil sector index beyond a certain threshold are bound to fall back.Whatever your idea might be,it is important to treat it as a hypothesisand test it against data to see if it holds up.This will allow you to build a strong foundation for your trading strategy.

### 02. M1L6 02 Momentumbased Signals V4-RedwbmYg6e4.en

Newton's first law of motion states that an object at rest stays at rest,and an object in motion stays in motion with the same speed and in the same direction,unless acted upon by an unbalanced force.While Newton's laws may seem to have little to do with the vagaries of the stock market,a similar behavior is often noted in security prices, that is,rising stock prices seem to continue to rise for some time,while falling stock prices continue to fall.This observed phenomenon is popularly known as momentum.But, stocks are not like physical objects,so what causes this effect?Honestly, no one really knows.There are several factors that seem to contribute to it including human behavior.People don't want to miss out on an opportunity to make money,and tend to follow the heart.People also tend to under-react to news.New information propagates over time leading to a prolonged effect on stock prices.There is a branch of trading strategies that attempts to capitalize on such trends,and while there is no standard method to quantify momentum signals,a few common techniques include: technical indicators such as moving averages,large price movements with volume,and stocks making new highs.In the next section,we shall look at one example momentum strategy andevaluate its potential using statistical analysis.The general premise of this trading signal is that,out-performing stocks tend to keep their momentum and continue to remainout-performers for some more time ina particular market and vice versa for under-performers.If you believe in momentum being a repeating market phenomenon,it may be a good opportunity to buy out performers and sell under-performers,capitalizing on the continuation of their movement.

### 04. M1L6 04 Long And Short Positions V3-TCOFgM-hxkQ.en

Once you have found a signal that seems to indicate the future performance of a stock,it is time to put it to action.For instance, if you think that a stock has upward momentum,you might want to buy some shares and hold onto it for a fixed period of time,or until you start seeing the stock fall.This is known as taking a long position on the stock.When you sell your stock, hopefully,at a higher price than you bought it,that is known as closing your position.But what if the stock has a downward momentum?And you believe that it is going to keep falling for some time?In this case, you can take what is known as a short position on the stock,selling first and then buying back later,hopefully, at a lower price.If this is the first time you're hearing about shorting,it might sound bewildering.But what it boils down to is borrowing shares from someone,usually your broker, and then promising toreturn them once your short position is closed.The brokers incentive here is that they typicallyearn a commission on the profit you make from the short sale.At the same time, they are taking a risk.What if you bail out and never buy back the shares?For this reason, brokers typically require you to keep some money ina margin account that they can charge if you fail to keep your promise.The whole process of short selling is a bit more complex with interests,fees, and margin calls coming into the picture.But, all you need to know when evaluatinga potential trading signal is that shorting is one possible action you can take.So, when you think a stock is going up,you can buy or go long,and if you believe it is going to keep falling, you can short it.However, dealing with individual stocks can be tricky and unpredictable.You don't want to put all your eggs in one basket.Therefore, it is recommended to adopt a cross-sectional strategy,where you invest in multiple stocks at the same time.One smart way to do this is to use your signal to rank order the stocks,and use the rankings to select stocks for long and or short positions.For instance, if you're using a momentum signal,the top performers are likely to keep rising,so you can go long on them,and the bottom performers,that are falling, are likely to keep falling.So, you can short them.This has the added benefit that you're usingthe relative performance of stocks to compare them.

### 06. M1L6 06 Trading Strategy V2-rrCHC20FkIc.en

Here's how you might approach the problem of formulating the complete trading strategy.Our goal is to construct a stock portfolio of long and short positions,and the selection process for stocks to go into the portfolio isbased on the stock's returns performance relative to other stocks.For this example, we shall assumewe are periodically re-evaluating and re-balancing stock holdings every month-end.Let's use the S&amp;P stocks universe.The top 500 stocks trading in the US tracked by the S&amp;P 500 index.A stock universe, is a general term in finance that refers toa group of stocks that share certain common features,belong to the same market or simply a set of stocks thatare used in verifying or simulating trading strategies.First, we fetch the daily closing prices of each toxins mid-2013.Please note that it is important that we useadjusted closing prices for analysis purposes.Also note that the composition of stock universes changes over time.For example, around 10 to 40 companies leave or enter the S&amp;P 500 every year.When you test your strategy,it is important that your dataset for 2013 for example,contained the companies that were part of the universe in 2013.In fact, your dataset for any point in time,should contain the companies that were in the universe at that time.If you use the current composition for example,the current S&amp;P 500 index in 2017,then you will effectively be testing your strategyon the subset of stocks that survive to 2017.These stocks probably performed better than the ones that left the universe.So the performance of your strategy will look better than it should.This is a subtle error that analysts often make,and is known as survivorship bias.Now, since we are only interested in month-end prices,we can re-sample the daily closing prices into monthly prices,then we can form a log returns time-series from the monthly prices.For each month-end observation period,we rank the stocks by their month-end returns from the highest to the lowest.Select the top performing end stocks intoour long portfolio and the bottom performing end stocks into our short portfolio.You could also choose a fraction of stock say,the top and bottom 10 percent.To simplify this example,we'll assume every stock gets an equal dollar amount of investment.This makes it easier to compute the portfolios monthly returnsperformance as the simple arithmetic average of the stock returns.Lastly, the combined portfolios monthly returns isthe difference between the long portfolio's return and that of the short portfolio.Now, continue doing this for each month andperiod and you have a momentum based trading strategy.

### 09. M1L6 09 Statistical Analysis V10-_p1m_q8jE6E.en

Now we are ready to perform our analysis.The resulting returns time series isthe theoretical monthly performance of our long-short portfolio.Our goal is to see if the mean monthly return is greater than zero.Let's calculate that.So, our mean is indeed greater than zero at 0.53 percent,but does this mean that we got this value becauseon average this trading strategy yields positive returns?That is to say that the true mean is greater than zero?It could be that on average this trading strategy will not yield positive returns.Maybe the true mean is less than or equal to zero,and this value is just a random fluctuation.Well, one approach to assess this is to perform a statistical test on our hypothesis.A t-test is a way of testing the probability of getting as bigger mean as we did,assuming all the assumptions we made to build our model of strategy returns were correct.In our case, we can compute the t-statistic by dividingthe mean return x bar by the standard error of the mean SE x bar.Using this t-statistic, we can measure the probabilityof getting a mean monthly return of 0.53 percent orlarger if the true mean monthly return iszero given that the assumptions we made to build our model are correct.This probability is called the p-value.If the probability is very small,we might infer that it's unlikely that the true mean is zero.Now, before running the test,we should decide how small the p-value needs tobe for us to conclude that the true mean is not zero.To denote this threshold,we usually use the Greek symbol alpha.A commonly used value is 0.1.Setting alpha sets the false positive rate for the test.So, if we set alpha equals 0.1,we're effectively saying that if we use this threshold,we are incorrectly rejecting the null hypothesis when the true meanis zero in 10 percent of many hypothetical users of this test.By performing a one-tail t-test onthe alternative hypothesis that the mean ofthese returns is statistically greater than zero,we calculate the p-value withthe stats package using a degrees of freedom of n minus one equals 47,and we find that the t-statistic is about 1.618,with the p-value of 0.0566.Meaning that the result is not significant at P less than 0.05 level,but insignificant at P less than 0.1.This means that we are fairly unlikely to get a mean of0.53 percent if the true mean is zero.This result shows some initial promisethat there could be some alpha contained in the strategy,and thus serves as a quick sanity check to justify investigating further andfine-tuning the strategy leading to a full backtesting exercise in the end.If we measure the monthly performance of the SPDR,S&amp;P 500 ETF, SPY over the same period,we would find that the mean monthly return is 0.83 percent,which is actually greater than that of the strategy,at least at this preliminary stage.To be clear, comparing our momentum strategy to the strategy ofinvesting directly in S&amp;P 500 ETF is not really a fair comparison.These are two very different strategies.For one thing, ours involves taking long and short positions,while investing in the S&amp;P only involves taking long positions.However, this gives us a vague idea of what values portfolio returns might take.Also, while our strategy's obviously simplistic to begin with,this shows that it is actually notan easy task to find a strategy that can outperform the market.Note that if you get a very large p-value at this early stage,you wouldn't want to go back and make meaningless changes toyour strategy like changing a parameter a little until you got a very low p-value.Doing this is called data snooping or p-hacking,and would just fine tune your strategy to exploitthe particular random fluctuations in the dataset you have.A strategy device that way would be unlikely to perform well on future data.What you should do instead is make meaningful changes to your strategy.In other words, you should come up witha different idea for a strategy and test that instead.

### 13. M1L6 12 Finding Alpha V1-r8lfWVhfQC0.en

Formulating trading strategies often starts with an observation.A pattern that seems to be recurring in the market over time.At that point, your creativity andintuition tell you that there might be an opportunity for monetization.Your job as a contrator then,is to turn this observation into an expression,both mathematically and programmatically and verify it using historical data.This is alpha research.Statistical analysis lets you very quickly and scientifically test whetheran observed pattern or trading signal that you come upwith has the potential to turn into a profitable trading strategy.Once this is proven,then you can proceed to define your trading strategy in a more detailed manner.Which will lead to a full back-testing exercise,as the last step of the research process.As a point of clarification,the Alpha of the t-test is not the same as the Alpha in Alpha analysis.The same Greek symbols are just used to represent many different things.Later in the program,we shall look more closely at the caveats andtechniques of finding trading signals using time series.

### 14. MV 13 Global Talent Is Equally Distributed V1-QwDJbbBl_48.en

We believe that the community of students who are takingthis nanodegree are doing more than just gettingthemselves ready to enter the quant trading industry.You and your community of classmates are helping us toprove that talent exists in every corner of the world.I have to admit that from the outside of the finance world there's a sense thatbanking and finance tends to be more suited for students from elite universities,but when we step inside the financial industry,we actually see that there are people from all kinds of backgrounds,former nonprofit policy analysts,chemists, and also very successful people who never got a college degree.So, I think that your community of classmates are collectively setting out to provethat no matter where you're from or what your parents did or what school you went to,you have the capability to find Alpha.

### img

## Part 01-Module 01-Lesson 09_Project 1 Trading with Momentum

### 01. MV 03 Transition To Project 01 V1-dcps5Bg4bZE.en

Congratulations on getting through your first batch of lessons.Next on your agenda is your first project, trading with momentum,where you'll implement a momentum trading strategy and performa statistical test to conclude if there's alpha in the signal.That sounds like a really interesting project.Does a bode on the learning we've been doing up until now?It does. This project will give you an overview of the Quant workflow.By preprocessing data with pandas,implementing a trading signal,and evaluating it's performance.Sounds like students will get a chance touse all the skills they've been building up until now,but in a really industry-relevant way.Completely. We'll use this foundation in later lessonswhere you'll learn more advanced techniques to generate trading signals,construct a portfolio, and evaluate its performance.That's awesome. Projects are a super powerful learning tool,they give you a chance to bring together all the skills and knowledge you've builtup until now and apply them in new and interesting ways.Totally. I love doing projects.They certainly can feel a bit challenging,but even though I don't always see the solution straight away,they help me grow in really meaningful ways.Totally. That's a great point.Since projects are these big learning opportunities,it's totally normal to find them challenging.Luckily, in your Nanodegree Program,you get to iterate on your projects just like you do with work in the real world.That's right. You'll get personalized feedback on each project you do fromour expert project reviewers that will help you see whatyou've learned already and how you can keep learning and improving your work.If your project doesn't meet the specifications the firsttime or even a number times beyond that, that's totally normal.Yeah. Again, projects themselves are meant to stretch your knowledge and skills,not just give you a chance to show off what you've already learned.It's the same as with the quizzes in the Udacity classroom,you get to play around and practice with them,trying them as many times as you want to or need to to keep growing.Everything you do at Udacity is intended to help you learn,and you are in the driver's seat.Good luck with project one, and have fun.

## Part 01-Module 02-Lesson 01_Quant Workflow

### 01. MV 05 Intro To Module 2 V1-92JzOXda9Q8.en

Welcome back. In this set of lessons,we're going to expand on the fundamentals even building,to prepare you for some neat stuff we'll do later.We're going to begin with a bird's eye view of strategy development,so that you understand the stages of this andhow the activities of each stage fit together.We'll talk about weird data points, outliers,a discussion that will help you start to get a feel forthe type of data used in quantitative trading.We're also going to discussthe very important concepts of regression and time series analysis,which are basic building blocks we'll need to introduce later concepts.We'll discuss the very important concept of volatility or variability in returns,which is frequently used as a measure of risk,which people in the trading world are constantly talking about.Finally, we'll discuss Paris trading,another very important type of trading strategy.This set of lessons is designed to help you build out your understanding of the ideas,data, tools, and concepts,fundamental to trading strategies. Let's go for it.

### 02. M2L1 01 Starting From A Hypothesis V3-yjlt4yerB9I.en

Congratulations on making it this far.The content you've seen previously,will serve as a strong foundation of knowledgefrom which to build your quantitative finance career.We have a few more concepts to discuss.But before we get into the weeds,let's take this opportunity to step back for a moment.I want to describe the series of steps,quants go through when developing trading strategies from a big picture of perspective.So you'll be able to see,how each step can contribute tothe ultimate goal of profiting from a successful trading strategy.This will help you understand the point of each school year learning andmake concepts discussed in the next few videos more clear.Fundamentally, quantitative trading is the process of using statistical analysis andmodeling to predict market behavior and usingthose predictions to make trades with the goal of profit.Like any research process understanding and predictingmarket behavior begins with a new idea for how the world might work.In other words, a hypothesis.A good hypothesis is the first step to making predictions about future behavior.Coming up with a good hypothesis means being brutally specific.For example, the hypothesis stocks that are discussed inthe news are likely to go up is not very useful by itself,because it's so vague that it doesn't lead to testable predictions.In what news outlet?At what time?In what manner must the stocks we discussed and for how long will they go up?An improved hypothesis would be stocks whose company name or tickerappear in the landing page ofThe Wall Street Journal website will increase by price by one percent,one day following this appearance.Although it's far from perfect,we can actually test this hypothesis byfinding the tickers and checking for the change in price.So how do you come up with a good hypothesis?Well, your hypothesis should be based on your current understanding of how markets work.So the more you know about the markets,the more likely you are able to come up with a new potentially profitable idea.It can take time to build up the domain knowledge to come up witha good hypothesis in any field of study and quantitative finance is no exception.So how do you build up this domain expertise?Well, you can try to immerse yourself in the markets,observe the markets, watch financial news,read financial newspapers, read books, read blogs,and study the known strategies of famous quants and discretionary traders and investors.Attend meetings and conferences and readacademic papers such as those on the Social Science Research Network.An online repository of scholarly research inthe social sciences including economics and finance.But no matter who you are or what you know,if your hypothesis makes predictions that are inconsistent with the real observed data,your hypothesis is wrong.The precise manner in which it is wrong,when and how, we'll show you how to focus your efforts to improve it.

### 03. M2L1 02 Quant Workflow V3-lZfCCRv2rEE.en

A hypothesis that forms the basis of a trading strategy mustsurvive several phases of increasingly rigorous testing.This process can be different at different companies,banks, or hedge funds, but usually,there is an initial exploratory research phase during whichideas are generated and validated in a fairly basic way.The goal here is to come up with promising ideas relatively quickly.In the research phase,you will focus on determining what set of positions to enter,on which assets at which times.Such that you have the potential to get positive future returns.Later on, you'll incorporate other factors critical to a full-fledged trading strategy.How much money to invest in each asset,under what conditions to exit positions,factoring in the costs of making trades,and what risks constraints to impose?The process of rigorously simulating the entire flow inan automated fashion using historical data is called Backtesting.It is important to resist the temptation to jump right into full-fledged backtesting.We'll talk about the reasons for this when we discuss overfitting in depth.

### 04. M2L1 03 Flavors Of Trading Strategies V4-uCCx8I9u_Nk.en

There are several flavors of trading strategies.The most basic type is based on buying and selling a single asset.An example of this type of strategy might be just trading the S&amp;P 500.You could track the performance of the S&amp;P overtime and enter positions based on past performance.For example, if the S&amp;P has been doing well for a while,you might enter a long position under the assumption that it has momentum.Another common strategy is to find pairs of assets that seemed to berelated and trade based on their relative movements.These are called pairwise strategies.For example, say there are two big companies in the beverage industry,these companies are probably subject to many in the same market affects,the price of ingredients,the cost of shipping et cetera.If one stock starts to appreciate more than the other,you might think that the laggard will eventually catch up.A pairwise strategy would make trades to try to capture this differential.Another class of trading strategy extends this idea to groups of stocks,these are called cross-sectional strategies also known asequity statistical arbitrage or equity market neutral investing.This is a popular type of strategy and involves comparinghundreds to thousands of stocks to determine which to holdin long and short portfolios with the goal of benefiting fromtransient market phenomena without being subject to overall market movements.The comparison is often based on price and or volume or fundamental information.A momentum signal where you rank stocks based on the strength ofprior returns over a given period is an example of a cross-sectional strategy.Finally, there's a class of strategies based primarily on new types of data,such as satellite imagery, social media,geolocation or consumer transaction data.Large hedge funds and asset managers are most interested in strategies three and four.These are the two we spend most of the time on in this nano-degree.Why is this? Two reasons: first,large professional market participants bydefinition have large amounts of capital to put to work.As such, they desire strategies with high capacity,this means the strategy can be put to work at a meaningful asset level andthis is most often achieved by trading very many stocks.Cross-sectional strategies all other things being equal,have the highest capacity.Second, professional market participants are looking for differentiated ideas.Given the proliferation of alternative data,many funds hope to uncover signals in hard-to-find,expensive and difficult to work with data.We will talk about that later in this nano degree.

### 05. M2L1 04 Anatomy Of A Strategy Part 1 V5-cnJK8c2zfq4.en

Let's zoom in on one of the types of trading strategies we talkedabout earlier, cross-sectional equity investing.This is a deep and important type of trading strategy.So, let's talk in detail about the pieces needed to build this type of strategy.In general, you can think of the process of developing this type ofstrategy as having six stages: data selection,universe definition, alpha discovery,alpha combination, portfolio construction, and trading.In the first stage, you need to decide what data set or data sets you want to use.As we've said, you should start with a hypothesis.So, you're going to want to get access to the data relevant to testing it.In the second stage,you need to pare down your data set to a subset thatcontains the stocks you wanted potentially trade.You might exclude stocks that have low training volume,and are therefore, hard to trade.But you also want to construct your portfolio,so that the stocks in it have similarities,so that ranking them is a reasonable thing to do.However, since you're looking to benefit from their movements relative to each other,they shouldn't be too similar.You're also going to want to limit your universe tothe stocks to which your hypothesis logically applies.For example, let's imagine you want to work with the hypothesis discussed in the paper,Geographic Momentum by Quoc Nguyen.This hypothesis is that a momentum effect is created for multinational US companies,because investors do not pay attention to foreign market developments.So, there is an opportunity to predict increases inthese company stock prices when markets and countries they're operating in improve.If you're working with this hypothesis,you would only want to work with companies who actuallyhave significant foreign market operations.

### 05. M2L1 04 Anatomy Of A Strategy Part 2 V1-v3w4JZKQixc.en

The alpha discovery phase is where the fun really starts.This is when you start looking for alphas,but what are alphas exactly?An alpha is an expression applied tothe cross-section over your universe of stocks which returnsa vector of real numbers whose values areproportional to the size of the position you will take on each asset.You can think of the numerical output of an alpha as providing an indication,or metric of conviction aboutfuture returns which will ultimately inform a trading decision.When you have discovered a successful alpha,the value in that vector for each stock is directly proportionalto the rank of its return at a future time across the universe of stocks.For example, in a cross-sectional momentum strategy,we rank the stocks according to how much momentumthey have as measured by a momentum indicator.We use the ranks to decide which stocks to put in alongportfolio and which stocks to put in a short portfolio at every time interval.In this scenario, you can think of the logic that produces the ranks as the alpha,and the ranks themselves as the alpha vector.Taken together, the ranks are a vector of numbers that helpinform the trading decision of which docs to hold long and short,and in what amounts.An alpha is one type of trading signal.A trading signal is a general term forany numerical signal that can be used to inform a trade.It could just be a single number.Therefore, alphas which are vectors are a subset of trading signals.The alpha discovery phase can also be called the signal research phase.This stage is where you test your hypothesis and see if you can come up withevidence that your idea may lead to strong future returns.This will be an iterative process.However, in modern markets,it's rare that a single alpha will providesufficiently consistent positive returnsto provide the sole basis of an investment strategy.Typically, several alphas will be combined together to generatean overall alpha that has better performance than the best individual alpha.This is akin to the ideas of modelstacking and ensembling in traditional machine learning.Combining alphas that have diverse inputs andunderlying hypotheses can lead to a high-quality combined alpha vector.For example, a price-driven alpha such as the momentum alpha maycombined well with an alpha based oncompany fundamentals since the inputs are very different.In this phase, alphas may be combined using simple logic like adding ranks are averaging,or through more complicated waiting schemes like findingthe weights that lead to the lowest possible variance for the combined alpha.Another possible method is to translate your alphas into features and use them asinputs to a machine learning classifier to capture the relationships between the alphas.The output of this stage is a single alpha vector thatincorporates the information of many individual alphas.

### 05. M2L1 04 Anatomy Of A Strategy Part 3 V1-vSxnkduTWWY.en

Eventually, you will arrive at the stage whereyou want to start thinking about how your strategy will work in practice.This is the portfolio construction stage,he stage where you have to think about usingyour combined alpha vector to generate and update an actual portfolio.At every time iteration,your strategy is going to take an existing portfolio,make predictions about market behavior using your combined alpha vector and usethis information to make trades that move the existing portfolio to an updated portfolio.There are more questions that must be answered when turningyour market hypothesis into traits such as,how does your strategy take into account in various forms of risk?What quantities should your portfolio seek to optimize?Are there any additional constraints on your portfolio?How do real-world constraints like transaction costseat into the theoretical return and how can these be mitigated?Let's talk in a bit more detail about some of these questions.You probably already have an intuitive sense of what a risk is.It's uncertainty about the future, and in particular,the possibility that something bad will happen,like losing the money you invested.In finance, risk usually refers to uncertainty orvariability in returns and there are many different ways to quantify it mathematically.There are different types of risks.Risks inherent in the entire market are called systematic risks.One form of systematic risk is risk inherent to individual sectors,like the technology sector or the energy sector called sector-specific risk.For example, the success of the energy sector as awhole might be affected by fluctuations in the price of oil,but it's unlikely that this would affect the technology sector nearly as much.Risk inherent to individual stocks is called idiosyncratic or specific risk.For example, a single oil company might be uniquely affected bypolitical events in a region of the world in which it is exploring for oil.Your strategy will likely incorporate a mathematical model in these risks,which will help you understand and manage the different types of risksand attempt to avoid being overly exposed to anyone.Different investment funds havedifferent mandates driven by the goals of their investors.For example, some funds will want to be neutral to all systematic risk.Whereas, other funds have a specific target for that risk.Your portfolio could be designed to maximize different objectives.As one example, you could seek to maximize expected return and minimize return variance.In this case, if two portfolios have the same expected return,you would choose the one with lower variance.Finally, your portfolio may be subject to additional requirements or constraints.For example, the policies of your investment firm may dictatethat you can't hold stocks below a certain level of market capitalization,or you may only be allowed to enterlong positions due to government regulations in certain markets,or there may be a limit on what percentage ofyour portfolio can be invested in any single stock.These must be taken into account during portfolio design.Finally, in the training stage,you make actual trades in the market.After portfolio construction, you'll havean ideal portfolio and a list of trades to make.In this stage, you'll have to figure out mechanics like howfast to trade and to which time horizon you believe your alpha applies.You may decide to trade quickly and aggressively,or more slowly and passively.You'll also have to take into account the fact that the tradesthemselves incur costs which will eat into your returns.This is the realm of market microstructure and high-frequency trading.Finally, spends market conditions don't allow you to place the exact trades you intend.As you can see, there's a lot to think about during this process.So, let's get on with it.

## Part 01-Module 02-Lesson 02_Outliers and Filtering

### 01. M2L2 01 Intro V1-OGx1aYHMgbs.en

Welcome back. In this lesson,we're going to discuss common sources of Outliers,a few ways to spot them and some methods you can use to handle them.Outliers can be a real nuisance.If not handled intelligently,they can make results look better than they really are.They can lead to false trading signals which when used toconstruct trading strategies result in profits and losses that at best,differ from simulation results and at worst, are completely negative.We don't want that, so let's see what we can do about Outliers.

### 02. M2L2 02 Sources Of Outliers V8-gXKhKQ2_TaA.en

What are we talking about when we talk about outliers?We're talking about extreme or unexpected values inmarket data which may or may not represent real events.For example, if we're tracking a solar energy company stock price,we might see unexpected movements in the price on a day when a solar eclipse occurs.Outliers can show up fora million different reasons but some reasons are more common than others.In this video, we're going to talk about a few of the most common types of outliers.Outliers can always appear due to human error,either during manual data entry or in the form of computer bugs.These are known as fat finger errors.To cite a few examples that had newsworthy consequences,in 2001 a Japanese traitor cost his employer UBS Warburg,71 million pounds when he sold 610,000 shares at60 yen instead of six shares at 610,000 yen as he had intended.In 2015, a junior member ofDeutsche Bank's foreign exchange sales team processed a trade usinga gross figure rather than a net figure causinga payment to a U.S. hedge fund of six billion dollars,orders of magnitude too high.Sometimes data can be missing completely,entered as zeros or duplicated from previous states.This can happen due to lax quality control of market data vendors but sometimes exchangesthemselves can be missing data.Data maybe missing because the stock was suspended fromtrading for a day or more often, for part of a day.Trading on a stock can be stopped for regulatory reasons,to reduce volatile trading prior to a companies news announcementor when there's uncertainty about whether the stock continues to meet listing standards.It can also be stopped for non-regulatory reasons.For example, if there's a significant imbalance and pending buy and sell orders.In these cases, one way to check if trading actuallyoccurred is to check if the trade volume was zero.Trading for an entire exchange can also be stopped.There are mechanisms called trading curves or circuitbreakers that halt trading if an index drops by a certain percent.The idea is to prevent stock market crashes,events when the prices of stocks acrossa significant cross section of the market drops suddenly and significantly.You might also see extreme price movements that are due to real events such as earnings,mergers or other announcements.Announcements represent information that the market previously did not know.So these can surprise the general market if the results are muchbetter or worse than the market expected, so to speak.When the market adjusts to this information,the stock price fluctuates asthe new information starts to be reflected in the trading price.For example, the athletic apparel company, Puma,made an unexpected earnings announcement on October 18th,2017 in which it reported stronger than expected third-quarter earnings.That day, its stock closed four percent higher thanthe previous day's close and the subsequent dayssaw fluctuations as the price rose still further.Outliers can occur due tothe intended or unintended actions of computer trading programs.For example, in 2010 the market experienceda flash crash which lasted only 36 minutes and was later linked to trading algorithms.While extreme price fluctuations occurred during this short period,they were due to real events.Sometimes quants must work withunadjusted or nominal price databecause adjusted price data are unavailable through their data source,when working with real time data directly from a stock exchange for example.If you are working with unadjusted price data,you will see discontinuities in prices on dates when companies issue dividends,split their stock or took other corporate actions.Companies may issue dividends on a regular scheduleor may decide to issue one-off so-called special dividends.In this example, you can see that in 2017,Costco issued several regular dividends aswell as a large seven dollar per share special dividend.These discontinuities can look like outliers if returns are calculated on these data.A stock trades ex-dividend on or after the ex-dividend date,so on this date, non-adjusted prices decrease.This represents the fact that the net company value hasdecreased because cash has been transferred from the company to shareholders.However, after prices are adjusted,change demand for company stock may also cause prices to change.Price changes due to change demand are real market effects.To deal with discontinuities in unadjusted prices,whoever is cleaning the data must go back,look at corporate events and cumulatively adjustfor each dividend distribution to generate adjusted prices.

### 04. M2L2 03 Outliers Signals And Strategies V5-zyVgpsRy-mU.en

Thinking about how to deal with Outliers is an important part of signal research.Let me give you a couple examples.Sometimes, the prices of stocks that aren't traded verymuch undergo dramatic changes when they are actually traded.As an example, let's look atsome small market capitalization stocks traded on the Hong Kong Stock Exchange.One example is Easyknit International Holdings Limitedwhich has the ticker symbol 1218.HK.I'm using Easyknit as an example of a thinly or infrequently traded stock,but what does it mean to be thinly traded.Well, one way to quantify how much a stock is traded is to look at its typical volume.This is the number of shares that were traded during a given time period.However, the number of shares traded may not tell you verymuch since a company can issue arbitrary numbers of shares.A better metric called turnover is the volume traded multiplied by the price per share,in other words, the total amount of money that actuallychanged hands when the stock was traded during this period.This metric is more comparable across companies since it's in units of currency.Easyknit's average volume was only about10,000 shares at the time we made this video and traded ataround 4.8 Hong Kong dollars for a turnover of around 48,000 Hong Kong dollars.For comparison, the turnover of Tencent Holdings,a leading provider of Internet related services inChina and one of the most traded stocks on the Hong Kong Stock Exchange,was around five billion Hong Kong dollars.It doesn't take much trading to move the prices of thinly traded stocks up and down.A single trade will cause a significant change in price,so the prices can fluctuate unpredictably.These fluctuations are real,they are due to market activity,but they are unpredictable.You could avoid exposure tothis unpredictability by excluding these stocks from your trading universe.Alternatively, if you want to trade these stocks,it might be a good idea to keep these price fluctuations in your trading data inorder to be sure that your strategy performs well despite this unpredictability.This example illustrates why it's important to be careful whichstocks you chose to include in your trading universe in the first place.Including data from periods when the market isbehaving in an unusual way can skew your signals.For example, during and after market crashes,stock prices are volatile and can reach extreme values.Because market crashes do not happen often,they are statistically infrequent outlying events.If we calibrate trading signals including data from these periods,the results will be highly skewed and the signalswon't perform optimally on normal trading days,in other words, most of the time.But if we don't include such periods when calibrating trading signals,and one of these rare events happens,the signal may perform really poorly or even in a toxic manner.There is no golden rule to resolve this,some traders prefer to alleviate these problems inthe strategy formulation phase bycontrolling the sizes of the long and short positions theytake and establishing thresholds called stoploss levels at which they will exit positions to prevent further losses.It's also possible to design training strategies thatattempt to take advantage of sudden extreme price movements.People who trade with so-called contrarian strategiesseek to identify sudden price movements ordislocations that they think are not justified byreal information or events and therefore won't last,and they trade against such movements.One could try to apply this strategy to data of any level of time resolution,but minute level data are more prone tothese sudden movements than daily data or monthly data.This is a risky strategy,it can be difficult to determine whether the price will settle back to where it was,in which case the trader profits,or whether the price will continue to move inthe same direction which would not benefit the trader.

### 05. M2L2 04 Spotting Outliers In Raw Data V3-kFIB0YIW1TQ.en

Finding and handling Outliers inraw price data and signal returns are slightly different scenarios.In this video, we're going to talk about spotting Outliers in raw data.Outliers to look for in raw data include large changes in stock prices and volumes,missing dates, missing prices and missing volumes.One basic approach to finding extreme values in raw data is to screen the data for them.Doing this by brute force,by looking at every row,for example, is very inefficient but maybe necessary if time is tight.Plots can be helpful but not by much if the data-set contains hundreds of stocks.One way to screen for Outliers is to create rule-based searching and filtering methods.For example, you might set up a filter to catch instances when prices change by more thansome value that seems reasonable givenyour signal and the scale of typical price movements.Percent change thresholds should not be relied upon too heavily,as they are likely to yield many false positives,extreme yet legitimate price movement data-points.Nonetheless, using such thresholds is one way to screen data quickly.If the price change is accompanied by a large change in volume,it's less likely to be wrong.So, you can use volume information to improve the accuracy of your filter.The challenges of this task will always be the need to process large amounts of data,minimize false positives and decide how to deal with data values that are missing.

### 06. M2L2 05 Handling Outliers In Raw Data V3-3l6kQZqlVJA.en

So, what do you do when you find outliers and raw price data?Well, although at least large institutions,co- traders usually have a team of people to clean data for them.They may run into situations where they need results fast.The easiest and quickest way to determine if the extreme value is real,or fill in a missing data point is to cross check with another data source.If it's a one off point,you can remove or replace it with data from a secondary source manually.You might think of replacing these data with some mean of the surrounding data.This is not done often because it risks incorporatinginformation from the future into those days data.Inaccuracy due to the use of information that would not have been known oravailable during the period being analyzed is called Lookahead Bias.Think about it. If you were trying to fill in that missing datum by averagingdata from the surrounding days on the day for which the datum is missing,you wouldn't be able to because you wouldn't know what the price would be in the future.Lookahead bias is a bias because usingunknowable data from the future will consistently make your results look better.The main problem with using future data in signal research is thatany kind of strategy based on that research would be impossible to execute.One common mistake is to use closing prices ofthe current day or a future date to calculate the trading signal for the same day.But of course, a strategy that says to make a trade duringthe day based on a closing price for that same day would be impossible to implement.It may not seem to make much of a difference in cases where youwere using the data for some kind of historical analysis,but to be safe, it makes sense to simply substitutea missing closing price with the previous closing price during signal research.However, keep in mind that it is recommended to keep the missing dataduring back testing because they may represent a real non-tradable event.For example, stock A may not have traded fora day and thus it's closing price would have been missing.In your back test your data should reflect this andyour strategy should not attempt to place trades on stock a during that day.

### 07. M2L2 06 Spotting Outliers In Signal Returns V4-BSLGZz0RzTg.en

After you calculate returns from a trading signal,you may suspect the presence of outliers when youexamine the distribution of your signal returns.Let me explain what I mean by that.Imagine that we've written some code to generate a trading signal and we'vecalculated our monthly returns from a couple years of trading based on this signal,what would we expect the distribution of returns to look like?Let's figure out what we would expect to see in a few limiting scenarios.What if we picked our buy and sell times at random?In that case, we wouldn't expect to make any money because we'll beequally likely to buy or sell when the market is going up or down.We'd expect our returns to have a normal distribution with mean zero,but what if we designed a trading strategy that performed well?Ideally, if a trading signal looks like it will perform well,the distribution of its returns should look likea slightly positively skewed normal distribution.For the signal to make money,returns should be positive and non-zero on average.So, the distributions mean should be above zero.However, sometimes the return distribution canlook a little too good or just plain weird.This should arouse your skepticism.Extremely skewed shapes or bumps at either tail of the histogram spell trouble.One tool you can use to compare your distribution of returns toanother distribution like the normal distribution is the QQ plot.A QQ plot is a plot of the quantiles ofthe first data set against the quantiles of the second data set.What are quantiles?Well, if we split a data set into four equally sized groups,the dividing lines are at the 25th, 50th,and 75th percentiles, these are usually called quartiles.The 50th percentile is the median,the value below which 50% the data fall.Quartiles divide the data set into four groups,but with quantiles, the dataset can be divided into any number of equally-sized groups.For example, you could have ten quantiles,these are usually called deciles.The word quantile is usually used to denote the cut points,but it's sometimes used to refer to the groups themselves.Let's get back to the goal of comparingyour distribution of returns to the normal distribution.If you wanted to use QQ plots,you would first first the set of quantiles you want to use,then you would plot the nth quantile of your distribution against the nth quantile ofthe normal distribution and continue for all the values ofn. If you're comparing your distribution to the normal distribution,and your distribution is approximately normal,points in the QQ plot should fall along a straight line.If the distribution has fatter tails than the normal distribution,the QQ plot will reveal deviations from a straight line at the extremities of the graph.Distributions with skew will also have QQ plots that curve away from a straight line.A good quant should try to understand the root cause of outliers and returns.The first step in dealing with a situation like this is tofind out where and when the outlying data points occurred,for which stock or stocks and for which dates.The next step is to ask why.Depending upon the nature of the extreme data,the source might be obvious or less so.Was it a data error?Was an illegitimate movement due to a real event?If an extreme datum looks like it could be real,you can check the news for the stock on that day.Was there an announcement?You'll want to think of every possibility you can toexplain why this might not be a market data vendor problem.A good strategy is to crosscheck with another market data source.

### 08. M2L2 07 Handling Outliers In Signal Returns V4-ILdnNi4CgZM.en

After you've identified anomalous skew inyour signal returns and you think you know the cause, what do you do?The best strategy for dealing with these types ofoutliers varies on a case-by-case basis.If you know it's just bad data from the data vendor,you can try to fix it by replacingthe affected data with correct values from a different vendor.If you can't do this,you might try to determine if your research result will be greatlyaffected if you replace the value with any reasonable value.For example, the previous day's values.If this isn't going to work,you might try to figure out whether removing this data row,all together, will have any significant effect on your signal research results.You are the one in the best position to make this judgment call.After all, you know the signal the best.If you think the extreme values are due to legit market events,there are a few different things you should think about.You might try to find out whether there are many similar occurrences in other stocks.If there are and if you can identify a common cause for all the extreme values,you could consider isolating this group of stocks out of the research stock universe.You should think about what effect this might have on your signal overall.Here, we're showing several thinly traded small market cap Asian stocksexperiencing large sudden unpredictable price fluctuations.Another type of stock that experiencesdramatic unpredictable price fluctuations isthe stock of single drug biotechnology companies.These type of companies evaluation hingesupon the success of various milestones: phase one,two,and three drug efficacy trials and FDA approval.With only one product,the company is worth a lot if it is successful and very little if it fails.However, the outcome of these binary events is very difficult to predict and to handle.So, quants frequently exclude these stocks from their trading universes.As one example, the price of the stock of Sage Theraputics,a company that makes a postpartum depression drug,jumped from $93 to $167 ina single day last year when the company announced positive test results.You should also think about whether events you think are legitimate are due tospecial market events like corporate earnings or central bank announcement's.Realistically, if you eventually use this signal to build a trading strategy,you might want to think about whether you can avoid these events by,for example, pausing the strategy in advance of them.If it's possible to do this,then you can probably remove these rows and proceed with the research.If you can't pause the strategy ahead of these events,then you're going to need to think about how you will be able toavoid losing money when executing this strategy.As you can see, a lot of this is common sense.You'll always need to think about how you can preventthe quality of your research results beingdegraded by outliers and how you can avoid losing money when trading your final strategy.

### 09. M2L2 08 Generating Robust Trading Signals V3-1ikkZmVkjl0.en

If possible, we'd like to generatetrading signals that are robust to outlying data points.For example, imagine you have an outlier in the closing price time series,a huge positive spike.If you're using a momentum based strategy and your strategies buy or selldecision is based solely on individual closing price values,then the single value might induce a buy decision.If instead you take the moving average of closing pricesaveraged with a fixed duration rolling time window as the input to your signal,it will average out that single extreme datum andreduce the effect of individual outliers on buy or sell decisions.The trade-off here is that your signal actions might begenerated with a slight delay relative to the stock price movement.The duration of this delay will depend on the window size you use.You can take an analogous approach for portfolio level strategies,or strategies that are based on the movements of an entire portfolio of stocks,.You can average out the extreme movements of individual stocks by basingyour buy or sell decision on the accumulated movements of many stocks,or even an entire sector.This will also reduce the effect of outliers on your signal.If you are looking for alternative solutions,there have been ongoing efforts from within the industry to incorporate Bayesian methods,or machine-learning, into outlier detection.This is outside the scope of this lesson,so, we will leave the subject to you to explore.

### 10. M2L2 09 Outro V1-r1SWu-7Rzf0.en

In this lesson, we've discussed common sources of outliers and data,some ways to spot them,and a few things you can do to handle them.However, it's worth noting that your strategy for dealingwith outliers will probably vary on a case-by-case basis.In general, pay attention to your dataset and investigate when you see something unusual.

## Part 01-Module 02-Lesson 03_Regression

### 01. M2L3 01 Intro V4-C7vWJH05tKA.en

This is what wedo in Statistical Arbitrage.Statistical Arbitrage is a trading technique that involves simultaneously buying andselling two related assets based on the analysisof how these two assets move in relation to one another.It's important to keep in mind,the signal to noise ratio in our data.The signal is the meaningful part ofour input data that helps us predict our dependent variable.The noise is the random part of our data,that does not help us make better predictions.With financial data, the signal is relatively low and there's a lot of noise.In other words, the signal to noise ratio tends to be low.When the signal to noise ratio is low,predictive models tend to overfit the data that it's trained on.This means that when the model is used to make forecasts in real life,they often pay too much attention to aspects ofthe data that are not actually useful in prediction.This results in predictions that are not accurate enough to be useful.Moreover, the relationship betweenthe independent variables and the stock returns can change over time.In other words, their relationships are not stationary.In practice, this means that your models become stale aftersometime and need to be periodically retrained with more recent data.This also means that some independent variables,that produced a useful predictive signals in the past maynot necessarily do so in the future and vice versa.This is partly why a certain trading strategies fadeafter awhile and be become useful again in the future.

### 02. M2L3 02 Distributions V2-ZlRGxq5I9BU.en

Many statistical models assume that the data follows a normal distribution,also referred to as a Gaussian or a bell curve.This is important when checking whether our models are valid.There are various tests that we use to checkthat our models describe a meaningful relationship.These tests assume that the data are normally distributed.If the data are not normally distributed,then these tests tend to conclude that the model is valid when in fact it is not.We will review the concepts of normality,show how to check for normality,and introduce some ways to transform our data so that they follow a normal distribution.What is a random variable?A random variable is a variable that can take on a random value.The probability that the random variable takesa particular value is determined by its probability distribution.You can think of a probability distribution as a range ofnumbers each with a probability associated with it.With data from the real world,we don't actually know its underlying probability distribution.However, we can often approximate its probability distribution with an equation.If we say that a random variable is normally distributed,how do we visualize what this means?We can imagine the random variable as a tennis ball machine.This machine shoots tennis falls ontoa number line that ranges from negative infinity to infinity.The number line has buckets of equal size placed on the line,so that tennis balls will collect in these buckets.If the random variables distribution is centered around zero,this means that tennis balls are more likely to hit the number line at zero,rather than say 500.If we shoot enough tennis balls along this number line,then we end up with a hill shaped pile of tennis balls.This plot is called a histogram.A histogram resembles the shape of its probability distribution.

### 04. M2L3 04 Parameters Of A Distribution V3--akdmiLDny4.en

A probability distribution is defined in math by an equation.We call this equation a probability density function or PDF.For every number from negative infinity to infinity,the probability density function gives a probability that this number will be generated.Using math notation, X tilde D meansthe random variable X follows a probability distribution D.The capital P with a lowercase x vertical bar and capital Dis read as the probability of x given D. So,what does it mean when we say the probability of x given D?Let's say we have a number two that we observed from a dataset.We assume that the dataset followsa particular probability distribution such as the normal distribution.The equation P of x tells us how likely the random variable would take on the value two.This P of x outputs a number between zero and one which represents this probability.For instance, here is the standard normal distribution written as a fancy-looking capitalN. The standard normal distribution has a mean of zero and a standard deviation of one,but what if you had a distribution that was not centered around zero?Or, what if you had a distribution that had a flatter wider distribution?These distributions can still be modeled bythe same equation by adjusting its parameters.To describe these variations of the normal distribution,we can adjust two of its parameters.Calculate the mean of the data and assign that value to mu.Calculate the standard deviation and assign it to sigma.The resulting function approximates the distribution of our data.

### 06. Testing For Normalilty-Sa1MJegyYfc.en

We've just seen how to model our data if we assume that it is normally distributed.But how do we decide if our data can bedescribed by a normal distribution in the first place?A quick way to visually check,is to plot a histogram of our data.We can then compare the histogram to a plot of the normal distribution.Does this data's distribution look normal?How about this one?Does it look like we can still describe it with a normal distribution?We can also use a boxplot to check for normality.As you'll soon see,the boxplot helps to check for symmetry in your data.Normal distributions are symmetric around their mean.Here is a boxplot.There is a box with a line inside.The bottom edge of the box,the middle line in the box,and the top edge of the box,are quantiles that divide the data in two groups.We call the dividing lines quantiles becausethey split the data into groups with the same number of points.Since the three dividing lines create four groups,we call these lines quartiles.Remember that the quartiles are the three dividing lines,not the four groups themselves.The line inside the box is the median.If you ordered all of your dataset from smallest to largest,50 percent of your data points are less than the median.The bottom side of the box represents the first quartile.Twenty five percent of the data are less than the first quartile.The top side of the box is the third quartile.Seventy five percent of the data are less than the third quartile.We can calculate the interquartile range by taking the third quartile,minus the first quartile.The boxplot has small lines on either end.We call these lines whiskers.The lower bound whisker is defined as the first quartile,minus 1.5 times the interquartile range.Similarly, the upper bound whisker is set to the third quartile,plus 1.5 times the interquartile range.Points lying outside of the whiskers can be consideredoutliers and are visualized as individual points.Going back to our test for normality,if you look at the normal distribution, it is symmetric.When a distribution is symmetric,it's median is equal to it's mean.The first quartile and third quartile are also the same distance away from the median.Also, the whiskers are the same distance away from the median.By visualizing your data with a boxplot,you can see whether it's symmetric or not.If it's not symmetric,it's not normally distributed.We can look at the data using both a box whisker plot and a histogram.Here, it looks like it follows a normal distribution.This other dataset is not normal.In fact, we say that it is skewed because it hasa long tail of data to only one side of the distribution.When there is a long tail of data to the left,the mean of the distribution lies to the left of the median.We say that the distribution is left skewed.In fact, stock returns tend to exhibit left skew and fat tails.This means that extreme negative returns occur withgreater frequency than would be expected by a normal distribution.Please note that even if your data is symmetric,it's not necessarily a bell-shaped curve of a normal distribution.So, it helps to see the box whisker plot,as well as the histogram.To do a more thorough check for normality,we can use a QQ plot.The QQ plot checks if the shape of the datamatches the shape of our probability density function.The first Q in QQ plot stands for quantile.The second Q in QQ plot also stands for quantile.Common quantiles are quartiles, deciles, and percentiles.Recall that when we sort the data points from smallest to largest,we can find boundaries that divide the data into groups with equal number of points.If we divide a data into 10 buckets,the nine boundaries are deciles.If we divide the data into 100 buckets,the 99 boundaries are called percentiles.Notice, that this is not the same as plotting a histogram.With a histogram, the buckets have the same width.With quantiles, the buckets all have the same number of data points.QQ plots let us compare any two distributions to check if they have the same shape.Since we want to check if a data is shaped like a normal distribution,we can plot it's quantiles against the quantiles of a normal distribution.As an example, let's take the 50th percentile of our data as our y-coordinate.Next, we take the 50th percentile ofthe standard normal distribution as our x-coordinate.We plot this xy point on the QQ plot.We repeat this for all the other percentiles.If the two distributions have the same shape,then the plotted points will followa straight line that goes from the bottom left to the top right.In this QQ plot,we can see that the data does not look like a normal distribution.This is because the plotted points form a curve instead of a straight line.So far, we've seen plots that help us visually check if our data looks like it's normal.What if we wanted a single number to represent how normal the data is?If we had a single number,we could choose a cutoff point.Anything on one side of this cutoff point,and we decide that it's normal.Anything on the other side of the cutoff point,and we decide that it's not normal.The Shapiro-Wilk test and the D'Augustino-Pearson tests,are hypotheses tests that give a p-value.The p-value ranges from zero to one.These are hypotheses tests in whichthe null hypothesis assumes that the data is normally distributed.If the p-value is larger than 0.05,then we will assume that it follows a normal distribution.If the p-value is less than 0.05,then it likely does not follow a normal distribution.There is a another more general test to decidewhether any two distributions are the same.This test is called the Kolmogorov-Smirnov test.Given two distributions, the Kolmogorov-Smirnov test returns a p-value.If the p-value is greater than 0.05,we will assume that the two distributions are the same.If the p-value is less than 0.05,then we say the two distributions are not the same.Remember the reason why we care about the data being normal.When we use statistical models such as regression,we use hypotheses tests to check if we can trust the model parameters of the model.These tests assume that our data is normally distributed.If the data are not normally distributed,these tests tend to tell us that our model is valid when in fact it is not.

### 09. M2L3 08 Heteroskedasticity V2-wias9OZ1tU4.en

Recall that we also need to check if our data is stationary over time.By stationary, we mean that the mean,variance, and covariance are the same over time.In particular, we want to check that the variance of our data is stable over time.This is important because if the variance changes over time,then the tests that we use to validate our model will be incorrect.The term for constant variance over time is homoscedasticity.The term for a changing variance over time is heteroskedasticity.To check if our data is homoscedastic or heteroskedastic,we can use the Breusch-Pagan test.If the Breusch-Pagan test,yields a small P value,say less than 0.05,then we have to assume that the data is heteroskedastic.If the Breusch-Pagan test yields a P value greater than 0.05,we can assume that the data is homoscedastic.

### 10. M2L3 09 Transforming Data V3-N8Fhq8wiQZU.en

So, now the question is,what do we do when our data is not normally distributed?Similarly, what do we do when our data is heteroscedastic?To reshape our data and make it more normal,we can feed our data into the log function.To get data that is homoscedastic,we can take the time difference between periods.By time difference, I mean that we can viewour data as the rate of return from one day to the next.Similarly, we could get the time difference bysubtracting each previous day's value from the current day's value.In practice, we take the rate of change for each period and then apply the log function.You may have seen this earlier when learning about why wemodel financial data with log returns.A way to make our data both normally distributed and homoscedastic,is by applying the Box-Cox transformation.To preview what the Box-Cox transformation does,let's get a conceptual idea of what a transformation looks like.Imagine you place all your data points on a number line,they are not evenly spaced and have odd clusters insome parts and no data points in other places.Think of this as a necklace in which the beads are not spaced out in a very nice way.Now, imagine if we could nudge each of these beads a little bit tothe left and to the right to even out the spacing in a nicer way.Notice that since these beads are all on the same string,the relative order of each data point remains the same,we're just spacing them out in a way that's easier to work with.In math terms, we just applied a monotonic transformation.A monotonic transformation changes the values ofa dataset but preserves the relative order.The Box-Cox transformation takes a datasetand outputs a dataset that is more normally distributed.You can see that the Box-Cox transformation has one constant value, lambda.If you choose lambda to be zero,then the transformation function is defined as the natural log.You can try different values for lambda to transform the data,then perform tests for normality and homoscedasticity.Once you can say your data is normally distributed,you can use it in the various models that we'll discuss in the rest of the lesson.

### 11. M2L3 10 Linear Regression V4-GRY4eakMBJ8.en

Now, we'll look at how to use one random variable to predict another random variable.We'll cover the basics of regression as this forms the basis forseveral models that are used to analyze stock returns over time.If we want to estimate the price of a house,we may assume that home buyers are willing to pay more for a bigger house,all other things being equal.So, we may find data on the area covered by each house as well as its price.We want to find a coefficient that we can multiply by the area,then add a constant term,which we refer to as the intercept.This is the equation for a straight line.Fortunately, we don't have to guess the best values for the coefficient and intercept.When we plot the price against the area,we can draw a line that tries its best to pass through most of the points.We can measure how well the line fits the data bymeasuring the vertical difference between the point and the line.An optimal regression line is one that manages to reduce these differences.This process that finds the optimal regression line is called ordinary least squares.Even after we find the best regression line,we can expect to see differences between the data points and the line.These differences between the best fit regression line ateach point are called residuals or error terms.In other words, the residual is the difference between the actual value,and the predicted value.We can check if the residuals follow a normal distribution.If the residuals follow a normal distribution witha mean of zero and a constant standard deviation,then these residuals can be considered random.By random, I mean that the model's predicted value is equallylikely to be higher or lower than the actual value.If however, the average of the residuals is not zero,this gives us a hint that the model has a bias in its prediction errors.One way to improve our model is to look for other independent variables.This is called multiple regression,when we use more than one independent variable to predict a dependent variable.Now that we fit a regression model,we want to check if we can rely on it for future predictions.One measure of our model's ability to fit the data is the R-squared value.The R-squared is a metric that ranges from zero to one.R-squared of one means that all the variation in the dependent variable,can be explained by all the variation in the independent variables.A better metric is the adjusted R-squared,which helps us to find the minimum combination ofindependent variables that are most relevant for our model.Another way to check our model,is by performing an F-test.The F-test checks whether our coefficients andintercepts are not zero, and therefore, meaningful.If we get a P-value of 0.05 or smaller,we can assume that our parameters are not zero.When parameters are not zero,then we can say that our model describes a meaningful relationship.

### 14. M2L3 12 Multivariate Linear Regression V3-WbCGVF7SAN0.en

Earlier, we touched on usingmultiple independent variables to predict a single dependent variable.We call this multiple regression.In this example, we are using a house's area,number of rooms and number of years since the house was built.All three are variables that we use to predict the house price.We can go one step further and choosemore than one dependent variable that we wish to predict.Now, we're not just predicting the house price,but also trying to predict the home's electricity consumption and gas consumption.When we tried to predict more than one dependent variable at the same time,this is called multivariate regression.When we're using more than one independent variableto predict more than one dependent variable,this is called multivariate multiple regression.This appears rather complicated,but being familiar with multivariate regression will helpyou later when you learn about time series and pairs trading.

### 15. M2L3 14 Regression In Trading V2-bcOGRWxg7qQ.en

So, how is regression used in stock trading?In practice, using regression to predict a stock's return isdifficult because the signal and financial data is low compared to the noise.Moreover, the models are sensitive to some choices you make about the model.For instance, you'll have to decide how much of your previous data touse since more recent data is more relevant than very old data.Regression models are pretty sensitive to these design choices.Regression is also sensitive to outliers in the data,as it adds more noise to the training data.However, there are still important reasons to learn regression to analyze stock returns.Learning regression is useful because we can applythe same regression techniques to analyze time-series data.Time series analysis looks at data that is collected at regular intervalsover time and uses that to predict its value in the near future.One more note about why regression is important to learn,if we learn the details of how neural networks work,many of the same principles apply.In fact, if we think of regression as a fundamental building block,then a neural network is a building that is composed of many regression building blocks.

### 17. M2L3 15 Summary V1-n2VxcEcw0GY.en

Congratulations on making it through this lesson.You've built a foundation that will help you with the next few lessons.Coming up, we'll discuss time series analysis in more depth.Time series analysis includesstatistical techniques such as auto-regression and moving averages.Time series analysis also includesmachine learning techniques such as Kalman Filters and Recurrent Neural Networks.See you in the next lesson.

### 18. MV 14 What Happens In Your Brain V1-ioDP7ndd40Y.en

You've been busy learning here,building up your knowledge and skills through your nanodegree work.But, what's actually happening inside your head to enable this growth?Well, each time you think about something,you activate particular neural pathways in your brain,and that activation strengthens the pathways themselves.As a simple example,if I just learned basic addition so that I canfigure out that three plus two equals five,the more I do problems similar to this like,one plus six equals seven,the easier it will be for me to remember how to add digits together.Essentially, we grow our knowledge and make it easier toaccess the more we recall relevant information.Research also shows that applying new learning indifferent contexts makes it even stronger,and it allows you to retain it better.So, if I move from problems like three plus two equalsfive to trying out something like 4 plus 20, plus 1,plus 6, I have the opportunity to apply my addition skills ina new context with bigger numbers and more things to add up.This will make my brain distill the core conceptsnecessary for understanding how addition really works,which means that in the future,when faced with even trickier addition problems,I'll have a strong foundation to tackle those challenges.This is exactly what we want to prepare you to do,albeit with much tougher problems.Our job at Udacity is to help you build deep knowledge andflexible skills that you can apply to all sorts of novel problems in the future.In order for that to happen your brain needs to actively engage with problems,questions, and ideas that will push it to grow in the desired direction

### img

## Part 01-Module 02-Lesson 04_Time Series Modeling

### 01. M2L4 01 Time Series Modeling V4-QeIu7GMZl20.en

Welcome to the lesson on time series analysis.Time series are data that are collected at regular intervals.We will cover two statistical methods,autoregression and moving averages.This will give us the foundation to cover two more advanced methods,autoregressive moving averages and autoregressive integrated moving averages.From there, we will cover two machine-learning methods.The first is Kalman filters,and the more generalized, particle filters.The second is recurrent neural networks.Let's get started.Let's think a bit about what a stock price time series looks like.Most stock prices increase over time.Sadly, some stock prices also decrease over time.This means that the price shows a trend.This makes analyzing the prices difficult since the prices are non-stationary.By non-stationary, we mean that the data's mean and standard deviation change over time.The goal of time series analysis is to use past data to predict future values.So, if the properties of the data change over time,the past is less useful in predicting the future.To work with data that is more likely to be stationary, and therefore,easier to model, we use stock returns and not stock prices.Moreover, to work with data that is more stationary and more normally distributed,we use the log of stock returns,which we call log returns.

### 02. M2L4 02 Autoregressive Models V5-9jE7S4b-oIU.en

When we look at log returns of a stock,we assume that the previous period's value gives ussome insight into the next period's value.It's also reasonable to assume that the past couple ofdata points give us hints as to what the next value will be.This is the main principle behind Auto-regressive Models.An autoregressive model, also called an AR model,tries to fit a line that is a linear combination of previous values.The AR model includes an Intercept which representssome constant that is independent of the previous values.The AR model also includes an Error term,which represents movements that cannot be predicted using the previous values.Let's see how we prepare the time-series data,to feed it into an Autoregressive model.As an example, our AR model will usethe previous two values to predict the current value.For example, if we have daily data for Monday, Tuesday,and Wednesday, then we use Monday and Tuesday as the independent variables.We also define Wednesday as the dependent variable that the Model tries to predict.Then, when we move on to Thursday,we use the previous two days to predict Thursday's value.If data from a previous period has some predictive value,then this coefficient will be non-zero.The number of past values used in the model is known as the Lag.We define an AR model by its Lag.For example, a model that only uses yesterday's value and ignores the rest,is an AR 1 Model.Whereas, a model that usesthe two previous days' values and ignores the rest is an AR 2 Model,and a model that usesthe previous three days' values and ignores the rest is an AR 3 Model.We write Auto-regressive Models as ARP,where p stands for the lag.We can choose different lag values to trainyour model and see how they perform on test data.We can check if the coefficients are significantly different from zero.If some coefficients are likely zero,then you can reduce the lag and focus the model on the more recent values.As with any regression model,you can also check the adjusted R squared to get a sense of howwell the independent variables explain the movements of the dependent variables.Note that an AR model is designed to represent a single time series.If you have several different time series,you could build a separate AR model for each.But what if the movement of one stock has some relation to the movement of another?You may be seeing something similar with multivariate multiple regression.To account for interdependence among more than one time series,we can use the multivariate version of autoregression.This is called a vector autoregressive model.Note that having some familiarity withthe vector autoregressive model will help youas you learn about pairs trading in a later lesson.

### 03. M2L4 03 Moving Average Models V5-1FkCP_dwxjI.en

Another way to model time series is to think ofthe stock return hovering around in moving average.As an analogy, imagine that you're walking at night while holding a lantern,a moth flies around the lantern,moving a bit randomly,but still following the general path of your lantern.In this analogy, the lantern represents the average position.The moth's smaller movements relative to the lantern are the residuals.The moth's movement relative to the ground isa combination of the lantern's movement and the moth's relative movements.In the moving average model,often called an MA model,we started with the average Mu.To get the value at time T,we add a linear combination of residuals from the previous time periods.In financial time series,the residuals represent the new unpredictable informationthat cannot be captured by the past data points.The residuals are the difference betweenthe model's past predictions with the actual values that occurred.Moving average models are defined as MAQ where Q is the lag.To decide the best value for Q in an MAQ model,you can draw an autocorrelation plot.Correlation between two variables is a measure ofhow much one variable moves when the other variable moves.Correlation ranges between negative one and one.Autocorrelation is a measure of how muchthe current value moves in relation to one of its previous values.For example, let's say we noticed that the current stock return is usuallypositive when its previous value is positive, and vice versa.Then, we can say that the stock has a positive autocorrelation with it's T minus 1 value.Note how an autocorrelation plot is different from performing a multiple regression.Correlation measures the pairwise relationship between exactly two periods at a time.Multiple regression measures how a set ofindependent variables collectively influence the value of the dependent variable.When we view an autocorrelation plot,we want to use the lag that contains highly positive or highly negative correlations.When we reach some time periods with little correlation,we can choose the lag to ignore those valuesand any other time periods further back in time.

### 04. M2L4 05 Advanced Time Series Models V5-cj1RCBTDog8.en

Autoregressive and moving average models tend to capture different relationships.The nice thing is, you can get the best of both by adding them together.An autoregressive moving average is defined with a p and q.The p is the lag for the autoregression,the q is the lag for moving average.A variation of the autoregressive moving averageis the autoregressive integrated moving average.This concept is used in a trading strategy called Pairs trading.First, let's build some intuition that will help us whenwe study autoregressive integrated moving averages.Let's say we want to describe the position of a turtle.The turtle is walking at a regular pace.So, its position goes from zero,then one, then two, and so on.We noticed that the turtle is moving at a regular pace.So, we can describe its speed,or the distance over time,as a single number.For example, this turtle is moving at one meter per second.If we plot the turtle's position over time,it is not constant.Its position over time will look like a line that slopes upward.However, if we plot the turtle's speed over time,it is a constant one meter per second.The plot of speed over time is a horizontal line.Now let's think about how the turtle's position is related to its speed.If we look at the turtle's position over time,the slope of that line is its speed,or meters per second.Going the other way,we look at the plot of the turtle's speed over time and take the cumulative sum.So, for instance, at each second,we add one meter.So, we get one meter,two meters, three meters, and so on.This is actually the area under the horizontal line,and is also the position of the turtle as time goes on.So, what did we learn here?In general, taking the difference between each periodis called the time difference or item wise difference.If you take the time difference of your data,you may be able to describe your data more easily as a constant number.If you remember from calculus,taking the derivative of a straight line gives the slope of that line.This slope gives us a way to describe the line with a constant.To go from speed back to position,we can take the integral.This is finding the area under the curve or taking the cumulative sum.Taking the integral lets us translate from the speed back to position.We will now apply these concepts of time difference to learnabout the autoregressive integrated moving average.Recall that regression-based time series models require the data to be stationary.When data is not stationary, the mean, variants,or co-variants may change over time,and it's hard to use the past to predict the future.One way to get a stationary time series is bytaking the difference between points in the time series.The time difference may also be called the rate of change or the item wise difference.We call that the rate of change between periods,or the rate of return,can be calculated by taking the ratio of the current price divided by the previous price.When you express your data in terms of logs,this ratio becomes a difference betweenyour current log price and the previous log price.When working with financial data,we usually find that asset price time series havea property such that their time difference is stationary.In other words, we like working with returns andnot prices because the time series are more stable.In math terms, we say that the original price data is integrated of order one.We also say that the log returns of this data is integrated of order zero.So, when working the time series,you can check if it is stationary usinga statistical test called the augmented Dickey Fuller test.If the augmented Dickey Fuller test gives a p-value that is 0.05 or less,then we can assume that the time series is stationary.If the data is not stationary,we can take the time difference then runthe augmented Dickey-Fuller test to see if the time difference is stationary.If it is stationary,then we can say that this time difference is integrated of order zero.We can also say that the original time series is integrated of order one.You may need to take the time difference multiple times.Then, once you find a time difference that is stationary,you refer to the original data as integrated oforder d where d is the number of times that you had to take the time difference.Also, once we have a stationary time series,we can model it with an autoregressive moving average.Note that being familiar with the integrated orderone and integrated ordered zero time serieswill help you as you learn about co-integration and pairs trading.

### 06. M2L4 07 Kalman Filter V4-CLJhgfMI4Ho.en

We'll now discuss the Kalman filter,which is used for time series in self-driving cars and even flying cars.But first, let's look back atregression and autoregressive moving averages to see how Kalman filters are different.Recall that with autoregressive moving averages,we must choose the lag number forautoregression and also the lag number for moving averages.Our choice of the lag parameters can affect how the model performs.What if, instead of trying to find the best lag parameters,we had a single state that represented all of the relevant information from the past?In other words, instead of choosing values for P and Q,what if we had a set of variables at T minus 1 to represent the past?I'll give you a hint, Kalman filters.Also, you may recall that financial data hasa lot of noise relative to its useful signal information.It's often the case that we want to measurea specific thing that can only measure something else that's related.For example, we may wish to measure oil production levelsbut can only measure oil pipeline flows near the production sites.So, how do we make predictions when we have noisy indirect measurements?If you guessed Kalman filters, then you're right.When using Kalman filters,we can assume that the stock returns properties can be summarized by set of values.We call these set of values the state.Within the state of the time series,there is some hidden property that we can't measure directly.We can think of this hidden property as a smooth curve whichrepresents the value of the stock return if there was no noise.On the other hand, where we actually measurethe stock return includes this hidden state plus noise.So, what we have to work with is a more jagged curve with some randomness in it.The Kalman filter is designed to handle this kind of real life noisy data.The Kalman filter repeats the following steps in a loop.The first step is called the predict step,the second step is called the measurement update step.First, the Kalman filter predicts the hidden stateor value of the stock return as a probability distribution.Next, it takes measurements such asthe actual stock return data and then updates its belief about the hidden state.Note that the Kalman filter stores the relevant information in what's called the state.Also notice how the Kalman filter is dynamicallyupdating its underlying model every time it performs a measurement.The Kalman filter uses both the previous time period's state andthe measurement of the latest stock return to predict the next state.So all the relevant prior history of the time series is stored in the T minus 1 state,and there's no need to look at the earlier time periods.

### 07. M2L4 08 Particle Filter V4-4KhDUAvwI74.en

Next, we'll discuss the particle filter,a type of genetic algorithm that is alsoused for self-driving cars as well as time series.By genetic algorithm, I mean that we apply natural selection to improve our estimates.This will become more clear in a bit.Let's start with a thought experiment.Imagine that we can hire many little helpers,each with a certain view on where the stock returns are going based on market data.Each of these little helpers predict the stock return for the next day,and on the following day,you can see how correct they were.Each day, you pay more to the little helpers who arecorrect and pay less to the incorrect helpers.Over time, only the accurate helpers remain to make predictions.So by the process of natural selection,we find the helpers who are mostaccurate and average their predictions as your best estimate.Since this process looks like natural selection and evolution,particle filters are considered a type of genetic algorithm.The little helpers are called particles.These particles are individual models whose parameters are set randomly.When most particles make very similar predictions,this also shows more confidence in the average prediction.When there are significant changes in the data distribution at any point in time,the particles may make predictions that are more different from each other.In those cases, when the individual predictions are more spread out,we are less confident of the prediction.Particle filters are pretty good at handling a variety ofdata because they don't assume the data to be normally distributed.Particle filters also do not assume a linear relationship,so they can better fit non-linear data.

### 08. M2L4 09 Recurrent Neural Networks V5-5cYAAHyRHDo.en

The last model we'll introduce here are recurrent neural networks,which are used in natural language processing as well as time series.Recurrent neural networks are as the name implies neural networks.Neural networks are models that can be thought of asmany regressions stacked in series and in parallel.When I say the regressions are in a series,I mean that the output of one regression is fedin as the input to another regression in a chain.Also, when I say parallel,I mean that there are multiple regressionswhose outputs are fed into another layer of regressions.A recurrent neural network is called recurrent because it takes some ofits intermediate output and uses this aspart of its input as it trains itself on incoming data.The recurrent neural network takes an input and outputs in prediction.The recurrent neural network also outputs an additional signaland intermediate output which it feeds back into itself.At the next time step when the RNN receives another input from the data source,it uses both the input as well asits previous intermediate output to help it calculate its next prediction.You can think of this signal as a way forthe RNN to remember relevant information from the past.To train a recurrent neural network,you feed it lots of data from the past,then you see how well it predicts data from the future usinga process called gradient descent to adjust the coefficients in the network.One commonly used version of recurrent neural network is madeup of one or more long short-term memory cells.The long short-term memory cell,or LSTM cell consists of several neural networks each that perform a specific task.The LSTM cell can be thought of as an assembly line,and specific tasks are performed by different assembly line workers.The LSTM cell takes data as input and alsotakes its own signals that it generated from the previous period.The signal that it takes from its previous periodcan be thought of as its memory from the past.Some of the assembly line workers, remove some memories.Other assembly line workers add more memories based on incoming data.Still other assembly line workers decide what to output.Remember that the recurrent neural network has two kinds of outputs.One, is its prediction for the variable in question.The other is an intermediate output for its future self.You can think of this intermediate output asthe memory that it wishes to pass on to the next time period.

### 09. M2L4 11 Outro V1-6sheR92KUU8.en

In this lesson, we've seen statistical methods such as regression,autoregression, and moving averages.We've also seen Kalman filters,particle filters, and recurrent neural networks.Congratulations on making it through this lesson

## Part 01-Module 02-Lesson 05_Volatility

### 01. M2L5 01 What Is Volatility V3-brGVwpDSuG4.en

Welcome back. In this lesson,we're going to tell you all about volatility.Volatility is an important measure of risk, but what is risk?We've talked a little about it already,and you may have an intuitive idea for what it means.It's basically uncertainty about the future and in particular,uncertainty about bad things happening,like losing all the money you've invested in something.In finance, risks can be thought of as uncertainty about future returns.Uncertainty, that sounds kind of hard to measure.How do you measure what you don't know?Well, we can get started by thinking of the log return asa random variable that can assume different values with different probabilities.An important way of characterizinga probability distribution is with it's expected or mean value.The mean is the average of many measurements of the random variable,and you already know thatthe standard deviation is a measure of the spread of a distribution.Volatility is simply the standard deviation ofthe probability distribution of log returns.It's a measure of the spread of this particular distribution.It measures the dispersion of log returns around the expected log return.Volatility gives you a sense of the range of values log returns are likely to fall into.For example, say you measured log returns oninvestment for several years and found that they happento be distributed more or less normally with a mean or expected log return of about 5%,and an annual volatility of about 6%.If you remember, that for a normal distribution,about 95% of observations fall in a range fromnegative two to positive two standard deviations around the mean.This would mean that 95% of the annual returns wouldbe between approximately negative 7% and positive 17%.So, why spend all this time learning about volatility?Basically, every investor trying tounderstand what level of return they can expect to get onaverage will also care about how often and byhow much their returns are going to differ from that average.In essence, volatility is a simple metric that characterizes variability in returns,that has many different users all over the financial world.Let's list a few examples of the ways you might see it used.As I said, volatility is one way to measure risk.Traders can use volatility ofa trading instrument to define position sizing in a portfolio.Volatility can be used in the design of alphas, that is,as part of the logic that defines which stocks to trade and when to trade them.Volatility is very important for determining the prices of options contracts.Finally, it's possible to trade volatility directly.For example, you can trade an instrument that tracks the volatility of the S&amp;P 500,we'll learn a little more about this later on.

### 02. M2L5 02 Historical Volatility V3-BOPhsYLHkUU.en

Say we want to get a sense of how our returns are going to vary in the future.That is to say, we want to estimate the volatility of a stock we've invested in.How do we do it? Well, the simplest way is to calculatethe standard deviation of the log returns of some price data that we have for this stock,eg data from the past.This is called historical volatility.The first step is to calculate log returns from prices.The first log return datum will be undefinedsince there is no price datum from the time point proceeding it.So, if you start with n plus one price data,you will now have n log returns.Then, the formula for volatility is the formula for standard deviation,where the observations are log returns.One thing to keep in mind is that,since volatility is defined as the standard deviation,it treats log returns that are above and below the mean the same way.This is because in the calculation of standard deviation,price differences relative to the mean are squared,so that negative and positive differences are combined into one quantity.So, after doing these calculations for this small dataset,we'd get sigma equals 0.025.Since our dataset consisted of daily prices,this is the estimate of the daily volatility.Sometimes though, instead of having a price for every day,you might only have a price for every week,or a price for every month.If you take these data,calculate the log returns,and then calculate the standard deviation,you would get a different number for the volatility.For example, for stocks,the standard deviation of daily log returns is typically around 0.006 to 0.03,while the standard deviation of weekly log returns is typically around 0.01 to 0.07.If you think about it, this makes sense.This is just telling you that stock prices changemore over the course of a week than over the course of a day.So weekly returns vary more widely than daily returns.So your volatility estimate depends on the time frequency of the underlying price data.But we want to be able to come up witha volatility number that we can compare across different situations and datasets.In order to do this,people typically calculate a value thatcorresponds to the standard deviation of annual log returns.If you don't have several years of data,and you can't calculate the standard deviation of annual log returns,you can calculate the standard deviation of log returns ofanother frequency and extrapolate it to an annual one.This is called calculating the annualized volatility.And we're going to talk about it in the next lesson.

### 06. M2L5 06 Rolling Windows V3-4EuMKqeNXA0.en

So, we know how to calculate volatility for a given period.This gives us one number to capture return variability for this period,but we also know that the market changes over time.We know the market goes up and down,and while sometimes it seems highly variable,other times it seems relatively placid.For example, during the last financial crisis in 2008,stock prices were extremely volatile.Whereas 2016 and 2017 have marked a relatively low volatility period.So, what do you do if you want to understand how volatility is changing over time?One option is to use a rolling window.So, if you wanted to estimate today's volatility,you would calculate the standard deviation of a set oflog returns from sometime in the past up to yesterday.This is a practical method that is commonly implemented,but how long should the time window be?When you take a look at the volatility of previous days to estimate today's volatility,how important is the volatility from one day ago?How about from one week ago?Or how about from one month ago?A long window will mean that the value youcompute may not react to changes in market conditions quickly enough.To short window may mean that the computed value becomestwo variable over different window periods to be used reliably.Your choice of time window is going to depend on your application.In general, if your signal or strategy involves holding onto purchases for long periods,you can afford to use a longer time window forcomputation and vice versa for short holding periods.You can also use windows of different sizes to gain insight into how volatility evolves.

### 08. M2L5 07 Exponentially Weighted Moving Average V4-VBPitTHzYRI.en

When we're predicting today's volatility,we may think that data from the past month are important,but what happened yesterday is even more important than what happened last week.This introduces another possibility for how to estimate volatility.What we can do to increase the influence of the more recent past is weight,log return observations from different days, differently.One common method is to weigh the observation from yesterday, the highest,the one from the day before a little less,the day before that a little less,and so on into the past.In fact, the weights decrease by the same percent withevery step which means they decrease exponentially.As a result, this is called an exponentially weighted moving average.Let's see how to calculate it.Let's start with the formula for historical volatility.Let's imagine we're estimating today's volatility using a window of n past days.We'll add the subscript t to indicate that we are calculating today's volatility.Let's square both sides.Remember, Sigma squared is the variance but we can always takeits square root to get the standard deviation i.e. the volatility.Now, let's make some simplifications.Let's assume that the mean log return is zero.This is okay to do because when you havelog returns calculated over a short time interval,like daily log returns,their mean is generally small compared to their standard deviation.Now, let's change n minus one to n,this simplification won't influence the result much when n is large,and allows us to write the estimate of the variance this way.As you can see, this is just an average ofthe squared log returns in which each squared log return is weighted equally.Let's change the arithmetic average to a weighted average and makethe weights decrease with each step into the past like we discussed earlier.We're going to introduce a new parameter and lets call it Lambda.Lambda is a number between zero and one,and it represents the factor by which the weights inthe weighted average decrease as we go back in time.So, let's construct the sum.The first term will be yesterday's log return squared,multiplied by Lambda to the zeroth power which equals one.The next term will be Lambda times the log return of the day before squared.And the next term will have another factor of Lambda.This keeps going until we have a term for every log return in our window.Now, to make it a weighted average,we just have to divide by the sum of the weights, and that's it.That's the formula for an exponentially weighted,moving average estimate of the variance of the log returns.So to get the volatility,we would just take the square root of that estimate.

### 10. M2L5 09 Forecasting Volatility V3-82v4v_PKDAE.en

It would be great to know what will happen before it happens, wouldn't it?Traders sometimes try to forecast volatility in order to anticipate the market.The general thinking is that it's easier to predict volatility than price,volatility tends to be sticky.A volatile market trading day is often followed by another volatile one.When you're trying to model volatility,you can use a special form of the autoregressive model called ARCH,which stands for, autoregressive conditionally heteroscedastic.Okay, now those are some really big words,so let's break it down.Autoregressive simply means thatthe current value is somehow related to the recent past values.Heteroscedastic means that the variable we're trying tomodel may have different magnitudes of variability at different time points.The magnitude of variability is commonly measured using variance.Finally, conditional here, refers to a constraint we place in this model,which limits the heteroscedastic property to beconditionally dependent on the previous value or values of the variable.An arch model looks really similar to the formulawe had for an exponentially weighted moving average.The current variance is a weighted sum of past squared log returns,but now we think of the weights as being parameters of a model.For example, in an arch one model,if rt represents the log return of a stock at time t,then we say that its variance is equal to alpha zero,plus alpha one, times r t minus one squared.Alpha zero and alpha one are the two parameters of the model.You can think about alpha zero as representing a baseline variance,and alpha one as waiting the contribution tothe model of the log return at the previous time point.This can be extended to an arch m model,where the variance is conditionally dependent on m past timesteps.Note that here, the variance is being modeled only based on the squared log returns.We can also add terms to makethe current variance dependent on previous estimates of the variance.To address this, you could modify the expression to includeterms with previous variance values denoted as sigma squared.This is known as the GARCH,or generalized arch model.And you can parameterize it as GARCH m, n,where m is the number of log return terms,and n is the number of variants terms to include in the model.Once you're satisfied with a model you've built,it's time to think about how you might use it to make trading decisions.You may choose to generate buy,sell signals directly based on your predictions,or you may use it in an indirect way.You can analyze past signal returns and see if it has somehow correlated with volatility.For example, it's well known that certain strategies do better with higher volatility,and then you can choose to activate that strategy only during high volatility periods.

### 11. M2L5 11 Markets  Volatility V3-jEHJkZUX9s4.en

At this point, you may be wondering,well, what causes volatility anyway?A natural assumption may be that volatility ofa stock is caused by new information reaching the market.New information causes people to revise their opinions about the value of a stock.The price of the stock changes which causes volatility.However, research hasn't supported this conclusion.Research studies have compared the variance of stock price returns betweenthe closing price on one day and the closing price on the next trading day,when there are no non-trading days in between,and the closing price on a Friday and the closing price on a Monday.You might expect the weekend variance to bethree times the midweek variance because the real elapsed time is three times as long.However, this has not been found to be the case,even for assets affected by news that is equallylikely to arrive midweek or on a weekend.The weekend variants of orange juice futures prices which are mainly affected byweather news is only about 1.5 times the midweek variance.The implication of all this is that volatility is,to a large extent,caused by trading itself.This idea is consistent with the observationthat volatility and volume are frequently correlated.All other things being equal,greater trading volume means greater volatility.But during a time when volatility is high becauselots of trading is happening and the price goes up,you might reasonably expect the price to go back down.If there was no news,the price probably didn't increase because the company's performance improved.It probably increased because people wanted to trade large numbers ofstocks and sellers charged a premium for that ability to trade, or liquidity.This means that for equity strategies in high volatility times,a type of strategy considered more likely to do well is a type of strategy based onthe idea that prices will return to their running mean when they go way up or down.These are called mean reversion strategies.In low volatility times,momentum strategies may work better.There are other general patterns in market behavior with respect to volatility.One is that market volatility tends to be high whenthe market is going down and low when the market is going up.Because of this, volatility is often quoted in the popular media as the gauge for fear.To see this most clearly,let's look at an index that essentially measures market volatility.You might have heard of the VIX Index,which is a market index measuring the volatility of 30-day S&amp;P 500 Index options.The value of the VIX index represents the annualized volatility of the S&amp;P 500 Index.This plot of the VIX Index and S&amp;P 500 Index overthe same periods shows that the VIX spikes when the S&amp;P dips.For example, between 1990 and 2015,VIX spiked to a record 80 during October and November of 2008 when,as you remember, the markets were roiling from a huge crisis.I know what you're thinking,can you trade the VIX itself?You guessed it. Humans will turn anything into tradeable securities.You can trade futures and options on the VIX Index whichessentially amounts to making a bet on the volatility of the S&amp;P 500.The VIX is published by the Chicago Board Options Exchange,which publishes a range of other volatility indices on stockand commodity indices currencies and some individual stocks.There's even a Volatility Index of the VIX Index, the VVIX.

### 12. M2L5 12 Using Volatility For Equity Trading V5-Vh9ajVRedvY.en

Volatility is a very general metric of variability in returns,so it's used in many ways.In this video, we're going to mention a few possible ways that might be used in trading,but there are many more possibilities.First, you can classify stocks as highly volatile or less volatile.Some strategies might work best on low volatility stocks,so you could use the volatility metric to limit your universe.For example, one type of strategy called a mean reverting strategy is to takea short position on an asset when itappreciates because you think it will depreciate again.If a stock has low volatility,you may think there is less uncertainty in its fair value.So, after a large deviation from its fair value,its price is more likely to revert.Another interesting observation about low volatility stocks isthat they actually seem to perform well compared to high volatility stocks.In finance in general,greater risk is thought to accompany greater returns.So, the empirical observation thatlow volatility stocks outperform high-volatility stocks is considered a mystery.One explanation is that people ignore really boring things in favor of exciting stocks,like the tortoise and the hare.The low volatility tortoise is chugging along,but people are buying the high-volatility hare stocks.Some exchange traded funds specifically optimized forlow volatility and seek to include low volatility stocks.We'll talk more about exchange traded funds in future lessons.Another way to use volatility is to normalize another metric.Let me explain what I mean by that.Say you have a momentum signal consisting of returns over the previous year,say both Netflix and Walmart have a return of 30%.This may seem to mean they're doing equally well,but in fact, Netflix is much more volatile,it can move 30% in a week.So, you could normalize your momentum signal by dividing by the volatility.The idea is to make the comparison apples to apples.This theme exists all over the place.If you want to compare any signal across your entire universe,you usually get some benefit by standardizing it in this way.You can also use volatility to help you determine position sizes.In general, quant traders utilize smaller position sizes intheir strategies when markets are volatile to minimize the volatility of net profits,commonly called profit and loss, or P&amp;L.Moreover, the volatility of overall portfolio, P&amp;L,is often used as a gauge of the professional ability ofportfolio managers managing multiple trading strategies.So, the ability to allocate capital,taking volatility into account becomes very important.If you believe you can forecast volatility with a certain level of confidence,you can adjust your portfolio sizing in anticipation of a pending storm in the market.But how would you decide how much money to move?Well, you can include volatility ina formula used to determine the size of your positions.Every trader might do this slightly differently,but as an example,a trader might use a formula like this.R divided by sigma times M times last close equals position size,where r is the dollar amount the trader is willing to lose ifan M sigma event occurs against her or his position.Sigma is the annualized volatility of the security or strategy in question,M is a trader to find integer,last close is the last closing price ofthe security or the equivalent for a portfolio of stocks,and position size is the number of stocks to trade.This will lead to smaller position sizes for more expensive and more volatile assets.If you determine your position using a formula like this,then when you notice market conditions changing,you might decide to recalculate and adjust your position sizes accordingly.Sometimes trading strategies define thresholds at which toautomatically sell a stock to take a guaranteed profit or limit losses.An upper threshold is called the take profit level,while a lower threshold is called a stop-loss level.These might be defined as a target price or as a percent change from an entry price.However, if market volatility increases,many stocks might reach these levels more frequently causing more frequent trades.During volatile times, traders might adjust these thresholds accordingly.

### 13. M2L5 13 Breakout Strategies V4-9eamk40DMu0.en

Trading might use metrics availability to define trading signals.For example, you can take a fix length rolling window andcalculate the mean and standard deviation of prices within that window.If we draw a line,two standard deviations above and below the rolling mean,we can get a sense of the trend and tendency to fluctuate of prices,these lines are called Bollinger Bands.We can use Bollinger Bands to generate trading signals.For example, we might think that if the price is belowthe lower band and starts to cross back inside towards the mean,that the price is still fairly low and on the rise,this might be a good time to buy.We might also think that if the price is abovethe upper band and starts to move back towards the mean,that this might be a good time to sell.This is one type of strategy you might use to take advantage ofsituations when the stock price continually returns to its running mean.Alternatively, when the price hits the upper band,you may think that is going to keep going that it's breakingout to take advantage of these situations,you can define a slightly different type of strategy.If the price of a stock is not very variable but continues to increase,you can get a situation where it keeps increasing but never hitsthe upper Bollinger Band defined by the standard deviation of the prices.Instead, you can use an upper band defined by the maximum value within a rolling window.If the price continues to increase,it will continue to reach new maxima and will hit that upper band.One strategy you might take defined by this signal is to enter a long positionon the stock when it hits the rolling maxand enter a short position when it hits the rolling min.

### 14. M2L5 15 Outro V1-FMXL37CkTgg.en

Well done on finishing the lesson.In this lesson, we talked about a lot of different things.We gave you some key insights into volatilitythat will really help you construct trading strategies.Understanding volatility is essential to understanding the markets,and this lesson provides a solid foundation.

## Part 01-Module 02-Lesson 06_Pairs Trading and Mean Reversion

### 01. M2L6 01 Intro V3-CQ6QGAxbUF8.en

In this lesson, we'll learn about two properties of financial assets,mean reversion, and co-integration.Based on these properties,we will learn about the trading strategy called, Paris trading.From there, we will examine the more general strategy called, mean reversion trading.First, let's preview some concepts.What is mean reversion?A mean reverting time series is one that tends tomove back and forth around some constant value.When applying mean reversion to a single stock,we could check when the stock price deviates significantly from the mean.For example, we may see a stock decline significantly below its historical average.Assuming the stock will revert to its mean,we may buy the stock and anticipate that it will increase back to its original price.There is still the risk that the stock price does not return to its historical mean.For example, the stock may settle at its new, lower value.Even worse, it might just keep declining.In practice it's more common to apply mean reversion to a pair of stocks.Let's imagine that two companies are economically linked.By economically linked, I mean that they may be in the same line of business,may be operating in the same country,and may have similar types of products or customers.We can imagine that the stock prices ofthese two companies may also move up and down in a similar way.Since these companies move up together and down together,they appear to have a consistent relationship thatwe can expect will continue into the near future.So, when we see the prices of these two similar companies diverge significantly,we may make trading decisions based on the assumption that this divergence is temporary.The view of our trading strategy is that later in the future,the prices will settle back to their original relationship.We'll examine these concepts in more detail in the rest of this lesson.

### 02. M2L6 02 Mean Reversion V5-zQ08lFcZa_A.en

Let's look at mean reversion in detail.We can plot a stock's price movements against its moving average.We tend to see that when the price increases ordecreases significantly from this moving average,the price then reverts back towards this moving average.To describe a mean reverting process with some math,we'll use what's called the Drift and Volatility model.To guide our understanding,we'll imagine the stock movement as a boat that is floating passively in the ocean.The change in price of an asset over a period of time isa sum of a long-term average plus some randomness.This long-term average is calledthe drift term because it changes over time, but relatively slowly.Hence, the long-term average is likethe ocean current that slowly and steadily carries the boat.The amount of drift depends uponthe current price and the amount of time that has passed.The random part of the price change is called the Volatility Term.We can think of this volatility component as all the fish,dolphins or even whales that bump into the boat.All of these little bumps move the boat in a more random path around it stripped.The volatility term depends on the current price,a standard deviation and the amount of time that passes.The standard deviation is small when the markets are calm,so it's like encountering a small group of fish.The standard deviation may become large during a market crash.This would be like encountering a group of wales.With math, the Drift and Volatility model looks like this.The dp_t on the left represents the change in price over the timet. The first term to the right of the equal sign a is the drift term.The drift term is made up of the current price,a constant average mu,and the change in time.The second term on the right is the volatility term.The volatility term is made up of the current price,a standard deviation sigma,a random noise factor epsilon and the square root of the change in time.If this equation feels intimidating, don't worry.For now, just think about the boat drifting in the ocean,encountering fish, dolphins and maybe even some tropical islands.

### 03. M2L6 04 Pairs Trading V3-7lEm_tFXcBk.en

Let's take a closer look at pairs trade.When two companies have economic links,this implies that their stock prices may also move in a similar manner.For example, company A sells frozen green peas,and company B cells frozen carrots.Since frozen peas and carrots are often combined and sold together,we can see how an increase in sales of peas and carrots would affect both companies.Most of the time we see the two stocks moving similarly.However, we may see the two start to diverge.When this happens, we use our assumption about their economic ties,and also our assumption about mean-reversion.We assume that the divergence is temporary and thatthe two stock values will revert back to their original long-term relationship.The pairs trade involves going long the asset that is priced relatively low.At the same time,the strategy involves going short the asset that is priced relatively high.If and when the prices start to converge we can close our positions.When I say we close our positions,I mean that we sell the asset that we originallybuy and we buy the asset that we initially shorted.Also, if we see the stocks do not converge but continue to diverge over a period of time,we may also decide to close our positions to cut our losses.You may be wondering how long we should wait before expecting to see the pairs converge.It helps to look at historical trends.Sometimes pairs of socks diverge and thenconverge in cycles that may last weeks or months.Some traders may decide to trade in shorter time horizons such as in days or even hours.For these shorter time periods traders may look forsmaller movements to determine what they consider diverging, or converging trends.There are benefits to trading a pair rather than a single stock.Trading a pair reduces our exposure to the overall market.For example, with a single stock,we may expect a stock's recent price decline to be temporary.However, if a negative jobs report causes most stocks to declinethen this overall market movement would carry this single stock downward with it.With partnering, we reduce some of the impact of large market movements.So, even if both stocks in the pair are pulled down by a general market downturn,what matters to us is the relative difference between the pair of stocks.To describe this relative difference between a pair ofstocks we will look into the spread and the hedge ratio.While the pair of stocks are still converged or linked,we can imagine holding a long position in stock A and a short position in stock B.Our positions are in a certain proportion so thatour combined position has a constant value over time.In other words, movements in our long position and short position cancel each other out.The proportion that lets us create this neutral position is called the hedge ratio.There are two ways to calculate the hedge ratio and they tend to have similar results.One way is to take the price of stock B divided by the price of stock A,or the price ratio.A second way is to run a linear regression in which stock A isthe independent variable and stock B is the dependent variable.The coefficient of this regression is our hedge ratio and is similar to the price ratio.The difference is that the regression accounts for a series ofprevious prices while the price ratio uses just the most recent price of each stock.Once we have a hedge ratio we can calculate the spread.The spread is the price of B minus the product of the hedge ratio and price of A.This may look familiar from our studies of regression.When we used a regression model to estimate the dependent variable we comparedthe prediction with the actual value and called it a residual or error term.Here, we can think of using the hedge ratio toestimate the price of B using the price of A.We still expect a difference between the estimation and the actual price of B.This difference between the estimation and actual value is called the spread.

### 04. M2L6 07 Finding Pairs To Trade V4-6hQtoElcnGM.en

To find good pairs to trade,we want to find pairs of companies that have economic links.It's common to find stocks that are in the same sector and country.It's also good to look for a less obvious but still meaningful connections.For instance, companies that trade internationallymaybe affected by trade decisions by their respective governments.For example, steel companies in the United States and steel companies inChina maybe influenced by political decisions made in either or both countries.Two companies may be suppliers and customers within the same supply chain.For instance, a textile supplier sells to a clothing retailer.A decline in clothing sales may impact the retailerfirst and then impact the textile supplier a bit later.Two similar companies maybe trading in separate countries and time zones.For example, a pharmaceutical company in India may seeits stock rise due to strong international demand for its medicines.A few hours later,a similar pharmaceutical company in Germany may also see its stock rise.Notice in the last two examples,we see a lag between when one stock moves and the other moves in the same direction.A time lag is a good scenario to look for in pairs trading.If we see stock A move upwards and expect Stock B to move upwards later as well,then we can place a trade after Stock A moves,but before stock B moves.After Stock B moves as expected,we can close our positions.Once we found a pair of stocks based on economic links,we can check if those stocks are in fact good candidates for pairs trading.We check the pair by calculating the spread.If the spread appears constant over time, it's likely stationary.Recall that a series is stationary when its mean,variance and co-variance are stable over time.

### 06. M2L6 09 Cointegration V6-N4ZI5SyFMOc.en

Let's add more detail to describe the relationship between a pair of stocks.Recall that some time series such as stock prices are integrated of order one.By this, I mean that if we take the time difference of this time series,we can get a stationary series.When a series is stationary,it is also integrated of order zero.Another way to get a stationery process is tofind some linear combination of a pair of stocks.For example, if stock X and stock Y have an economic link,then if we subtract one from the other,we may get a time series that is stationary.More generally, we multiply X by hedge ratio using regression.Then we take Y minus the estimate of Y based on the hedge ratio and the value of X.This difference is the spread.If the spread is stationary,then it is integrated of order zero.Also if the spread is stationary,then we say that the original series of X and Y are cointegrated.The hedge ratio is also called the coefficient of cointegration.Note that cointegration is not the same as correlation.Correlation is the covariance re-scaled to range from negative one to positive one.A strong positive correlation means that when stock A moves up,stock B moves up at the same time and vice versa.However, if the magnitude of movement in one stock is greater than the other,then they can still drift apart even though they are correlated.In other words, two correlated stocks may not be cointegrated.Cointegration means that, over a range of days,the relative increase in A is matched by a relative increase in B.Let's imagine that we use $100 to buy stock A and $100 to buy stock B.Assuming these are cointegrated,the value of both positions would remain similar over time.So keep in mind that a pair of stocks may be correlated but not cointegrated.Similarly, a pair of stocks may be cointegrated but not correlated.For pairs trading, we want to find pairs of stocks that are cointegrated.The method for checking if two series are cointegrated is called the Engle-Granger Test.The Engle-Granger Test involves two steps that we saw previously.One, find the hedge ratio using regression.Two, calculate the spread and test if the spread is stationary.If the spread is stationary,then the two series are cointegrated.To check if the spread is stationary,we use the Augmented Dickey-FullerTest.The Augmented Dickey-Fuller Test outputs a P-value.If the P-value is small, say,0.05 or less, then we can assume that the spread is stationary.Therefore, we can assume that the two are cointegrated.

### 08. M2L6 11 Clustering Stocks V3-LkgCK_qPqWE.en

Let's see how we would find cointegrated pairs from a universe of stocks.For example, we might look at companies listed in the S&amp;P 500.If we were to compare every pair of stocks this would take a long time.More importantly, we would end up with spurious false positives just by chance.It's important to partition our universe of stocks into meaningful groups.One way is to group stocks by sector or industry.However, since grouping by sector is quite popular,we want to find other relationships that are not as obvious but still meaningful.One way to find meaningful but less obvious groups is to usea class of unsupervised machine learning algorithms called clustering.The inputs to the clustering algorithm are the time series themselves.More similar time series get grouped together.Note that after clustering we still want to analyzethe fundamental relationships between pairs of stocks that exhibit co integration.

### 09. M2L6 13 Trade Pairs Of Stocks V6-i1yVMrgjtB0.en

It's also important to decide how wedefine when two stocks are diverging from there spread.For this, we want to choose a threshold.If the spread exceeds the threshold,then we can assume that the spread istemporary and will revert back to its historical mean.We can have two kinds of thresholds.One threshold, tells us that the spread is much wider than its historical average.Another threshold, can tell us that the spread ismuch narrower than its historical average.When the spread is unusually wide,the action we take is called shorting the spread.We say we're shorting the spread,because we expect the spread to get smaller in the future.Conversely, when the spread is unusually narrow,the action we take is called going long the spread.We say we're going long the spread or buyingthe spread because we expect the spread to get larger in the future.Let's discuss what it means to go long or short spread.When the spread widens,we decide to short the spread.This means, that we short the asset that has increased relative to the spread.We also buy the asset that has declined relative to the spread.Some of our previous examples involved shorting the spread.Conversely, when we see this spread narrow,we decided to go long the spread.This means that we buy the asset that has declined relative to the spread.We also short the asset that has increased relative to the spread.If this concept sounds a bit confusing,just to remember to buy low and sell high.In other words, buy when it's on sale,and sell when it's overpriced.The way we define these thresholds,is by calculating how many standard deviations the spread is from its historical average.This is called the Z-score of spread.A Z-score of positive one,means that a value is one standard deviation greater than its historical mean.To calculate the Z-score,we subtracted the mean and divide by the standard deviation.As with all modeling,is necessary to perform backtesting.By backtesting, I mean that we want to test our model on historical data.We want to test in a way that simulates the model making predictions in real time.We don't want to expose the model to data that occurs in the future.To perform backtesting, we divide our historical data into a training set,a validation set, and a test set.The training set has earlier data up to a cutoff date,the validation set has data that occurs afterthe training set up to and even later cutoff date,the test set contains data that occurs after the validation set.We train our model on the training set,then do intermediate checks using the validation set.This allows us to adjust our model to help it perform more accurate estimates.When we've adjusted our model to the point that we think it's ready to use,we then check it against the test set to see if it really works.Note that, it's important not to use the test set until we're done optimizing the model.If we were to adjust the model after using the final test set,this would be like letting our model cheat by showing it information about the future.

### 14. M2L6 20 Summary V2-wuzha8SU2jw.en

Now that you've reached the end of this lesson,we hope you've gained a better understanding ofpairs trading and mean reversion in general.Remember that mean reversion trading follows these general steps.Find a pair of assets or a pair of portfolios that have some economic link,compute the hedge ratio,use the hedge ratio to compute the spread,test whether the spread is stationary,if the spread is stationary,consider the pair a candidate for mean reversion trading.Next, choose thresholds for the spread.When the spread widens beyond its historical average, short the spread.When the spread narrows beyond its historical average,go along the spread.Most important of all,remember to backtest your model.We hope you've enjoyed learning about mean reversion and pairs trading. Bye for now.

### img

## Part 01-Module 02-Lesson 07_Project 2 Breakout Strategy

### 01. MV 7 Transition To Project 02 1 V1-nkAcx2X_lfs.en

Now that you made your way through the material,you have a solid foundation in researchinga trading strategy which will help you with finding an alpha signal.The next step on your journey is to do Project2 where you'll implement a breakout strategy.You'll take everything you learned from Project 1 to build this strategy,along with finding and filtering Outliers.Transitions are really easy.In transitioning from lessons to projects and back again,sometimes it's difficult for an entity students.That's true. I've gone through multiple Eudacity Nanodegree programs as a student,and I have to admit that starting a project wasone of the loneliest parts of the process.I was a Eudacity student in the data analyst,machine learning and deep learning Nanodegrees.Every time I started a new project, I struggled.I didn't really know if I would ever finish.Yeah.I would spend hours alone in the library just begging my code to magically work,and searching the web and online forums for hints.So, there are definitely moments when I started to ask myself,"What am I doing here?Why am I doing this to myself?"That sounds really tough.How did you handle those moments?Well, whenever I felt like I couldn't bring myself to keep trying,I would watch online guest speakers or interviews bypeople who did something really cool in the field that I was studying.So, it helped to remind me why I was interested in the Nanodegree program to begin with,and helped me to keep going forward.Yeah. That's a really great idea.If you're ever feeling burnt out or tired froma day of debugging code or working on tough problems,take a few seconds to remind yourself ofwhat initially brought you to this Nanodegree program.What do you want to do? Who do you hope to become?Watch some YouTube videos about this innovative field in which you're growing,maybe throw in a few cat and puppy videos along the way,and when you feel that urge to move forward with your learning,dive back into your program.Who knows where you'll go with your new skills and knowledge?

### 04. MV 10 Transition From Project 02 Int V1-DYjOsL3VYfY.en

Starting open-ended projects like some of the projects inthis nanodegree program or even like the project oflearning a whole new subject can feel bewildering.For example, starting the project of earning a PhD will certainly bewildering to me.It can be hard to proceed when you don't know what partsof the information you're trying to learn will be important in the future.Frankly, there's an infinite amount of information inthe world and any part of it could lead to interesting places.That's one of the reasons why we try in this nanodegree program,to give you context for why you're learning things,how they're relevant to your goals and how they'll be useful in the future.Also, with unstructured problems,you may not really know when you have found the answer and are done.When I worked on unstructured problems during my PhD,it felt like trying to climb a set of stairs in the dark,feeling around with my foot until I found a stair to step onto.What was helpful to me in this type of situation was to let goals be my guide.So, if I was trying to build something,I would know I was done when it was built and it worked.If I was trying to analyze some data,it was important to decide what question I was trying to answer,and so I would know I was done when I had an answer to that question.Sometimes, the goals are obvious,like finish a course,but sometimes you have to decide what the goals are,like when you have to decide what questions you're trying to answer.We've provided you with some goals in your projects inthe form of rubric criteria to help you direct your efforts,but bigger picture, deciding what you are trying toachieve can give you a way to measure your progress.So, now that you've got some substantial progress underyour belt in this big project that is your nanodegree program,and growth journey that that's a part of,how are you feeling about the goals you've set out to achieve?Recognize the headway that you've made so far,in terms of what you've learned,reasoned through and created,you're making great progress.

## Part 01-Module 03-Lesson 01_Stocks, Indices, Funds

### 01. MV 11 Intro To Module 03 Difficulties In Learning V1-kqjFkUVZwEc.en

Hi there. These next few lessons we'll give you a broad overview of mutual funds,hedge funds, and exchange traded funds.The second half will be a deep dive into portfolio optimization,which is really exciting because it will help you build themathematical skill sets that you'll want to have as a quant.Now, I have to admit that math tends to scare me sometimes.Well actually, okay, math always scares me a lot.But math is also very cool.When you learn the math,its like gaining superpowers that let you extract meaning from the chaos of numbers.With that said, we'd still like to acknowledge thatdiving into the math can still feel scary or overwhelming,especially when you feel like it's just you alone in a library or a coffee shop.I've been through several Eudacity Nano degrees as a student,and I also felt like sometimes it was just me and my computer in a library.I'd like to share a bit of advice for getting throughthe more challenging material in an online learning environment.What really helped me get through some ofthe more challenging subjects was reaching out online to interact with my classmates.I'd either ask questions when I was stuck,or tried to answer other student questions.I would also post curriculum suggestions,and sometimes I'd get a response from someone at Eudacity.Whether your online chats directly answered the question you're trying to solve,they help you by building a community that supports you,and also motivates you.It's one of the important side benefits that you'vealready experienced when you take in-person classes,because you're surrounded by other people with shared goals and shared experiences,because they have empathy for you and your struggles.In the Eudacity classroom,your classmates and mentors are there with you too just connected by modern technology.Also, we want to remind you that there's a whole team of people at Eudacity,many of whom may not necessarily be on camera,that are working to support you on your journey.We encourage you to regularly connect with your fellow classmates so thatyou can support each other on your journey through the program.

### 02. L1 00 Intro V2-JA4WBd6sHF4.en

Welcome to this module,and congratulations on getting through the previous modules and projects.In this lesson, we'll learn about Stocks, Indices and Funds.We'll cover how Indices are defined and categorized.We'll also learn about market capitalization and price-weighted indices,and walk through an example of constructing an index.Then, we'll learn about funds,such as hedge funds and mutual funds.We'll cover active versus passive fund management as well as smart beta.We'll also learn about open-end and close-end mutual fundswhich will prepare us for the next lesson about exchange traded funds.In later lessons of this module we'll calculateportfolio risk and return and study optimal risky portfolios.Finally, we'll cover portfolio optimization which will equip you with the skills toconstruct portfolios with a goal of maximizing returns and minimizing risk.

### 02. L1 01 Stocks V2-XHo5iyxDxOQ.en

Let's first define the terms Equity, Stocks and Shares.What does it mean when an investor buys stocks.When you get a new job and the company pays you in shares what does that mean?Equity, is the value of a company's assets minus it's liabilities,is the net value of the company after counting for everything in the company owes.Such as debt, payments to suppliers, or employee wages.A company's equity, is divided into stock certificates that investors can buy.These can be called shares of the company,or stock in the company.Stock and shares pretty much refer to the same concept.When you say you own stock,people know you're referring to shares in one or more companies.When you say you own shares,you normally specify shares, any specific company.So, when an investors purchase stocks,or if employees receive shares from their employers,they're receiving partial ownership of a company's equity.Now, let's say you're trying to follow how well the overall markets are doing.So, you look at a bunch of stocks and you may seethat Google was down yesterday but up today,while IBM was up yesterday and then down today,and Amazon was up both yesterday and today.How do you consolidate all this informationinto a single number in a way that's meaningful?As you'll see next, we can use an index to help us track not only these companies,but also hundreds and even thousands of companies using a single number.

### 03. L1 02 Indices V2-BRv5B78YBGs.en

What is an index?If you're listening to financial news about the U.S. markets,you may hear about the S&amp;P or the Dow.For news about Latin America,there is IBOVESPA or MERVAL.For markets in Asia,there's the NIKKEI or HANG SENG,and in Europe, there's the FTSE or EURO STOXX.These are all examples of indices.Indices are created by financial research,and credit rating companies such as Standard and Poor's,Dow Jones and Financial Times.They're also published bystock exchanges such as the Nasdaq or the London Stock Exchange.Indices can also be created by banks such as Hang Seng bank.Business news will often refer to whether the index went up or down.For instance, we may hear that the Dow went up by 200 points since yesterday.If the Dow was at 25,000 yesterday,that means it is now 25,200 points today.The 200-point increase, a bounce to an increase of about 0.8 percent.An index is a metric that representsthe aggregated value of a group of stocks as a single number.Note that you can refer to more than one index as indexes or indices.

### 03. L1 03 Indices Are Virtual Portfolios V2-oAd_szbBNWc.en

An important point to keep in mind is that an index is a virtual portfolio,and not an actual fund that people invest in.We can think of an index has a fantasy football team in whichsports fans pick their favorite players and put them in a virtual team.We can track the performance of this artificially constructed team by combiningthe performance metrics such as number of points scored for each player.But keep in mind that the team only exists in paper not in real life.There's no team owner,no coach and the members in the virtual team don't actually practice together.Similarly, an index represents the aggregated performance of a group of stocks.But they're not funds they're not managed by professional money managers,and investors don't pay index creators to invest their money in the indices.

### 03. L1 04 Indices Describe The Market V2-jNzwxE3el7I.en

Indices may track stocks within a specific stock exchange or country.For example the CSI 300 includesstocks that are listed on the Shanghai and Shenzhen Stock Exchanges in China.Stocks can also be related by sector for example the Nasdaq 100 technologyindex or Nasdaq financial 100index track the performance of stocks in their respective sectors.Indices are important to investors because ata glance we can quickly check the status of various markets.In a way it's checking the pulse of a region orsector to evaluate the health of that market.Professional investment managers also use indices as benchmarksthat they either tried to match or outperform with their own portfolios.

### 04. L1 05 Market Cap V2-PE0UgUc0f0U.en

So far we've seen some major indices that track a large collection of stocks,from big to small with different sectors such as technology or banking.What if we wanted a more refined look atjust the smaller companies or what if we wanted to track just high-growth companies.Having a more specific index would help ifour investment research is focused on companies with similar characteristics.Index creators often make indices that group stocks bymarket capitalization defined as large-cap, mid-cap or small-cap.Market capitalization or market cap of a company isits number of shares outstanding times the price per share.In other words; market cap is the market value of the company.

### 05. L1 06 Growth Vs Value V2-ZCjre5YTD0s.en

Indices can also include stocks based onwhether companies are considered growth or value stocks.So, what's a growth stock,and what's a value stock?A growth stock tends to have high growth insales or earnings and potential for future growth.For example, if a company creates new ways for people to makepurchases with bitcoin or new ways for people to commute to work,it might be considered a growth stock becauseof its potential for increased future sales.A value stock tends to be a mature company that has stable sales, revenue, and earnings.For example, a company that sellshousehold and bathroom products may have a stable flow of earnings,but has fewer expectations for rapid growth.

### 06. L1 07 Ratios V2-Dfbwep-tkok.en

So, how do we define whether a company is growth or value?We can look at valuation metrics such as price to earnings ratio,price to sales ratio,and price to book ratio.Then, within a list of companies in an index,we can rank these stocks by these metrics and say that a stockis more on one end of the spectrum relative to the other stocks.The price to earnings ratio,is the stock price divided bythe company's earnings per share over the past four quarters.The price to sales ratio,is the stock price divided by the sales per share over the past four quarters.The price to book ratio,is the stock price divided by the book value per share.The book value is the company's accounting value which is assets minus liabilities.The book value differs from the market value,which we've been referring to as the market capitalization,which varies as the stock price changes.Growth stocks tend to have high price to earnings,price to sales and price to book ratios.Conversely, value stocks tend to have lower price to Earnings,price to sales, and price to book ratios.Note that these ratios tend to vary by sector and industry.Generally speaking, software and biotechnology stocks have higher PE ratios,while agriculture and construction companies tend to have lower PE ratios.

### 07. L1 08 SP Index Categories V2-D3VGIvti71g.en

Indices may use both the market cap and the growth versus the value classification,to decide which stocks to include.So as an example,the S&amp;P devised a list of stocks into the S&amp;P 500,S&amp;P MidCap 400 and the S&amp;P SmallCap 600.The S&amp;P 500 contains large-cap stocks such as Lockheed Martin, an aerospace company.The S&amp;P MidCap 400 contains stocks such as Delfi and automotive technology company.The S&amp;P SmallCap 600 contains a small-cap stocks such as The New York Times.Within each of these indices,the stocks are then ranked as more growth or more value according to price-to-earnings,price-to-sales, for price-to-book value ratios.These rankings are used to create more subgroups defined as either growth or value.For example, from the S&amp;P SmallCap 600 list,stocks are selected forthe S&amp;P SmallCap 600 growth index or the S&amp;P SmallCap 600 Value Index.

### 08. L1 09 Price Weighting V2-2SFbwJ19NhA.en

So, let's say an Index like the Nikkei 225,tracks the price movements of 225 stocks.To get a single Index number,we could add up the single share price of all of the 225 stocks,and there's your index number.That's more or less what the Nikkei 225 Index does,and this is called Price Weighting, or Equal Weighting.The Dow Jones Index is also a Price-Weighted Index.Note, that Index creators usually rescale the number,but we'll see that later in the lesson.

### 09. L1 10 Market Cap Weighting V2-7qVVA5yLFnY.en

We can also take a weighted average ofthe stocks in a way that does not treat all the stocks the same.A sensible choice for weights is to givemore weight to stocks of bigger companies rather thansmaller companies because a larger company's price movinghas a greater effect on the overall change in the stock market.This is called market cap weighting.Remember, that market capitalization of a companyis the number of shares times the market price per share.So, for example, the Wilshire 5000 Index ismarket cap weighted and contains both Nvidia and iRobot.Nvidia has a market cap of about 160 billion.While, iRobot has a market cap of about 1.8 billion.This means that a one percent change in the price of Nvidia will affectthe Wilshire 5000 Index more than a one percent change in the price of iRobot.Major indices that are market-capitalization weighted are the S&amp;P 500,Ibovespa, MERVAL, Hang Seng,FTSE and EURO STOXX.

### 10. L1 11 Adding Or Removing From An Index V2-_bWIZWa20j8.en

You may wonder what happens to an index if one of its listed companies goes bankrupt,is acquired, or merges with another company.For example, in June of 2018, Monsanto,a US based agriculture company was acquired by Bayer,a pharmaceutical company based in Germany.Monsanto was listed in the S&amp;P 500,but due to the acquisition,Standard &amp; Poor's removed Monsanto from the S&amp;P 500,which is called an Index delete.Standard &amp; Poor's also added Twitter to the S&amp;P 500,which is called an Index add.When the list of companies in an index changes,the index needs to be rebalanced.In other words, the weights applies to each stock must berecalculated based on the new total market cap of all stocks in the index.Now, we'll cover how an index is constructed and calculated next.Industry balancing can be scheduled at predetermined intervals,such as every month,every quarter, or every year.An index may also be rebalanced on the day of a major event such as during a merger,acquisition, privatization, or bankruptcy.

### 11. L1 12 How An Index Is Constructed V2-dsbi4dvdU9c.en

Let's see how an index is constructed andupdated to see where the index number comes from.The Hang Seng Index was created by a banker,Stan Lee Kuan at Hang Seng Bank in 1969,and tracks the 50 largest companies that are listed on the Hong Kong Stock Exchange.The index selected the 50 largest companies that were traded onthe Hong Kong Stock Exchange and added up their market capitalization.Then, they set that sum equal to 100 points for the index.So the index began in 1969 with the number 100.Note that we will refer to the sum of market caps ofall these 50 companies as the total market cap.Each trading day, the percent change ofthe total market cap is multiplied to the previous day's index,to calculate the index for that day.For example, let's pretend the total market cap onthe very first day was 100 billion Hong Kong dollars.The following day, let's say the total market cap changed to 102 billion.Recall that we can get the percent change bytaking the current day divided by the previous day.So in this case,102 billion divided by 100 billion is 1.02 or a 2% increase.If we multiply the first day's index of 100 points by 1.02,the second day's index value is now 102 points.

### 12. L1 13 Hang Seng Index Construction V2-rdGdC-meRLU.en

Note that the Hang Seng Index is a capped free float adjusted market cap weighted index.Okay, that was a lot of words.Let's start with free float adjusted market cap,in particular the free float part.When we measure the market cap of a company,we take all outstanding shares held by investors and multiply by the price per share.However, some of these shares are not readily available in the marketbecause some stockholders are legallyprevented from selling their shares during a lock-up period.For example, employees who are paid intheir company's shares may not be allowed to sell their sharesa few days before the company's quarterly earnings reportor for a year after an initial public offering.By using only their readily tradable or free float shares to calculate market cap,we can get a better measure of how the market is moving,since the non-free float shares can't be bought or sold at that time.So in practice, an index will estimate what fraction of shares arefree float and use that in this calculation of free float shares from day to day.For example, let's say about 90 percent of $0.10 outstanding shares are free float.If today, we see that $0.10 has 10 billion shares outstanding,we'll estimate that nine billion of those are free float.

### 13. L1 15 Calculating Index After Add Or Delete V2-hiAHRE6JY0k.en

Going back to the issue of index deletes and index adds,when Monsanto was removed from the SMP 500 and replaced by Twitter,what happened to the index?Was it reset back to 100?Is there's some fancy math to update the index?Well in fact, we use the same math as we did before.Prior to the replacement Monsanto was worth about $50 billion.On the day of replacement Twitter was worth about $30 billion.Let's make up a fake company to serve as a placeholder for Monsanto before,and Twitter after the change.Let's call this fake company, MonsanTwitter.Between the previous day and the current day,our placeholder MonsanTwitter changed its market cap from $50 billion to $30 billion.We calculate the percent change of the total market cap asusual and apply that to the previous index to get the current day's index.

### 14. L1 16 Funds V2-s9f2Bzc9lnk.en

Now, let's define an investment fund.An investment fund is a collection of investor money thata professional money manager allocates toward a portfolio of assets.So, what's the benefit of investing in a portfolio of stocks?Why not just pick your favorite 10 stocks and watch them grow?Well, portfolio's benefit from diversification enable investors to get the same level ofreturn with lower risk or similarly more return for the same level of risk.In effect, a portfolio let's you optimize your Sharpe ratio,which is the measure of how much you're rewarded for buyingrisky assets divided by the risk of that portfolio.You'll learn more about the Sharpe ratio in a later lesson, but for now,just remember that a smart combination of assets together can giveyou better returns or less risk than a single stock can achieve.You may notice that there are many mutual funds to choose from.Each of these target a certain subset of assets,have specific investment strategies and also aim for a certain level of risk.Investors can choose different funds based on their level of risk tolerance,the amount of fees charged by the fund and also based on the fund's past performance.

### 15. L1 17 Active Vs Passive V2-QzoHmUzJ5zw.en

Let's learn about active fund management versus passive fund management.When the goal of a fund is to outperform an index,it's called an actively managed fund.For example the Fidelity Contrafund targetslarge cap growth stocks and seeks to outperform the S and P 500 index.When the goal is to match the performance of an index,it's called a passively managed fund.A passive fund usually chooses an index as a benchmark that it tries to track.So they're often called index funds.The fund managers do this by buying the same stocks that arelisted by the index and in the same proportion as the index.Note that an index fund is not an index.It's a fund that attempts to match the performance of an index.For example: The Vanguard 500 index Investor Fund is a fund thatallocates investor money towards a portfolio that tries to match the S and P 500 index.

### 15. L1 18 Alpha And Beta V3-CcVdfrr5nD8.en

Note that Active Funds,are sometimes called Alpha Funds.While Passive Funds are sometimes called Beta Funds.So, where do the names alpha and beta come from?Well, alpha and beta refer to the intercept and coefficient of a regression line.In particular, this is the regression ofa portfolio's return against the markets return in excess of the risk-free rate.In general, a portfolio's excess return,is the portfolio's return minus the risk-free rate.A beta coefficient of one means that a two percent increase in the market,will also result in a two percent increase in the portfolio.An intercept or alpha of one means thatthe portfolio has a one percent return that exists,even if the market's excess return is zero.You'll get more details aboutthis particular regression line in a later lesson about portfolio optimization.But the goal of an active fund,is to generate excess returns above the market's excess return.Hence, is called an Alpha Fund.The goal of a passive fund is toclosely match the returns of the market that is tracking.So, it's called a Beta Fund.

### 17. L1 19 Smart Beta V2-Rc9NEmNMzk8.en

So, here's the question,can a fund only be active versus passive,only alpha or beta?Well, if you're at a restaurant,do you have to choose between soup or salad?Does your beverage have to be either milk or tea?What if you wanted a little bit of both?In fact, you can order a half soup and a half salad combo.You can also order a milk tea.Similarly, a portfolio can combine both active and passive management.This is often referred to as Smart Beta.Generally speaking, the idea is to start with the weights based onan existing index and use that asa starting point from which we can try to make improvements.So, for instance, if we can nudgethe weights a little up or down from their original values,then we can aim to either improve returns or reduce risk.We call these Smart Beta portfolios because they'rebeta portfolios that have some alpha sprinkled on top.

### 18. L1 20 Mutual Funds V2-LgaylDkS92Y.en

Now, that we've had an overview of funds.Let's discuss two general types of funds: mutual funds and hedge funds.Mutual funds are available toeveryday investors and have restrictions on certain strategies,such as shorting stocks or using derivatives.So, they're usually long only,which means fund managers buy and holdstocks that they expect will perform well in the future.Investors are allowed to put money into or pullmoney out of most mutual funds on any business day.In other words, there's no lock-up periods that prevent investors from withdrawing.Some popular mutual funds include the Vanguard Equity Income Fund,the T. Rowe Price Blue Chip Growth Fund,and the BlackRock Technology Opportunities Fund.

### 19. L1 21 Hedge Funds V4-AgGPqvDFTHw.en

Then, there are hedge funds.Hedge funds have fewer restrictions on their trading strategies,which allow them to take short positions and alsouse derivatives such as options or futures.Hedge funds typically take money fromhigh-net-worth individuals or institutions such as pension funds.Hedge funds generally require a higher minimum investment,and may require lockup periods duringwhich investors are not allowed to withdraw their investments.So, here's a question for you to ponder.Why are hedge funds called hedge funds?Is it A, hedge funds are named after hedgehogs,who play in the hedges of a rose garden or B,hedge funds are named after hedging strategies designed to limit portfolio risk?You'll find out the correct answer by the end of this lesson so stay tuned.

### 20. L1 22 Relative Returns V2-m4MvYRlyPoU.en

There are two ways that funds are evaluated on their performance,relative versus absolute returns.Relative returns refer to how the fund compares to a benchmark,which is usually an index.Mutual funds, both active and passive,are evaluated relative to their benchmark.Let's say for example,that an actively managed fund chooses to focus on a portfolio ofstocks that are also listed in the S&amp;P 500 index.If the S&amp;P 500 has an annual return of two percent and the fund returns three percent,then the fund outperformed its benchmark by one percent,and we say that the relative return was one percent.Since this is an actively managed fund,we also refer to this relative return as the active return.For a passively managed fund,the performance is based on how closely the fund attracts index.For example, if the S&amp;P 500 had an annual return of two percent,and so did the passively managed fund,then the fund is meeting its investment objective.The relative return is zero,and since this is a passively managed fund,we say that the fund has a zero percent tracking error.

### 20. L1 23 Absolute Returns V3-wbb6WSyXLdU.en

Hedge funds are usually evaluated by absolute returns.Absolute return funds use the cash interest rate suchas the US three-month Treasury rate or LIBOR as the benchmark.Since the cash rate is relatively small,it doesn't fluctuate much compared to the equity markets.Absolute return funds are measured by their returns themselves,not the difference between the fund and the benchmark returns.The average return of hedge funds in 2017 was 8.5 percent,and 5.4 percent in 2016,according to the Hedge Fund Research group.

### 21. L1 24 Hedging Strategies V3-8bzw4ZMGpWU.en

Hedge funds employ hedging strategies,in an attempt to deliver a market neutral returns.By market neutral, I mean that the portfolio's gains orlosses are less affected by overall market movements.Hedging generally means entering intoa transaction in order to reduce exposures to price fluctuations.This is done by buying derivatives such as options or futures.For instance, let's say we hold 100 shares of Daimler,the owner of car brands such as Mercedes-Benz.We want to ensure against losses if the stock price of Daimler drops.So, we buy 100 put options on shares in Daimler.As an example, each put option gives us the option tosell one Daimler share at a fixed price of 60 euros.So, one would be a good time to exercise that right to sell.Well, if the market price of Daimler drops below 60 euros such as 50 euros.We could buy Daimler stock at 50 euros per share.Use the put option to sell the same stock at 60 euros.We could keep that 10 euros per share as profit.This could cancel out the 10-euro per share lossthat we would incur on our existing 100 shares.Hedging helps money managers devise investment strategies that are market neutral,which means that gains or losses are less sensitive to movements in the overall market.

### 22. L1 25 Net Asset Value V2-hBnY2DmEFo4.en

When an investor puts money into a fund,they receive shares in the fund which increase ordecrease in value as the funds portfolio value goes up or down.The share represents a fraction of the funds fair value.To get the fair value of a fund's share,we start with the value of its investments,also called its Assets Under Management,or AUM for short.We take the AUM minus expenses,these expenses are the costs of running the fund such as employee salaries,transaction costs, and taxes.We then take the net value of the fund and divide by the number of shares in the fund.The fair value per share that we get is called Net Asset Value or NAV.This is often just pronounced NAV.

### 23. L1 26 Expense Ratios V2-SHZ0AhJq134.en

Usually, the expenses associated withthe fund are defined as a fraction of assets under management,which is called the gross expense ratio.Sometimes, when funds are just starting out and trying to attract investors,they give discounts to their new customers.So, these customers actually pay less than the gross expense ratio.This is called the net expense ratio.Note however, that discounts are usually not permanent.So, the gross expense ratio isa better indicator of what investors will pay in the long run.

### 24. L1 27 OpenEnd Mutual Funds V2-T4_mmjEKUAo.en

Mutual funds receive investor money,which they invest in a portfolio of stocks or other assets.In exchange, investors receive shares in the fund.For example, when a fund starts,investors may pay $100 for each share of the fund that they receive.There are two types of mutual funds;open-end funds and closed-end funds.Open-end mutual funds, allow investors to buy into the fund,after the fund has already started operating and investing.Open-end funds, also let existing investors withdraw their money from the fund directly.When people want to invest,the open-end fund creates new shares for the investors to buy.When investors wish to withdraw their money,they turn in their shares and the fund returnsthe cash value of those shares back to the investors.The fund also removes these shares from his total shares outstanding.Let's walk through an example.Let's pretend a new open-end fund initially has10 investors and each investor buys one share of the fund at $100 per share.So, the fund takes this $1,000 to invest in a portfolio of stocks.We say it has $1,000 in assets under management or AUM.A month later, let's say the portfolio's value has increased to $1,200.So, each of the ten shares is worth $120.A new investor wants to buy a share of the fund.So, they pay $120 for a share and not 100.In other words, the new investor pays for the current price of the share.The fund has now issued 11 shares and has $1,200 plus 120 or $1,320 in his portfolio.

### 25. L1 28 Open End Mutual Funds Handling Withdrawals V2-46NGAQHY-Mc.en

Note that open-end mutual funds must keep part of their investments in cash,in case investors wish to redeem their shares.For example, let's say an open-end fund has issued10 shares and the shares are worth $100 each,one investor wishes to return their shares for cash.The investor gets their $100 in cash and the fund has nine shares outstanding,with investments valued at $900.In anticipation of potential withdrawals,the fund has kept some of its $1,000 in cash and not invested in stocks.

### 25. L1 29 Open End Funds Holding Cash For Withdrawals V3-RU8-ZRBJ2Cw.en

Holding cash means that the portfolio cannot makeuse of all of its assets in the given investment strategy.Also, if many investors wish to redeem their investments at the same time,the fund may need to sell their existing investments to get cash and increase liquidity.These issues may reduce the overall potential return of the fund.To see this, imagine you invested a million dollars into an open-end mutual fund.The fund allocates half of this investments into a portfolio of equities,earning a 10 percent return.The fund keeps the other half of its investments in cash, earning two percent.What do you think will be the total rate of return on your million-dollar investment?Well, first of all,congratulations on having a million dollars to invest.Second of all, you'll probably guess that your rate of return will be less thanthe 10 percent return on the portfolio ofequities because only half of your money is allocated to it.The return on the fund is a weighted average of the active investments and the cash.In other words, a half of 10 percent is 5.A half of 2 percent is 1.5 plus 1 is 6 percent.So, the total return of the fund is 6 percent,which is less than the 10 percent return of the equities.

### 26. L1 30 ClosedEnd Mutual Funds V3-y2VhtrF6vdc.en

Because the need to maintain cash tends to erode the total return of the open-end fund,the financial services industry created closed-end funds to address this issue.Closed-end mutual funds are closed ended because they accept investor money whenthe fund is initially created and don't createnew shares or handled withdrawals afterwards.By making the fund closed ended,the portfolio managers no longer have to keep cash on hand and nolonger have to liquidate existing positions to satisfy redemption requests.This also means that money managers canuse all of the cash towards their investment strategies,potentially improving the overall return of the fund.So, what do investors do if they wish tostop investing in the fund and get their cash back?Investors can sell their shares to other investors.For investors, shares of closed-end funds can bebought and sold on the stock exchange just like a stock.This means that investors do not need to waitfor the mutual fund to process their share redemption.However, the price of closed-end funds is determined bythe supply and demand of that funds shares on an exchange,and often will not precisely reflect the fair market value of its underlying portfolio.In other words, closed-end funds often trade ata discount or premium to the net asset value other underlying holdings.In a later lesson,we'll see how exchange traded funds are designed toimprove upon both open-end and closed-end funds.

### 27. L1 31 Transaction Costs V2-JGYAv7tQpyY.en

An important consideration for any investor ora portfolio manager is the cost of buying and selling stocks.These are called transaction costs.Transaction costs can be commissions paid to brokers or bargain-making investment banks.Transaction costs can also be the costs ofmoving the market price by trading a large block of shares.Large institutional investors are primarilyconcerned with moving the market price by making large trades.For example, if a fund wants to buy a stock that it considers underpriced,a large purchase of many shares will push up the market price ofthat stock making the original purchase decision less attractive.Usually, traders will break up the order into smaller pieces and make the purchases withdifferent sellers on different exchanges inorder to prevent these sellers from raising their prices.Portfolio managers need to consider howdifferent strategies affect their overall transaction costs.For instance, if a strategy requires buying or selling based on daily price movements,this will incur more transaction costs compared toanother strategy that triggers trading based on quarterly financial reports.When a portfolio manager decides to adjust their investment allocation,this adjustment is called portfolio rebalancing.When rebalancing a portfolio,a portfolio manager may decided that due to transaction costs it makes more senseto skip trades for which the transaction costs outweigh the benefits of rebalancing.Moreover, for large institutional investors that manage multiple funds,they can trade internally between funds that are within the institution,thereby reducing the cost of rebalancing.

### 28. L1 32 Summary V1-Pt2sVftdwS0.en

In this lesson, we learned about indices,mutual funds, and hedge funds.We learned how indices may either be market cap-weighted or price-weighted.We also saw an example of how to constructan index and update its value from one day to the next.We learned about mutual funds and hedge funds.We discussed the concept of actively managed funds versus passively managed funds.We also introduced the idea of Alpha,Beta, and Spark Beta.Moreover, we defined a fund's relative returns asthe fund's performance relative to its benchmark,which is usually an index.We also learned about open-end mutual funds and closed-end mutual funds.We saw how open-end funds need to handlecustomer withdrawals and therefore hold cash reserves.We also saw how closed-end funds are designed to address this issue.We also pointed out that the market price of a closed-end fundmay trade at a premium or a discount to its fair value.In the next lesson,you will build upon these concepts and learn howexchange-traded funds are designed to improve upon mutual funds.See you soon.

## Part 01-Module 03-Lesson 02_ETFs

### 01. L2 01 Intro V2-utlPzT8MEsM.en

In this lesson, we'll introduce Exchange Traded Funds or ETFs,and get an understanding of how ETFs work.ETFs are considered a significant innovation in finance.One might say that ETFs did for the financial services industry,what smartphones did for the tech industry.As the introduction of ETFs led toevermore products that customers just couldn't get enough of.The world's first ETF was created by the Toronto Stock Exchange in Canada in 1990.The market value of ETFs in the US market grew from $100 billion inthe early 2000s to $3.4 trillion by 2017.Globally there are about $4.5 trillion invested in ETFs.Broadly speaking, as ETFs have gained popularity due to their low fees and accessibility,they've been taking market share from traditional mutual funds.ETFs have lower fees because they have lower operational costs,and their transactions are structured in a way that can avoidsome of the taxes that traditional mutual funds normally pay.The lower operational costs are usually due to passive investment management,which requires less frequent trading and therefore lower transaction costs.We will point out other cost savings as we cover more details about how ETFs works.

### 02. L2 12 Shortcomings Of Mutual Funds V2-oEqsaex31Qg.en

Recall that there are some shortcomings in Open-End and Closed-End Mutual funds.Open-End Mutual Funds may need to maintain parts of their assets undermanagement in cash to let investors withdraw on any given day.This dilutes the funds overall performance.Also, Open-End Mutual Funds may limit the number oftimes that you can invest or withdraw within a certain time period,such as 90 days.Moreover, the price you get for withdrawing isbased on the closing price at the end of that day,not the best price at the time that you initiate the transaction.Closed-End Mutual Funds addressed thisby making their shares tradeable in the stock market.But the market value of these shares may diverge from the value of the funds portfolio.Is it possible to do better?Can we set up a fund that does not need to hold a reserve of cash andwhose market value matches is portfolio? Yes we can.We can do better than mutual funds with ETFs.ETFs like mutual funds,allow us to invest in a portfolio of stocks.Thereby benefiting from diversification.Even better, ETFs shares can be traded on a stock exchange as if their stocks themselves.Plus, the market price of an ETF closely follows the value of the underlying portfolio.We'll see why later in this lesson.

### 03. L2 02 Commodities V2-gc_GMqbCC2Q.en

There are some common ways that investors use ETFs,and these include investments in commodities or international stocks,and also using ETFs for hedging.Investing in commodities is easier with ETFs.Commodities are raw materials that are interchangeable and tradable.By this, I mean that a commodity such as a barrelof oil can be exchanged for another barrel of oil,whereas a handmade tailored wedding dresscannot be swapped out for another wedding dress.Commodities include energy, such as oil and natural gas; precious metals,such as gold and silver;and agriculture, such as corn and even cows.If you wanted to bet that the price of gold will increase,you could buy gold from a jewelry store and hide it under your bed,then if the price of gold increased as you expected,you could sell it for a profit.

### 03. L2 03 Commodity Futures V3-qvSubjxMGJ0.en

For most investors, it's easier to buy Futures contracts that are tied to commodities.Buying a futures contract is an agreement between two parties to buy orsell an asset at a future date and at a fixed price.If you trade futures directly,remember to mark your calendar,because you'll need to either close the positionor roll it over by the time the contract is due.Let me explain what it means to close the position in a futures contract.Let's say you enter into a futures contract with a farmer to buy one metric ton of cocoa,at a fixed price,at a fixed date six months from now.Since you're buying you are long cocoa futures.The farmer who is selling is short cocoa futures.At that six month date,you're required to pay cash and the farmer is required to send you one ton of cocoa.If you want to close your long position,then before the due date you should enter into a short position tosell one metric ton of cocoa at a fixed price and on the same due date.There's a commodities exchange that handles the paperwork.But, it's as if you paid for a ton of cocoa,then immediately sold that cocoa to someone else to get your money back.So, that was how to close a futures position.Rolling over contract is like renewing your gym membership for another six months.To roll over a futures contract,you start by canceling the contract when it's near the due date.Next, you enter into a new contract to buy cocoa at an even later date,such as six months after the original contract due date.

### 03. L2 04 Commodity ETFs V2-UpgX6INJ6nU.en

If you don't cancel or roll over your futures contract by the due date,you're obligated to buy the asset.Whether it's a barrel of oil or a flock of sheep.So, imagine that you have bought multiple gold futures contracts,that have different due dates.You'll have to set up a lot of reminders on your calendar,to help you remember when to roll over or cancel each contract Fortunately,there are ETFs that track commodities bybuying and rolling over futures contracts for you.You can buy shares of a gold ETF and hold it however long youwish and save yourself the hassle of managing individual futures contracts.

### 03. L2 05 International ETFs V2-OL2p8S-82mY.en

Investing internationally is easier with ETFs.Normally, if you wanted to trade stocks in another country,you might set up a brokerage account with a firm in that country.Also, you would be trading when markets are open in that country's Stock Exchange,which is tough if the time zones are different from yours.For example, let's say you are in New York and making trading decisions for shares inTencent or China Construction Bank which are listed in the Hong Kong Stock Exchange.Since the time zones are far apart,you would be spending your late evenings until early mornings managing your trades.What if you like having a regular sleep schedule as well as a social life?Well, financial firms create ETFs forinternational stocks and sell these ETF shares in the local US Stock Exchange.For example, BlackRock issues the iShares China Large-Cap ETF,which is called FXI for short.The FXI tracks the 50 largest stocks that are traded in the Hong Kong Stock Exchange,including Tencent and China Construction Bank.So, you can trade at the FXI on the New York Stock Exchange during the day,then at night you can get some well-deserved sleep or party with your friends.

### 04. L2 06 Hedging V3-4k1bdohhawI.en

Now, let's discuss how investors can use ETFs for hedging.Recall that hedging generally means entering intoa transaction in order to reduce exposure to price fluctuations.Let's see how hedging works with ETFs.If you have a portfolio that holds many stocks in the S&amp;P 500,you can take some short positions in an ETF that tracks the S&amp;P 500.The most popular is the SPDR S&amp;P 500 ETF,which is issued by State Street Global Advisors.Note that it's spelled S-P-D-R and pronounced spider.When the overall market moves down,your portfolio goes down with it.However, your short positions inthe SPDR ETF gain at the same time limiting the impact of the market downturn.Note that the reverse happens when the market goes up.Your long investments will go up with a market,while your short positions in the ETF will lose reducing your net gain.Active fund managers may also short ETFs in order to reducetheir portfolio's exposure to certain sectors, industries, or regions.For example, a hedge fund whose portfolio includes tech stocks may wish to short an ETF,such as the Fidelity MSCI Information Tech ETF,in order to cancel out some of the price fluctuations in the tech industry.

### 05. L2 07 ETF Sponsor V2-v5vfAP1nJ10.en

Financial institutions that issue ETFs are called ETF sponsors.ETF sponsors could be banks or institutional investment firms.The largest ETF sponsor is BlackRock's iShares.iShares was created by Barclays' Global Investors,which in turn was acquired by BlackRock.As of early 2018,iShares had more than $1.75 trillion of assets under management,which made up more than one-fourth of BlackRock's total AUM.The other major players are Vanguard,State Street Global Advisors,Deutsche Bank, and Investgo.ETF sponsors, like mutual funds,earn fees as a percentage of assets under management.That is, if an ETF sponsor is managing one billion dollars in investments,while charging 0.1 percent in fees annually,then the ETF sponsor is earning 0.1 percenttimes one billion dollars or one million dollars per year.ETF fees are much smaller percentagescompared to the fees charged by mutual funds and hedge funds.But the large volume of assets invested make ETFs very profitable for ETF sponsors.The ETF sponsor designs and maintains the ETF portfolio,so you can think of them as the fund manager.So far, ETFs sound pretty similar to mutual funds.

### 06. L2 08 Authorized Participant And The Create Process V4-u4thSf3Uxsc.en

Now, here's where ETFs start to look different from mutual funds.Recall that with mutual funds,investors give their cash to a mutual fund andthe mutual fund then invest that cash in a portfolio of stocks.With ETFs, the ETF sponsor only makes transactions withspecial partners called authorized participants or APs for short.Major APs include Merrill Lynch,Morgan Stanley, Goldman Sachs and Fortis Bank.An AP goes to the stock market and buys stocksthat match the portfolio defined by the ETF sponsor.Then, the AP makes a trade with the sponsor.The sponsor creates ETF shares and givesthose shares to the AP in exchange for the bundle of stocks.Notice that the sponsor issues ETF shares similar to what a mutual fund does.The difference is that the sponsor only deals witha set of APs who are financial institutions.Also, unlike a mutual fund,the ETF sponsor doesn't receive cash in exchange for its shares.Instead, the sponsor receives a bundle ofstocks that matched the portfolio designed by the sponsor.Okay, so what do APs do with their ETF shares?Also, so far, the investors haven't been involved in any of the transactions.How do investors get in on some of the action and own some ETF shares for themselves?Well, the APs go to the stock exchange and sell their ETF shares on the open market.Investors can buy these shares and now have investmentsthat are linked to all of the stocks that are in the ETF portfolio.What we've described so far is called the create process.In summary, the create process involves these steps.The AP buys stocks and gives them to the ETF sponsor.The ETF sponsor creates ETF shares and gives them to the AP.The AP then sells the ETF shares to investors.This is called the create process because it effectivelycreates more ETF shares that are available on the stock exchange.

### 07. L2 09 Redeeming Shares V3-ZSVgU7DBarc.en

If investors hold ETF shares and want their money back,they just sell their shares on the stock exchange.There's no need for them to interact with ETF sponsor.For reasons we'll see soon,an AP may decide to redeem ETF shares for the original portfolio of stocks.This is essentially reversingthe original ETF share creation process and is called, the redeem process.So, the AP buys ETF shares from investors in the stock market.Then, the AP tradesthe ETF shares with the ETF sponsor in exchange for the original stocks.Then, the AP sells these stocks on the stock exchange.This is called the redeem process,because the AP redeems the original basket of stocks by trading in the ETF shares.Notice, that the redeem process effectivelyreduces the number of ETF shares in the open market.

### 08. L2 10 Lower Operational Costs And Taxes V2-UlJusglK0h0.en

Recall earlier that we mentioned how ETF sponsors are more tax efficientand that their transactions are not taxed as much compared to traditional mutual funds.One reason is that ETF sponsors do not deal directly in cash.During ETF share creation,an ETF sponsor creates shares and receives a bundle of stocks of equal value.Similarly during the redemption process the sponsor givesa bundle of stocks in exchange for ETF shares of equal value.Since the two items being exchanged are ofthe same value there's no capital gain that is recorded.This is important because capital gains are taxed by the government.We can contrast this with an open-end mutual fund.If an investor requests to withdraw their investmentthe mutual fund may need to sell some stocks for cash to give tothe investor the capital gains taxes on the stock sales add to the operational costs ofthe fund and these costs are reflectedin the higher fees that the mutual fund charges investors.

### 09. L2 11 2 Arbitrage Farmers Market V1-hHxp16mQNGA.en

Hi, there.Do you remember Betty from module one?She used to sell fruit at the local farmers market.Hi, Betty. What are you up to these days?Hi, Eddy. Well, I've been working asan investment analyst at a new hedge fund Roulant and partners.Well, that's cool. What brings you to the farmers market?Well, I like to check out the real-time pricing andtrading that goes on in the farmer's market.It reminds me of the trading that goes on in the financial markets.That's an interesting connection.I noticed that one of the farmers here,Brock cells oranges for $1 each whileanother farmer Cindy sells a bag of 10 oranges for $9.I was wondering how you look at this pressing from the view of a hedge fund analyst.Well, I could buy 10 individual oranges from Brockat $1 each or I could buy from Cindy and pay $9,but still receive 10 oranges.I've also notice that there are tons of customerswalking around who want to buy an orange for a dollar.This looks like an opportunity for arbitrage.Can you walk me through what you would do?Sure. In general, I like to buy low and sell high.So, I'd like to buy a bag of 10 oranges for $9 and then sell them for a dollar each.I also want to make sure that I don't get stuck holding the bag.How would you avoid getting stuck holding a bag of oranges?Well, first I'll try to find 10 customers who want an orange.At the same time I'll make an agreement with Cindy to buy the bag.So, you're buying and selling at the same time.Exactly, and I earned a dollar profit by using arbitrage.So, now the question is what will you do with your $1 in profit?Well, I'm starting to feel a little bit hungry, I think I'll buy an apple.

### 09. L2 11 Arbitrage V2-yp-CcGrMzYQ.en

You may be wondering why an AP would bother creating or redeeming ETF shares.After all, what's in it for them?Do they charge a percentage fee the way an ETF sponsor does?No, but they do find other ways to make the transaction worth their while.This benefits investors too,because the actions of the AP helped to alignETF share prices with the stocks that they represent.Before we go into how this happens,let's review the concept of arbitrage.Arbitrage is the act of buying and selling assets thatshould be of the same value but that are priced differently in the market.When an investor finds a price differencebetween two assets that should be interchangeable,then they can buy the cheaper one and simultaneouslysell the more expensive one and make a profit on the difference.

### 10. L2 12 Misaligned ETF Pricing V3-5-pBZ3fyv6I.en

Let's see how an AP might seek out arbitrage opportunities with ETFs,and how that results in stable pricing of ETF shares.We'll look at the iShares STOXX Europe 600 Retail ETF,which tracks eurozone retail companies.These include companies such as Inditex,which sells the Zara clothing line and is based in Spain.It also includes Henness and Mauritz,which sells the H&amp;M clothing line and is based in Sweden.The iShares STOXX Europe 600 Retail orUCITS for short is tradable on the Bolsa Mexicana de Valores,which is the the largest stock exchange in Mexico.Let's say that the investors in Mexico are very eager to invest inthese European retail companies through ETFs and bid up the price of the ETF shares.This leads to a discrepancy betweenthe ETF share price and the underlying prices of the stocks.We say that the ETF is trading at a premium to the net asset value orNAV because it is relatively expensive compared to the underlying stocks.If the ETF shares were cheaper relative to their NAV,we would say that they're trading at a discount.Notice that this is the type of mispricing experienced by closed-end mutual funds.The actual difference is often referred to as the basis.We use this term because a basis point is defined as 0.01 percent or 1 divided by 10,000.Usually, the premium or discount is smallenough that its unit of measurement is a few basis points.

### 10. L2 13 Realigning ETF Share Prices V2-aRXJxjQQSiI.en

Going back to the example with the UCITS ETF,the demand for the ETF has led to a premium on the ETF shares.An AP such as Morgan Stanley may see this price difference as an arbitrage opportunity.So, the AP triggers the create process,that is Morgan Stanley may buy shares in Inditex,H&amp;M, and all the other companies listed in the UCITS ETF.Then, the AP gives the bundle of stocks to the ETF sponsor,which is BlackRock's iShares.The ETF sponsor issues more ETF shares for the AP.The AP, Morgan Stanley,then sells the ETF shares to investors on the stock exchange,which is the Bolsa Mexicana de Valores.The create process injects more ETF shares into the marketplace.The large purchases of a company stocks push up the stock prices of Inditex,H&amp;M, and the other European retail stocks listed in the UCITS ETF.Moreover, the additional supply of ETF shares helps to bring down the price of the ETF.The AP profits from the difference betweenthe buy price of the stocks and the sell price of the ETF shares.As long as there is a price discrepancy that is large enough for arbitrage opportunities,APs will buy low and sell high until the prices are aligned.

### 11. L2 14 Summary V1-E5br2PiH8kY.en

Let's take a step back and see what we've learned.We've learned that ETFs charge smaller fees than mutual funds or hedge funds.Moreover, ETFs are more efficiently priced than closed-end funds becauseauthorized participants use arbitrage to bringthe ETF share prices in line with the underlying stocks.We hope you have gained a better understanding ofhow ETFs work as you'll be working on ETFs in your project for this module.Congrats on making it through this lesson and good luck on the next lesson.

### 12. MV 11 Guided Meditation V1-njp1mnEEv9s.en

We're going to take a minute to let our brains chill out.Learning requires active engagement of your brain and really your whole self,and research shows that giving yourself adequate time torest and recuperate helps your new knowledge put down roots,so you can use it effectively in the future.Getting enough sleep, spacing your learning out throughoutyour weekly schedule rather than trying to do it all in one chunk,and giving yourself permission to take a break when you feel tired orfrustrated can all help you maintain your motivation and your brain's learning readiness.Meditating is another great way to takea mental break from learning indirect the world at large.So, let's try it out, nothing to lose.Wherever you are right now,find a comfortable position to sit or stand in.Try to let your whole body relax,and others can be a little bit tricky especially when you'vebeen leaning in a very active way.But just start by relaxing your hands and your feet,let your arms and your legs relax,let your belly relax,let your shoulders roll up back and down,close your eyes or you can pick a spot right infront of you to just stare at in a more static way.Then just start to let your thoughts aboutyour learning and whatever else is going on in your life fade away.If it's hard to just let your brain empty itself,you can instead replace those thoughts aboutanything that's going on with instead thoughts about your breath.So, to start to tune into how your body is naturally breathing on its own,just noticing what it feels like to inhale and what it feels like to exhale.Maybe you're breathing more into your chest,maybe you're breathing more into your belly.Feel how your rib cage expands out in all directions as you breathein and then contracts back to the center as you breathe out.You don't have to control your breath anyway,it's something that your body is just doing automatically on its own.Or just a couple of moments here,noticing that breath being still,being quiet, and justfocusing on this emptiness instead of everything else that's going on outside of you.Then you can gently blink your eyes open if you close them.So, congratulations, you just meditated.This is something that you can use throughoutyour learning experience using this video or on your own,and you can take it into the rest of your life as well whenever youneed a moment just to ground yourself and feel a little bit more calm.So, I hope this was helpful and congratulations on trying something new.

## Part 01-Module 03-Lesson 03_Portfolio Risk and Return

### 01. L3 01 Intro V1-PxLJniuGyC0.en

Welcome back. So, far in this module,you've learned about Indices and ETFs,their application in the real world and how they work on a transactional level.Now, we're going to talk about the baskets of stocks that underliethese instruments and you will learn how todistribute money across the stocks in the basket.First, we'll discuss the risk and return properties of a collection of stocks.So, let's say, you've researchedyour trading signals and finally come up with a list of stocks to buy.You know how much money you have to spend.You're ready to buy the stocks needed to construct a portfolio for an ETF.But, how much money should you invest in each stock?Well, putting more money in stocks that you expect to have the highest returns,seems naturally more likely to give you higher returns.But what if the prices of these stocks undergo the most fluctuations.That is they entail the greatest risk.You might have great returns for a while,but then, you could lose it all.There must be a balance here.How do you distribute money to not only maximize returns,but also minimize risk.That's a question that's been challenging great financial minds foryears and you're about to embark on a quest in search of the solution.

### 02. L3 02 Diversification V3-tyzqlXddXd8.en

Since you've studied the stocks in your universe now for a while,you might already know which stock has been performing the best.Let's call it New Digital Corporation.At this point, you may want to put all your money in New Digital stock.But let's say you do this and New Digital stock keeps increasing in value for a while,but suddenly drops to half the price you purchased that.Since you used all your money to buy the stock,you just lost half your money.Ouch! What if instead you puthalf your money in New Digital and half your money in Big Pharma Company?Intuitively, this seems like it could smooth out the dips and value of your investment,because New Digital could start doing well when it launches a new tech product,while Big Pharma recovers from a failed drug trial.Big pharma might start doing well when it's research arm,deliver several new promising candidate drugs,while New Digital struggles during a change in management.What we've done here is reduced our portfolio risk by diversifying our portfolio.But why not keep spreading our money into more and more stocks?Can we reduce our portfolio risk indefinitely?If all the sources of risk are independent,we could in theory,reduce risk to zero by spreading our money between more and more stocks.This would be the case if each company is onlysubject to its own independent sources of risk,and there are no risks common to all companies.Risks specific to individual companies is called idiosyncratic risk or specific risk.However, in the real world,all companies are subject to common sources of risk thataffect the entire economy such as the risks of inflation,recession, or change in interest rates or GDP.Risk attributable to market-wide sources is called market risk or systematic risk.Let's see if we can quantify this risk andfind some guidelines for how to allocate our money.

### 03. L3 03 Portfolio Mean V3-vozlctvug7I.en

Let's see how this works in more detail,by constructing a portfolio that containstwo stocks and then calculating its mean and variance.Let's put a fraction xA of our portfolio in stock A and the rest xB in stock B.We call xA and xB the weights on assets A and B inour portfolio and we know these weights xA and xB sum to one.To calculate the mean and variance of our portfolio,we will again need to think of future log returns as random variables,that is as quantities that can take a variety of values with different probabilities.You can think of returns,at a future time as being indexed by i,where i is a possible scenario or alternate timeline of the future.R the log return,takes different values for different scenariosand different scenarios have different probabilities of occurring.The expected value of R,which we referred to before as the mean or expected log return,is given by this formula.The total return of the portfolio in each scenario isjust the weighted sum of the returns of each individual asset.The expected value of the portfolio return,equals the weighted sum of the individual stock's expected returns.To see this, we take the expected value of the portfolio returns.The summation symbol distributes across the sum,then we can pull out the weights because they are independent of scenario.Finally, we see that each term is the expected value of the individual asset.

### 04. L3 04 Portfolio Variance V2-LlxRypakop4.en

Now, let's calculate a metric of the total risk inherent to the portfolio.We'll measure risk with volatility or more specifically with the variance,the square of the volatility.Let's review the formula for the variance of one asset.This is a slightly different formula for variance than we've seen before,but it's similar in spirit.Instead of just summing the squared difference betweeneach observation and the mean and dividing by n,we multiply the squared difference bythe probability of that value occurring and sum those.The formula will be the same as dividing by n,if the probability of each scenario occurring is the same.So, now let's calculate the variance of the portfolio.This takes just a few simple steps,but for now we will skip ahead to the answer.You'll find the full derivation given as text on this page.We find that the portfolio variance equals thesquared of weights times each asset's variance,plus a term that includes the covariance of the assets multiplied by each weight.Remember that the covariance is a measure ofthe joint variability of two random variables.When stock A is above its average,if stock B is also above its average,they are varying together or covarying,so they have a positive covariance.This covariance term turns out to be very important.It basically underlies the benefit of diversification.Remember that the covariance is just the correlationbetween the two variables times each of their standard deviations.We know that the correlation coefficient takes values between minus one and one,where minus one means the variables are perfectly anti-correlated,and one means that they are perfectly correlated.Both correlation and covariance are measures of how much two variables vary together.With this knowledge, we can rewrite the portfolio variance in terms of the correlation.Let's see what happens to the variance ofour two-asset portfolio when the correlation between stock A and stock B is one.In that case, the expression to the right of the equal sign is a perfect square,and the portfolio standard deviation isjust a weighted average of the standard deviations of the individual stocks.In all other cases,the correlation coefficient is less than one.So the portfolio standard deviation is less thanthe weighted average of the individual standard deviations.How about when the correlation between stock A and B is minus one?In that case, the portfolio variance reduces tothe weighted difference between the two standard deviations squaredand the portfolio standard deviation becomesthe absolute weighted difference between the individual standard deviations.In the case when the correlation is minus one,we can get a perfectly hedged portfolio by solving this equation.If we plug in the relation that the weights x_A and x_B sum to one,we get that x_A equals Sigma B divided by Sigma A plus Sigma B.These weights drive the portfolio variance to zero.However, in reality, since every asset is affected by systematic risk,the correlation between two assets will never reach minus one.

### 07. L3 06 The Covariance Matrix And Quadratic Forms V1-as5lafBZ2CA.en

Congratulations, on getting through the first five nodes.You're now over a third of the way through this lesson.Because some of the topics in this lesson are so math heavy,we wanted to create content that contain the actual formulas.Beta testers suggested that content delivered as text is super-helpful.Be sure to let us know what you think.Udacity is all about creating an environment that encourages students,but we also want to learn from you.We want your feedback to be included in the process.Your feedback helps us to iterate and create better lessons.So, if we do something that you like, let us know,and if you think we could do something better,let us know that too.Good luck with the rest of the lesson.

### 10. L3 08 The Efficient Frontier V3-tEEyhU23bI4.en

Okay. So far you've learned about the importance ofdiversification and how to calculate portfolio mean and variance.You may wonder, what are the best ways to assign to each stock in your portfolio.Before we dive into portfolio optimization,let's take a look at the set of all possible portfolios.This will lead us to an important concept, the efficient frontier.Before we start, let's quickly recap.The expected return of a portfolio,equals the weighted sum of each stock's expected returns.The variance of a portfolio,equals the sum of the pairwise covariances weighted by the products of the weights.Okay. I think we're ready to go.Let's first take a look at a simple example with five stocks.Stocks A, B, C,D and E with given annual mean returns and covariances.One day your manager gives you $10,000 and tellsyou to invest in these five stocks whichever way you want.But taking only long not short positions.What would you do with this money?Of course, you would want to impress your manager and createa portfolio with the highest returns and the lowest risk.Is this possible?How would you do it? You've given the task some thought anddecided to just create a few portfolios with randomly assigned weights.For the first portfolio,you give each stock an equal weight of 20% of the total capital.The portfolio return equals 4% and the standard deviation equals 3%.Is this good enough?You are not sure.So, you just keep testing different combinations by assigning a little more weight toone stock and a little less to another andthen rechecking the portfolio's return and risk.The more simulations you do,the more interesting the problem gets.For example, both scenario one and two have the same risk,but scenario one has higher expected return.In this case, you will definitely prefer scenario one.Also, scenario four has the highest expected returnof 0.07 but it also has the highest risk of 0.04.Scenario four is less attractive because it is a riskier portfolio.This is interesting; isn't it?Let's try to run thousands more scenarios and plot theexpected portfolio return against portfolio volatility.Okay. Here is a plot representing 25,000 simulated portfolios.Each dot represents a possible risk return combinationthat can be generated by the basket of stocks.The x axis is the volatility and the y-axis is the return.What have you noticed? Let's take a look at a quick example.Let's take a look at these two portfolios represented by the red and yellow dots.Notice that both portfolios have the same risk of 0.035 butthe red portfolio has significantly higher expected return of 0.06.Furthemore, the red dot outperformsall the portfolios with risk of 0.035 but there's more.Have you noticed that all the dots that lie onthe upper boundary outperform the dots below it?This is the efficient frontier.The efficient frontier is the upper boundary of the set of possible portfolios.Portfolios on this boundary have the maximum achievable return for a given level of risk.Any portfolios above the frontier are unachievable.It's not possible to find a combination of stocks thatproduces a higher level of return at that same level of risk.On the flip side, portfolios below the frontierare certainly achievable but one might wonder whya rational investor would prefer a lower rate of return when shecould achieve a higher return while taking on the same level of risk.The portfolio that achieves the lowest level ofrisk is called the minimum variance portfolio,and the portfolios on the efficient frontier are known as market portfolios.

### 11. L3 09 Capital Market Line V2-BRO-vo3y0-U.en

In the last video,you learned about the efficient frontier.You know that any point on the efficient frontier representsthe portfolio that gives you the best risk return trade-off,and that these portfolios are known as market portfolios.But is that it? Is there a way to do better than the efficient frontier?In this video, we will be adding a risk-free asset intoour portfolio and we're going to introduce something called the capital market line.A risk-free asset is an investment instrument thatentails absolutely no risk or uncertainty.In theory, if you invest in such an instrument,you receive a guaranteed rate of return called the risk-free rate andreality entirely risk-free assets don'texist since all investments carry a certain level of risk.However, in practice people normally refer to the rate ofreturn on a three month treasury bill as the risk-free rate.Now let's add this risk-free asset into the picture.Let's imagine we construct a new portfolio with any ofthe market portfolios that you prefer and the risk-free asset.Which market portfolio would you choose?Note that this new portfolio isa weighted sum of the risk-free asset and the market portfolio.For example, if you choose the green portfolio as your market portfolio and the red.as your risk-free asset,the line between them representsthe potential portfolios that you can construct with the two assets.Now, which market portfolio would you chooseso as to achieve the best return for a given level of risk?Well, you should be looking at the efficient frontier,the top edge of the set of possible portfolios.It turns out that you should choose the market portfolio that allows you to drawa straight line starting fromthe risk-free asset that just touches the top of the efficient frontier.To see why this is the best market portfolio for the job,let's take a look at lines A, B,and C. Note that line A connectsthe red dot which represents the risk-free asset with the green dot,which represents the tangent market portfolio on the efficient frontier.Line A lies above the efficient frontier and intersects the tangent portfolio.All the portfolios represented by line A have higher returnsthan the ones on the efficient frontier given the same risk level.This is not the case for lines B andC. Portfolios on this tangent line are the best achievable.They even beat the efficient frontier which consists of purely risky assets.This line is called the capital market line.Now what is the expected return ofthe portfolio consisting of the risky portfolio and the risk-free asset?The formula for this quantity is also the formula forthe capital market line because each point on this linegives the return as a function of risk ofa possible combination of the risky portfolio and the risk-free asset.So let's examine the graph to determine the equation.You have two points 0,rf which is the risk-free portfolio and sigma m,rm which is the market portfolio.The slope of a line simply equals rm minus rf divided by sigma m.This quantity the difference between the market return andthe risk-free rate is called the market risk premium.The portfolio return is then just the risk-free rate of return asa baseline plus the slope times the portfolio volatility.The portfolio volatility will depend on what weights you choose toput on the risk-free asset and the market portfolio, i.e.where you choose to sit along the capital market line.The slope of the capital market line,which is of course the same for all points onthe line is a special quantity known as the Sharpe Ratio of the market.We're going to discuss that more in a future lesson.In fact, if you can borrow at the risk-free rate,you can achieve points onthe capital market line to the right of the tangent market portfolio.This amounts to setting a negative weight on the risk-free asset.That is to say, shorting it.This is why professional investors care almost only aboutthe Sharpe ratio because they can manufacture any level of risk or return with leverage.

### 16. L3 13 Summary V1-I7XKJf8t_0s.en

In this lesson, we learned how to calculatethe expected return and variance of a portfolio risky assets.We study the properties of all possible portfolios of risky assets,and what happens when a risk-free asset is added.What you learn in this lesson is essential foundational knowledge.Once you understand it,you'll be ready to learn how to obtainthe best possible risk return balance from your portfolio,which is what we'll study next.

### img

## Part 01-Module 03-Lesson 04_Portfolio Optimization

### 01. L4 01 Intro V1-CtIcmmR0YTs.en

In this lesson, you'll learn strategies for optimizing returns on your portfolio.That is to say getting the best possible combination of risk and return.Portfolio optimization consists of a diverse array ofwidely utilized and actively research techniques.In this and future lessons,we're going to take you from the very basics,all the way up to some of the most cutting edge methods. Let's get started.

### 02. L4 02 What Is Optimization V2-ISRlP1GeOjU.en

Previously, we studied the set of all portfolios and we know we're particularlyinterested in portfolios withthe highest possible return given the volatility in returns.How do we find the highest returns then?Well, this is a type of problem known as an Optimization Problem,you may have encountered this before.Intuitively optimization means making something the best possible right?So, what we're trying to do is find the maximum or minimum of a function.In fact, we can always turn a maximization problem intoa minimization one by optimizing the negative of the function.So, we can say that optimization involves,trying to find the minimum of a function.First, let's look at a simple example of a type you may have seen before,here we're looking at the function Y equals X minus one squared plus one.This is a quadratic function ofone variable X so we know it looks like an upside down U shape a Parabola.We can see that all we have to do to find the location ofthat minimum is to find the value of X that makes Y the smallest.We will find that point by observing that the graph has a special property at that point.At every other point on the line seems to be either going down or going up,but at that point the line looks basically horizontal.What I'm saying is that every other pointthe instantaneous slope of the line is some positive or negative number,but at that minimum the instantaneous slope of the line is 0.That is how we will find the point.Now, let's remember that we know how to get an equation thatrepresents the instantaneous slope of the function, the derivative.So, what we can do to solve for the value of X at this point mathematically,is to take the derivative of the function,set it equal to 0 and solve for X,so let's do that now.And check it out we got that x equals one,this is one very basic type of optimization problem that can be solved exactly.Later on, we'll see that there are many complexities we can add toan optimization problem and we'll require many more methods to solve them. Stay tuned.

### 03. L4 03 Optimization With Constraints V3-91WzhG6dti8.en

Earlier, we discussed a basic optimization problem.It's possible for optimization problems to have an additional wrinkle.Constraints can be placed on the problemto limit the region where you look for a solution.In other words, you try to find the smallest value ofthe function that also satisfies the constraints.Let's discuss how to solve an optimization problem with constraints.First, let's clarify some terminology.The function we are trying to optimize is called the objective or cost function.The variable we are trying to optimize,which in portfolio optimization is the vectorof weights on the various assets in the portfolio,is called the optimization variable.Then we may have various constraints.The constraints can take the form of inequality or equality conditions.If there are no constraints,we say the problem is unconstrained.If the objective function is x minus one squared plus oneand we apply the constraint that x must be less than or equal to zero,then the new minimum value is two because this isthe smallest value of the function that also satisfies the constraint.A vector x is called optimal or a solution of the problem if it hasthe smallest objective value among all vectors that satisfy the constraints.The set of points for which the objective andall the constraint functions are defined is called the domain of the problem.A point in the domain is said to be feasible if it satisfies all the constraints.The problem is feasible if there exists at least one feasible point,and it is infeasible otherwise.If there are feasible points for which the objective function reaches negative infinity,we say the problem is unbounded below.This is the case for the function f of x equalsthe negative natural log of x with no constraints.A general optimization problem can be very difficult to solve.You can have functions of many variables andthe objective function and constraints can be very complicated.So, instead of thinking about how to solve all types of optimization problems,let's just think about how to solve certain types.It turns out that there are well developed methods to solve some sub-types of problems,and they will be of great help to us as we try to optimize our portfolios.It turns out that a very important type of optimization problem is one for whichthe objective function and inequality constraints are convex.What does that mean? Well, it's actually very intuitive.A convex function curves upward everywhere.Any straight line you draw between two pointson the function has to lie above the function.So, this wiggly function would not be convex.An important property of a convex optimization problem is that it only has one minimum.So, if you find a local minimum,you know it is the globally optimal value over the whole feasible region.This is actually hugely helpful.When the objective function and inequality constraints are convex andthe equality constraints have the form f of x equals a times x plus b,the optimization problem is called a convex optimization problem.If you find a local optimum for this problem,you know it is a global optimum.There are well developed techniques for solving these problems,and the set of problems we deal with duringportfolio optimization will generally be of this type.

### 10. L4 11 Rebalancing A Portfolio V2-S5SPhBpG3b0.en

So far in this lesson,you've learned how to useoptimization techniques to come up with optimal portfolio weights.Is your job done? Not really.As a portfolio manager,the next step after portfolio construction,is to constantly monitor your portfolio,and make sure it is in line with your goals.Over time, assets produce returns that can differ from those originally estimated.So, the amount of money invested in different assets, changes.Therefore, the portfolio weights,which represent the proportion of money invested in each asset, change.In order to make sure the portfolio maintains the desired set of weights,we need two rebalance.For example, say we started with a strategy in which we wanted to invest50% of our portfolio in a solar energy company and 50% in a construction company.Let's say the solar energy company does really welland it's value appreciates such that the shares we've invested in it,now represent 70% of the value of our portfolio.If we wanted to return to the 50% ina solar energy company and 50% in the construction company,we'd sell some shares of the solar energy company,and buy some shares of the construction company to bring us back to 50-50.When you have determined your portfolio weightsvia optimization and later want to rebalance,you simply re-run the optimization with the same objective function and constraints,but using updated data.Before we discuss different rebalancing strategies,let's consider the costs incurred during rebalancing.Think about it, when you rebalance, you place trades.To place trades, you generally have to pay a broker commissions.These commissions are one type of transaction costs.Transaction costs, are probably the most important costs incurred during rebalancing.But, there are others, such as taxes,like capital gains taxes and the administrative costs of time and labor.Transaction costs can be potentially significant.In the year 2000,the Texas Permanent School Fund rebalanced its portfolio ofmore than 2,000 securities, 40 portfolio managers,500 million shares, and $17.5 billion were involved not including administrative costs,the transactions themselves costs $120 million.Now, you can see how crazy transaction costs can be.In general, it's hard to model transaction costs directly.To know the transaction costs,you must know what the trade is.But the whole reason to do the optimization,is to know what the trade is.So, instead of trying to estimate transaction costs,we can make the assumption that transaction costs will beproportional to the magnitude of change in the holdings.This is called portfolio turnover and it's basically,the sum total of the changes in the weights on all the assets.To calculate turnover between two time periods,you would take the absolute value ofthe difference in weight between two time periods for each asset,and then sum these over assets.To get an annualized turnover,you would calculate the average turnover per rebalancing event by calculating,total turnover for all the rebalancing events in the timeframe you're considering,and dividing by the number of rebalancing events,and then multiplying by the number of rebalancing events per year.

### 11. L4 12 Rebalancing Strategies V2-8u5gBx-fYr8.en

In the last video, we discussed the purpose of rebalancing,costs incurred during rebalancing,and we also chatted about a way to estimate those costs.But how do you know when to rebalance a portfolio?There are two types of events that trigger the rebalancing decision,cash flows and changes in the underlying parameters of the stock return model.Cash flows are movements of money into and out of a portfolio.For a portfolio of stocks,the most common forms of cash flows are dividends,capital gains and new contributions.For example, an investor receives dividend payments when he loans a stock.However, when he has shorted a stock,he is obligated to pay the balance of dividend paymentsissued to shareholders to the person who has loaned him the stock.Capital gains and losses are the increases ordecreases in value of the assets in the portfolio,but these are unrealized until the assets are sold atwhich time they become realized capital gains and contribute to cash flows.New contributions consist of new investments in the fund or portfolio under management.The simplest way to adjust for cash flows is tobuy and sell stocks using the current portfolio weights,but when some assets appreciate and some depreciate,the weights which represent the fraction of the portfolio invested in each asset change.A portfolio manager can take advantage of cash flows to adjust the underweighted or over weighted portfolios to return to the target asset allocation.Changes in model parameters do not automatically trigger rebalancing.In this case, the portfolio manager must decide whether the parameter change islarge enough to warrant incurring transaction costs to rebalance.As a portfolio manager,you need to constantly monitor the parameters usedin your portfolio and be ready to rebalance when needed.Parameters can change at anytime.For example, many types of corporate actions such as mergers andacquisitions or changes in management may impact the model's parameters.When a portfolio manager suspects that a parameter has changed,she will re-estimate the model to come up with the optimal weights,then she will decide if the portfolio should be rebalanced byweighing the benefits of rebalancing with the downsides such as transaction costs.The most common rebalancing strategies arebased on rebalancing at set temporal frequencies,or rebalancing when portfolio weights change by a certain threshold amount.One strategy is to rebalance at a set temporal frequency, daily, monthly,quarterly, annually or at another frequency,regardless of the changes in the sizes of the portfolio weights.This temporal frequency typically comes from the original time horizon of optimization.When a portfolio manager models his portfolio,he will often optimize it using data from a chunk of time of some duration.For example, if a portfolio is model based on month long chunks of data,the optimal weights are only valid for a month and so rebalancing is performed monthly.The threshold base strategy calls for rebalancingthe portfolio only when the portfolio weights havedrifted from the target ones by a predetermined threshold such as 1%, 5% or 10%.Since rebalancing does not occur at predetermined times,it can happen as frequently as daily,but also as infrequently as yearly.These strategies can also be combined,such that rebalancing is performed on a scheduled basis but only whenthe portfolio weights have drifted from the target ones by a predetermined threshold,thus, both criteria must be fulfilled.On the scheduled rebalancing date,if the change in the portfolio weights is less than the threshold,rebalancing won't be performed.Also, if the portfolio weights drift bymore than the threshold before the scheduled rebalancing date,rebalancing won't be performed.

### 12. L4 13 Limitations V2-UbbZa7-3iuk.en

So far we've learned a few common ways toformulate and solve a portfolio optimization problem.But there are a number of practical limitations to these methods.First of all, mean returns are notoriously difficult to estimate accurately.Past returns are frequently poor estimates of future returns.To mitigate this problem,people frequently avoid estimating returns directly altogether.We've already introduced one possible solution which is to simply minimizethe difference between your portfolio weights and another set of target weights.Another technique is to use an alternative quantity thought to be predictiveof future returns as a substitute for actual returns value estimates.We're going to talk more about these methods in a future lesson.There are also problems inherent to the estimation of portfolio variance.The first is that variants may not be a good enough measure of risk.This is the theme that might come up whenever you rely upon variants to estimate risk.To understand what I mean,think about the distribution of log returns.If the distribution has a different shape on the left side and on the right,this means there's a different probability of gettingvery high positive returns than getting very high negative returns,but mean and standard deviation won't tell you about this asymmetry.For example, these two distributions have the same mean and the same standard deviation,but obviously very different shapes.As a quantitative trader,you really care about the shape of that negative tail because that part ofthe distribution tells you how likely you are to get large negative returns.As a workaround for this,you can formulate your optimization problem using alternative measures of risk,some of which we'll discuss in future lessons.Furthermore, the need to estimate an n by n covariance matrix,where n is the number of assets in your portfolio,means the problem size grows very quickly as the number ofassets increases and the matrix becomes very large.Estimating large numbers of parameters introducesthe possibility of aggregating many estimation errors.When the number of stocks n is of the same order of magnitudeas the number of historical return data points per stock t,the total number of parameters to estimate in the covariance matrix is ofthe same order as the total size of the dataset, which is problematic.The need for a covariance matrix also means that we need to havea long enough historical stock price datasetto produce reliable estimates of covariances.For example, estimating a covariance matrix of50 assets requires at least five years of daily data.If there are not enough daily return data,estimates of variances and covariances will not be accurate.But in fact any estimate is noisy.How do you handle noise?Well, instead of using raw values of quantities like means,you can utilize their ranks so that you only keep track of their relative magnitude.You can also simply include a term in the objective function to penalize turnover.This means that by default you are less likely to trade,therefore your results will be less reactive to noise.Finally, there's a method called robust optimizationwhich takes into account confidence intervals of estimates.Another issue which we discussed when we mentioned rebalancing isthat portfolio optimization is typically doneusing a chunk of data from some time period,but then needs to be performed again as time passes and things change.In other words, the optimized weights are really only valid as longas the estimates of the parameters fed into the problem are accurate.There's no built-in way to account for change over time.But the changing nature of the stock market maylead to conflicting predictions over different time horizons.For example, what if your analysis tells you thata return over one day horizon will be positive,but over a one month horizon it will be negative?In a single period optimization framework,it's not clear how to account forthe different timescales while taking into account the conflicting predictions.One approach to account for the changing nature of the stock market over timeas part of optimization is called multi-period optimization.In essence, this involves modeling a plan for trading overthe next several periods of time usingthe best information available at the current time,and optimizing over this series offuture trades but only ever executing the trades for the first period.This basically amounts to a planning exercise,the idea is simply to try to ensure that we don't placeany traits now that will put us in a bad position in the future.In the example I just mentioned,where we have a prediction that returns over one day will be positive,returns over one month will be negative,we can see intuitively that it makes sense to golong and short to take advantage of these predictions,if transaction costs are low but to refrain from trading if transaction costs are high.If transaction costs are high,we may waste more money making trades than we couldgain by following the predictions of positive and negative returns.A multi-period optimization analysis that includestransaction costs would provide an optimal solution to this problem.Another difficulty of portfolio optimization isthe ambiguity about how to deal with transaction costs.Every time we make trades to rebalance the portfolio,we incur trading costs.But in general, they're difficult to precisely model.In order to account for them,we model them as being proportional to turn over,the sum total of changes and portfolio weights.But you can also include measures of turnover in the optimization problem formulation,either as a constraint,or as a term to minimize in the objective function.To address some of these weaknesses,factor-based models of risk are often seen as morepractical but if this discussion seem rather abstract to you, just hang on.Many of these topics will be elaborated upon in a future lesson.

### 13. L4 14 Recap V1-e3qJYCQfJD0.en

Hey now, you made it,and you are doing awesome.We talked about a lot in this lesson.We learned about a very important method for distributing money amongstseveral assets in a manner that maximizes return and minimizes risk.This knowledge will serve you well when you're deciding how to best invest your money.I really hope you enjoy the project.It's a great opportunity to cement what you've learned in memory.Looking forward to seeing you in the next module.

### img

## Part 01-Module 03-Lesson 05_Project 3 Smart Beta and Portfolio Optimization

### 01. MV 12 Transition To Project 03 V1-ClzlNlWqMQI.en

Congratulations, you're more than halfway through term one.You've done amazingly well to get this far.You've learned the basic mechanics of stock markets,the fundamental math and vocabulary that people use to describe markets,and a few types of trading strategies based on basic hypotheses about market behavior.This means that you're ready for a new exciting challenge.In this project there are two parts,in the first part you'll build a smart beta portfolio,then calculate the tracking error against its index.In the second part,you will re-balance the portfolio usingquadratic programming and calculate its turnover to evaluate the performance.The project will give you practical experience with the idea of smart beta,a popular type of investment strategy thatyou'll hear about when discussing quantitative trading,and enable you to practice the math of portfolio optimization,which is something you would do frequently as a quantitative analyst.Best of luck with this project. It's a good one.

## Part 01-Module 04-Lesson 01_Factors

### 01. M4 L3A 01 Intro To The Factors V2-OqhRUxHf6wo.en

Well, you made it.Made what?Made it to the,in my opinion, best part of the term.This is where it all comes to life.That's part of the term but they've already learned so much good stuff.They've already created their own trading strategiesand they're already developing strong market intuition.They have an idea for how quant research works andhow production quality, trading strategies work.They know about markets, volatility,outliers, indices and ETFs.They even know how to create optimal diverse portfolios,trading off volatility or risk with return.Yeah, that's true, and learning all of those things help bring them tothe point where they can fully appreciate what they're doing now.But now we're going to put it all together and they'refinally going to see the whole process end to end,from an initial hypothesis,maybe from an academic paper,we'll see a few real life examples to quantify non-hypothesis,to combining information of many types from many sourcesabout drivers of mean returns and drivers of volatility into a single model.Then using optimization tools we've already introduced,to turn that into a production quality,theoretical tradable portfolio ready for a barrage of further testing.Well you certainly have my attention.Is this how real quants do it?Absolutely. This is how real quants do it.Well, that's exciting.Wait, when you mentioned ways to quantify a hypothesis,are you referring to the alphas that we discussedback when we were talking about the quantum workflow?Yep. That's exactly right.We're going to talk about alphas in great detail.Okay. So, students are going to hear about all of these in these lessons.But will they get hands-on experience to?Absolutely, your final project for this term,will give you a chance to demonstrate your understanding of this whole process.Frankly, I'm excited that you'll have a chance to learn about these fundamental tools.I actually don't know of anywhere else where you can findthis kind of practical instruction in these methods.In this lesson, you'll get an inside view of how practitioners translateideas into alpha models and then implement those alpha models in code.Personally, I'm also very excited that Jonathan has designed this module to give youa real practitioner's perspective of how to combinedata with an idea and implement it in code.Yep, this is awesome.We're all super excited that you've reachedthis point in your journey with us and we hope you are too.Without further ado, let's get started.

### 02. M4 L1A 02 Intro V2-W7_llXQ2GhA.en

So, what are we doing here?What is this all about?As we heard about earlier,our work in this lesson builds upon previous lessons.While we'll still develop trading strategies withlarge baskets of stocks and use optimization tofind combinations of weights on stocks toseek the highest return for a given level of risk,we'll now learn how to develop models of portfolio returnsand risk and add these into our optimization framework.Before, we always talked about the distribution offuture log returns and it's mean and standard deviation like these were numbers,we can just read off of a list somewhere.But in general, this is not the case.Otherwise, all this would be easy.In addition, we aren't going to explicitly model returns in the way you might expect.The problem with explicit estimation of returns is that these estimates are verynoisy and noise causesunnecessary trading which we want to avoid because trades are costly.Instead, we will focus on what Jonathan mentioned earlier.Drivers of mean returns and drivers ofvolatility and these will be the bases of our model.Drivers of mean returns are alpha factors and drivers of volatility or risk factors.We'll come up with a way to quantify these concepts.In the next few lessons,we'll learn about factors,how to create and improve them,and how they fit into the model.We will also explore advanced portfolio optimization using factors.We've actually seen some ideas for factors earlierin this course without explicitly calling them factors.For example, factors can be based on momentum, or fundamental information.When we can process the data in such a way that they have the potentialto provide predictive information about the future movement of stocks,we call these factors.But what is a factor?A factor is a list of numerical values one assign to each stock inour stock universe that is derived to potentially bepredictive of an aspect of the performance of the stocks in the future.In essence, factors are signals that help suggest where to place bets in a portfolio ofstocks and further suggest what the relative magnitudes of those bets should be.Factors are the basis of quantitative portfolio management,as the search for effective trading strategies startswith a search for factors that drive stock movements.This lesson will introduce you to an example of a factor and show you how tostandardize it so that it can be interpretedas a set of weights for a theoretical portfolio.After that, we'll walk you through an open source Python library,Zipline which we'll use throughout the module.You'll also use it in the project to create and evaluate factors encode.Zipline is a Pythonic algorithmic trading library.It's an event-driven system for back-testing.If you start work at a fund or a bank,it's likely that you will learn proprietary software packages that arebuilt in-house and these will be unique to the firm that you work at.Learning how to use the open source packages in this program,will give you practice in learning and usingcustom libraries such as the ones that you may use on the job.By the end of this lesson,you'll have hands on practice with implementingyour first factor in code which will serve as a foundation for the rest of the module.

### 03. M4 L1A 03 Example Of A Factor V4-MJrwJDjWlAg.en

Let's walk through an example of a factor to learn more about what a factor looks like.We'll look at a momentum factor.So, let's say we have a hypothesis that the one year return ofa stock gives some signal about future returns over the next few days.To create a factor,we get price data for stocks within the stock universe of interests andcalculate the percent change of today's price from the price from one year ago.We repeat this for several days of data.The collection of one year returns for the set of stocksover time is what we'll refer to as a factor.It represents information that can potentially be useful in deciding how much weightwe'll give each stock in the portfolio and whether to long or short each stock.Now, the question is how do we use this factor?Do we use it to try and predict how much a stock will go up the next day?That's quite difficult when dealing with data that has a lot of noise.What we actually want to do is to use the factor value foreach stock to compare it toother stocks that are candidates to be included in a portfolio.So, let's say we have a portfolio of just two stocks, Apple and NVIDIA.On August 3rd, 2018,Apple's stock price reached $208 per share valuing the company at $1 trillion.Its price one year before was $156.So, Apple's one year return was about 33 percent.NVIDIA stock price on August 3rd,2018 was $252, valuing the company at $152 billion.NVIDIA's price one year before was $166 per share.So, its annual return was about 52 percent.To summarize, Apple's factor value is0.33 and NVIDIA's factor value is 0.52 for this one day.Now, what? Well, since the hypothesis ofour factor is that a higher number indicates higher future returns,then according to our factor and its associated hypothesis,NVIDIA's return may be higher relative to Apple's over the next few days.One way to act on this signal is to put more money on the stock for which the factor hasa more positive value and less money on the stock that has a less positive value.We could also short a stock that has a negative factor value.In this hypothetical example,we want to give a higher positive weight to NVIDIA relative to Apple.

### 05. M4 L1A 04 Standardizing A Factor V5-sLZY2SQ4uME.en

Now, let's see how to convert this raw calculation into a standardized factor.We want the resulting weights to satisfy two conditions.First, we want the sum of the weights to add up to zero.Second, we want the absolute values of the weights to sum to one.To satisfy the first condition,we want to find the mean or average of the list of raw factor values for all the stocks.Then, subtract that mean from each of the raw factor values.We say that we de-mean the factor,because we're subtracting it's mean.To satisfy the second condition,we want to re-scale the values.To re-scale, we divide each value by a number.This scalar, equals the sum of the absolute values.So, we re-scale the values by dividing each value by this scalar.To standardize a vector of factor values,we both de-mean and re-scale,so that the mean of this two list of numbers equals zero,and the sum of the absolute values equals one.We could do the de-mean or re-scale,one after the other in either order.Let's walk through an example with three stocks,each with a factor value such as a one year return.First, we want to de-mean the values so that they sum to zero.So, we calculate the mean by adding up the values of A,B, and C, then divide by the number of stocks.Then, for each stock,we get the raw factor value minus the mean.Next, we want to re-scale so that the sum of absolute values equals one.We can get the scalar by adding up the absolute values of the numbers.To re-scale, we divide each value by the scalar.This gives us the standardized values for the factor.We can verify the conditions that the sum is equal tozero and the sum of the absolute values equals one.Feel free to pause the video,if you want to study these steps in detail.Great. So, now you know how de-meaning and re-scalingcan convert raw calculations into candidates for alpha factors.This is actually how we standardize factors so that we canevaluate them and compare them with other factors.Next, we'll look at how to interpret,why we take these steps to standardize a factor.But first, here's a random question to ponder,why were the normal distribution and the raw factor bothvery unhappy when they were told that they would be standardized?One, I know the answer, because they thought it was de-meaning.Get it? De-meaning.

### 06. M4 L1A 05 Demean Part 1 V3-R3N8bd8U6TM.en

So, you just learned how to demean a factor so that the values add up to zero.But why do we do this,the short answer is that, later on,we're going to test the factor as if each valueassociated with each stock also determined,the stock weight in a theoretical portfolio.We want to make this theoretical portfolio dollar neutral, that is,the dollar amount of all long positions equals the dollar amount of all short positions.We want to do this to test the predictive power ofthe factor while excluding the influence of the overall market.So, why is a dollar neutral portfolio less influenced by overall market movement?Well, market movement refers tothe general direction of most stocks in a country or region.If a portfolio was long only,it would likely move in the same direction as the market.If a portfolio was short only,it would likely move in the opposite direction of the market.A long short portfolio means we can take long and short positions.A dollar neutral portfolio is a special case of a long-short portfolio.Is the case where the value of the longs equals the values of the shorts.If a portfolio were halfway along and have short,then the market movement would havea minimal effect on the movement of this dollar neutral portfolio.So, a dollar neutral portfolio is set up to approximately be market neutral.Note that, an assumption we're making is that on averagethe Beta of each stock to the market is one.Remember that, Beta inthe capital asset pricing model refers to how much a stock moves when the market moves.In general, the average Beta of stocks may not be exactly one,but when a portfolio contains thousands of stocks it's a reasonable assumption.So, a dollar neutral portfolio may move a bit withthe market but it would ideally be close to market neutral.Let's now switch from talking about portfolio positions in dollaramounts to talking about portfolio positions in terms of weights.Let's review the meaning of portfolio weights and introduce the concept of the notional.The notional or trade book value is the gross dollar amount associated with a portfolio.The amount of money we'd allocate towards a stock equals itsportfolio weight times the portfolio's gross notional value.If a portfolio notional is 100 million and the stock is given a weight of 0.01,we multiply the weight of 0.01 times 100 million which is 1 million.This would be the amount of money we'd allocate toward the stock.For a long or short portfolio,we can also use the weight times the notionalto see the dollar value placed on that stocks position,whether it's a long position or short position.For example, assuming a notional of 100 million,let's say we have a stock with a weight of 0.01this translates to a position of $1 million.Likewise, another stock with a weight of negative0.01 then has a position of negative $1 million.Notice in this example,assuming that there are only two stocks in the portfolio,these positions add up to zero.When all the positions add up to zero,the portfolio is dollar neutral.Remember, the goal of having a dollar neutral portfolio is to makethe portfolio market neutral or at least close to market neutral.

### 07. M4 L1A 06 Demean Part 2 V2-aaj1QVsSCIs.en

Any theoretical dollar neutral portfolio,we technically wouldn't be using any cash to create the positions.In fact, sometimes, in academic papers,they call these portfolios, zero investment portfolios.In real life, there are transaction costs,margin costs for shorting positions,and differences in timing of trades.For the theoretical portfolio used to evaluate a factor,let's not worry about those issues.So, now, let's visualize how we convert a portfolio into a dollar neutral portfolio.Let's say we have some portfolio weights andwant to adjust them so that they are dollar neutral.For example, let's say we have two stocks in the portfolio,the weights are both positive,so the portfolio is not dollar neutral.If you imagine the weights as actual weights on a balancing scale,the scale would be tilting because it's heavieron the positive side compared to the negative side.Notice that the relative difference betweenthese two stocks is the weight of A minus the weight of B.Keep this in mind for what's coming next.If we were to shift the weights to the left towards the negative direction,we can keep going until the weights are balanced.Notice that the relative difference betweenthe weights of the two stocks is still the same.So, how did we shift our weights to make the portfolio dollar neutral?Well, we actually took the average ofthe weights and then subtracted that average from each of the weights.Does this look familiar to you?Subtracting the mean of the weights,D means the portfolio weights.The sum of the resulting weights now equals zero.So, the portfolio is dollar neutral.

### 08. M4 L1A 07 Rescale Part 1 V2-BcsxA0vy3jA.en

Okay. Now, let's get an idea for why were-scale the weights so that the sum of the absolute values equals one.Our goal is for the portfolio's leverage ratio to be equal to one.So, let's first discuss the concept of leverage,and then we'll learn about what the leverage ratio is.Leverage is the act of borrowing in order to invest in assets.Usually, the goal when using leverage isto potentially increase the return on investment.Leverage is related to the word leverage as used in physics.You can think of a lever resting on a fulcrum.If you push the short side of the lever down by one meter,the long side will move by more than one meter.For a long/short portfolio,the money that is borrowed for leverage can come from borrowingcash and also from taking short positions in stocks.Recall that to take a short position,we borrow an asset and sell it inthe market with a promise of buying it back in the future.We can use the cash received fromthese short positions in order to buy other stocks in the portfolio.Without leverage, if we invested in a stock,then a 10 percent change in the stock price would lead toa 10 percent change relative to our initial invested capital.With leverage, if we borrowed cash to double our initial investment,then a 10 percent change in the stock price will lead toa 20 percent increase relative to our initial capital.Let's look at an example without leverage.Pretend that we have $100,000 as initial capital.If the stock price increases by 10 percent,then the portfolio gains $10,000.Relative to the initial capital,the investment increased 10 percent.Now, let's look at a scenario where we use leverage.Start with $100,000 of initial capital.Borrow 100,000 in cash.Use the 200,000 in cash to invest in the stock.If the stock increases by 10 percent,then the investment increases by $20,000.Relative to the initial capital of 100,000,this is actually an increase of 20 percent.Since the potential investment percent gain of20 percent is greater than the stock price gain of 10 percent,we say that this position is leveraged.Feel free to pause the video to study this.

### 09. M4 L1A 08 Rescale Part 2 V3-8Ix10U6MEug.en

At first glance, leverage looks awesome because if we take enough short positions,we could technically have enough cash to pay for all of our long positions.But does this mean that more leverage is always better?Unfortunately, the answer is no.If the stock price went down 10 percent,then our leveraged position would go down 20 percent,which is greater than the loss we'd incur without leverage.So, as the saying goes,there's no such thing as a free lunch.Which means that the benefits of leverage come at a cost.Using leverage can increaseour potential gain but can also increase our potential losses.A useful measure of the amount of leverage in a portfolio is the leverage ratio.In terms of dollars,if we added up the magnitudes of the long and short positions,and divided that sum by the notional,this would be the leverage ratio.For example, if a portfolio had a notional or initial capitalof $1 million and had $1 million in long positions,the leverage ratio would be one.If the portfolio had the same notional of1 million but borrowed cash to go long two million positions,its leverage ratio would be two million divided by one million or two.If the portfolio had long positions totaling$1 million and short positions also totaling $1 million,then the leverage ratio would also be two million divided by one million, which is two.If the longs were two million and the shorts were two million,then the leverage ratio would be four.So, I hope you noticed that the more positions we takeand absolute magnitude relative to the initial capital,the higher the leverage ratio.For theoretical portfolio, that's used for analyzing a factor,we assume a $1 notional.We add up the sum of the absolute values of the weights,then divide by the $1 notional to calculate the leverage ratio.Dividing by one doesn't change the sum.So, we can skip the division.For example, if we had three stocks in the portfolio with weights of negative 0.5,positive 0.3, and positive 0.2,the sum of their absolute values is one.So, the leverage ratio is also one.If on the other hand,the portfolio had weights of negative one,positive 0.6, and positive 0.4,then the sum of their absolute values is two.So, the leverage ratio is also two.Let's revisit our goal,which was to rescale portfolio weights,so that we have a leverage ratio of one.To do this, we can sum up the absolute values ofthe weights and divide each of our original weights by this sum.By doing this division,we are re-scaling the factor values so that the sum of their magnitudes equals one.

### 10. M4 L1A 09 Overview For Standardizing A Factor V3-0clT0lnrTrU.en

So, let's revisit the steps we take to convert factor values intoportfolio weights in a dollar neutral portfolio with a leverage ratio of one.To make the factor represent weights of a dollar neutral portfolio,de-mean the vector of values.Do this by subtracting the average from each value.To make the factor represent a portfolio with leverage ratio of one,re-scale the vector of values so that the sum of absolute values equals one.All right, I've talked enough.Let's give you a chance to practice coding up a factor.

### 12. Zipline Pipeline SC V1-DHTwIwVk_sc.en

Hello and welcome. In this lesson,we will introduce you to the zipline pipeline.Zipline is an open-source algorithmic trading simulator developed by Quantopian.In this notebook, we will see how to create a pipeline with screens,factors, and filters on how to run them using a pipeline engine.So, let's get started.So, why do we need a pipeline?One reason is that on any given trading day,the entire universe of stocks consists of thousands of securities.Usually, you will not be interested in investing in all of them,but rather you're most likely to only select a few of them to invest.For example, you may only want to invest in stocks that havea 10-day average closing price of $10 or less.In order to avoid spending a lot of time doingdata wrangling to select only the stocks you want,people often use pipelines.So, in general, a pipeline is a placeholder for a series ofdata operations used to filter and rank data according to some factors.Before we start building our pipeline,we will first see how we can load the stock data we're going to use into Zipline.Zipline uses data bundles to make it easy to use different data sources.In this notebook, we will be using stock data from quote media.In the Udacity workspace,you will find that the stock data fromquote media has already been ingested into Zipline.So, we will only need to load the data.To load the data, we will use zipline's load function from the bundles class.In order to load a previously ingested data bundle,we must pass the name of the data bundle to the load function.In this case, the name of our previously ingested data bundle is eod-quotemedia,which is specified here.The first thing the load function does is to look for the most recent ingested data.Therefore, we must also specify the location of the previously ingested data bundle.This is done by setting the zipline routevariable to the path where the most recent data is located.Before we load the data bundle,we must also register the data bundle and its corresponding ingest function.After we load our data,we're ready to build our first pipeline.We will now start building our pipeline step-by-step.We will start by building an empty pipeline with the screen.To build a pipeline,we will use Zipline's pipeline class.In this example, we have used a screen that selectsthe top 10 assets with the highest average dollar volume within a 60-day window.This screen acts as a filter to exclude data from our stock universe every day.The average dollar volume is a good first pass filter to avoid illiquid assets.This way, we can guarantee thatthe selected assets have enough daily trading volume to fill our orders quickly.It is important to note that this freshly constructed pipeline is empty.This means that it doesn't know yet how to compute anything,anyone produce any values is we ask for its output.The next step in building a pipeline is to add factors and filters.We will now take a look at two types of computations that can be expressed in a pipeline.Factors and filters.Let's take a look at factors first.In general, a factor is a function froman asset at a particular moment in time to a numerical value.A simple example of a factor is a most recent price of a security,because the most recent price of a security is just a number, for example, $10.On the other hand. A filter is a function from an assetat a particular moment in time to a boolean value.Boolean values are either true or false.An example of a filter is a functionindicating whether a security whose price is below $5.This is because at any particular moment in time,this statement will either be true or false.So, the difference between factors and filters is that factors return numerical values,while filters return boolean values.Zipline comes with some built-in factors and filters,but also allows you to combine and create custom factors and filters.Before we learn how to add factor some filters to our pipeline,let's take a look at a nice feature of the pipeline class.A neat feature, a Zipline's Pipeline class,that it comes with the attribute show graphthat allows you to render the pipeline as diagram.This diagram is specified using the DOT language,and consequently, we need DAG graph layout program to view the rendered image.In this notebook, we will use the package Graphvizto render the diagram produced by this show graph attribute.Let's take a look at the current diagram of our pipeline.Right now, our pipeline is empty,and it only contains a screen.Therefore, when we render our pipeline,we only see the diagram of the screen.We can see that our screen takes as input,the closing price and volume from the US equity pricing dataset to calculate the average solar volume in a 60-day window.At the bottom of the diagram,we can see that the output is determined by the expression x_0 less than or equal to 10.This expression reflects the fact that we only selecting the top 10 assets.As we are factors and filters through our pipeline,this diagram will get more complicated.In this diagram, we saw that our screen takes asinput prices and volume from the US equity pricing dataset.So, let's take a moment to talk about the datasets and data loaders.Another feature of Zipline is that it separates the actual source ofthe stock data from the abstract description of that dataset.Therefore, Zipline differentiates betweenthe actual dataset and the loader for that dataset.For example, the loader used forthe USEquity pricing dataset is the USEquityPricingLoader.The USEquityPricingLoader class can also be used to load, open, high,low, close volume data from other data sets,like the one from quotemedia.Therefore, we will set the USEquityPricingLoader as our data loader.Before we add our factors and filters,let's take a look at the raw data in our quotemedia data bundle.This requires a couple of steps.The first step is to build a pipeline engine.This is because, in order to execute a pipeline,Zipline employs pipeline engines.The SimplePipelineEngine class that we've used here associatesa data loader with a trading calendar and a corresponding data bundle.It is important to note that the get loading parameter must be a callable function,and this is the reason we have defined this function right here,that's [inaudible] are pricing loader.We will also use the trading calendar used by the New York Stock Exchange.Once we have chosen our pipeline engine,we're ready to execute our pipeline.We can execute our pipeline by usingthe.run_pipeline attribute from this simple pipeline engine class.In this example, we will run our pipeline for a single day.We can see that the output of the pipeline is a Pandas DataFrame with a MultiIndex,where the first index level contains a trading dates,and the second index level containsthe tickers for the stocks that have passed our screen.This tickers can be accessed and saved into a list.Once we have the tickers for the stocks that have passed or pipeline screen,we can get the historical stock data for those tickers from our data bundle.In order to get the historical data,we need to use Zipline's data portal class.A data portal is an interface to all the data that a Zipline simulation needs.Once we created the data portal like we've done here,we can get the historical data by using the get_history_window attribute.Here, we can see the historical data for the given start and end dates.It is important to note that when a pipeline returns a date,for example, January 7th,2011, this includes data thatwill only be known prior to the market opening on that date.Therefore, the price shown for January 7th,2011 is actually the closing price from the day before.Finally, let's see how we can add factors and filters to our pipeline.We can add both factors and filters toour pipeline using the add method from the pipeline class.The first parameter in the add method representsthe factor or filter we're going to add to our pipeline,and the second parameter is a string that determines the name in the column,in the output DataFrame for that factor or filter.Here, we have added factor that computes the 15-day mean closing price.Now, let's render our pipeline to see what it looks like.We can clearly see our factor in the pipeline now.If we want our pipeline,we now see a column that contains the output of the factor,namely the 15-day mean closing price for each stock that passed our screen.Now, let's add a filter to our pipeline.Here, we've created a filter that returns truewhenever the 15-day average closing price is above $100.Like I mentioned earlier,we can add this filter to our pipeline by using the.add method would use before.Now, let's run through our pipeline to see what it looks like.We can now see our filter in the diagram and if we run our pipeline,we can now see a column that contains the output of the filter with truefor every stock that had 15-day average closing price above $100,and that passed our screen.That's it. Now, you know how to create a pipeline with screens,factors, and filters, and how to run them using a pipeline engine.In the next lesson, you'll get some practice gradingpipelines with custom factors and filters.

### img

## Part 01-Module 04-Lesson 02_Factor Models and Types of Factors

### 01. M4 L1B 01 Intro To Lesson V1-ff0paDNA75U.en

Welcome to this lesson.Now that you've seen an example of a factor and how to create one from data,we'll give you a closer look at the theorybehind these methods by introducing the factor model.This will help you understandthe broader context around what you're doing when you work with factors.After that, we' ll walk you through some major categories of factors.These are factors such as price volume,fundamentals, sentiment, and alternative factors.Ready, ready.Ready. Ready, Liz?I sure am.Cool. Let's do this.

### 02. M4 L1B 02 What Is A Factor Model V4-K5QKPwU38Do.en

Okay. So, we've talked about factors in theabstract and in your head you're thinking, "Okay,it's a vector where the values for each stock areproportional to some aspect of the future performance of that stock."Yes, you're right.When I first learned this,in some ways, it wasn't very satisfying to me.But there's a sense in which that's what you need to know.But I want to give you more because there is more tothe story but that's where it gets more complicated.So stay with me and I promise it will paydividends in your learning for the next several lessons.There is a formalism behind factors,let me tell you a little about it.There's such a thing in the statistical literature called a factor model.A factor model is a statistical model used todescribe variability among observed correlated variables,in terms of potentially smaller number of unobserved variables called factors.This method is actually used in several disciplines including biology,psychology, and business, as well as finance.Whenever practitioners are looking to discover a smaller number ofso-called latent variables that explain correlations in a set of variables of interest.We could apply this type of analysis to many situations,but in finance, we're typically interested in modelling the returns of several assets.We might notice that the returns of some assets seem to vary together.Are these companies similar in some way such that someunderlying effect influences both of them in the same way?What if we could model the returns of a large group of stocks witha smaller set of variables which explain their common variability?This is what we attempt to do with factor models.The observed variables, in our case returns,are modeled as linear combinations of the factors returns plus error terms.So a generic linear factor model looks something likethis: we model the return on a stock,bond mutual fund or something else asa linear function of returns attributed to several factors.The values of these factor returns are the same for all stocks.The factor exposures tell you how much the return ofan individual asset should change per unit change in a factor return.Here, the last term, the error term,is the portion of the return on asset not related to any of the factors.In words, the return of any stock, i,can be decomposed into the returns offactors times the stock's exposures to those factors,plus an unexplained portion.

### 03. M4 L1B 03 Factor Returns As Latent Variables V3-LpHvJq6XTOQ.en

We left off with the idea that the return of any stock, i,can be decomposed into the returns of factors timesthe stocks exposures to those factors plus an unexplained portion.But wait, this is looking a lot like multiple regression.What's the difference?Well, regression is one tool you might use to build a factor model.Also, in a factor model,the independent variables or factor returns are usually things we think are there,but have an influence that we can't measure directly.These are called unobserved or latent random variables.We may need to take other steps to producethese time series before we can run a multiple regression.For example, what if we thinka company's size has something to do with the performance of its stock price?Maybe, smaller companies stock prices produced higher returns?Okay. That seems reasonable,so let's do a regression.But hang on, what data do we have?We have time series of returns for several stocks.You've seen those data before.Okay. So size, how do we measure that?Lets say, we're specifically thinking that market cap is the metric of interests.Market cap is the market value of a publicly traded companies outstanding shares.It's the share price multiplied by the number of shares outstanding.So, we'd have market cap for every company in our universe for every day,because it changes over time.That sounds like the data could be organized in a table or a matrix.But to create a single factor return time-series,we're looking for a single time-series of values thatrepresents the effect of the size factor across stocks.Do you see the challenge we're facing?How do we create a single time series that represents our ideathat small-cap companies generally outperform large-cap companies.Moreover, how do we quantify this effect on the returns of a whole set of stocks?A latent variable is something like this,something nebulous that we want to represent as a time series like a normal variable,but it's hard to directly measure.What we do in this case to create a single time series for our factor,is we create a theoretical portfolio.This portfolio long small caps and shorts large-caps every day.The time series we seek is the portfolio's daily return.This is how we quantify our idea that small caps should outperformlarge-caps and generate a single time series to represent this idea.

### 05. M4 L1B 04 Factor Model Assumptions V3-qEu3m_3eGWk.en

There are a couple of assumptions inherent to strict factor models.The first is that the residual return is assumed tobe uncorrelated with each of the factor returns.This does not restrict the set of possible models as much as it may seem.The factor exposures can be adjusted to achievethe condition where the residuals are uncorrelated with the factor returns.In fact, in simple settings using historic data,this can be achieved with multiple regression procedures.The key assumption of a linear factor model is that the residual forone asset's return is uncorrelated with the residual of any other asset.This means that the only correlations amongasset total returns are those that arise due to their exposures to the factors.The residual component of an asset's return is assumed tobe unrelated to the residual of any other asset,and hence totally specific to that asset.In other words, the risk associated with the residual return isspecific to or idiosyncratic to the asset in question.This assumption makes a linear factor model powerful inthe sense that it rules out many possible combinations of outcomes.But this power comes at a cost.The more restrictive a model is,the greater the chance that it may be wrong.For this reason, if using a strict factor model,it is important to try to account for the most important sources ofcorrelations by including a sufficient number of factors,while also attempting to include only the most important ones.That said, there's also value to simplicity in modeling,since the goal is always to model signal and not noise.

### 06. M4 L1B 05 Covariance Matrix Using Factor Model V3-_qfTLXoifsM.en

Okay. We talked about the assumptions inherent to factor models.Now, let's derive the covariance matrix of the returns,in terms of the factor returns,exposures, and residual term.We need this description of the return variants,because we want to be able to use our factor model todescribe and manage the portfolio variance,and use this information for optimization.We'll need to use the two assumptions; one,that the residual of any asset is uncorrelated with the factor returns,and two, that the residual of any asset isuncorrelated with the residual of any other asset.Before we begin, we subtract from each asset random variable its mean.Such that the mean of this shifted return distribution is zero.We start with this equation,which is a factor model of the stock return.If there are many assets then r is a vector of observable random variables.What I mean by that is,r1 is a random variable that represents the return of asset one.It can take on any value with a probability determined by a probability distribution.R, is a vector of length equal to the number of assets.B, is a matrix of fixed that is not random coefficients,it has dimensions of assets by factors.F, is a vector of random variables,each of which represents the value of a factor return.Finally, this is the vector of residuals.Which as we said, are specific to the assets.So, there is a residual random variable for each asset.Now, we want to calculate the covariance matrix of the asset returns.Remember, that the covariance oftwo random variables can be calculated with this formula.But if the means of the two random variables are zero,this simplifies to the expectation value of their product.The expectation value of a matrix is just a matrixwith the expectation value operator applied to each element.The matrix of products of asset returns,is just rr transpose.So, the covariance matrix can be written as the expectation value of rr transpose.This follows by our model for r.First, distribute the transpose to the two terms on the right.Here, we just expand out the matrix product.Now, distribute the transposes.We can pull the B matrix out of the expectation function,because it is not a random variable, it is fixed.Here, we use the fact that the residual errorsand factor returns are assumed to be uncorrelated.The first assumption we discussed earlier.This means that the expected value of the factor returns times the errors is zero.So, the two terms in the middle becomes zero.We will now simplify our notationto represent the covariance matrix of the factor returns.Finally, we write down the covariance matrix of the residuals,which we know to be a diagonal matrix.Meaning that only the variances of the residuals may be nonzero,but the covariances between residuals of different assets are zero.The reason for this, is the second assumption from earlier;that the error random variables are assumed to be uncorrelated across assets.So, their covariances are zero.

### 07. M4 L1B 06 Factor Models In Quant Finance V2-VeM2SudgZqc.en

Okay. So, we've learned a little about factor models.So, how are they used in practice?It might not be what you expect.Most practitioners don't use factor models to explicitly model asset returns.Factor modeling has a long history in academic research,and some of these methods were used in the past to explicitly model returns time series.But when devising and testing trading strategies,the models are typically used slightly differently.We've seen the equations for returns and for risk in terms of factors.One key goal is that we want to be able to usethese expressions in a portfolio optimization problem.Now, let me tell you about a couple of simplifications that quants make at this point.Let's take the matrix of factor exposures.As we've said, a value in this matrix representsthe sensitivity of an individual asset to a specific factor return.A portfolio with weights x has a portfolio factor exposure of B transpose x.Now, let's introduce a new idea.Let's say we think there are actually two types of factors.One type of factor is predictive of the mean of the distribution of returns.The other type is predictive of the variance of the distribution, but not of the mean.The first describes our alpha factors,while the second describes our risk factors.We'd rather our portfolio be minimally exposed to risk factors.The drivers of volatility.We can try to make this happen by placing constraints onB transpose x that only apply to factors that we think are drivers of volatility.We do this typically in the constraints section of the optimization problem.This will specifically reduce the exposure of our portfolio to these risk factors.However, we don't constrain the factors that are drivers of mean returns.So, we drop them from B.Hence, you can think of B now as the risk factor loading matrix only.The alpha factors aren't kept in here because we're notgoing to constrain them the way we do with the risk factors.What about F and S. We think about F asthe covariance matrix of factor returns that havelarge impact on variance across all stocks,and S as the variance that's left over.In fact, we define risk factors precisely asfactors that have large impact on variance across all stocks.So, we include only the risk factors in F. Anything that's left over,like variance from the alpha factors we took out and everything we can't explain,is accounted for in S. We can use F and S together to constrain portfolio risk.The key takeaway here though is that, in practice,B and F explicitly contain only information about risk factors.S says nothing explicitly about alpha.Practitioners will usually buy F, S,and B from a commercial provider,and now we can see why that is sufficient.We typically don't mix our alphas and risk factors.What about the alpha factors we took out from B?We are left with some number of alpha factors which are values per stock per factor.What do we do with those?This is where the creation of the objective function and optimization comes in.We need to combine these into a single vector,which we will do in later projects,and optimize our resulting weight to these.

### 08. M4 L1B 07 Risk Factors V Alpha Factors V2-9KUpH1MDC1k.en

In the coming series of lessons,we'll go into detail about risk factors and alpha factors.So, what's the difference,and why do we make this distinction?First, let's start with how factors,whether risk factors or alpha factors, are the same.The use of factors this is based on the assumption that stockswith similar characteristics may move up or down in similar ways.For example, stocks may exhibitsimilar price movements if they are also within the same sectoror are similar in market cap or based in the same country or have similar fundamentals.Factors represent these common characteristics such as sector,market cap, or market return or the country or book-to-market ratio.If we model the return of a stock as the sum ofthe contributions from risk factors and alpha factors,then we notice that each of these factors,both alpha and risk factors,contribute to the stock return.In other words, each factor is adding a little bit to the movement of the stock price.However, in practice, risk factors andalpha factors are used to accomplish different goals.We want to use factors to help us learn somethingabout the distribution of expected future returns.Factors could be predictive of the mean of the distribution of expected returns or not.They could also help to explain a significant amount ofthe variance of the return distribution or not.Factors that significantly help explain the variance ofthe return distribution are candidates for use as risk factors.We want to find a set of risk factors that explain much of the variance of a portfolio.The reason is so that we can adjust portfolio weights toreduce that variance that's caused by these risk factors.Reducing the movement of a portfolio that may be attributed torisk factors is how we control portfolio risk.One major risk factor is the market return from the capital asset pricing model.We saw earlier how adjusting portfolio weights to make them dollar neutral also helpsto make a portfolio less sensitive to overall market movement thereby controlling risk.Factors that are significantly predictive of the mean of the return ofdistribution are candidates for use as alpha factors.We want to find alpha factors that are predictive offuture price movements after we've neutralized price movements using a risk model.For example, if a factor signals thatthe mean of the expected return distribution is greater than zero,then this signal may indicate that the stock return will bepositive in the future which also means it's a signal to buy.One example of a former alpha factor is the market cap of a stock.Small-cap stocks tend to have higher returns compared to large cap stocks.Factors that are neither useful in explaining the variance nor predictive or the meanof the return distribution would probably not be used as either alpha or risk factors.One example would be days when there are full moons.Even though a full moon may tell you when toexpect werewolves or other magical creatures to appear,it probably wouldn't tell you much about which stocks to long or short.

### 09. M4 L1B 08 Risk Factors V Alpha Factors Part 2 V2-AApfsuSpnMY.en

We generally observe different attributes among risk factors versus alpha factors.Even though factors fall along a graded incremental spectrum of these attributes,it helps to put them into two groups to geta sense of how risk factors and alpha factors differ.One difference is in the magnitude ofthe factors contribution to the stock's price movement.When we choose a set of20 or so risk factors that explain a good portion of the variance of stocks,then taken as a whole,these risk factors tend to be major contributors to the price movement of most stocks.These are factors such as the market return,the country in which the company operates,the sector or line of business,and interest rates set by central bank.In comparison, a single alpha factor mayhave a smaller contribution to a stock's price movement.As an example, the book-to-market ratio multiplied by idiosyncratic volatilityor the trajectory of a stock's return over time are examples of potential alpha factors.You will actually get an in-depth look at these two alpha factors in a later lesson.But for now just assume that we're talking about two examples of alpha factors.The contribution of these alpha factors may be statistically significant,but in terms of magnitude,their impact on the variance of the stock may besmaller than that of a set of common risk factors.This is an important reason for why we want to finda useful set of risk factors and neutralize our portfolio's exposure to them.If we didn't, then the portfolio's movements due to these risk factors wouldlikely be so large as to overwhelm the effects of the alpha factors.

### 10. M4 L1B 09 Risk Factors V Alpha Factors Part 3 V1-UmdOVhcRCVU.en

Another descriptive attributes of common risk factors,is that they are well known by the investment community.This is not just a descriptive attribute,it also affects how risk factors drive price movements.When most of the investors in the market understandhow a factor drives the price movement of a stock,then it's unlikely that anyone will gaina competitive advantage in generating higher than expected returns based on that factor.Why is that? Because investors who are seeking to maintaina competitive position will monitor the movements of these risk factors,and we'll adjust their portfolios in response to changes in these factors.Researchers have looked into how the performance of funds that useparticular factors declines after those factors are widely published in academic papers.For example, the findings that small-cap stocks tend to havehigher returns than large-cap stocks is also known as the size effect.Research that details this effect was first published in 1981.A mutual fund was actually created to make use of the size factorand it was called the dimensional fund advisor small company portfolio.In a study conducted in 2003,researchers showed how this particular funds performance after the 1981 publication,no longer generated abnormal returns based on the size factor.One likely explanation, is that the act of publishingthe size factor and its ability to generate abnormal returns,made the size factor well-known to practitioners at investment funds.The practitioners then acted on this knowledge and over time,traded away this mispricing.That is one consequence of being risk factors.They're not likely to be used to enhance the portfolio return.In other words, risk factors may give an indication for how muchthe portfolio stocks bounce up and down like boats without sails,moving up and down with the ocean waves.However, just as a boat without salescan't drive its average position in a particular direction,an investor won't use risk factors to drive their returns in a particular direction.To summarize the idea,we say that risk factors are drivers of portfolio variance,but not drivers of the portfolio's mean return.

### 11. M4 L1B 11 How An Alpha Factor Becomes A Risk Factor Part 1 V3-p0cTudt8kXI.en

Alpha factors eventually lose their performanceas they become more publicly known among investors,and then eventually become risk factors.Let's look at an analogy to help frame our discussion of how this happens.Let's say I drive to work and I normally deal with a lot of traffic.So, I download this cool new app that tells me which lane I should choose,that will let me get to work faster.Since I'm one of the few people who has downloaded and used the app so far,I'm able to find lanes that have fewer cars andmove through traffic a little bit faster than the other drivers.You can think of this app as my alpha factor,because it's helping me improve my performance on the road.Now let's say this app becomes really popular.So, most other drivers now use it to help them find the fastest lane.Now, when the app tells me to take a particular lane,many other drivers next to me also get the same signal,so they also choose that same lane.Many drivers actually get into the lane before or at the same time that I do,so I no longer have the ability to get ahead of the other drivers.Moreover, since everyone is following this app,the app is now driving the variance of these cars on the road.You can imagine many cars now constantly changing lanes concurrently,as a group like a school of fish, and yes,I just used a fish analogy to explain the car analogy,which in turn I'm using to explain alpha factors.We can now think of this driving app as a risk factor,because it is driving the movement of many cars, and therefore,the variance, but is no longer helping any of us improve our individual performance.

### 12. M4 L1B 12 How An Alpha Factor Becomes A Risk Factor Part 2 V1-9waaTtRaU-Y.en

We can see a similar process in the stock market.If you find an Alpha signal that suggests that a stock will have positive future returns,the assumption of future higher returns means that the pricetoday is lower then the expected future price.In other words, its current market price is cheap according to the signal.If you buy it now at this price,and if your hypothesis was accurate,you'll be expecting the price to rise in the future.Now, what if lots of other investors also findthe same signal and also decide to buy the stock now?The buying activity will push the stock price higher.So, by the time you get around to buying yourself,the price has already risen.If you buy it like everyone else,you may find that it doesn't produce the positive return you were hoping for.This is an example of the efficient market hypothesis,which states that assets are fairly priced based on publicly available information.The factor is no longer helping to drive the mean return of our investments.Instead, the factor is triggering the market price to move becauseit is known and being used for trade decisions by many other investors.So this factor is now driving the movement of the stock.Every time the factor signal changes,the market price also changes soon after.We now say that the factor is a risk factor.So in summary, good Alpha factors enable us to findmispricings and seek a competitive edge in the market,which drives the mean return of a portfolio.When Alpha factors become two well-known,then they end up triggering the market to move as a changes,which drives the variance of our portfolio's return,but no longer the mean of the return.So now it's a risk factor.Market participants historically have had to use their judgment and experience to decidewhich factors to use as Alpha factors and which factors to use as risk factors.Can we use AI to do this instead?Yes we can. We'll go in depth into Alpha factor combination in term two.

### 13. M4 L1B 13 Momentum Or Reversal V3-izTAHVF6V_g.en

Factors can be broadly categorized as either momentum or reversal factors.We've previously referred to reversal as mean reversion.A momentum factor suggests that an existing trend will continue.A reversal factor suggests that a trend will change and go in the other direction.Say we have a hypothesis thatthe trend of stocks one year return will continue in the same direction.In other words, winners keep on winning and losers keep on losing.Let's call it a one-year return momentum factor.When the one year return is higher for stock A compared to stock B,this indicates that we expect stock A to havea higher near-term return compared to stock B.Now, let's look at a possible reversal factor.Say our hypothesis is that weekly stock returns are being reverting, that is,when a stock increases over a week,we expect that it will give up some of those gains due to profit taking.What I mean by profit taking is that other investors maysell after recent price gains in order to lock in their profits.So, the weekly return for stock A is higher than the weekly return for stock B,then we expect future returns on stock A to be lower compared to stock B.Similarly, if the weekly return of a stock is negative,our hypothesis assumes that the stock may be oversold.It might trend backup as other investors try to buy the stock at a cheaper price.So, to write these calculated returns as factors,the momentum factor is just the one year return,whereas the reversal factor is the negative of the weekly return.Remember, that a factor is some manipulation of raw data plusa hypothesis on what that data means interms of the relative future performance of the assets.

### 14. M4 L1B 14 PriceVolume Factors V2-zaG0PDc3wsA.en

One of the most commonly used data sources in quant rating is the stock price and volume.We will refer to this as price-volume,but you may also hear it referred to as technical factors.The raw data for price-volume can includeanything that we learn from a quotes and trade feed.For example, adjusted and unadjusted prices,the open, high, low,close for each time period.Different time frequencies such as day, week,hour, minute, and bid-ask price quotes.There are operations that we can apply it tothe price volume data in which we also defined the window length,which is how many days worth of data to use in the calculation.We've already seen returns,which can be daily, weekly, monthly, or yearly.Returns can also be calculated at shorter time scales such as seconds, minutes, hours.Returns can be calculated from the close of one day to the close of the next day.Returns can also be calculated on the open price to close price for the same day.For example, the open price in the morningto the close price of the afternoon on the same day.Returns can be calculated on the close price ofone day to the open price of the next day.For example, the return from the close price ofMonday evening to the open price of Tuesday morning.The last example is referred to as overnight returns,and we'll discuss this in detail in a later lesson.We can also apply operations on a distribution of returns.For instance, we can calculate the first four moments,which are called the mean,variance, skew, and kurtosis.The mean describes the center of the return distribution.The variance describes the spread of the distribution.The square root of variance,which is the standard deviation,is a common measure of return volatility.Skew describes the amount of asymmetry.A positive skew, means there aremore extreme values in the positive side of the distribution.Kurtosis describes how much of the distribution occurs in the left and right tails.Stock returns distributions tend to exhibitlarger tails compared to a normal distribution.Kurtosis is often referred to as fat tails.There's also the minimum or maximum over a certain time window.Note that we'll discuss skewness in a later lesson about alpha factors.So, to summarize, pretty much anystatistical or time series calculation you can think of,is fair game for use in a price-volume driven factor.Using price and volume as a factor,has a benefit of having readily available data,since stocks trade on exchanges and many market data vendors sell trades,quotes, and bar data.The availability and regularity of the data source is nice to have,since other sources for factor generation suchas fundamentals are updated less frequently,and still other data sources such as news, social media,or analyst reports may not always be available for the stock that we're analyzing.It's very helpful to have a factor that can generatea signal for every stock in the stock universe.This is especially helpful in quant investing,since quant investing usually involves portfolios of hundreds or thousands of stocks.Keep in mind that the higher frequency of price volume data often leads to more trading,and therefore higher portfolio turnover.Since trading decisions are based on the data,the more often the data updates and changes,the more often the portfolio is likely to require rebalancing.This can be good or bad and requires careful study.Faster signals means more information.More information, all other things being equal, is good.However, more rebalancing means more trades and more transaction costs.So, strategy that is based onfrequently updated data generally has more transaction costs.As a researcher, a key thing you need todetermine is if the higher information content of the signal,more than offsets, the likely higher transaction costs.

### 15. M4 L1B 15 Volume Factors V1-1dTAV3Irxv4.en

Volume-based factors usually include price information,to categorize the volume in a given time period as a net buy or net sell.At first glance, the idea of net buying or net selling,may appear counterintuitive when you recall,that every transaction involves a buyer and a seller.Some examples will help to clarify what we mean by this.A volume factor may track whetherthe total trading volume of a stock is higher than normal,and use that to determine whether recent price movements are significant or not.For example, sometimes stock exchanges are open on or around a major holiday,and since fewer investors are participating in trading,volume may be lower on these days.So investors will take the low volume into consideration,when deciding whether price movements are significant or not.Volume can help add more context to a price-based factor,such as a moment factor.For instance, unusually high volumes that accompanies a major price movement,may be a sign that the momentum signal is more significant than usual.One could say that the price-based momentum factor,is conditioned on the volume information.One piece of volume data that is independent of price is called short interest.Short interest tracks the quantity of a stock shares,which are held short.When a trader shorts a stock,they borrow the stock from its owner and sell it,with a hope of buying the stock back at a later date when it's cheaper.If instead, the stock price increases,the short seller will realize mark to market losses.Mark to market is an accounting concept,that assess the value of a health asset at its market price.So mark to market losses,are unrealized losses that are tracked in accounting,even if the asset isn't sold to realize those losses.This can become expensive if the stock price keeps rising,so short-sellers may decide to cut their losses,by buying the stock at the market price,and closing out their position.This is called a short squeeze.If this sounds a bit complicated,consider the mirror image of a short squeeze,which is a long squeeze.If you bought stock in a retail company,and watch the stock price decline day after day,you might have decided at some point to cut your losses and sell the stock.This long squeeze is the mirror image of a short squeeze.When a short squeeze occurs due to an upward price movement,the additional stock buying that it triggers,tends to add more upward momentum to the stock price.So if recent short interests is higher than usual,this might be considered a buy signal.Since an increase in price,may trigger a domino effect of additional increases.This could motivate a nice Alpha factor.Find stocks that have high price momentum,and are highly shorted.In this case, short interest data could act as a conditioning information,to make a momentum signal stronger.

### 16. M4 L1B 16 Fundamentals V1-rPii5-ry8nc.en

The other commonly used data in factor construction are called fundamentals.These are measures derived from financial statements such as price to earnings ratios.Price volume factors andfundamental factors are the most commonly used factors for quant trading,and it's possible for some active fund managers torely primarily on these two types of factors.A practical difference with fundamental factors compared to technical factors is thatfinancial statement data is usually updated on a quarterly basis, not daily.This means that if a fund is trading at a daily frequency,they use the most recent fundamental data forthe three months until it's refreshed in the next quarter.Fundamental factors allow for higher capacity,which means that one can potentially put more capital on that particular trade.This is in part due to lower turnover,as we would only update our portfolio whennew financial statements arrive every three months.Lower turnover generally means lower transaction costs,which means we can put more capital behind a strategy based on fundamental factors.One factor you've seen before is the market cap,which is a measure of the size of a company.Generally, small cap companies have historically outperformed large cap companies.So strategy may involve overweightingsmall cap stocks and underweighting large cap stocks.

### 17. M4 L1B 17 Fundamental Ratios V2-Eo-faV9CsP8.en

Many fundamental factors are different variations of the price to earnings ratio.Note that [inaudible] typically use the data measured as earnings pershare divided by price instead of price divided by earnings.Why? Earnings could be zero or close to zero.So, the data using price divided by earnings may endup with numbers close to infinity or even null values.Using earnings divided by price helps to reduce these kinds of data issues.The book to price ratio may bea good alternative to earnings divided by price because earningsmay be negative while book to price ratios remainpositive as long as the company's net asset value is positive.Other variations of fundamental ratios try to remove accounting choicesfrom the metric by looking at direct cash flows instead of earnings.Example of these include cash flow,earnings before interest, tax depreciation, and amortization.You don't need to dive into accounting for this lesson,but it helps to get a sense of how earnings are different from cash flow.Cash flow is the amount of cash flowing into and out of a company.The cash flow can be a more volatile metric than earnings,but it's also more difficult for company executives to manipulate.Cash flow maybe more volatile due tolarge upfront capital costs such as buying expensive equipment.Accounting smooths out the earnings by spreading outthe cost of these purchases over the lifetime of the asset.So, earning numbers are more smooth and less jumpy compared to cash flow.The earnings numbers come from the interpretations ofaccounting rules and there can be some judgment in applying these accounting rules.This is not the case with cash.With cash, it's either there or it isn't.Since accountants decide how to spread out the initial purchase over time,there is a human element in earnings that may add noise to the data.Whereas cash flows are a more direct measure of the state of the company's finances.

### 18. M4 L1B 18 EventDriven Factors V1-2mnwjChH2hg.en

The previous factors that we've seen,such as price and volume, book-to-market,and analyst forecasts, tend to occur at regular intervals,whether it's daily or quarterly.Another class of factors are based on events that may not bescheduled and may have a significant impact on the stock price.For example, when the Deepwater Horizon oil rig exploded in the Gulf of Mexico in 2010,British Petroleum saw its share price drop from$60 to under $30 per share over the next 40 days.In 2016, when Microsoft announced its intention to acquire LinkedIn,the share price of LinkedIn jumped almost 50 percent on that day,while Microsoft shares decreased by three percent.Events can be either macro level events such as natural disasters,major changes in government,or changes in interest rates by central bank.Events can also be corporate events such as mergers and acquisitions,spinoffs, and new product announcements.Still other common events are index ads orindex deletions and even weight changes for major indices.Many kinds of events are one-off and rare events, but some,such as earnings releases, earnings guidance,and product announcements, can be expected with some regularity.

### 19. M4 L1B 19 Index Changes V1-C7QNfPZBXXo.en

Index adds can drive buying of the stock that was added to a major index,while index deletions can drive selling of that stock. So, why is that?You'll recall, that an index is not a fund.So, what causes this buying and selling when an index changes?Recall that funds and ETFs,particularly passive funds that track a major index,want to create portfolios that match that index.So any changes in the index requires a change in the funds portfolio.Even Spark Beta or actively managed funds may consider adjusting their portfolios afteran index add or deletion is announced sincethey may use that particular index as their benchmark,and the stock universe for the portfolio may depend upon what's in the benchmark.For example, when Monsanto was acquired in June of 2018,the S&amp;P 500 removed Monsanto and added Twitter.Twitter's stock increased by about five percent after the announcement as funds andETFs that track the S&amp;P 500 bought Twitter's stock in order to match the index.We could follow the news for index adds and buy the stock anticipating thatother funds who need to buy that stock will addmomentum to the price increase in the short term.Similarly, index deletions could be a signal to sell or sharethe stock since funds will also selltheir holdings to match the index that they're tracking.As always, you need to use care and judgment here as it is likely thatthis information is very well known and therefore might not be a strong Alpha factor.

### 20. M4 L1B 20 Pre And Post Event V1-Olz9QZQaBxs.en

Event driven strategies are often combined withfundamental and sentiment factors which help us interpret the significance of the event.For example, companies typically will release their earnings once every three months.The dates are scheduled in advance so there are opportunities tomake trading decisions both before and after the event.For pre-event trading, we could use analyst sentiment and fundamentals todecide if we expect the earnings release to exceed or fall short of expectations,then we can buy or short before the event.For example, after analyzing financial statements,we may think that the news or analyst sentiment is overlypositive and hypothesize that the actual earnings will be less than what's expected.So this would be a signal to short or sell.For post-event trading, we could determine whetherthe actual earnings release was better or worse than expected.This is also referred to as an earnings surprise.For example, if the actual earnings were abovewhat was expected based on analyst's ratings and fundamentals,then this could potentially be a buy signal.We could also combine this with price movementsimmediately following the earnings release todecide whether other investors consideredthe event to be positive or negative for the stock price.In other words, we could treat the price movements immediately followingan earnings announcement as a measure of investor sentiment towards the stock.To summarize, events in combination with price,volume, sentiment and fundamentals may inform a strategy.One commonly known strategy,which doesn't work so well anymore because it is very well known,is the post earnings announcement drift.When firms announced earnings,if earnings are above or below expectations,they're referred to as earning surprises.If we assume an efficient market,we would expect the market price to adjustquickly and stabilize after an earning surprise.However, in reality, is often possible thatprices continued drifting in the same direction,upward for a positive surprise or downward fornegative surprise for about two months after the event.Even though this factor is well-known and therefore are notas likely to be used to see abnormal returns,it is instructive to study this phenomenon to getsome intuition of a market mechanics and behavioral psychology in the markets.

### 21. M4 L1B 21 Analyst Ratings V1-cHkJo8qBKes.en

Analysts factors are derived from the published reportsof sell-side research analysts at investment banks.Their reports include ratings such as buy, hold, or sell.They also include earnings estimates or price targets for the stock.Sell-side analysts may be assigned to a few companies within the same industry,and may even meet with the management of the companies that they evaluate.This research is intended forbuy-side investment firms such as mutual funds and hedge funds,who execute their trades with these sell-side investment banks.Since many large investment firms value the opinions or research analysts,their published analysis can have significant effects on stock trading.If you read financial news and the news mentioned thata particular stock has a buy or hold rating,this is likely an average rating of multiple research analysts.We may also refer to analyst ratings as a kind of sentiment factorbecause each research analystsaggregates their information about the company's financials,it's growth prospects as well as its current price,and then summarizes this into the analysts overallconclusion or sentiment about that stock.Investment banks choose their own rating scale.For example, one has buy, neutral, or sell.Another has overweight, equal weight,underweight, or more volatile.Yet another has trading by, recommended list,market out performer, market performer and market underperformer.Sell-side ratings are usually aggregated tocreate a consensus sentiment about a particular stock.It's also useful to focus on when analysts change their ratings.For example, if multiple analysts startchanging their ratings of a stock from hold to sell,we might expect investors who follow analyst ratings to start selling that stock.Analyst ratings may experience a heard mentality.It's difficult for a single analyst to publish a sell ratingwhen most other analysts in the industry are publishing a buy rating.In other words, it's safer to be wrong when everyone else is also wrong,but more costly for an analyst to be wrong when everyone else is right.So instead of taking a simple average of analyst ratings,it may be useful to focus onthe most reputable star analysts and give their ratings more weight.One way to combine analyst sentiment into a trading signal is to aggregateall the ratings that have changed over a certain period such as the past three months.If we count all the ratings upgrades and subtract all the ratings downgrades,then we have a number that is either positive, negative or 0.A positive number represents a positive sentiment,whereas a negative number is a negative sentiment.To rescale this sentiment between negative 1 and 1,we could divide this number by the total number of analysts that we're tracking,whether they upgraded, downgraded or cut their rating the same.

### 22. M4 L1B 22 Alternative Data V1-p6NxGZnkrdc.en

In the history of finance,there has been a continued search to findnew sources of information and to derive meaning from them.For instance, before the 1980s,stock traders used to get their financial data from newspapers.However, stock exchanges began to use computers to automatetheir record keeping and Bloomberg terminals wereinvented to deliver financial data directly to trading desks.Similarly, finance professionals had been collecting and analyzingdata that goes beyond financial statements and stock quotes.These include data collected on the Internet such as social media or online reviews.This also includes satellite images,consumer transactions, and mobile app usage.Any data that are relatively new sources for generatingtrading signals such as data not including price volume,financial statements, or analysts reports are referred to as alternative data.

### 23. M4 L1B 23 Sentiment Analysis On News And Social Media V1-Jph7h2Yl0yg.en

If you are starting out in the field of finance andyou ask any experienced portfolio manager,investment banker or research analyst for advice on how to improve your skills,it's very likely that one of their suggestions would be to read financial news.Most people working in financial services readfinancial and business news regularly in order to stay in touch with the markets.Watch for trends and look out for unexpected events that may affect their portfolios.The rise of social media has createdanother source that we can use to measure the pulse of the markets.For example, negative tweets about accompany whether thereby investors, or customers,may provide a lead for investors signaling that it may bea good idea to follow up and analyze that stock in more detail.Companies that receive lots of positive attention from both news,media and social media may have a higher chance of being over bought.As much of the general public hear as positive mentions about a particular company,they may also jump in to buy the stock hoping toride the wave of a stock that appears to just keep going up.This may push the stock to a price that's above what's supported by fundamentals.At which point the stock may be a potential candidate to short sell.Notice how we don't have to assume that every news article ora social media post is the definitive truth about the health of a company.This data is really measuring the sentiment of people whether they're investors,customers, or public figures with lots of followers.Financial analysts have been applying natural language processing orNLP on news and social media to analyze public sentiment towards companies.In other words, they're trying to turn raw text intoa signal about whether people have a positive or negative view of a stock's future.This is similar to tracking the sentiment of financial research analystsexcept with news and social media there isn't always a clearly labeled buy,hold or sell rating.NLP can be used to estimate how likelya news article blog post or social media posts can be categorized as buy, hold or sell.In term two of this program,you will learn how to use NLP and deep learning to analyzesocial media data and apply these skills in order to generate Alpha factors.

### 24. M4 L1B 24 NLP Used To Enhance Fundamental Analysis V1-9zMWuZ9j7rI.en

Advanced natural language processing techniques arebeing used to enhance fundamental analysis.Fundamental analysis is the kind of work done bytraditional investment analysts who work for financial institutions,like mutual funds and brokerage firms.Much of the text-based analysis that fundamental researchers perform on companies,whether it's reading financial news,reading quarterly earnings reports,listening to earnings calls,or reading forms submitted to regulatory agencies,this work can be enhanced with natural language processing.For example, a human analyst usually tracks 10 to 20 companies.Computers could act as a first-pass filtersifting through the same data source for thousands of companies.The automated methods could filter,categorize, and label the information.This could help fundamental analysts decide which companies orinformation sources to prioritize for further analysis.So, the pre-processing performed with NLP could support andenhance the existing workflow of fundamental research analysts.One can also look for insights inthe required paperwork that companies send to government regulators.In the United States,the Securities Exchange Commission or SEC receives10-K forms from companies once a year and three 10-Q forms per year.The 10-K details the companies view of its business,its past financial results for the year,as well as its current or potential future business risks.One potential use of these 10-Ks is to track how a company's business evolves over time.For instance, one study used natural language processing to analyzeAmazon's 10-Ks from the years 2007 to 2016.The researchers attempted to determine which sectorsthe company most closely resembled based on its 10-K reports.In 2007, the company's description ofits business most resembled the software and hardware sectors.Over time, the company's description based on it's 10-K forms resembled moreof the retail and software sector and to some extent the media sector.You may be wondering how we could make use of this information.The sector that companies closely resemble arefactors that help explain the price movement of their stocks.You could imagine that retail stocks may be correlated to some extent,and technology companies may also exhibit similar price movements.In a later lesson,we'll learn how the sector is a particularly important factorand we'll learn about adjusting a portfolio to be sector neutral.Another application of natural language processing on 10-K forms is sentiment analysis.You may be familiar with sentiment analysis on movie reviews or restaurant reviews,usually to categorize user-generated text as positive, neutral, or negative.Sentiment analysis can be used with other categories as well.For instance, one could apply sentiment analysisto estimate how much little risk a company's 10-Kdescribes or how much uncertaintythe company faces from competitors, customers, or suppliers.One could imagine creating factors based on positive outlook,negative outlook, little risk,business uncertainty, or any number of categories.These labels could also be used to create Alpha factors.In fact, you will be applying natural language processingon corporate documents in term two of this program.For now, we want to first build-upa comfortable foundation in factor risk modeling and Alpha factor research.This will give you the context to understand whereNLP and deep learning techniques can fit into the quad workflow.

### 25. M4 L1B 25 Other Alternative Data V1-hMw3AuPVSSs.en

Alternative data could be generated by scraping social media profiles of companies,tracking building permits or hospital purchases,and analyzing satellite imagery to name a few examples.Construction companies file permit requests withlocal governments when installing components ofa house and also when upgrading components.This information could be tracked to better understand the housing market.Some hospitals provide information about purchases of medical equipment and supplies.This could also be used to assess the sales ofcompanies that supply medical equipment to these hospitals.These days, companies have public profiles in major social media platforms.Information about their employee count, job postings,employee ratings of the company,store locations, number of social media followers,number of active users of their mobile apps,number of check-ins by customers,and so on, these can be tracked over time to gain insights into a company.An interesting hypothesis is that companies,which are struggling with reduced sales,may be more likely to increase marketing on social media.So, it's possible to look forrecent increased marketing efforts as a potential alpha factor.Satellite images can also be used to get a high level overview of parking lots,mining sites, oil rigs,shipping ports, and construction zones.A retailer's customer base can be estimatedfrom counting how many cars visit their retail stores.Empty parking lots likely suggest fewer customers,full parking lots suggest more customers and potentially more sales as well.In fact, the parking lot data generated by a startup, Orbital Insight,closely tracked the downward trend of JC Penney's stock price from 2012 to 2017.Crude oil storage can be tracked by analyzing satellite and drone images.Now, you may be wondering why oil storage is relevant if you're trading equities.Well, crude oil storage is tracked closelyin order for oil traders to set prices on a barrel of oil.Storage of commodities in general gives an indication ofhow much is being produced relative to how much is being consumed.Prices are based on both supply and demand.Storage levels and changes instorage levels provide a signal about both supply and demand,and therefore a signal about the price.Price changes of oil trickles down to impactthe operation costs of any businesses that rely on petroleum,jet fuel, gasoline, diesel,heating oil, and other products that are derived from crude oil.These include airlines, trucking,shipping, and utility companies to name a few.In the US, oil traders closely monitorthe weekly oil and natural gas storage reports issuedby the Energy Information Administration or EIA.The EIA surveys crude oil stockholders,which may use floating roof tanks,fixed roof tanks, underground caverns,trains, trucks, and pipelines to store crude oil.Some companies have also looked into tracking oil pipelineflows in order to estimate changes in crude oil storage levels.With aerial footage, it's possible to track floating roof tank storage levels.Floating roof tanks are cylinders with a roof that floats on top of the stored oil.Computer imaging can track shadows to estimate the height of these roofs andsubsequently estimate the volume of crude oil stored in a floating roof tank.Orbital Insight produces energy storage estimates based onhis analysis of this image data for the US, OPEC, and China.

### 26. M4 L1B 26 Summary V1-yuLQA24Thms.en

Hey, you made it.You've learned quite a lot in this lesson,from the principles of factor models to the covariance matrix of factor returns,to the application of factor models and quant finance.Yes. Congrats on building up your understanding of factors.You've also learned about the distinction between alpha factors andrisk factors and have an overview of various data sources from which to derive factors.This foundation will help you inthe upcoming lessons where you will learn about risk factor models,alpha factors, and advanced portfolio optimization.Yes. Keep up the good work.See you in the next lesson.

## Part 01-Module 04-Lesson 03_Risk Factor Models

### 01. M4 L2A 01 Intro V1-DgsD3yL9Yy0.en

Hi there. So far,you've been introduced to the concept ofa factor and the framework of the multifactor model.We've also touched upon how in practice we defined some factorsas risk factors and other factors as alpha factors.In this lesson, we will focus onrisk factors and the fundamentals of the risk factor model.Risk factor models are used to describe the volatility or risk ofassets such as stocks based on the movements of the risk factors.The goal of a risk factor model is to measure,control and possibly neutralize a portfolio's exposure to major sources of risk.In order to achieve this goal,we need a way to model the portfolio's risk in terms of the common risk factors.To model a portfolio's risk,we want to model individual asset variances andpairwise asset covariances in terms of these risk factors.To model asset variance and covariance in terms of risk factors,we need a couple pieces of information;the variances and covariances of risk factors,the factor exposures which measure how exposed each asset is to each risk factor,and the specific variance of each asset that is not explained by the risk factors.To calculate the variances and covariances of the risk factors,we also need to calculate the returns of the factors which are called the factor returns.We'll also make use of the asset returns which will help us either estimatethe factor exposures or the factor returns depending on the type of risk model.We can then use these pieces of information toestimate the specific variants of each asset.To start off, we also should decide what factors we'll use in our model.By the way, don't worry if these terms don't look familiar yet,you'll see a couple of ways of getting all the pieces that go into a risk model.So along the way,you'll see what these words mean.We'll begin with the motivation for using risk factor models.Factor models have some benefits overmodelling portfolio risk directly from the covariances of its individual assets.Next, we will cover the framework for the risk factor model.The risk factor model attempts to explain the variance ofa portfolio as the sum of two parts;the variance that can be attributed tothe common risk factors plus variance that is not explained by those risk factors.I just want to point out that risk factor modeling combined withalpha factor modeling are both crucial to successful portfolio optimization.Just as a car needs both a brake pedal and an accelerator ora good basketball team needs both a good defense and also a good offense.An effectively optimized portfolio needs to both control risk using risk factors,while seeking higher than expected returns using alpha factors.

### 03. M4 L2A  02 Motivation For Risk Factor Model V2-jAQRjxK8PyQ.en

We've previously seen how volatility is a common measure of risk.Variance is just the volatility squared.If we were to estimate the variance of a portfolio of two stocks,we would calculate this as the weighted sum of the variance of the returns ofeach stock as well as the covariance between the returns of these two stocks.If we were looking at the US stock market,we may have 9,000 stocks in the stock universe.Recall that we can organize these variances and covariances into a covariance matrix,which contains the pairwise covariances ofeach stock with itself and with all the other stocks in the portfolio.This means we have a covariance matrix that has 9,000 rows and 9,000 columns.Multiplying 9,000 by 9,000 results in 81 million elements.However, since the covariance matrix is symmetric,we will have about 14.5 million unique values to fill in the matrix.That's still too many values to estimate and maintain.Notice that this issue of having too many values to estimate isa general problem referred to as the Curse of Dimensionality.In one dimension, there are a couple of thousand stock returns to estimate.In two dimensions, to fill in the two dimensional matrix,we end up with a couple million values to estimate.This process of calculating the covariance matrix ofassets is called a historical measure of a portfolio's risk.It becomes difficult to do when there are many stocks in the portfolio.The challenge with estimating the covariance matrix ofassets motivates the need for a different approach.That approach is the Risk Factor Model.

### 05. M4 L2A  03 Factor Model Of Asset Return V2-7UnllxDmLj8.en

To get us ready to look at the risk factor model,let's review the factor model of returns.Note that some of this may look familiar from an earlier lesson, which is great.Our team is really excited about what you'll be ableto do by the time you get to this next project.But we also recognize that it'sa pretty steep climb to reach the top of the mountain of knowledge.So, we're trying to guide you up the mountain as gently as possible.Let's begin by looking at a factor model of return.For a single stock and a single factor,the return of a stock can be modeled as the sum oftwo parts: the factor contribution to return plus the specific return.The contribution of the factors is also called the common return.The common return is defined as the stock's exposure to that factor times the rate ofreturn of the factor is the part ofthe stock's return that can be explained by the factor.The stock's exposure to the factor has a couple commonly used names: factor exposure,factor loading, factor sensitivity, or factor beta.We'll refer to this as the factor exposure.The rate of return of the factor is the percent change of a particular factor.We'll call this the factor return.An example of a factor that we've seen before is the market return,as approximated by an index such as the S&amp;P 500.The other contributor to the stock return is the specific returnwhich is the return of the stock that is not explained by the factors in the model.This may also be referred to as the idiosyncratic return or idiosyncratic shock.We'll call this the specific return.So, to summarize, the stock return equalsthe factor exposure times the factor return plus the specific return.A model can have multiple factors contributing to the return of the stock.So, for instance, the return can be modeled as the contributionof one factor plus the contribution of a second factor.As before, the specific return represents the part ofthe stock return that isn't explained by the chosen factors.In general, we can choose any number of factors in order to model stock's return.We can refer to this as a multi-factor model of returns.Cool. So, that was how we model the return of a single stock,but as you may guess,we're going to be more interested in a portfolio of stocks.Let's see how factor models describe a portfolio of stocks next.

### 07. M4 L2A  04 Factor Model Of Portfolio Return V3-HEoPljS1wD0.en

Already know, let's add one more layer of complexity,by using a factor model to describe a portfolio of stocks.A portfolio's return can also be modeled as a sum of two parts,the returns that can be explained by a set of factors,and the returns that are specific to each stock within the portfolio.Let's look at a single factors contribution to the portfolio.Assuming we had the factor exposures of each stock within the portfolio,and we also had the weight of each stock in the portfolio,how do we get the portfolios factor exposure?Well, a portfolio's exposure to a factor isa weighted average of the exposures from his individual stocks.For each stock in the portfolio,we multiply the stocks portfolio weight,by that stocks exposure to that factor,and we sum up all these weighted Betas.This process may look familiar to you.We do the same with portfolio returns,by calculating a weighted average of the individual stock returns.The difference here is that we're calculating the portfolio's factor exposure,instead of the portfolio's returns.So, now we can get the portfolio's exposure to one factor,exposure to a second factor,and exposures to each factor in the model.If we have each factors contribution to the portfolio return,we can sum up the contributions of all the factors.The remaining part of the portfolio's return is the specific return.Can you think of how to get the portfolios specific return,if we have the specific return of each stock?The portfolio's specific return is the weighted sum of the specific return of each stock.So, we take each stock's weight in the portfolio,multiplied by the stock's specific return,and add this up for all stocks in the portfolio.This weighted sum is the portfolio's specific return.This is the portfolio's return,modeled in terms of factors.The point is to explain most of the portfolio return,using a set of common factors.Ideally, the chosen factors would explain more of the portfolio return,and the specific return would explain less.From here, we'll get the portfolio's variance,modelled in terms of these common factors.We'll look at how to model portfolio variance for the rest of this lesson.

### 08. M4 L2A 05 Covariance Matrix Of Factors V3-llA1A0vjSuI.en

We can also model the portfolio's variance as a function of factors.The portfolio's variance can be broken up into two parts,the risk contribution of the factors,and the specific risk that is not due to the factors.Note that is similar to what we saw earlier with the factor model of returns.The end goal is to become familiar with this formula,which models portfolio variance in terms of common factors.These variables represent matrices.The point of these matrix multiplications is to add upthe individual stock variances and pairwise covariances,to get the variance of the entire portfolio.Also, to remember the various pieces and to make the math easier to digest,I'll use ice cream as an analogy to describe the variables.Here's the covariance matrix of factors,we can think of this as the main ingredients for making the ice cream,such as milk and sugar.Here is the matrix of factor exposures,as well as its transpose,which we can think of as measuring spoons for the ingredients.Here's the matrix of specific variances,also known as idiosyncratic variances.Which we can think of as ingredients that are specific to each ice cream,such as pecans or chocolate chips.Finally, there is the matrix of portfolio weights, and its transpose.We can think of these as ice cream scoops,which help us choose how much of each flavor of ice cream to put in our bowl.

### 10. M4 L2A 06 Variance Of One Stock V3-rxaABg4wVZo.en

Let's begin with our matrix notation,and consider a portfolio with two stocks,in a model that uses two factors.The variance of the portfolio is the weighted sum ofthe variances and pairwise covariances of its stocks.We can think of each stock as a different flavor of ice cream,such as butter pecan or mint chocolate chip.We'll start with the first stock.Its return is modeled as a linear combination of the factors plus the specific return.These factors are used in the model to describe all the stocks.So, in the context of risk modeling we call these common factors.Risk factors, or common risk factors.Let's take the variance of both sides of the equation.This becomes the variance of each factor plus twice the covariance of the factors.Plus the variance of the specific return.The model assumption is that the specific return is not correlated with the factors.So, this specific return doesn't show up in any covariance operator.You can pause the video to study these formulas as needed.For constant like the factor exposure,we can move it to the outside of the variance operator if we square it.Similarly, we can move the factor exposures to the outside of the covariance operators.The sum of the first three terms containing the factors is the systematic variance.The last term is the specific variance also called the idiosyncratic variance.If we think of this first stock as butter pecan ice cream,then the systematic variance describes the main ingredients such as sugar and milk.While the specific variance describes the pecans.Okay. That was the variance of one stock.Let's look at the second stock,and see how these two mixed together to create a bowl of ice cream.I mean, a portfolio of stocks.

### 11. M4 L2A  07 Taking Constants Out Of Variance And Covariance Optional V3-M9R9870m_o0.en

If you haven't looked at variance operators in a while,it may not be obvious why we're able to takethe constant factor exposure out by squaring it.Just a reminder of why we'd square theconstant when putting it outside of the variance operator.Remember, that variance takes the square of each observations difference from the mean.For example, let's pretend there arejust two data points for factor one with equal probabilities,then the variance of factor one looks like this.If we take the variance of a constant times the variable,then it looks like this.Notice that this constant shows up next to every variable.If we take the constant out one step at a time,then we'll eventually factor out that constant from the squared terms,which means we'll also apply the square operator on this constant,then we'll factor out the squared constant a bit more.Notice that the variance of a constant times a random variable is thesame as the squared constant times the variance of that variable.You can pause the video here if you want to look a bit longer.So, that's why we can take the constant out of variance operator by squaring it.For similar reasons, we can also take the constants out of the covariance operator.

### 12. M4 L2A 08 Variance Of 2 Stocks Part 1 V3-PlPusmuR20k.en

Recall that I'm referring to the first stock as butter pecan ice cream.The second stock can be modeled in a similar way.We can think of it as another flavor of ice cream.Like mint chocolate chip.Similar to before, the second stocks variance can be modeled as two parts.The part that is explained by the common risk factors,and the part that is not.I'd like to point out something very cool.Which is that all the values that you see inside a variance operator orcovariance operator contain just the risk factor as one in two.Which we've nicknamed sugar and milk.The variance and covariance functions don't needto include information that is specific tothe stocks because we can takethe factor exposures outside of the variance and covariance operators.This is very helpful because as you'll see later on,it greatly simplifies the number of pairwise covariances that we'll need to estimate.So, now that we have models that describe the individual variances of the two stocks,what is the pairwise covariance of these two stocks?To get the covariance of the two stocks,we substitute their returns with the factor models of their returns.Now, we have a covariance of two expressions and each expressionis some weighted sum of two factors and it's specific return.To help keep track of these formulas,notice that for the first stock,factor 1's contribution is labeled with white sugar,while factor 2's contribution is labeled with plain milk.Also, for stock 2,factor 1's contribution is brown sugar,while factor 2's contribution is chocolate milk.Our next step is to apply the pairwise covariance operator on each pair of terms.We see that each stock is the sum of three terms.This would end up with three times three or nine covariances.You can pause the video for a bit to study this.By definition, the specific returns are uncorrelated with the factors.So, the five pairwise covariances that include the specific returns will become zero.In other words, wherever we see a covariance that contains pecan or chocolate chips,we'll assume the covariance is zero.The remaining four covariance pairs that include only factors may still be non-zero.So, we'll keep these.These are the four pairwase covariances written out.For example, here's the covariance between the contribution of factor 1 to stock 1,and the contribution of factor 1 to stock 2.The images here are to help you keep track of the notation.You can see the four pairs of covariances among the white sugar,brown sugar, plain milk, and chocolate milk.

### 13. M4 L2A 09 Variance Of 2 Stocks Part 2 V4-tSMutw0f6OE.en

Let's clean up the formulas a bit.We can move the constant factor exposures outside of the covariance operators.Also, the covariance of a factor with itself is also called the variance of that factor.So, the expressions now look like this.So, to summarize, we can expressthe pairwise covariance of two stocks as a sum of four terms,written in terms of the two factors.Now we have the variance of the first stock,the variance of the second stock,and their covariances written in terms of the two factors.So, that's pretty great,because we actually have the pieces we need to fill in a covariance matrix of assets.Recall that another way to get these four values is to usea time series of the stocks to calculate the variances and covariances.But as we mentioned earlier,this approach doesn't scale well when handling thousands of stocks.So, instead, we have modeled the covariance matrixof assets in terms of the common risk factors.We can plug the formulas for the variances andcovariances into the four elements of this covariance matrix of assets.It becomes a bit hard to read.So, let's use these images of ice cream to help us keep track of the different pieces.

### 17. M4 L2A 11 Types Of Risk Models V1-SHj2VzJggAE.en

Coming up, we'll cover major flavors of the risk model.We'll start with a risk model that uses a single risk factor, the market return.This is the capital asset pricing model which you've learned earlier.Then, we'll build on this to learn about the Fama French three factor model.The Fama French model which was proposed by Eugene Fama andKenneth French is the basis for the movement towards the multifactor model of assets.In fact, many papers that study or propose potential alpha factors,can trace some of their methodology to the Fama-French three factor model.The cap m and Fama French three factor model areboth examples of time series risk models.We'll then introduce a cross-sectional risk model andsee how they're different from time series risk models.In a later lesson,we'll also introduce a model of Leighton risk factors usinga kind of unsupervised machine learning principle component analysis.In quanta investing, you may come across all three types of risk models,times series, cross sectional or PCA.So, it helps to understand each one.You will use the PCA risk model in this next project and we'lluse one of the time series models in term two of this program.

### img

## Part 01-Module 04-Lesson 04_Time Series and Cross Sectional Risk Models

### 01. M4 L2A 12 Time Series Risk Model Factor Variance V2-hjVBXeZmA0w.en

So, you've seen the structure of the risk factor model and how itattempts to explain a portfolio's risk in terms of the common risk factors.Let's see how this works with a factor that we've seen before,the market return which was introduced when discussing the capital asset pricing model.Our goal is to use the market return as a single factor inour Risk Factor Model and then fill inthe values that we need to calculate the portfolio variance.First, there is the covariance matrix of factors.Since there's only one factor,it has one value which is the variance ofthe market's excess return above the risk-free rate.To make our example more manageable,we'll work with just two stocks.For the matrix of factor exposures,we want the factor exposure of each stock to the single factor,which in our model, is the market return.So the factor exposure matrix is a column vector with two rows,and the transpose is a row vector with two columns.We'll also fill in the matrix of specific returns for each stock.Let's see how we get the values that we'll put into the matrices of this risk model.For the variance of the market factor,we can collect a time series of an index.For instance, the S and P 500 if the stock universe is focused on the US stock market.We can choose a time window of four weeks,eight weeks, or 12 weeks.Then calculating the variance of this time series gives an estimate for this value.There's a bit of a judgement call as to what time window to use.Too short of a sample,and the data is mostly noise.Too long of a sample,and the earlier data points may no longer beuseful for estimating the current variance of the market.Also, the time window used for analysis scaleswith a holding period and the frequency at which we expect to trade.So, for daily trading,we may try different windows of several weeks.For strategies with longer holding periods,we may use a longer time window such as one,two, or three years.So now we've estimated one value in the risk model.

### 02. M4 L2A 13 Time Series Risk Model Factor Exposure V4-WPBSMptBrfw.en

To estimate the other variables,we'll also collect a time series of data foreach stock and make use of the capital asset pricing model.Remember that the capital asset pricing model assumes thatthe stock's excess return above the risk-free rate can be modeled asits exposure to the market's excess return abovethe risk-free rate multiplied by the market's excess return above the risk-free rate.To make this discussion less wordy,I'll just say market's excess return whenreferring to the market excess return above the risk-free rate.I'll also just say stock's excess return whenreferring to the stock's excess return above the risk-free rate.Also, notice that the market exposure,which is what we've been calling this Beta variable when discussing the CAPM,is called a factor exposure when we're discussing factor models.The term factor exposure is a more general termthat describes any factor including the market factor.We're going to demonstrate one way to estimate factor exposures using regression.But keep in mind that factor exposure estimation is not a settled science.So, there are also other approaches toestimating a stock's exposure to a particular factor.We'll get a time series of the market's excess return,a time series of the first stock's excess return and run them through a regression.The coefficient Beta is an estimate of the first stock's exposure to this one factor.We can do the same with a time series of the second stock's returns andestimate the factor exposure of the second stock to this one factor.

### 03. M4 L2A 14 Time Series Risk Model Specific Variance V2-I0uJLfh_OgQ.en

We can also use the results of the regression toestimate a time series for the specific return.For each time period,the specific return isthe residual from taking the actual minus estimated excess return of the stock.The actual return is data that we just used to input into the regression.The estimated return is calculated by using the market return andthe regression estimates for the factor exposure and the intercept term.If we calculate the difference betweenactual and estimated returns for multiple time periods,this is an estimate for the specific return time series.The variance of the specific return can be used in the risk model.Similarly, we can also calculate the specific return of the second stock,then calculate the variance of that specific return.Note that calculating specific variance is also not a settled science.The time window chosen affects the result.This difference between actual and estimated return,which we often refer to as the residual return,is what we're calling the specific return in this context.If you're working in quant finance,you may hear an expression like,the returns that are not explained by the modelor the returns that are not explained by the factors.Since the goal of risk factor models is to explain where the returns of assetscame from by breaking them down into well-defined components or factors,anything left over that is unexplained is represented by the residual.

### 04. M4 L2A  15 Time Series Risk Model V2-Lz3RMLmov8o.en

If we look again at our risk model,we can see that we've got estimates for eachof the variables in several of these matrices.In particular, we are able to fill in the matrix of factor variances and covariances,the matrix of factor exposures and the matrix of specific variances.The product of factor exposures to factor variances and covariancesplus specific variances is essentially the covariance matrix of the stocks.The stock weights in the matrices at the left and right end of this expression canbe multiplied to this covariance matrix of assets to get the portfolio variance.In a later lesson,you'll learn about choosing optimal portfolio weights.But for now, you can think of that inner core,the covariance matrix of assets,as the plug-and-play component that we can insert intovarious portfolios that contained a subset of the assets contained in that matrix.

### 05. M4 L2A 16 Fama French Size Factor V2-94a2ugitC_E.en

Now, we'll build upon the previous risk model byincluding two additional factors for a total of three.This is the famous Fama-French model,which can be considered the model that started the movement towards multi-factor models.The original three-factor Fama-French model includes the market return,and adds two additional factors,which the other's called size and value.Let's look at these two additional factors,and how the data for them are generated.The first additional factor we will look at is size.The size of a company is measured by its market cap.There's been evidence that shows,that small-cap companies have higher average returns compared to large-cap companies.In other words, if we can determine that company ABC is a small-cap stock,valued at one billion dollars, and company XYZ,is a large cap stock valued at $10 billion,then we could expect that company ABC will havehigher risk adjusted returns, compared to XYZ.The question is how much more return can we expect for companyABC for being a fraction of the size of XYZ?Remember that we're trying to fill in the factor risk model.The factor covariance matrix uses the time series of factor returns forthe three factors: market, size and value.Well, if you had evidence,that small cap stocks had higher than average returns,than large-cap stocks, what would you do asa rational investor to make use of this knowledge?One way that seems to make sense is to buy small-cap stocks,and to short large-cap stocks.If we had a portfolio in which you bought small-cap stocks and shorted large-cap stocks,we could track the portfolio's value over time so we'd have a time series.We could also calculate the daily return on this portfolio.The return of this portfolio is the return of the size factor.Let's think about what this portfolio tells us.If the portfolio's return is positive,it means that on that day,favoring small cap stocks had a positive effect on the portfolio's return.If the portfolio's return is negative,then it means that on that day,favoring small cap stocks had a negative effect on the portfolio's return.So that's pretty cool.We have a way of quantifying whether a smaller sizeactually adds to positive or negative returns on each day.

### 06. M4 L2A 17 Fama French Size Factor V3-FXZuHsn0bx4.en

Notice that while we calculated our returns for one particular portfolio,if we chose different stocks,we could create another portfolio to represent the size factor.If we decided to buy in short twice as much,this could potentially be another portfolio representing the size factor.How do we generalize this to other portfolios thatbuy small-cap stocks in short large-cap stocks?Well, rather than trying to create an infinite number of long-short portfolios,we can use a single standard portfolio and then see how other portfolios co-vary with it.In other words, portfolios that are just like the standard portfoliowould move up or down in the same percent as the standard portfolio.Some other portfolio that may not have such an extreme bet onthe market cap of stocks may move a fraction as much as the standard portfolio.In fact, we don't have to limit ourselves to portfolios.We could check to see how much a single stock movesin relation to this long-short portfolio.If we perform the regression in which the independent variable wasthe return of this size portfolio and the stock return was the dependent variable,then the beta coefficient of that regression would give us a measure of howexposed the stock is to the movements of this size-based portfolio.Let's look at what Fama and French did to create the size factor.Start with an estimation universe,which is a set of stocks that are representative of a region or countries stock market.Sort the stocks by market cap.The top 90 percent by market cap are put into a portfolio called "Big".The bottom 10 percent by market cap are put into a portfolio called "Small".Calculate the equal weighted return of the Small portfolio,and the equal weighted return of the Big portfolio,then take the difference,the return of small minus the return of big.This difference gives us a measure ofthe return that can be attributed to investing basedon the hypothesis that small-cap stocks tend to outperform large-cap stocks.We can think of this in terms of a long-short portfolio which buysthe small cap stocks and shorts and equal dollar amount of the large-cap stocks.The daily returns of this long-short portfolioare the factor returns of this size factor.So, we now have a factor time series that we can use in a factor model.Note that in the Fama-French notation,this size factor is written with the acronym SMB.Can you guess what that stands for?SMB stands for Small Minus Big.

### 07. M4 L2A 18 Fama French Value Factor V4-IcbsQ4QRGbs.en

Now let's look at the third factor, the value factor.There is research to suggest that stocks that have a high book value,relative to their market price,tends to outperform stocks with lower book value.Put another way, stocks that are cheap relative to their fundamentals,tend to outperform stocks that are expensive,relative to their fundamentals.Stocks with high fundamental valuations,relative to market price,are often called value stocks.Stocks with high market price,relative to their fundamental valuations,are called growth stocks.Remember from the previous discussion ofthe size factor that we can create a theoretical long/short portfolioin order to express the hypothesis that valueindicates an additional boost and returns relative to growth.To do this, first,sort the stocks in the estimation universe by their book-to-market value.Stocks with high book-to-market values,those that are above the 70th percentile,are grouped into a portfolio called value or high,where high refers to high book-to-market value.Stocks with low book-to-market values,below the 30th percentile,are placed into a portfolio called growth or low,where low refers to low book-to-market value.Note that these percentile cutoffs are the ones used in the Fama-French model.Next, calculate the equal weighted return ofthe value portfolio and equal weighted return of the growth portfolio.The long/short portfolio can be constructed by buyingthe value portfolio and shorting an equal dollar amount of the growth portfolio.The daily returns of this long/short portfoliogives us the factor return of the value factor,which we can now use in a factor model.In the Fama-French model,the value factor is written as HML.This stands for High Minus Low,where High refers to the value portfolio and Low refers to the growth portfolio.

### 08. M4 L2A  19 Fama French SMB And HML V2-fnncnimScFc.en

So, now we've seen how Fama and French usedtheoretical portfolios to create factor returns,time series that represents size and value.What Fama and French did when combining these factors was to definetheoretical portfolios that were based on both size and value.Let's see what I mean by that,first sort by market cap,define the group of small stocks and also define the group of big stocks.Then, for the small group,sort by book to market value.Divide this small group into three groups: value, neutral and growth.Do the same for the big group.Sort the stocks by market cap.Sort them by book to market value,then divide them into three groups: value, neutral and growth.Now, we'll effectively have six portfolios which are six building blocks,with which we'll use to constructthe long short portfolios representing the size and value factors.The small minus big size factor is the sum ofthe three small portfolios minus the sum of the three big portfolios.The high minus low value factor is the sum oftwo value portfolios minus the sum of two growth portfolios.Notice that the two neutral portfolios aren't used in this formula.Okay, one more thing.Notice that small minus big was constructed usingthree building blocks forthe long position and three building blocks for the short position.However, high minus low was built usingtwo building blocks for the loan and two for the short.Since we plan to use both factors in a multiple regression,we want to even out the effects of each factor.So, we'll divide the small minus big by three and divide the high minus low by two.You can pause here to study these formulas.

### 09. M4 L2A  20 Fama French Risk Model V3-tepvGkpNKrI.en

Now, let's see how we can usethe Fama French three-factor model to fill in the values we need for the risk model.We have three factors,so we'll fill in the covariance matrix of factors that is three by three.Notice that the first row has the market factor in all three columns,the second row has the size factor,and the third row has the value factor.We want time series of three factor returns;market, size, and value.The market factor can be the excess return of a market index such as the S&amp;P 500.The size factor is the return of a theoretical portfolio that long,small stocks and shorts, big stocks.So, it's going along the three small portfolios and shorting the three big portfolios.The value factor is the return ofthe theoretical portfolio that longs value stocks and shorts growth stocks.So, it's going long two value portfolios and shorting two growth portfolios.You can pause here to study these formulas before continuing.Now, let's look at the matrix of factor exposures.We'll assume just two stocks.Each stock has a factor exposure for each of the three factors.The transpose of the factor exposure matrix looks similar,just turned on its side.To estimate factor exposures for the first stock,we can use regression,this time a multiple regression.For the first stock,the regression looks like this.The independent variables are the factor returns for the market, size, and value.The dependent variable is the return of the first stock.The same procedure can be used to estimate the factor exposure for stock two.The specific variance matrix holds the variance foreach stock that is not attributable to these factors.For the specific variants of the first stock,let's first get a time series of the specific return.The specific return for each day isthe actual return of the stock minus is estimated return.The estimated return is calculated usingthe factor returns and factor exposures that we calculated earlier.We can do the same for the second stock.Taking the variance of the specific returns gives usthe values that we can put in the matrix of specific variances.You can pause here to study the formulas a bit.So, we did it. We have all the pieces that go into the factor model ofportfolio variance and we estimatedthese values using the Fama French three-factor model.Just a reminder that this approach is called a Time Series Risk Model.Next, we'll look at another common type of risk model,which is the cross-sectional risk model.

### 10. M4 L2A 21 Cross Sectional Risk Model V3-mpnRAt8qUus.en

What we've seen so far are called time series risk models.Most practitioners tend to buycommercial risk models that are built and maintained by other companies.These commercial risk models tend to be a different type of risk model,a cross-sectional risk model.The use of cross-sectional risk models is quite common within the investment industry.So, even though it is unlikely that you will have toimplement a cross-sectional risk model from scratch,it is good to have a sense for how they work.So, what is a cross-sectional risk model,and how is it different from time series risk model?Well, let us first see how a cross-section differs from a time series.If we have lots of data on a single stock over time,this is a time series.If we have lots of data on many stocks in a single time period,such as one day then that is a cross-section.For risk models, a cross-sectional factor model also differs froma time series factor model in terms ofwhat is known and what is estimated by a regression.Let us look at the factor model has a bunch of factor exposures and factor returns.When we use a time series model,the factor returns were calculated first using theoretical portfolios.Then, given the factor returns data and the stock returns,the factor exposures were estimated using regression.With the cross-sectional factor model we would actually calculate the factor exposures,the betas before doing a regression,then the factor returns are estimated using regression.This may sound pretty confusing at first, I have to admit,it was confusing to me after hearing it for the first time,and also second time and the third time.Let us look at this topic a bit more.

### 11. M4 L2A  22 Cross Sectional Risk Model A Different Approach V2-LauZ7h4bgKE.en

Let's step back and look at an equation z equals x times y.If we knew both z and x,then we can solve for y.Similarly, if we knew both z and y,then we can solve for x.In other words, if we know any two of the three variables in this equation,we can solve for the one that's missing.So, now let's revisit the factor model,keeping in mind that we can think of the stock return,the factor exposure, and the factor returned as three variables.If we can calculate two of these,then we can use regression to estimate the third variable that's missing.In the case of a time series risk model,like the Fama-French example,we get the stock return in factor returns first.So, we use regression to estimate the factor exposure Beta.For a cross-sectional risk model,we get the stock return and factor exposure Betas first.Then we use regression to estimate the factor return.Again, this might take some getting used to,because when we look at regression formulas,we're probably used to seeing a letter y for the dependent variable,a letter x for the independent variable,and a Greek letter Beta to represent the coefficient that we're estimating.Instead, we can shake things up and let our brains be more flexible.So, we'll think about what data is available as inputs forthe regression and what variables we don't have,and we'll estimate with regression.

### 12. M4 L2A  23 Categorical Factors V2-F76juAxHVIk.en

Let's look at a few examples factors we can use with this cross-sectional risk model.There are categorical factors such as the country in whichthe company is based or the sector that it is a part of.Intuitively, we can imagine that the country ofa company may affect its stock price movement in some way.For instance, if a country's government added orremoved regulations that impacted only countries thatoperated in that country then the impact would beconcentrated on companies that are exposed to that country.Similarly, it makes sense thatcompanies sector is a useful indicator about its stock price movements.For instance, if oil prices increased sharply,we can imagine this having a positive impacton the stock price of most companies that operate inthe oil and gas production sector but have a negative impact onbusinesses that buy oil and gas products such as airlines or trucking companies.What you may notice about categorical variables such as countryor sector is that we can't sort categorical variables.Sorting only makes sense if we have numerical data.Recall that if we were to try using a time series risk model,our first step would be to createa theoretical portfolio whose daily returns represent the factor returns.But to construct the theoretical portfolio representing the factor,we first need to sort the values so thatwe can divide them into separate quantile groups.This isn't possible with categorical variables like a country or sector.Let's talk about how we normally handle categorical variables.You may have heard of dummy variables or one-hot encoding.If the country category has 12 unique countries for instance,then we could expand the single country variable into12 separate variables each representing one country.So, if a stock where in the USA,it would have a value of one assigned to the USAvariable and zero for all other country variables.When we think in terms of the factor model,the value of one or zero that is assigned to each countryvariable is the factor exposure for that stock to that country.

### 13. M4 L2A 24 Categorical Variable Estimation V4-50hvTluqz3U.en

Let's focus on just one country variable,such as the USA country variable and see howwe can estimate the values that will plug into the risk model.For the factor exposure matrix and its transpose,we want each stock's factor exposure to the USA country factor.This can be set to one,if the company is based in the USA and zero if it's based in another country.You could also imagine choosing decimal values between zero and onebased on how much of the company's revenue is generated in the USA.In any case, the point is that we can estimate the values for the factor exposures.Next, we want to fill in the co-variance matrix of factor returns.So, we want to obtain a time series of factor returns.To make it easier to follow,I'm going to simplify our model so that it contains only one factor.So, the factor co-variance matrix containsjust a single variants for the factor return of country USA.Also, the matrix of factor exposures has just one column for the single factor.The number of rows equals the number of stocks.We want to get the factor return,time series of the country USA,so that we can calculate a variance from it.The journey to estimating a time series starts with a single step.So, lets first estimate the factor return for a single time period.Once we estimate the factor return for one time period,we can repeat the process to estimate factor returns for more time periods.Okay. We're going to estimate the factor return for a single day.So, we take a cross section by collecting data pointsfor a single day but across multiple stocks.Each stock has a factor exposure associated with it.The factor exposure is one,if the company is 100 percent exposed to country USA.The factor exposure is zero,if the stock is completely not exposed to the country USA.It's somewhere in between,if the stock is partly exposed to USA,but also to other countries.We'll also have the return for each stock on that time period.So, given the stock return and factor exposure,we want to estimate the factor return.We have several data points,one for each stock in the stock universe.So, we can use a regression model to fit a line throughthe plot of stock return versus factor exposure betas.The slope of that line is the estimate for the factor return for that one time period.We can interpret this estimate of factor return as the amount of return that canbe attributed to being exposed to the risk factor of country USA.So, that was one factor return fora single time period using a single cross-section of data.Yeah. We've now estimated one point.Remember that we want to find factor returns over time.If we repeat this cross-sectional regression for multiple time periods,we'd get a time series for the factor return.That's great. If we calculate the variance of the factor return time series,we can plug it into the co-variance matrix of factor returns.

### 14. M4 L2A 25 Specific Variance V2-JwA9g3NBglE.en

Now that we have the matrix of factor variances and covariances,we also want to get values to fill in the matrix ofspecific variances which contain the variances of the specific returns for each stock.If there are two stocks in the stock universe,then there are two specific risk variances to calculate.Let's look at the specific risk variants of the first stock.The data to input into the variance isa time series of the specific returns for the stock.The specific return is the residual or the difference from taking the stocksactual return minus the stocks estimated return using the chosen risk factors.So for each time period,we have the actual stock return and can calculatethe estimated return based on the chosen risk factors.We can get the estimated return of the stock ateach time period by inputting values that we've already calculated.The estimated return of the stock fora single time period is a stock's factor exposure to country USA,multiplied by the factor return of USA for time t,plus the intercept term for time t. This is great.Now we have one data point for the specific return of stock one.We can do the same thing for multiple time periodsand get a time series for the specific return of the stock.The variance of this time series is the estimate that wecan use in the matrix of specific variances.So now we have all the pieces that go into the BFB transpose and S matrices.

### 15. M4 L2A  26 Fundamental Factors V2-fndhL2Tolac.en

Fundamental factors can also be used in the cross-sectional risk factor model.In fact we'll discuss two that we saw in the Fama-French time series model,the book to market value and market cap.This is an interesting example because we can see thatthere are two approaches to working with the same factor.There's the method we saw using a time series risk factor model,and now we'll see how to do this with the cross-sectional risk factor model.Just to revisit our goal,we're going to fill in the values to calculate the portfolio variance.For starters, let's see what data we already have to work with.For each stock, we have a book to market value,which may be updated once every three months if the company is based in the US.We also have a market cap,which may be updated daily.These values can be set as the factor exposures.Notice that in the Fama-French time series model,the book to market and market cap,were used in theoretical long-short portfolios to set the factor returns.However, for the cross-sectional model,these values are used directly as the factor exposures instead.We can also obtain the stock returns,and we want to estimate the factor returns using regression.So, we'll take the following steps.For all the stocks in the selected stock universe,obtain each stock's factor exposures,which are the book to market value and market cap.Also get each stock's return for one time period.Next, request the stock return against the factor exposures.The regression estimates the factor return for each of the two factors.Since this factor return is estimated based on all the stocks in the stock universe,it is general enough to apply to all those stocks.Notice that in practice,even if the stock universe is 9,000 stocks,we may use a subset of those stocks to perform the regression.This subset is called the estimation universebecause it's the stock universe that is used for estimating these model parameters.To get a time series of factor returns,repeat this process of performing a multiple regression for each day.Once we obtain a time series of factor returns,we can calculate factor variances andcovariances to fill in the covariance matrix of factors.To fill in the matrix of specific variances,we can calculate the specific return from the stock returnminus the estimated stock return using the chosen factors.The variance of the specific return time series for each stockis what we'll use to fill in the matrix of specific variances.

### 16. M4 L2A 27 Summary V1-rdqINNkTlqs.en

It's important to be familiar withthe cross-sectional approach as it's the one that most commercial risk models are based,and commercial risk models are widely used by institutional investors.In academic research, it's common to see the time series approach, whereas in industry,practitioners usually either purchase a commercial risk model such asthe ones built and maintained by MSCI Barra, Axioma,or Northfield, or they may use a third approach,principal component analysis, which we'll learn about in the next lesson.One challenge of constructing a risk model on your own is that you needto decide for yourself which risk factors to use in the model,and don't have an easy way to enforcethe assumption that the factors are independent of each other,or that the specific returns of the stocks are independent of each other as well.Commercial risk models can be purchased off the shelf so thatinstitutional investors can focus their attention on researching Alpha factors.The machine-learning approach with PCA,which we'll learn in the next lesson,is nice because it derives latent factors whichare by definition explaining the most variance inthe distribution of returns while also enforcingthat each latent factor is independent of all the others.

### img

## Part 01-Module 04-Lesson 05_Risk Factor Models with PCA

### 01. M4 L2b 01 PCA Statistical Risk Model V1-lDxqJ0JYUzs.en

By now, you've seen a few important types of factor models.You know that the goal of factor models is to modelroughly things you think have a similar underlying effect on your variables of interest.You want to represent a variable or variables in terms ofseveral important underlying variables or factors.In our case, we are trying to represent a large number of similar variables that returnstime series of several financial assets in termsof a smaller number of common underlying factors.There's another way to do this that relies on the machine learning method,principal components analysis or PCA.PCA is a technique you can use to represent your dataset in terms of hidden latent features or dimensions,and potentially reduce the number of dimensions ofyour data set by dropping the least informative dimensions.We'll show you what we mean by that.If you've been reading about machine learning or artificial intelligence,you may have heard of this method before because it's usedwidely across all types of industries and fields.What we're going to do now is refresh the math you'll need to understand PCA and thenexplain in detail how this method is typically used for risk modeling and finance.Let's start by reviewing some of the important math we'll need to learn PCA.

### 02. M4 L2b 02 Vector Two Ways V3-mlw6FnCUloU.en

Okay, we're doing math now,so let's start with something super simple; a vector.Oh my gosh you're thinking,a vector dah, I've seen this a million times.Well, I just want to remind you that there area couple of different ways of thinking about a vector,that we will transition between when discussing analysis methods.First, a vector is a collection of numbers,as in you might make a bunch of measurements and put them intoa column in a spreadsheet application and think of that as a vector.For example, you could think of the returns ofseveral companies stocks on the same day as a vector.But the other way to think of a vector,which you may also be familiar with,is as a direction in space.Remember like a 3 D vector is also an arrowpointing from the origin to a point in 3 D space.These are both valid views and we'll need both to get to where we're going.I want you to be comfortable with the idea thatwe'll need to switch back and forth between them,as we move through this content.

### 04. M4 L2b 04 Bases As Languages V3-yEL0-AE3mjo.en

PCA, amounts to finding a new and special basis for your dataset.But hang on, what does it mean to find a new basis?I'm guessing that you've heard about and even done this yourself before.But before we move on, let's just briefly review exactly what this means.Let's return to thinking about vectors as directions in space,and start in the 2D plane.You know that we can represent a point living in the 2D plane with two coordinates.The coordinates tell you how to get to the point from the origin.For example, the vector 1,2 means go one unit to the right,and two up to get to this point.So, we see that we can decomposethe overall movement represented by the vector into two movements;one to the right, and one up.Once we know the directions to go,we can specify how far to go in each direction.Towards this goal, let's write down two vectors,i hat and j hat.The first one, we write as 1,0 which means move one unit to the right.The second one, we write as 0,1 which represents moving one unit up.Our original vector is made up of one i hat and two j hats.We can write it as 1 times 1,0 plus 2 times 0,1 or 1,2.In other words, our vector can be written as a linear combination of i hat and j hat.We call i hat and j hat a set of basis vectors for the 2D plane.A set of vectors is a basis for a space,if no vector in the set can be written as a linear combination of the others,and any vector in the space can be written as a linear combination of vectors in the set.If we want to express this using matrix multiplication,we could write it this way.Now, we see that the number in the top position of this vector means how much of i hat,or how much of the first basis vector.While the number in this bottom position means how much of the second basis vector.So, 1,2 is how we write our vector in the i hat, j hat basis.Although we use i hat and j hat most of the time,there are other bases we can choose for the 2D plane,because there are different ways to get to our point.We could shoot out over here,and then go back over here.Remember, that our choice of basis also effectivelysets up a coordinate grid for us to represent our vectors in.We are used to seeing a grid made up of square boxes,the usual X, Y coordinate system,with the basis vectors i hat and j hat.But if we use these red and blue vectors as our basis vectors,a more sensible grid would be made up of these parallelograms.Thus, we can represent our original vector with a different combination of basis vectors.We can think of this as another language for writing down our original vector,because we see that we can write down a new set ofinstructions for creating the original vector.Now, all we need is a way to translate between the two languages,the original language, where we use i hat j hat,and this one with the blue and red vectors.

### 05. M4 L2b 05 Translating Between Bases V4-lrE4VOJ2RCA.en

Let's take a close look at these red and blue vectors.In fact, let's try looking at them in the original i hat j hat basis.In the original basis,the first vector can be written (1,0.5).That is to say it can be created with one i hat and a half a j hat.The second vector is (-1,1) so it can be created with minus one i hat and one j hat.Here, we've learned how to write the vectors ofthe new basis in the language of the old basis.So, we effectively know how to translate between them.Now, let's see how we can use this information to take a vector expressedin one basis and find out how to express it in another basis.We know that we need two copies of the blue vector andone of the red vector to represent our original vector,let's write that down.When we multiply it out,we see that we recover our vector as written in the i hat,j hat basis, which makes sense since we know thatthese vectors are also written in the i hat j hat basis.So, now we can see how translation between the two languages works.We need to write the new basis vectors in the language of the old basis,then the formula for building the vector in the language ofthe new basis gives us our vector in the language of the old basis.Conversely, if we were starting with (1,2) in our original language of i hat,j hat and we wanted to translate to this new basis,we'd have to find out how to write i hat and j hat in the new basis.Then, we'd multiply it by that matrixand outward pop our original vector in the new basis.It turns out that the matrix that tells you how to translatefrom the language of the old basis to the language of the new basisis just the inverse of the matrix that tells you how totranslate from the language of the new basis to the language of the old basis.This makes sense because translating from the old basis to the new basis andback should have the effect of doing nothing and give you back what you started with.I'll let you verify this for yourself.

### 06. M4 L2b 06 The Core Idea V3-0KwLkaKHAvg.en

Okay. Let's get back to what we really want to talk about, PCA.In a nutshell, what is PCA?PCA is a series of calculations that gives us a new and special basis for our data.Why is it special?Well, the first dimension is the dimensionalong which the data points are the most spread out,we say they have the most variance along this dimension.What do we mean exactly?We mean that, if we start with a dataset and we considera new axis in the 2D plane represented by this line through the origin,then we find the coordinates of our points along this new axisby projecting them by the shortest path to the new axis.Consider the variance or spread of this set of coordinates along the line.When we do PCA or trying to choose our new axis insuch a way that the new coordinates are as spreadout as possible or have maximum variance,it turns out that the choice of line,where the coordinates are most spread out,is also the choice of line that minimizesthe perpendicular distance of each coordinate to the line.We say that the basis minimizes reconstruction error.Maximizing variance and minimizing reconstruction error go hand in hand.The squared distance from the origin to the projection,plus the squared distance from the projection to the point,equals the squared distance from the origin to the point,this is just the Pythagorean theorem.So, when you change the orientation of the line,if one increases the other must decrease.The orientation of the line chosen by PCA is the one thatmaximizes the squared distances along the linefor all points and simultaneously minimizesthe squared perpendicular distances to the line for all points.This is how we find the first basis direction.The next basis direction must be perpendicular or orthogonal to the first.In our little example,there is only one choice for this dimension,because we are working in a two-dimensional space.But if we were working in a higher-dimensional space,the requirement for the next basis direction would be that it beorthogonal to the first and also maximizethe variance of the points along that dimension and soon until we have as many new dimensions as we had dimensions to start with.So, if we had data in four dimensions,PCA would give us four new axes.

### 08. M4 L2b 08 Writing It Down Pt 1 V3-NyDNFqm8c_s.en

Okay. So, what I told you is that we want tomaximize the variance of the data in the new basis direction.Now I want to show you exactly how what I showed youin pictures translates into symbols that we write down.I don't think this is as essential for you to know,because you already have the core idea.But I want to show this to you,so that if you read about PCA in books or online,you'll be able to follow what's written,and because for myself I like to go through these thingsstep-by-step to make sure I really understand exactly what's going on.I think this is important for being able to makeinferences about the results of mathematical calculations,and for being able to compare among techniques that do similar things.I want to be able to afford to you the opportunity tounderstand on a deep level to. So, let's get started.What we're going to do here is go through what we already described in a bit more detail,in a bit more rigorously.So, let's start with something I left out earlier.The very first thing we need to do when we do PCA is makesure the data are centered around zero. What do we mean by that?Let's say the dataset we started with was over here in the 2D plane.The coordinates of this point are a and b.Here's the mean of all the points x coordinates,and here's the mean of all the ports y coordinates.When we begin PCA,if these means are non-zero,we want to subtract the mean ineach dimension so that we get a new dataset centered around zero.First, we subtract the x mean from each data point.Then we subtract the y mean from each data point.Now the points are centered around zero,.This is called mean centering or mean normalizing the data.

### 09. M4 L2b 09 Writing It Down Pt 2 V2-TSH3hTAHsIg.en

Now for simplicity, let's look at just one of these mean centered data points.We're looking for a new basis for the data and we want towrite down the coordinates of this point in a new basis.This is the same thing we discussed earlier,we are looking for two new basis vectors for the 2D planewhich give us an alternate way of describing our original points location.Okay. Let's say we've run PCA and it has spit out the new basis directions,these are called Principal Components PCs or just components.They are the directions of the new basis as written in the language of the old basis.It's required by the algorithm that these havelength 1 and be perpendicular to each other.So, let's show them slightly smaller.We'll consider them to be length 1 now.Now, let's find our points' coordinates in the new basis.Let's do one coordinate at a time.We'll start with the red basis vector.Let's write down some names,let's call the vector pointing to our original data point X andthe red basis vector W. Notice that what we've formed here is a right triangle.If X is the length of the X vector and Theta is the anglebetween the X vector and the W vector and you remember your trigonometry,you'll see that this distance is X cosine Theta.Now, let's remember the formula for the dot product.X.W is the length of X times the length ofW times the cosine of the angle between them, Theta.So this length equals X.W divided by the length ofW. This quantity is the projection of our original point onto the new basis direction,and it turns out it's exactly the quantity we're looking to maximizeover all the data points when we pick our new basis direction.In fact, when I say maximize the variance,I actually mean that if we have all these dot productsfor all the X vectors pointing to each of our data points,we want the variance of this new set of numbers to be as large as possible.

### 10. M4 L2b 10 Writing It Down Pt 3 V3-kSl0j4QIMIU.en

Variance, okay we know the formula for that.It's just the sum of the squared deviations from the mean,divided by n minus one.But if the mean of the original data coordinates is zero,so is the mean of their projections onto the new direction.So, we have that this term I call mu is zero.So, we're trying to maximize this thing.Now, I'm going to show you how to write this in a more condensed fashion,that you will see if you're reading about PCA.First, let's notice that if each of these quantities is a number and a vector,the variance above is just the squared length of the vector.Now let's look at the thing inside the double bars.If you don't follow this step,try writing it down for yourself and multiplying it out.But see that we can write this as a matrix where x_1 is the first row,x_2 is the second row and x_3 is the third row times the w vector.Remember that this w is just the length of the w vector,so it's just a number.I'll just pull it out over here.Now, let's write this in a more condensed form.See that the matrix on the left isjust our data matrix where the rows are the observations,so the number of rows is the number of data pointsand the columns are the features or dimensions.The vector on the right is just our w vector.Another way you might see this written is like this.Be sure to go through these steps on your own if you find you're a little fuzzy on them.But now we've arrived at where we wanted to.This last quantity is called a Rayleigh quotient.This is exactly the quantity we try to maximize in PCA,when we're looking for w;the direction of the first dimension.

### 11. M4 L2b 11 Writing It Down Pt 4 V3-7XO-syqIpCE.en

Let's summarize.What I'm trying to tell you is that the goal of maximizing these links,the projections of our data onto the first dimension can be written this way,where this thing in here is the variance of this set ofprojections of the X vectors onto the W direction,up to a constant factor.Then, we choose W so as to maximize that variance.This is how we choose the first principle component,what we would do to find the next principle component is,subtract the component ofeach data vector in the direction of the first principle component.Remember that when you subtract a vector from another vector,you line up the first vector's tail with a second vector's tip.Then, we'd get a new set of data vectors that livein the space orthogonal to the first principle component.In our example here,this new space is only a straight line orthogonal to the first principle component.If we were working in three-dimensions,and we found the direction of the first principal component,we'd find the component of each data point along that direction,and subtract that from the data point.These new data vectors would all live inthe 2D plane perpendicular to the first principle component.Then we'd apply the same procedure to find the next principle component,until we had one for each dimension.

### 12. M4 L2b 13 Principal Components V3-XtecKk58CLs.en

Great. So, we found this new and special set of basis vectors,which are the principal components.What now? Well, let's look at what we have.We have these new basis dimensions.What are the significance of these?How do we interpret them?What do they represent?Well, we've been talking about vectors in geometric space so far.So, we picture the principal components as directions in space,and they are that.But when we have a data table,the dimensions have another meaning as well.Each dimension represents a feature in a dataset,one type of measurement.For example, if we're talking about stock returns,each dimension represents the returns of an individual company.The new basis dimensions,the PCs, are combinations of the original dimensions.Therefore, they have funny units,some amount of returns of company A plus some other amount of returns of company B.These new units may or may notcorrespond to any quantity that makes sense in the real world.We say they may or may not be interpretable.Whether or not we think the new basis dimensions representsome real-world quantity and whether or not wecare depends on the problem we're trying to solve.

### 13. M4 L2b 14 Explained Variance V3-OdHeReNUqoQ.en

So, you have a sense of what the PCs are.Now, I have an important thing to tell you about how we use them.In fact, we frequently don't use all the PCs.Instead, we decide to use some fraction of them starting with the first,which we think explain most of the variance,and I'm going to explain what I mean by that in a moment.So, for example, with the 2D dataset I'm showing you here,if we decide to only use the first PC,instead of an x-coordinate and a y-coordinate for each data point,we just use a single coordinate that represents howfar along the first basis dimension the point falls.This is a lower dimensional representation of the same dataset and itmakes the most sense if the data approximately fall along a line to begin with.In this new representation,we lose the information about how farthe original data points lie in the direction perpendicular to the first PC.However, if those distances are small,we don't lose much information.But if we have many dimensions to begin with,how do we decide how many PCs to use?Well, it can depend on the application,but one way is by calculating how much variance each ofthe PCs account for and by dropping those that account for the least variance.That seems reasonable, but how do we implement that quantitatively?Well, we know that the variance along each dimension is an important quantity in PCA.It turns out that the total variance is thesame in the original basis as it is in the new basis.Let me explain what I mean by that.Let's start with the three data points we had before,where we've already mean centered the data.The variance along the original horizontal dimensionis the sum of the squares of these lengths.The variance along the original vertical dimensionis the sum of the squares of these lengths.But because of the Pythagorean theorem,the sum of the squares of all of these lengths equals the sum ofthe squares of the distances from the origin to each data point.When we find the new PC basis,we can calculate the variances along the new dimensions.But since each dimension is still orthogonal to every other dimension,we still have that the sum of the variances ineach dimension equals the squared distance to each data point from the origin.Since the sum of the variance ofall the data points is determined by their distance from the center,the sum of total variance is the same,regardless of which basis you choose.This quantity, the sum of the squares ofthe distances to each data point from the origin,is called the total variance of the data.As you have already seen,each principal component, that is,each new dimension for the data,is associated with some fraction of the total variance.The first PC is associated with the most,the second with the next most,and so on down the line.So, in order to decide how many PCs to keep,we might look at the variance of the data alongeach dimension and drop the dimensions along which the data vary the least.It is in this sense that PCA is used as a dimensional reduction algorithm.When we drop the dimensions that capture less of the spread of the data,we have lost some information,but retained most of the spread and thus most of the information.

### 14. PCA Toy Problem SC V1-uyl44T12yU8.en

Hello and welcome. In this notebook,we will learn how to use principal component analysis for dimensionality reduction.Dimensionality reduction, is one of the main applications of PCA.In the previous lessons,you've already learned how PCA works and about eigenvectors and eigenvalues.In this notebook, we will see how to apply PCA to a small dataset.Let's begin by understanding what dimensionality reduction is all about.Let's suppose we had some two dimensional data that looks like this.We can see that most of the data points lie close to a straight line.We can also see that most of the variation in the data occurs along this direction,but there's not much variation along this direction.This means we can explain most of the variation of the data byonly looking at how the data points are distributed along this straight line.Therefore, we could reduce this two-dimensional data toone-dimensional data by projecting all these data points onto this straight line.By projecting the data onto a straight line,we can actually reduce the number of variables needed to describe the databecause you only need one number tospecify a data point's position along a straight line.Therefore, the two variables that describe the original data can bereplaced by a new variable that actually encodes this linear relationship.It is important to note that the new variable isjust an abstract tool that allows us to express this data in a more compact form,and may or may not be interpreted as a real-world quantity.Now, let's see how we can do this in code.For simplicity, we will use a small two-dimensional dataset.In a later notebook,you'll get a chance to apply what you learned in this notebook to real stock data.We will start by creating some randomly correlated data.In this code, you can choose the range of your data and the amount of correlation.The code outputs a plot with the data points and the amount of correlation.In this case, we chose our data range to be between 10 and 80.Therefore, the datapoints range between 10 and 80 in both the x and y axis.Remember, a correlation of zero means no correlation at all,and a correlation of one means complete correlation.You can vary the amount of correlation and create the data that you like.Once you have created your data,the next step in PCA is to center your data around zero.It is also customary to normalize your data.This is known as mean normalization.While centering the data is necessary,normalizing the data is optional,and this is what I have done here.Mean normalization not only centers your data around zero,but also distributes your data evenly,in a small interval around zero.As you can see here,the data is no longer in the range between 10 and 80.But after normalization, the data is now distributed between minus three and three.This will help your algorithm converge faster.With our data centered,we're ready to perform PCA.To do this, we will use a package called Scikit-learn.Scikit-learn's PCA class allows us to easily implement PCA on data.The first thing we need to do is to create a PCA object witha given set of parameters including the number of principal components we want to use.We'll start by using two components because we want to visualize them later.Here, we can see the parameters that the PCA algorithm is going to use.The next step is to pass the data to the PC object using the fit method. A quick note.In Scikit-Learn, the PCA algorithm automatically centers the data for you.So, you could pass the original dataset tothe fit method instead of the normalized data as we have done here.Once we fit the data,we can use the attributes of the PCA class to seethe eigenvectors also known as the principle components, and its eigenvalues.One important attribute of the PCA class is the explained variance ratio.The explained variance ratio gives us the percentageof variance explained by each of the principal components.In general, the principle components withthe largest eigenvalues explain the majority of the variance,and this is usually the ones that we want to keep.For example, here we can see thatthe first principle component explains 94 percent of the variance,and has the largest eigenvalue.Now that we have the principle components,we can visualize them.Here, we see the data with the first principle component,and the second principle component.We can see that the first principal component liesalong the direction in which the data varies the most.One question that you will frequently faceis how many principal components you should use.For example, suppose you had a dataset with 1,000 dimensions.Should you reduce this dataset to500 dimensions or could you do better and reduce it to 100 dimensions?Usually, the number of principal components is chosendepending on how much of the variance of the original data you want to retain.For example, you may want to retain 90 percent of the variance,or you may only want to retain 50 percent of the variance.You can use the explained variance ratio attribute of the PCA classto determine the number of components you need tokeep to retain a given amount of variance.For example, if you wanted to retain 90 percent of the variance,you can add up the elements inthe explained variance ratio array until the desired value is reached.The number of elements you had to add up to reachthe desired value determines the number ofprincipal components needed to retain that level of variance.For example, if you had to add up five elements to retain 90 percent of the variance,then you will need five principal components.Now, that we have seen what all the principal components look like,we will now use PCA to perform dimensionality reduction.Since the data we're using in this simple example only has two dimensions,the best we can do, is to reduce it to one dimension.So, now choose the number ofprinciple components in our PCA algorithm to be equal to one.Once we ran the PC algorithm with only one component,we can see what the transform data looks like byusing the transform method of the PCA class.In this simple case,the transform method projects the data ontothe first principal component so we will just get a straight line.When working with higher dimensional data,the transform method will project the data ontothe lower-dimensional surface determined bythe number of principal components you used in your algorithm.

### 16. M4 L2b 15 PCA As A Factor Model Pt 1 V3-4E3C5E-MmkI.en

So, let's return to the main subject of this lesson,which is factor models of risk.So, how do we use PCA to create a factor model of risk?What are our data?They are a set of time series of stock returns for many, many companies.Our main motivations for using PCA are to reduce the dimensionality of these dataand also find a representation of them that captures a maximum amount of their variance.We'll use this representation or model of risk later whenwe seek to minimize risk as part of an optimization problem.We're going to be talking about a lot of matrix multiplications here,so I'm going to try to always write down the dimensions.I think keeping track of the dimensions will help you keep track of what's going on.It's certainly helps me.When we use a factor model, remember,we have a representation of the returns that looks like this.In order to produce a factor model of risk using PCA,we need to look at what the PCA algorithm givesus and map its outputs to each of these matrices;the factor exposures, factor returns,and the idiosyncratic risk matrix.So, let's look at what these symbols represent in terms of matrices.The returns matrix has dimensions of number of companies by number of time points.The matrix of factor exposures has dimensions,number of companies by number of factors.The matrix of factor returns has dimensionsof number of factors by number of time points.Finally, the matrix of specific risk hasdimensions number of companies by number of time points.Let's take a look at what we get from running the PCA algorithm.Remember that PCA finds a new basis for the data.If we keep all the PCs,then multiplying the representation of the data in the PC language bythe matrix of PCs completely recreates the data in the original basis.On the other hand, if we drop some of the PCs then multiplyingthe data matrix in the new basis by the matrix of PCs almost recreates the data.This is the compressed representation of the data.If we add in what's leftover,then we have a complete representation of the data.Now, this is looking a lot likethe factor representation we discussed a little bit earlier.In fact, we're just going to use this as our factor model.

### 17. M4 L2b 16 PCA As A Factor Model Pt 2 V2-sDbmO0kHx9A.en

The matrix that describes the coordinates of the principal componentsin the original basis is the matrix of factor exposures.The matrix of data transformed into the new language is the matrix of factor returns.How do we calculate the factor returns?Well, we multiply the original data by the inverse of the factor exposures matrix.Since we required this matrix to be orthonormal,its inverse is just its transpose.Remember that for the risk model,we need to calculate the factor covariance matrix. How do we do that?Well, we have the matrix of factors,we just calculate their covariance the same way we always calculate a covariance matrix.Remember that since the factors are projections ofthe data onto the PCs, they are orthogonal.This means that the factor covariance matrix is a diagonal matrix.This is true for PCA,but not necessarily true for other types of factor models.Finally, remember that the factors are returns time series.They don't necessarily have direct interpretations in the physical world,but you can think of them as returns time series for some kindof latent or unknown driver of return variance.If the returns time series are daily returns,then when we calculate their variances,we have estimates of the factor returns variances on a daily basis.Typically, we annualize these by multiplying the whole matrix by an annualization factor.Remember, we don't need the square root of the factor becauseour numbers here are variances not standard deviations.So, if we were going from daily to annualized,we multiply by 252.Lastly, we need to calculate the idiosyncratic or specific risk matrix.We know that what's left over from our model isthe original returns data minus the part we represent with the PCs.To calculate the specific risk matrix,we calculate the covariance matrix of the residuals,and set the off-diagonal elements to zero.Since we set the off-diagonal elements to zero,we now do not have a perfect recreation of the covariance matrix of the original data.But this is reasonable because we try to capture as much ofthe correlation in the data as possiblewith the principal components that we decided to keep.

### 19. M4 L2b 19 Outro V1-nfVnAkndJCY.en

Well, I hope you're excited because you have just learned a whole stack of things.You've learned about a really cool and commonly used algorithm,and also about one way it's used in finance to model risk.Now, hold on to your hats because next we're going to talk aboutpossibly the most exciting part of this whole series of lessons, Alpha factors.Finally, in the last lesson and in the project,we're going to put it all together. Stay tuned.

## Part 01-Module 04-Lesson 06_Alpha Factors

### 01. M4 L3a 01 Intro Efficient Market Hypothesis And Arbitrage Opportunities V3--YpXAt7zuh8.en

Welcome to the overview of alpha factors.This is the extra exciting part where you finally get to startturning hypotheses into code and testing that code against data.Alpha factors are tools operating on data that give ussignals about how stocks may perform relative to one another.You've heard a little about alpha factors already and how they differ from risk factors.Remember that alpha factors are hopefully predictive of future mean returns,while risk factors impart information about commonality of return variance.The search for alpha factors is essentiallythe search for deviations from the efficient market hypothesis.The efficient market hypothesis has different forms,but generally it means thatall available information is reflected in the current price of an asset,which suggests that assets are priced fairly.When searching for exceptions to the efficient market hypothesis,we're looking for mispricing in the market.We learned about arbitrage before,which is the simultaneous buying and selling of a perfect substitute to make a profit.In the context of this lesson,we'll also use the term arbitrage.But here, it refers to buying an asset that may beunderpriced or shorting one that may be overpriced,where the expected return is in excess of what it should be for the risk we bear.In this lesson, we'll start by discussingacademic research as a source for hypotheses and for alpha factor generation.We'll cover techniques for processing a raw alpha factor including methods,such as sector neutralization, ranking,Z-scoring, smoothing, and conditioning factors on other factors.Each of these techniques is important inthe process of turning the numbers in our alpha vectors intosignals that represent whether to buy or short each stock in a portfolio and by how much.That is to say, these techniques are relevant tothe goals that we have for real-world portfolios.We'll also cover techniques for evaluating factors,such as the sharpe ratio,information coefficient, information ratio, and turnover analysis.These will help you decide whether your alphas provide enough return relative to risk,and whether they will cause too much trading to likely be profitable in the real world.The topics we discuss here will prepare you for the following lesson,in which we extract alpha factor ideas from academic research papers,and then implement those ideas in code.This is exactly the type of work that quants do in the real world.In this lesson, we step into the typical life of a quant researcher.You'll see how quants generate alpha factor ideasthat may make their way into real hedge fund portfolios.

### 03. M4 L3a 02 Alpha Factors Versus Risk Factor Modeling V2-qsahBvhVTkk.en

Since we just talked about risk factors,let's talk about how both the risk andalpha factors help us create successful portfolios and how they're different.We use risk factors in risk factor models to helpus model the movement of our stocks prices due tocommon publicly known factors that may explain price movements ina broad cross-section of stocks defined as systematic returns and systematic volatility.Since these risk factors may significantly impactour portfolio volatility withoutproviding appropriate compensation in the form of returns,our goal is to neutralize the portfolio's exposure to these risk factors,so that their remaining movements of stocks inour portfolio can be attributable to our alpha factors.We can use an analogy where factors aresounds that you hear when eating lunch at a busy restaurant.Risk factors are like the background noise,like the conversations of all the patrons,the noise from the servers and the kitchen noises.The alpha factors are likethe soft voices of your friends who are sitting with you for lunch.If we don't neutralize the background noise,it may drown out the sound of your friends voices.Similarly, alpha factors have important information that may be overwhelmedby the movements of risk factors such asthe movements of the market or movements of a sector.We want to isolate the effect of alpha factors.To do that, we eliminate the effect of risk factors.

### 04. M4 L3a 03 Definition Of Key Words V4-zySdIQTPTGo.en

Note that the word alpha is used to mean different things within the world of finance.In the context of factor models,we'll refer to an alpha model asan algorithm that transforms input data into a list of numbers,one number for each stock under consideration per time step.A higher positive number means we want to put more money on that stock law.A negative number means we want to short the stock.An alpha vector refers to this list of numbers for a single time period, such as a day.Each number in the alpha vector is proportionalto the amount of money we want to allocate towards each stock,according to the alpha model.The alpha vector at each time step is standardized,so that it has mean zero and the sum of the absolute values adds up to one.We'll use the term alpha value to refer to a single number in the alpha vector.So, it's a number assigned to a single stock for a single time period, such as a day.We'll refer to an alpha factor as a time series of alpha vectors.So, it's the set of alpha vectors over multiple time periods,such as multiple days.A raw alpha factor is the initial output of the alpha model,excluding additional processing that is done to improve the alpha.We'll use the term raw alpha factor to distinguish froman alpha factor that has been processed to improve its signal,and make it easier to work with.Note that within the finance industry,there is no single consistent way that practitioners use the term alpha or alpha factor.Though, our use is the most common.We'll use these definitions to help us identifywhich step in our alpha generation we're referring to.We'll also be talking about our stock universe,or simply our universe.This refers to the set of stocks whichwe are considering in our portfolio at each time step.These are stocks that we may or may not hold positions in for our portfolio.

### 05. M4 L3a 04 Researching Alphas From Academic Papers V4-te0UTxemLBE.en

You may be wondering how to come up with alpha factors and trading strategies.Some sources of inspiration,maybe reading financial news,observing the markets for curious behavior,studying the methods of famous investors bothdiscretionary and want or talking to industry practitioners.We'll explore academic research papers as a source foralpha factor ideas through the remainder of this lesson and in the next lesson.It's important to understand a key point before we do this.We should not expect to getstrong production-ready alphas ''as is'' from academic papers.Why not? Because the very act of publication which makes the paperpublicly available also diffuses this information into the market.As participants adopt the trading strategy,the strength of the effect is diffused.Why then do we look at academic papers at all?For four reasons.First, these papers can spur idea generation for us to create derivative works.Second, the work conserve as baseline factors to compare with our own alpha generation.Third, we can learn how professional full-timeresearchers approach problems and validate their findings.Fourth, perhaps we can learn about new sources of data or novel ways to work with data.Academic research can be published in general interests economic journals,investment focused journals, or open source journals.General interest journals and investment focused journals bothrequire peer review and require a paid subscription.Open source repositories usually include working papers thathave not yet been peer reviewed and are also free.Open source repositories for research papers include,SSRN and the archive.One benefit to using open source journals is that there'sa broader selection of papers and their ideas more recent,since peer review is a process that takes time.It helps to figure out which other papers cite a particular article,to see how other researchers attempted toreplicate critique or improve upon the original idea.Once you've found some papers of interests,scan the abstract and find out if the data use is accessible to you.Focus on papers for which the data isaccessible so that you may try to replicate their work.Check that the methodology could potentially be practical.Academic papers almost never includereal-world constraints like transaction costs or liquidity directly.You need to make a judgment quickly if you think addingthese constraints will completely invalidate the ideas presented.Some papers include these aspects in a roundabout way.They conducted Turnover Analysis or they choosea stock universe that includes more illiquid stocks.Some examples of impractical methods mayinclude choosing a large universe of say 8,000 stocks,in which most of the factor returns are derived froma subset of the small and micro cap stocks.Remember that small and micro cap stocks are less liquid and factors thatrely on illiquid stocks may be difficult to implement in practice.Note that the process of finding academic papers, studying papers,implementing the papers and then evaluating themis a very important skill to have as a quantitative researcher.We hope to walk you through some of these examples,so that you will eventually have the confidenceto conduct your own research after this course.

### 06. M4 L3a 051 Controlling For Risk Within An Alpha Factor Part 1 V3-raeVfAbBXnA.en

Okay. Let's say we've read a paper,extracted some ideas, and have come up with an Alpha factor.Now, let's talk about some of the common early transformations we mightdo to that factor in order to move it closer towards our goal.A vector of numbers that represents the weights we'd use to create a portfolio.These transformations help to enhance, improve,and prepare our Alpha factors so that they may be usedduring portfolio optimization to choose actual portfolio weights.When we actually implement traits for our portfolio,we want that portfolio to be neutral to common risk factors.We should not wait however for the optimization step to think aboutcommon risk as we do not want to rely exclusively on optimization to make this happen.Rather, it's best to consider obvious common risks even at the Alpha research stage.The most significant common factor risks are market risk and sector risk.We control market risks explicitly by the definition of an Alpha factor.The sum of the values is zero implying zero exposure to the market.Note that an important assumption that we'll make is that onaverage the Betas or exposure of the stocks to the market are all one.So even though in reality the regression Beta of stock ABC to the market may be1.2 or the Beta of stock XYZ maybe 0.8.When dealing with stock universes of hundreds or thousands of stocks,we often can take the simplifying assumption that they all have the same Beta of one.To get the values to sum to zero,we subtract the mean from each Alpha value in the vector.When the value sums to zero,the theoretical portfolio is said to be dollar neutral,and so general market movements that affect all stocks are cancelled out.This may not precisely eliminateall market risk as market Betas may not all be equal to one,but this is often a fine assumption at this stage.

### 07. M4 L3a 052 Controlling For Risk Within An Alpha Factor Part 2 V2-Ks8HiHcflPs.en

So now that we've controlled for market risk,how can we control sector risk at this point.We often want our alpha factor to be sector neutralso that it's not exposed to overall movements withina sector and this means that we want our short weights inthe sector to be equal in magnitude to our long waits in the sector.We achieve this by calculating the average of all weights in the sectorthen subtracting the sector mean from each weight within that sector.Again, will make the simplifying assumption thateach stock's Beta to its sector is equal to 1.Let's look at a simple example withonly two stocks in the tech sector; Apple an Alphabet.To make our alpha factor neutral to the tech sector,we could go along one of these stocks and short theother with the weight of the same magnitude.If we think about our raw Alpha factor,let's say that Apple's raw Alpha value is0.33 and Alphabet's are raw alpha value is 2.31.The average is 1.32 and we subtract this mean from each stock's raw-value.So Apple's sector neutral Alpha value is now negative0.99 and Alphabets is positive 0.99.So by subtracting the industry average,we're able to convert this raw Alpha factor into an Alpha factor that is sector neutral.Again this is because the sum of the negative weights isequal to the magnitude of the sum of the positive weights in each sector.If the entire sector were to move up or down,the sector neutral portfolio would be unlikely affected by this movement.A portfolio would likely consist of multiple sectors.So we'd repeat the sector neutralization for each sector.Note that we can neutralize the Alpha vector by market and also by sector.So in practice, we could first neutralized bythe market and then take that re-scaled vector and neutralize it by sector.

### 09. M4 L3a 06 Ranking Part 1 V4-4j2hIB7WHY4.en

You may notice, that if the amount we invest in each stock ofour portfolio is tied to the Alpha value that we get from daily data,then we would be constantly buying and sellingevery day in order to follow the signal faithfully.In other words, as the Alpha vector changes every day,we'd have to adjust our portfolio weights every day also.We also have to address what happens to our Alpha vector when we encounter outliers,or extreme values in the data.If we have had a large increase in the office signal for one stock,then a sharp decrease the next day,this would effectively tell us to buy a lot ofthat stock and then sell a lot of that stock the next day.This may or may not be warranted.In the real world, trading costs money.So, we want to be very confident that if we go to make a trade and bear that cost,that it is indeed warranted.Some ways to keep extreme values from leading to unnecessarily large trades,are by clipping very large and small values at forexample the 95th percentile and the fifth percentile.This process is called winsorizing.Here's an example of winsorizing inAlpha vector which has Alpha values for each stock, for a single day.For any number that exceeds the 95th percentile,we've replaced that outlier with a number at the 95th percentile.Also for any values that are below the fifth percentile,we replace those with a number at the fifth percentile.Again, this is called winsorizing.We can also deal with outliers,by setting a maximum magnitude allowed waits for any single stock.Note that we would handle outliers for each Alpha vector which may be updated each day.Even when we've dealt with outliers,there's still the issue of whether it makes sense to buy and sellbased on the signal if their relative magnitudes for the Alpha values don't change.Let's again take the example of Apple and Alphabet.One day, Apple's Alpha value is 0.33 and Alphabet's value is 0.31.What if on the next day,Apple's Alpha value increased by 0.01 and Alphabet's Alpha value also increased by 0.01.If we translated these directly into portfolio weights,the weights would change slightly from day one today two.But the important thing to notice is that we'd still beputting more money on Apple relative to Alphabet.So, maybe we wouldn't actually want to change our positions at all.Often what we want to do,is have a more robust version of the signal which is able to withstand outliers,handle noise in the data,and also keep us from making potentially excessive traits.We can handle this with a ranking.

### 10. M4 L3a 07 Ranking Part 2 V2-uwPUV5LBhWY.en

Ranking is a broadly useful method in statistics tomake calculations more robust and less sensitive to noise.So, how do we use ranking here?If we have just two stocks in our portfolio,we would sort them by the office signal in descending order.So, for instance, let's say we had a Raw Alpha values for Apple, Alphabet, and IBM.If we sorted their values and gave a rank of one to the lowest value,two for the next highest value,and three for the highest value;we've converted an Alpha Vector of decimals into an Alpha Vector of ranks.Note that we use descending order to preserve the property thatthe highest numerical Alpha valueis proportional to the stock we think will rise the most.Now, let's see how ranking helps us deal with outliers.If Apple's Alpha value surged from 0.33 to 0.50 from day one to day two,and the Alpha Values of the other two also increased by 0.01 to 0.02,then the relative order or ranks of the three stocks wouldn't change.When this gets translated into portfolio weights,it means that we wouldn't be expected to make a trade based on these ranks.We can also see how ranking helps us deal with noise.If Apple's signal increases by one percent from day one to day two,and Alphabet signal also increases by one percent,their relative values don't change.So, the rank of Apple stays at two onboth days and the rank of Alphabet stays at one for both days.Again, ranking is a broadly useful method for making data analysis more robust.We'll see ranking used again later in this lesson.

### 13. M4 L3a 08 Z Score V3-6_cKCoLa92o.en

It's common for researchers to normalize alpha factorsby subtracting the mean and dividing by the standard deviation.We can also refer to this process as Z-scoring because the result is a Z-score.Z-scoring helps to standardize the data,so that it has a consistent range and distribution.This makes it easier to compare and combine more than one alpha factor.For instance, if we assume a somewhat Gaussian distribution,then about 95 percent of the values ina Z-scored distribution would be roughly between negative two and positive two.Notice that a ranking also hasa similar benefit of setting different alpha vectors on the same scale.Whatever the original raw alpha values are fortwo different alpha factors when we convert to ranks,if we had a universe of 100 stocks,then the ranking for any alpha would be the integers 1-100.Note that Z-scoring has an additional benefit that you wouldn't get from ranking.Z-scoring sets alpha vectors on a comparable scaleeven when we're dealing with differently sized stock universes.For instance, certain alpha factors may be generated on different stock universes.So, if we had one alpha factor calculated on a universe of100 stocks and another alpha factor calculated on 500 stocks,the rankings for the first would range from 1-100,while the rankings for the second would range from 1-500.Converting the ranks into a Z-score would rescalethe two alpha vectors so that they could be comparable and more readily combined.Recall that ranking still makesour alpha vectors more robust against outliers and noisy data,and Z-scoring does not provide the same benefit.So, a valid approach would be to use ranking onlywhen you know that all your alpha vectors are using the same stock universe.It's also useful to apply ranking and then Z-scoring,if you think that some of your alpha vectors maybe generated on different stock universes.One thing to remember is that when you work in a large asset management company,you may be one of many alpha researchers,and there may be other team members,portfolio managers, who use your alpha vectors.In this case, the portfolio managers want toget standardized outputs from all the quants.Z-scoring is one way to achieve this.Academic research often uses Z-scoring,as many papers are based on the work of Fama and French,who designed one of the first multi-factor models.Since this paper has influenced many subsequent papers in the study of factors,it's likely you'll see Z-scoring in academic papers.

### 16. M4 L3a 09 Smoothing V2-mAfrjpZOf7Q.en

Financial data is noisy,and sometimes the data we're working with is sparse.For instance, it might have many missing values.We can apply smoothing techniques acrossthe time dimension to help make the factor more robust to noise and sparse data.Fundamental data is also sparse in nature,since it is only updated in the US,for example, once every three months.If we wish to use in a higher frequency,such as daily frequency,we should copy the most recent data over toeach new day until the next new data point arrives three months later.We could also apply smoothing so that the daily data pointsincorporate some weighted averaging of not only the most recent update,but also the value from previous quarters.To make our Alpha factor more robust to noise and sparse data,we can apply a rolling window average.A rolling window is a kind of smoothing technique.A variation of a rolling window average is a weighted average,where the most recent Alpha may be given more weightand prior Alpha values are included but given less weight.This is called linear decay.For example, if we choose a window length of two and tookthe weighted average of the most recent Alpha values and the prior days Alpha values,we could give the newest value a weight of two and the earlier value a weight of one.In general, for a window length of T,we would give the most recent Alpha value a weight ofT. Then for the earlier alpha values,we'd give them smaller weights.This technique can be very effective.In fact, I've seen several cases where the application ofa smoothing operator both increase the Sharpe ratio and decrease the turnover.By the way, we'll cover the Sharpe ratio anda proxy for measuring turnover later in this lesson.These are both important evaluation metrics that help us find out ifan Alpha factor that we create has the potential to enhance a portfolio's performance.

### 19. M4 L3a 10 Factor Returns V5-enyeTpyCS-o.en

Once we've calculated an Alpha factor,we can calculate some evaluation metrics,that let us compare it to other Alpha factors and get a sense for how itmight perform when used to design a real-world portfolio.These metrics include factor returns,sharpe ratio, information coefficient,information ratio, quantile analysis, and turnover analysis.We evaluate Alpha factors to help us decide whether we'lluse them in creating a combined Alpha factor,which we'll then use within portfolio optimization.One useful construct, is the return of the factor,which is called the factor return.The idea is that this is the return that a theoretical portfoliodesigned to capitalize on the arbitrage idea presented by the factor would produce.You can think of this as a way to directly measure the returnsyour portfolio would have if their weights were determined purely by the Alpha factor.To calculate the factor return we create a theoretical portfolio in which the weights foreach stock on each day are set equal to the Alpha value for each stock on that day.So, every day we use the prior day's known data to calculate an Alpha vector,which we would standardize to have mean zero,and the sum of absolute values equal to one.We use the Alpha value of each stock i,as the weight for that stock on the current dayT. At the end of that day we check the returns of each stock i,for that day T, which we'll call the single day return.Then, we can calculate the weighted average of the returns,using the Alpha values as weights.This gives us the factor return for a single day.We can repeat this for multiple days for a window of a year or more.This time series of daily portfolio returns is the factor return.Note that we'll use historical data to calculate the factor return.So, the example of calculating this one day at a time,is to help you see that we're simulating daily portfolio decisions that areonly based on information that existed at the time of the theoretical trade,and these portfolio decisions are refreshed every day.Notice that the factor return depends uponthe stock universe and time window of our theoretical portfolio,but it's a useful way to compare various Alpha factors to eachother if we choose the same universe and time window.

### 23. M4 L3a 11 Universe Construction Rule V3-Cr0-k7gUSNg.en

One important thing to note is that when we say "universe",we really mean Universe Construction Rule.Meaning, we cannot pick a static list ofstocks and just use those for all time periods historically.If we chose a static list of stocks that exists today,or that have data for the time window of interests,that would include look-ahead bias.Look-ahead bias occurs when we simulate a trade onhistorical data using information that wouldn't be known at that time.A particular kind of look-ahead bias is survivorship bias.Survivorship bias occurs when we have the benefit today of choosing stocks for the past.In order to have an unbiased sample for our stock universe,we would not want to exclude companies that,for example, went bankrupt or were acquired because in the past,we would not know about those future events.Instead, we have to use a rule thatcreates the universe as it would have been for each day.One way to do this is to use a commercial index providerand use the components of an index as they were historically.That's actually what we do here.We're using the S&amp;P 500 index.The S&amp;P 500 is an index that's managed by Standard and Poor's.They managed index, adds, and deletes.By choosing our stock universe to include the stocks when they're inthe index and exclude them when they are no longer in the index,we can more faithfully simulate a portfoliothat would have existed in that point in time,thereby reducing look-ahead bias.

### 24. M4 L3a 12 Return Denominator Leverage And Factor Returns V3-QxHrP5LoXAI.en

Let's get back to the interpretation of the factor return.When initially learning about alpha factors,the magnitude of the return can create some confusion.In the alphas we work with below the annual returns range from 1%-4% or so.Let's be honest, that seems really small,is that what institutions really want from their researchers?The key to placing these returns and context is to understand the concept ofthe return denominator in research andthe actual leverage likely to be used in production trading.Let's first define return denominator,when we normalize an alpha vector one of the stepsis to sum up the absolute values of the alpha values,then divide each value by the sum in order forthe normalized vector to have the sum of absolute values equal to one.The sum of the absolute magnitudes ofthe alpha values is the denominator that I'm referring to.Leverage is the act of borrowing money in order to take positions in assets like stocks.The leverage ratio is the sum of the absolute value of all positions long andshort divided by whatever capital we actually devote to supporting those positions.This concept will likely become more clear when we study backtesting in term two,until then, think about it this way,a trading strategy produces dollars of gains and losses,to convert those dollars in terms of returns or in return space,we need to divide by some denominator,which in this case is called the capital denominator.At the research stage,we ignore this and simply assume one dollar of capital to one dollar of longs and shorts.When we increase leverage by puttingless than one dollar of capital against one dollar of positions,the leverage ratio increases.A leverage ratio of one means we apply one dollar of capital for one dollar of positions.A leverage ratio of four means,we apply 25 cents of capital for one dollar of positions.In research, we are looking at the return of the normalized alpha factor,the denominator is one,which implies a leverage ratio of one.In a real trading,we would combine several factors and then apply leverage,depending on the final portfolio,an institution might apply leverage of anywhere from two times to six times.In the context of that leverage the magnitude of the alpha factor returns start to makesense and likely look comparable tothe published returns observable from hedge fund return indices.This insight should also help explain why I continuallyrefer to implementing this style of trading as inside an asset management firm.It would be very unlikely that an individual,could obtain the necessary leverage andlow-cost market access to allow him or her to trade in this style,in an individual brokerage account.

### 27. M4 L3a 13 Sharpe Ratio V4-W8nfg1fkloA.en

The sharpe ratio, sometimes referred to as the risk adjusted return,is a key metric for evaluating alpha factors.This is the ratio ofthe daily factor return divided by the daily standard deviation of the return.As an example, if we had three years worth of daily data,we could calculate the average daily return over those three years anddivide by the sample standard deviation of those daily returns,and then analyse that by multiplying bythe square root of the number of trading days in a year.If we have two factors that have the same universe and similar turnover profiles,we will prefer the one with the higher sharpe ratio.Sharpe ratio is the key metric that institutional asset managers are judged on.Note that the Sharpe ratio,like many evaluation metrics for alpha factors,help us to compare the relative performance of alpha vectors.The sharpe ratio is key,not the magnitude of factor returns.Because as we learned in the immediately preceding section,we can amplify or dampen returns by the use of leverage.It is the sharpe ratio that gives us the confidence to apply that leverage.

### 30. M4 L3A 141 Ranked Information Coefficient Part 1 V4-_huNulOIuB0.en

A useful evaluation metric is the rank information coefficient,often referred to as rank IC.The rank IC tells us whether the ranks ofour alpha values are correlated with the ranks of the future returns.In other words, if the alpha factor suggested that we betmore on stock ABC and less on stock XYZ,was the future return of ABC relatively high?Was a future return of stock XYZ relatively low?If the future performance of the assets matchedthe expectations that was suggested by the alpha factor,then the information coefficient would be higher.Otherwise, it would be lower and possibly negative.For some insight, let's first look at this without ranking.Let's pretend there are just two stocks in our universe, ABC and XYZ.Stock ABC has a high positive alpha value andXYZ has a very negative alpha value as calculated before time,t. Between time T to time T plus one,the return of stock ABC is positive and XYZ's return is negative.These alpha values appear to be correlated with the forward asset returns.Note here, when we say asset return,we're referring to the return of each stock for each time period.We also specify that the asset return is a forward return.We say, forward asset return to specify that the return is inthe future or forward in time compared to when the alpha value was calculated.So, if the alpha values are calculated before time t,then the forward asset return is calculated with data thatoccurs later from time t to time t plus one.To make our evaluation more robust,we want to use ranks instead of the original alpha values and returns.Stock ABC is alpha value before time t is high and positive,while stock XYZ is alpha value, is negative.So, ABC has a higher rank of two and XYZ has a lower rank of one.The forward asset return of ABC from time t to time t plus one is higher compared to XYZ.So, the rank of ABC's forward asset return is two,and the rank of XYZ's forward asset return is one.If the ranks of the alpha values and the ranksof the forward asset returns are highly correlated,the rank IC metric would be close to one.In this example, since the ranks ofthe two alpha values are equal to the ranks of the two forward asset returns,then the rank IC in this example is one.

### 31. M4 L3A 142 Ranked Information Coefficient Part 2 V5-WKGmog0Nzgo.en

More formally, here are the steps we'll take to calculate the Rank IC.For each time period,rank the raw Alpha factor.For instance, with a stock universe of three stocks,the lowest Alpha value has a rank of 1,and the highest Alpha value has a rank of 3.Also, calculate the forward asset returns and rank them.The stock with the lowest forward return has a rank of 1,and the stock with the highest forward return has a rank of 3.The correlation of ranked values is calledthe Spearman rank correlation to distinguish it from the Pearson correlation.Let's clarify these two terms.The Pearson correlation is what you're probably familiar with,is the covariance of two variables which is thenre-scaled using the standard deviations of the two variables.The denominator makes the correlation range from negative 1 to 1.The Pearson Correlation is also the square root of the R-squared in a linear regression.Recall that the R-squared represents the proportion ofvariance in one variable that is explained by the second variable.The Spearman rank correlation is the same as the Pearson correlation,except the variables x and y are converted toranks before calculating the covariance and standard deviations.To get the rank IC,calculate the correlation between the ranked Alpha vectorand the ranked forward asset returns for a single time period.Repeat this over multiple time periods to get a time series.You may be wondering why we usethe Spearman rank correlation as opposed to the Pearson correlation.That is why do we use ranking?The answer is because we don't care about being wrong in the right direction.Let's say stock ABC is our top Alpha value in a universe of 2,000 stocks.Our alpha value will likely be a small positive value,then imagine that ABC outperforms all the other stocks not by a small amount,but by a very large amount.The Pearson correlation would calculate a lower scoreeffectively penalizing us because we didn't get the relative magnitude correct.On the other hand, the Spearman rank correlation would not be affected by this.We prefer to use ranking to evaluate our performance becausewhat matters is that our Alpha is still profitable as we had hoped.

### 34. M4 L3a 151 The Fundamental Law Of Active Management Part 1 V4-iCW_vqvrTlw.en

A metric that is a special application of the Sharpe ratio is the information ratio.Recall that we are separating known factors as risk factors,which drive the variants of stocks and Alpha factors whichdrive portfolio returns after neutralizing to those risk factors.The return driven by risk factors is the systematic return,whereas the return driven bythe Alpha factors after taking out the effect of risk factors,well, this is the specific return.The specific return is also referred to as the residual returnbecause it is the residual amount of return thatremains after taking out the systematic return.The information ratio is the Sharpe ratio applied to the specific return.So, it's the average specific return divided by its standard deviation annualized.The information ratio can be interpreted as the Sharpe ratio thatmeasures the performance that the fund manager contributes to the portfolio.Note that in the context of what we're doing in this lesson,that is Alpha modeling,we are creating a market and common factor neutral portfolio.Note that in this case,the Sharpe ratio equals the information ratio.We simply introduced the term information ratiohere because you will see it often in academic literature.As I stated previously,the most important metric in evaluating an Alpha factor andultimately a trading strategy or fund is the Sharpe ratio.So, some good advice might be, okay,just create Alpha strategies with high Sharpe ratios.That advice though is pretty useless because it's descriptive not proscriptive.In other words, it tells us what we want to achieve,but it says nothing about how to get there.So, how do we actually create high Sharpe Ratio strategies?Well, the fundamental law of active management gives us some insight.From a philosophical standpoint,perhaps the most important thing you will learn inthis program is the following relationship;IR equals IC times the square root of B.This comes from the seminal work a Richard Grinold,which is aptly titled,the Fundamental Law of Active Management.This is further detailed in the subsequent book by both Richard Grinoldand Ronald Khan titled, Active Portfolio Management.By the way, this formula IR equals IC times the square root ofB sometimes referred to as the E equals MC squared of finance.I just want to make a point about this formula.This result is based on some simplifying assumptions and should be thought of as a guide.It would be more correct to say that IR isproportional to IC times the square root of B as opposed to equal.This is not a formula we will use directly.Meaning, we don't actually use it to make calculations.Rather, we use this identity to direct how we structure our investment process.Also, don't worry if the relationship between IR, IC,and B doesn't look obvious to you as it's not supposed to be.If you're interested in the derivation,you can check out the original paper or the book by Grinold and Kahn.I haven't defined what IR, IC,and B mean in the context of this relationship.But stay tuned, it's coming up.

### 35. M4 L3a 152 The Fundamental Law Of Active Management Part 2 V7-CMc4ujA8Ahs.en

The breadth is the number of independent trading opportunities annualized.The key insight on this is the word "independent."For example, if we arelong 30 oil services stocks and short 30 semiconductor stocks for a year,how many independent bets is that?The answer is one.Not 60, one.Why? Because we expect the stock holdings in each sector are diversifiedto such an extent that we are likely not going toget any material impact from any one stock,and we have simply one bet thatthe oil service sector will outperform the semiconductor sector.The key takeaway here is that in order tomaximize the number of independent bets in our portfolio,we need to remove exposure to common risk factors.Again, risk factors are factors which indicate which stocks have strong commonality.If we hold positions with strong commonality,then we are not holding independent positions.At the Alpha research stage,this is precisely why we often demean by sector.One way to improve our Sharpe ratio is to increasethe probability that our Alpha factor can accurately choose the correct stocks.This is measured by the information coefficient.We should always strive to do this but recognize that this is indeed very challenging.The other way to improve performance is toincrease the number of independent bets we make.This is the breadth.This is the key advantage quants have over everyone else;the ability to achieve high breadth.In general, a reason whyinstitutional investors take a cross-sectional approach of tradingmultiple stocks is to maximize breadth andimprove their performance which can be measured as the information ratio.Additionally, institutional investors will combine many Alpha factors,again to maximize breadth.

### 36. M4 L3a 161 Real World Constraints Liquidity V3-eu0YZRMu_3w.en

So far, we've looked at evaluation metrics thatassume unlimited liquidity in zero transaction costs.In other words, we've assumed that we can buy or short the stocks when wewant and at the market price without cost or impact to that marketplace.We also saw that, increasing the number of stocks and the number of tradescan improve one evaluation metric, the information ratio.However, when we include real-world constraints and costs,there are reasons why more trades aren't always better.When trading in the real market,there are constraints due to both liquidity and transaction costs.Liquidity of a stock refers to how likely one canbuy or sell the stock when they want to in the amount that they want to.A proxy for liquidity is the bid-ask spread.The bid-ask spread is the difference between the price thatan investor can buy versus sell immediately in the market.High liquid stocks that trade in high amounts every day havemany shares that are accessible to the public and can be borrowed for short positions.Examples would be large market cap stocks in highly developed stock markets like the US.The bid-ask spreads in liquid stocks maybe around three to five basis points.For example, if the price at which one can sell a stock is$100 and the price at which one can buy a stock is 100.05,then the bid-ask spread is said to be five basis points.Low liquidity stocks havefewer accessible shares or are traded on a less active exchange.You may also refer to low liquidity as highly illiquid.Bid-ask spreads for less liquid stocks maybe 20 to 30 basis points.Remember, a decent Alpha factor might havean annual return of four percent or 400 basis points.When you think about that,a bid-ask spread of 20 to 30 basis points on a single trade is massive.

### 37. M4 L3a 162 Real World Constraints Transaction Costs V2-HAif7xSh8z0.en

Another real-world constraint is the total transaction cost,the implicit and explicit costs of making trades.Transaction costs add up and are oftena substantial contributor to the success or lack of success of a trading strategy.Most people immediately think of commissions,the explicit cost, as the transaction cost.However, for an institutional market participant,the commission is usually by far the smallest component of the true transaction cost.The biggest component, the implicit component,is what is called the market impact.Institutions need to make large trades.Market impact is the effect thata market participant has when he or she buys or sells an asset.It is the extent to which buying or selling moves the price against the buyer or seller.More buying will move the market price upward.More selling will move the market price downward.For instance, if a fund is trying to sell a large block of shares to an investment bank,then the larger the block of shares,the more risk the investment bank is taking in holding the opposite side of the trade.So, the investment bank would ask for a better deal in the form of a lower price,which would then diminish the profit of the fund.To avoid adversely moving the market price,they will likely slice the trade into smaller blocks.Then we'll trade those blocks over time.Taking more time to trade all blocks also entails the riskthat the market price will move over that time.So to summarize,real-world transaction costs can diminish the performance of an alpha factor.So, we want some way to evaluatean alpha factors potential impact on a portfolio's total transaction cost.

### 38. M4 L3a 171 Turnover As Proxy For Real World Constraints V2-6xo8sZjoSVk.en

Since liquidity and transaction costs aredependent upon market conditions at the time of a trade,it's difficult to simulate actual transaction costs when evaluating an Alpha factor.So a useful proxy for these real-world constraints is to measure turnover.Generally, factors that are updated quarterly such as fundamental factors,have lower turnover compared to factors that areupdated daily such as those based on price and volume data.Turnover measures what fraction ofthe portfolio's total value gets traded in a time period.For example, let's say a portfolio is valued at $100 million.On a single day, let's imagine one million dollarsare brought and another one million dollars are sold.So, the turnover in this case is two million dollarsdivided by 100 million or two percent.Since we're out the Alpha research stage and not dealing with an actual portfolio,turnover can be defined in terms of the change in portfolio weights.We calculate turnover by taking the difference betweenweights for each stock between yesterday and today,then we sum up the absolute values of the weights for all stocks in the portfolio.Recall that when constructing an Alpha factor,we're likely to convert raw factor values into ranks.So, there is another way in which we can measureturnover called the Factor Rank auto-correlation. We'll see this next

### 39. M4 L3a 172 Factor Rank Autocorrelation Turnover V2-QBvbMiVW100.en

Another way to measure turnover,is called the factor rank autocorrelation.The factor rank autocorrelation is close to one,when the ranking of stocks doesn't change much from day to day.To give you an illustration,let's imagine our universe consists of just two stocks, apple and alphabet.If over several days,the rank of apple,based on its alpha signal is always two,and the rank of alphabet is always one,then the previous ranks of each stock areperfectly correlated with the current day's ranks.In other words, the ranks are highlycorrelated or the factor rank autocorrelation is close to one.When the ranks of stocks don't change much from day to day,this also means that the weights of our theoretical portfolio don't change much either,which means less trading,and therefore less turnover.To calculate the factor rank autocorrelation for a single time period,we'll get the ranked alpha vector for the previous day,and also the ranked alpha vector for the current day.Then we calculate the Spearman rank correlationbetween the prior days and current days ranked alpha vectors.We can repeat this for multiple days and a time window,so that we have a time series of factor rank autocorrelations.A high factor rank autocorrelation is a proxy to indicate that the turnover is lower.A low, or even negative factor rank autocorrelation,would then indicate high turnover.Let's look at how we would use a turnover measure,such as factor rank autocorrelation in the context of other evaluation metrics.If two alpha factors have similar sharpe ratios,similar quintile performance, and similar factor returns,we'll always prefer the one with lower turnover.If an alpha factor has high sharpe ratio,but very high turnover,we may need to consider whether the alpha factor would survive backtesting,paper trading, and real trading.As a reminder of why lower turnover helps us,it makes it more possible for us to execute trades ifthe stocks are liquid and can reduce our transaction costs.Managing turnover is a delicate balance.On one hand, we want high turnover because thatmeans alpha factor is taking advantage of new information,and making more trades which likely implies higher breath.However, on the other hand,trading costs money andexcessive turnover could mean the alpha is simply capturing noise.Over time, the more you work in alpha research,the more you will develop intuition about this.Evaluating alpha factors is both an art and a science.Keep in mind that factor rank autocorrelation is different than rank IC.Even though these metrics both use ranking and correlation,rank IC is measuring whether an alpha factor is correlated with forward returns,while factor rank autocorrelation is measuring howstable the ranked alpha vectors are from day to day.

### 41. M4 L3a 181 Quantile Analysis Part 1 V2-oT5GFbg0G8g.en

We saw earlier that the rank IC can tell us overall how wellan Alpha vector's predictions alignwith the subsequent stock returns for the next period.You may be wondering if it's possible to drill downdeeper to look at the Alpha values assigned each stockto see which subset of stocks actuallycontribute most or least to the factor return of the portfolio.Ideally, if our Alpha model and hypothesis were accurate,the stock with the highest Alpha value for that daywould also have the highest positive return the next day.Similarly, the stock with the lowest Alpha value forthat day would also have the lowest return in the next day,which would be good for our theoretical portfolio since we would be shorting that stock.Remember, that we are working withperhaps hundreds or even thousands of stocks in our universe.So, a good middle ground is to divide our Alpha vector intoquantiles and analyze their returns and volatility within those quantiles.An ideal Alpha model would be one where the group of stocks containingthe highest Alpha values for that day would also have the highest average return,and possibly, the highest risk adjusted to return.Similarly, the group of stocks containingthe lowest Alpha values would ideally have the lowest returns.For example, if we have 25 stocks in our universeand wanted divide them up each into five groups of equal size,we would be using five quantiles also called quintiles.For each day, we'll sort the stocks by their Alpha value.The five highest values go in the fifth group,which are above the fourth quintile.The stocks with the lowest five values go in the first group,which are below the first quintile.Similarly, we fill groups 2, 3,4, with five stocks each based on their Alpha value.We can keep track of the individual returns withineach of the five groups over a time window such as one,three, or five years.Then we can calculate the mean return withineach group as well as the standard deviation.We can call what we just did quantile analysis or quantile performance.Since we chose five quantiles,we can also call this quintile performance.

### 42. M4 L3a 182 Quantile Analysis Part 2 V3-NF18kx0sfBE.en

Here's an example of an ideal Alpha factors quintile performance.The fifth group containingthe highest Alpha values would ideally give the highest returns.There would also ideally be a smooth progression,down the fourth, third,second, and first groups.That is the fourth group would have lower returns than the fifth group,but higher than the other groups.The first group would ideally have the lowest returns,and the second group would have the second lowest returns.It's common for the tails to show predictive power;that is group five may have higher returns,group one low returns, but groups two, three,and four have returns not that much distinguishable from zero.In these scenarios, the Alpha model's performance ismore reliant on a smaller subset of the Alpha vector.In other words, on any given day,the factor return is due to the model being right on a subset of the stocks.Similarly, it's also possible for the tails to showless predictive power and for mostof the factor return to be attributed to the middle groups;two, three, and four.Again, this means that a subset ofthe entire Alpha vector appears to get it right on any given day,and it's usually less correct when the Alpha number is either very high or very low.You must diagnose these situations very carefully.In most cases you want to see a monotonic relationship.Each successive quantile produces a higher return than the previous one.In general, if you don't see this it couldindicate that your factor is not robust and invalid.However, there are exceptions and you should thinkabout this before you run the quintile analysis.One example is the Sloan's accrual anomaly which you can read about on the link below.Whenever the factor return relies ona smaller subset of the Alpha vector having predictive power,then there's higher risk associated with using that Alpha.

### 45. M4 L3a 19 Quantiles Academic Research Vs Practitioners V2-AwL7cV2VyhM.en

As practitioners, we were reading academic research papers for insights.So, it's helpful to knowhow academic papers and practitioners view quantiles differently.Academic papers tend to take raw alpha signals,which are not ranked,and split them into quantiles only.Their analysis usually focuses on the tails.That is, the highest and lowest quantiles.The reason that academic researchers are focused on this,is that they want to detect broadly applicable market phenomenon,and are not necessarily interested in how to implement a trading strategy.Because of this, you may notice that certain academic researchwill find an effect in just one tail such as the lowest quintile,while they find less of an effect inthe other tail quantile or even in the other quantiles in between.Note that this means that the abnormal returns that they detect in such a paper,may only work on a subset of the stock universe.Practitioners have the goal of implementing a trading strategy thatcan be applied to all the assets in their universe.This means that with quintile analysis,practitioners care about how the factor performs across all quantile groups,not just one or two tails.This is because practitioners want to see how the factorinfluences all stocks and there trading universe.So, even if a research study found that an alpha factoronly worked in the subset of the quantile distribution,the practitioner will view this asa graded spectrum in order to generate an alpha number for every stock in their universe.

### 46. M4 L3a 20 Transfer Coefficient V3-4rZ0MWQzlIs.en

Recall that we prefer to control for risk within the Alpha factors,rather than waiting for their risk model optimizer to do it completely for us.This is why we make the Alpha vector dollar neutral and often also sector neutral.The reason we wish to control for risk early on withinthe Alpha vector itself is so that after the optimization,the portfolio weights are not too different from the original Alpha vector.It's natural for you to wonder why is it an issueat the portfolio weights are nothing like the original Alpha vector?To understand this, remember that we're evaluatingeach Alpha factor in isolation of other Alpha vectors.We evaluate the Alpha factor to see if it may bea good candidate to help improve the performance of a portfolio.If after optimization, the risk model significantly changesthe weights for stock so that they nolonger follow the weights chosen by the Alpha vector,then we can't expect the performance of the Alpha to carry over into the portfolio.As the example, imagine if an Alpha vector gives most positive weight to stock ABC.But after the risk model neutralizes various common risks,the final portfolio weights end up shorting stock ABC,then the signal from the Alpha vector which essentially says,"Buy stock ABC" is no longer being followed.This means that the performance that we originallyhoped to gain from using this Alpha vectormay not carry over or transfer to the portfolio itself.Remember that the Alpha vector is an expression of expected returns.If the portfolio weights after every optimizationare very different from the original Alpha factors,then the portfolio itself may not becapturing the information expressed by those Alpha vectors.To see how closely the optimized portfolio weights still match the original Alpha vector,we can calculate something called the transfer coefficient.The transfer coefficient measures how closelythe optimized portfolio weights match the original Alpha vector.To get the transfer coefficient,we need to use portfolio optimization that includes a risk factor model.We will learn about the portfolio optimization step in a later lesson.For now, let's just assume that we havea portfolio optimizer that outputs portfolio weights.We start with our Alpha vector,pass it through the portfolio optimization and get portfolio weights.The transfer coefficient is the correlationbetween the Alpha vector and the resulting portfolio weights.A high transfer coefficient is a good sign thatthe Alpha vector has survived the portfolio optimizers risk neutralization.The transfer coefficient is also a useful measure foranother desired attribute of an Alpha factor independence from risk factors.Ideally, we would like to findAlpha factors that are independent of the common risk factors.If Alpha factors were too correlated with the risk factors,then they would also be neutralized by the risk model.

### 48. M4 L3a 21 Its All Relative V2-VBcOrT7TuFA.en

In some alpha factor papers that we'll discuss in the next lesson,the academic research discusses findings that are specific toone direction or one tail end of a particular attribute.A practitioner's approach is to think aboutextending the interpretation of the academic conclusion,so that it can be more readily applied to general situations.For example, in an alpha factor that we'll discuss,the curvature of a stock's price path over timehas some predictive value of its future expected performance.Even though the paper itself doesn't discuss linear trajectories,a practitioner would want to find some meaningful hypothesisabout straight lines and how they compare to convex or concave functions.In other words, when a straight line is compared to a convex curve,the line is relatively more concave than the convex curve.By thinking how an academic finding can be applied morebroadly to a full range of a particular attribute,a practitioner has a better chance of creating an alpha factor thatcan more generally be applied to the entire stock universe.

### 49. M4 L3a 22 Conditional Factors V2-2J1aUwGq6tc.en

So far, we've been looking at simple individual alpha factors.But there are times when we can create factors that are conditioned on another factor.We can create a conditional factor by multiplying two factors together.For example, if factor A gives an alpha value for a single stock,and factor B gives a different alpha value for that same stock,then we can create a conditioned,alpha value for that stock by multiplying those two separate alpha values together.Early on in my career,it was the case that some fairly simple,at least as thought of in hindsight today,factors could be profitable in live trading.Markets though have become much more competitive as time has marched on and as data,tools, and knowledge have become more available.Today, it's likely that you need to findconditionals to make really good alpha factors as such,this section is critical.So, how do we interpret what this multiplication is doing?We'll first look at a common practice in academic papers,which we'll refer to as quantiles of quantiles.If a research paper is looking at the effects of one factor condition on another,they would divide the data into quantiles for one factor.Let's say we're using 10 decile groups,then within each decile group they would splitthe stocks into another 10 decile groups based on the second factor.So, now we have 100 quantile groups conditioned on two factors.Because we have 10 deciles based on one factor andeach decile group has 10 deciles based on the second factor.Now, let's see what happens if we create a condition factor based on two factors.Let's say we have a stock universe of 100 stocks and we'll convertsignal A and signal B into scores from one to 10 for each signal,then we'll calculate the conditional factor as signal A times signal B.Let's pause a moment to think what multiplication means visually.If you remember learning your multiplication tables as a kid,you might remember seeing a rectangular grid with a column ofnumbers 1-10 along the left edge and the numbers 1-10 above the top.Multiplying the two numbers for each row and columnallows us to fill all the boxes inside the multiplication table.This is exactly what we're doing with our two factors.As you can see, the product of A times B,ranges from a value of one to a value of a hundred.Notice that, not all the values in between one and 100 are represented,as there are some numbers that occur twice.However, overall, you can see that there is a range of values between one and a hundred.Does this look familiar to you?When we discuss the process of calculating deciles nested inside deciles,we also ended up with 100 groups.Even though multiplication of the two factors does not givethe exact same groupings as we would get from nested quantiles,they have the same goal of creatinga composite factor that is conditioned on the value of both the individual factors.We will seek some conditional factors in detail when weanalyze some research papers to generate factor ideas.When you read ideas or hypotheses like,stocks that go up on high volume are likely to revert,or momentum is more pronounced on stocks with low volume,you need to use conditionals.Also, whenever you see the word or concept of end,it is likely that you need to use a conditional.

### 50. M4 L3a 23 Summary V3-FZYqdaqoiZk.en

Congratulations on making it through this lesson.We reviewed some foundational building blocks that you will see again and use when wetranslate academic papers into alpha ideas and then implement those ideas in code.But let's just remember that, even after testing,if your alphas look promising,this is really only the first stage of their lives so to speak.Lets step back a bit and see how the search for alpha factorsfits into the big picture of the quant workflow.We first propose and generate alpha factors,then evaluate them to find those that might show some promise.Then we perform out-of-sample testing ofthe alpha factors using historical data that wasn't used to construct the alpha vector.If that looks promising,then we would conduct paper trading in which we don't use real money,but we follow the factor as if we're making theoretical trays overtime on newly arriving live market data for some period.If that showed promise,then we would put the alpha into production in a real portfolio with real money.The alpha at that stage would be blended withother alphas and the final alpha vector would pass through a portfolio optimizer.We would likely start by giving that alpha factor a small weight in the combined vector,and given more weight if it performed and improved portfolio performance.We would monitor the alpha factor over time knowing that at some point,the factor's usefulness will erode because we're trading in a competitive market.Then, we would remove the alpha factor and go back tothe beginning to search for new promising alpha factors.Note that when I say "we" in those steps,after the first one,the "we" could be a human portfolio manager or itcould be an artificially intelligent agent and AI.Even in the first step,we can utilize AI techniques to help us recover both these AI applications.The application for alpha generation and the AI application forportfolio construction in term two of the API for trading nano degree program.We will cover later stages of this process in future lessons.We will cover backtesting,out-of-sample testing and other methods to avoid overfitting.It can't be overstated how important out-of-sample testing is in practice.If our goal is to seek above average performancein real production trading with real money and real world constraints,then it is critically important to avoid overfitting to passdata to the point that our alpha factors cannot generalize to current market conditions.Even within the academic scientific community,both in finance and in the general sciences,there are serious discussions aboutoverfitting and about validating the results of research.The rest of this lesson will focus on generating and evaluating alpha factors.But I do hope you keep in mind that it's perfectly normal ifyou feel the need to go back and review the concepts covered in this lesson.I know there was quite a lot of material,but I'm also very excited for you as you're learning the techniques andthe type of thinking that real quant researchers deploy in their search for alpha.See you in the next lesson.

### 51. M4 L3a 25 Interlude Pt 1 V2-SMQwc5kwSr0.en

Well, hello there. Thanks for joining me.I'm just going to pop in here and share some thoughts about reading research papers.I remember when I got started doing academic work,reading a research paper felt like a daunting task.I understood that reading a research paper meant sitting down witha cup of tea and reading the paper like I would a book from cover to cover,trying to absorb every word.When I accomplished this successfully,it took me several hours.When I failed, I would leave off in the middle,often not remembering what I learned at the beginning.It must be said that being able to read and learnfrom a research paper quickly comes with experience,but also comes with experience in the specific field you're reading about.This is because research projects ineach very specific niche field are highly interdependent.When you see that one paper is basically using the same method as another,and you already understand that method,you can skip that entire method section.When you see that a paper references another paper for some idea,and you've already read that other paper,because it's a very influential or so-called classic paper in the field,you'll understand the reference.There's no easy, quick way to gain this knowledge that I know of.It's one reason that developing expertise in a field is a long, hard one process.But there are some things to know about research papers andsome strategies you can take to make the process a little easier.I think the most important thing to remember is that you don't have to read everything.Research papers are designed in such a way as to show you,the reader, where to find different types of information.Perhaps the second most important thing to rememberis that research papers are written by people.They are not infallible,they're not gospel, and they are notperfect at communicating what they aim to communicate.What I mean to say is that if you don't understand something you're reading,it may be that you lack the background to understand it,but it may also be that the paper's author wrote it in a way that they understand it,but not in a way that anyone else can understand it.Often, the author is writing to an audience thatalready knows the field, not to a new comer.Instead of banging your head against the wall, so to speak,or rereading the same section many times,you may have to go elsewhere for a better understanding of the topic.

### 52. M4 L3a 26 Interlude Pt 2 V2-1a60RPqhO8k.en

Let's talk about the structure.Do you remember grade school science fairs and learning about the scientific method?Well guess what, what you learned actually applies to the real world.Papers are actually structured with title, abstract,methods, results, conclusions, and discussion sections.Let's start at the beginning.The very first thing that you'll read is the paper's title.You've probably picked out the paper because the titleindicates that the topic has to do with something you care about.Sometimes, it has an important keyword.The second thing that you should pay attention to isthe list of authors. Why does this matter?Well, it tells you something about the provenance of the ideas within.Usually, the first author is the main author,the person who did most of the work.But sometimes there are multiple main authors, indicated with asterisks.Usually, the last author is this person's adviser.Typically, a more senior person in the field.The ideas in the paper maybe offshoots of this more senior person's other ideas.So, they may be related to this person's older papers.When there are only one or two authors,it could be that this person or people are alreadyestablished in this field and they worked on this project on their own.Sometimes the list of authors is huge.This means that many teams of people typically indifferent places around the world had to cooperate to accomplish this work.Okay. The third thing you'll read is the abstract.This is a short summary of the paper.Sometimes this is as far as you get.The abstract should communicate basicallythe question or problem the author tried to address,what the author did, the main important finding,and what they think their result means.That is how it addresses the original question or problem.If you read the abstract and the paper isabout something different than what you thought it was,or doesn't indicate that they found something meaningful,you can stop right there.Sometimes the abstract looks interesting,but doesn't give you enough information.If you get past the abstract,you come to the introduction.In this part, the author is supposed to give some background about the field,and explain why they decided to study the topic of the paper.It's supposed to help you follow the flow of ideas that led to the question,which in turn led to the idea of their paper.Sometimes the author does a great job at this,and you can really learn something about the field in this part.Sometimes the introduction is short and not very helpful.Often, in this section,the paper will cite other papers that may be useful to look at to.The next section is usually the methods section.Do you have to go through this line by line?No. In fact, you can skip this entire section.You might skip ahead to the results and conclusions toknow what they actually found and come back to this section later.You might end up reading this,if you want to know that they were careful to control for some important confoundingvariable which you would care about if you were trying to decide for yourself,if their results are meaningful.Or you might read this, if you want to replicate what they did in whole or in part.Next, is usually the result section.This section should communicate the main measurements the authors made.Ideally, everything is well labeled and clear.But sometimes it's not,and you have to skip back to the methods to find out what some measurement is.Finally, the conclusions or discussion should communicate why the results are important,what they mean, and how they relate to the original question or problem.

### 53. M4 L3a 27 Interlude Pt 3 V2-v6cLkoJhujU.en

Another thing to remember,is that journals themselves impose their own editorial influence on papers too.They look for papers of certain types.They ask for papers to be structured in certain ways.They impose word counts and style guides.Often within a field,certain journals are known to be the most competitive toget your work published in and the most high profile.Typically these journals have the largest most diverse readership.So, a paper must be of sufficient importance andbroad interests to be published there and be circulated to the most eyeballs.The other important thing I wanted to tell you is that,if what you are trying to do is understanda whole field and not just an individual experiment,or project, what you may want is a review paper.This is a paper that has a different purpose.It's meant to be a summary,or discussion of several papers,or ideas on the same topic.These are like gold for newcomers to a field.It's not always clear where and how to find them.Sometimes a journal that normally publishes regular papers puts out an issue of reviews.Other times you'll find helpful reviews and journals that only publish reviews.For example, the journal annual reviews.A final thing to know,when you are reading research papers is that,yes a paper is a piece of communication or documentation about a study.But it is also an assertion that science has progressed,that knowledge has advanced,in other words it's an assertion that the workdocumented in the study means something important about the world.When a paper is complete enough to be sent to a journal,it usually means that many people think the work is valid and meaningful.The lead author, her colleagues, her adviser,and potentially other people inthe research community who the person showed the work to or discuss it with.But then when a journal editor takes interest in a paper,the work is usually sent out to other people who do similar work.These people often raise very important concerns aboutthe validity or meaning of the work that nobody else has mentioned yet.Their assertions can block publication of the paper if not addressed.This is the peer review process.Finally, even after a paper is published,sometimes information comes out that cast the information in the paper in a new light.For example, sometimes other researcherstry to do the same thing and get different results.This isn't to say that you should necessarily always doubt the authors assertions,but neither should you believe them wholesale.The reality is far more nuanced than that.The vast majority of the time,they did what they say they did but there may be a caveat about how they didit such that what they did is not as meaningful as they think,this is just to say that the process of establishingknowledge is long and has many twists and turns.Remember that when you read a paper,you're somewhere in the middle of that process which isalso rather exciting because when you're reading papers,you maybe learning about things that are so newthat very few people in the world have heard of them yet.Getting back to the actual reading,sometimes it is important that you read an entire paper.For example, if you're doing a presentation on itand people are going to ask you detailed questions.But more often there's certain information you arelooking to take from a paper and that's all you need.I think the most important thing to remember in practice is to keep up your momentum.Don't get stuck on a tiny detail that you don't understand.If you think it's really important,take a note and look it up elsewhere later and move on.

## Part 01-Module 04-Lesson 07_Alpha Factor Research Methods

### 01. M4 L3b 01 Case Studies Intro V3-oWWrWbzDi2k.en

Hi, welcome to the lesson where we'll takea deeper look at four academic papers and see how we can code up Alpha factors from them.In building the material for this lesson,I browsed dozens of academic papers and chose some of these to implement.Note, that I didn't choose these papersparticularly because they produced great Alphas, rather,I chose them to give a diversity of style and showimportant techniques and building blocks for making Alphas.In fact, some of the performance is disappointing in the period we test,but the concepts that you'll learn are key to the work that you may do as a quant.I'll discuss each of these papers and we'll see how each can be implemented in code.I will also comment on instances where I takea practitioner's approach to interpreting orgeneralizing ideas from the academic research.In reading these papers and learning to implement Alpha factors based on their ideas,you will learn about Alpha factor building blocksthat we'll refer to as overnight returns,accelerated and decelerated gains or losses,positive skew, and idiosyncratic volatility.You'll gain insights into how psychology and market mechanicsshape the hypotheses upon which these factors were built,and also see different ways in which a factor can condition the effect of another factor.I hope that you can develop a good habit of reading academic research,proposing, and implementing, and then evaluating potential Alpha factors.These skills and experiences will serve youwell in preparing you for roles in quantitative finance.Let's get started.

### 03. M4 L3b 02 Overnight Returns Abstract V2-q5xidwa5W8w.en

Let's read the abstract of the paper,Overnight Returns and Firm Specific Investor Sentiment.We can see that the authors tried to analyze something called overnight returns.If we skip ahead to Section 2 titled sample,variable definitions, and descriptive statistics,we can see that overnight returns are defined as the percent change fromthe close price of the previous day to the opening price of the next day.The overnight returns are also called closed open returns.They also mentioned investor sentiment.If we jump to the introduction,we see that overnight returns may be used as a proxy for firm-level investor sentiment.In the context of the paper,firm level sentiment appears to refer to whetherinvestors have a positive or negative view of a stock's future price.It also helps to check the paper for signs of either a momentum or mean-reversion factor.It mentions short-term overnight return persistence.When we read the word persistence,we can interpret this as referring to some kind of momentum.The paper also says that stocks with high overnight returns underperform.We can interpret this as a kind of mean reversion.After reading the abstract,it's okay to quickly scan the paper for a preview of major sections.It's common for papers to have an introduction,methodology, results, and conclusion section.It's also okay to read the paper out of order.For example, you can start with the introduction,jump to the conclusion,then work your way through the methodology and results.

### 04. M4 L3b 03 Overnight Returns Possible Alpha Factors V2-QBCDr9q2rLo.en

From reading the introduction,we see that the authors refer toinvestor sentiment as the positive or negative views of investors,especially individuals who tend to cluster their orders around a market open.The authors also define overnight returns as the close toopen returns and describe the following hypothesis.Individual investors may notice attentiongetting events and may then choose to trade on those events.Since many of them have day jobs,they may place trades aftermarket clothes that would be executed the following morning.The hypothesis continues to assume thatthese overnight trades may be subject to mean-reversion by the middle of that second day.Based on this, you may start thinking about how this can be useful as an alpha factor.Let's say, a stock's overnight returns are high.So, that morning, you see an increase ina stock's open price relative to the previous day's close.According to this hypothesis,the stock is overbought and we may expect a correction in a form of a drop in the price.So, if you calculate the overnight return of the stock,you can short the stock that same morning when the overnight return is high.However, that's not the effect that we will look to capture here.There are ideas later in the paper that we will use to construct an alpha factor.Also, in the introduction,the authors state that they test for short-run persistence in overnight returns.This indicates some momentum in the overnight returns over a short term window.If we continue reading in section two titled sample, variable definitions,descriptive statistics, we can see that they'reinterested in a window of one week or five trading days.They calculate the average daily close to open return overa week as a measure of the persistence or momentum in sentiments.Again, based on this information,you can think about howthis weekly average momentum factor might be used in a theoretical portfolio.If according to the hypothesis,the weekly average close to open return for a stock is high and positive,we might want to buy more shares of that stock withthe assumption that the positive returns will continue in the short run.Finally, the abstract also identifies a third possible factor.With the hypothesis that in the longer term,stocks with high overnight returns under perform.In other words, in a longer time window,stocks exhibit mean reversion.If we go to section five titled longer-term returns,the hypothesis based onother papers is that stocks that are more attractive to speculatorsand therefore see more near termpositive returns may end up under performing over the next 12 months.However, this also is not the effect we will look to capture here.

### 05. M4 L3b 04 Overnight Returns Data Universe Methods V2-Y_lBDa1hRco.en

The paper identifies its dataset as the Center for Research and Security Prices,which is often abbreviated as CRSP and can be referred to as CRISP.This is a fairly well-known pricing dataset.And if you check out their website,you can see that the data is available via subscription.We don't use that data precisely,but we are using a pricing dataset which is very similar.It's also helpful to check what stock universethe authors think are most relevant to the factors they are analyzing.Even though a factor would ideally be applicable and useful for a broad range of stocks,it's also possible that certain factors only generatemeaningful signals for a certain subgroup of stocks.In the abstract, the authors mentioned that meanreversion or momentum factors that they'reevaluating are more pronounced for hard to value firms.If we look through the paper for mention of harder to value firms,we see how they define them in section four titled,Short-term Overnight Return Persistence,Hard to Value Firms and Institutional Share Holdings.According to other papers they cite,harder to value firms are more volatile,have a smaller market cap,are newer companies, currently less profitable,and considered high growth companies as exhibited by higher price to earnings ratios.Remember that I said when you see and, think conditional factor.If you wanted to implement these specific ideas,you would do so via the mechanics of a conditional factor.We don't do that in this exercise however.The authors provide some context for why harder to value firms may be more likely toexhibit overbuying at market open or weekly momentum of closed open returns.The hypothesis is that individual investors may rely moreon sentiment when the fundamentals of the company are more difficult to measure.This sentiment is expressed in the recent closed open returns of the stock.We can also scan the paper for useful methodology,which we can apply more broadly to any factors that we're evaluating.For instance, in section two,the authors define the weekly overnight return on the stock.If you haven't read this section yet,you may imagine taking a window of five days daily returns,adding them up to get the weekly cumulative close to open return.The authors do something slightly different.They take the average of the daily returns over five trading days,then they multiply by five.Can you think of how this is different or why they may do this?Notice that if we had missing data,then simply adding up the daily closed open returns over a five-day window,may not make each week comparable.If a certain week is missing data for one day,then we would only have four days worth of returns to add up.By taking the daily average and then multiplying by five,we can better deal with missing data and help makeeach week's cumulative return more comparable to other weeks.

### 06. M4 L3b 05 Overnight Returns Methods Quantile Analysis V3-4Js3mghq2mU.en

In section three titled Weekly Overnight Returns,the authors check if the weekly close-to-open returns persist one,two, three, or four weeks into the future.The authors make use of quantiles to analyze the data.They sort the weekly overnight returns and then dividethese into deciles or 10 quantiles.If we jump to table two,titled Short-Run Persistence ofWeekly Overnight Returns and Subsequent Weeks' Total Return,we can see that decile one contains the lowest overnight returns,while decile 10 contains the highest overnight returns.In the other columns,we can see that decile one has low overnight returns one,two, three, and four weeks later.Similarly, decile 10 has higher positive returns in its subsequent weeks.This is an indication thatweekly overnight returns may persist for up to four weeks into the future.We now have some ideas for potential factors and how to implement them.First, we'll want to calculate overnight returns,which are the returns from the closed price ofthe previous evening to the open price of the following morning.Next, we'll aggregate overnight returns by week to get weekly overnight returns.Based on the analysis of this paper,the hypothesis we'll use for this factor is thathigher weekly returns for a stock indicate higher subsequent returns in the near future.In other words, our strategy will be to overweight stocks that havehigh weekly overnight returns and to underweight stocks that have low weekly returns.

### 08. M4 L3b 06 Winners And Losers In Momentum Investing V2-84ygzbLENbE.en

Here's a question for you.If two stocks begin and end at the same point over a period of time,does it matter how they got there?If you ever heard the fable about the tortoise and the hare,then you might remember the saying,"slow and steady wins the race."Maybe that idea is relevant in trading.For example, let's pretend the return of two stocks.Let's call them stock tortoise and stock rabbit.Over the same year are both plus 20 percent.If we created a simple momentum factor based on the one year return,this would give us the same signal for both stock tortoise and stock rabbit.What if we were to look at the trajectory of both stock tortoise and stock rabbit andnotice that the path they each took toarrive at the plus 20 percent was rather different?For example, let's say stock tortoise has a linear trajectory,where it incrementally increases throughout the year at a steady pace.Let's say that stock rabbit increases at a higher rate,say plus 40 percent earlier in that year,but then drops by 50 percent later in the year.Even though the tortoise and the rabbit reached the same point at the end of one year,both plus 20 percent,which one might you think will do better in the next few days, weeks, or months?In this case, do you think it's fair to hypothesizethat "slow and steady wins the momentum race"?

### 09. M4 L3b 07 Winners And Losers Accelerated And Decelerated Gains And Losses V2-cdSdKl4uxVM.en

The paper titled, The Formation Process of Winners and Losers in Momentum Investing,discusses how the trajectory of a stock isan indicator for whether its momentum is accelerated or decelerated.When a stock is recently showing higher returns,this paper refers to these as accelerated gains.When a stock is recently showing minimal positive returns,the paper refers to these as decelerated gains.If you look at a stock price trajectory of an accelerated gain,the shape will be convex.In case you can't remember what a convex curve looks like,the plot of y equals x squared is an example of a convex function.Essentially, the trajectory is showing an accelerated increase.On the other hand,a stock price trajectory of a decelerated gain will have a concave shape.As a reminder of what concave means,a plot of y equals the square root of x is an example of a concave function.The trajectory is showing a decelerated increase.We can also describe accelerated losses and decelerated losses as concave or convex.An accelerated loss has a concave shape.You can imagine the plot of y equals negative x squared as a concave function.A decelerated loss has a convex shape.You can imagine the plot of y equals negative square root of x as a convex shape.So far, we've described four general shapes.What we want to point out is which type oftrajectory may be preferred for going long or going short.We'd say that an accelerated gain may havehigher future returns compared to a decelerated gain,if the starting and end points are the same.So, we would prefer to go long the accelerated gain,a bit more than the decelerated gain.We would also say that an accelerated loss ispredictive of more negative returns compared to a decelerated loss.So, all else being equal,we would prefer to short the accelerated loss more than the decelerated loss.An important point that might not be obvious to see in the paper is that we canview the convexity or concavity of a curve in a relative terms.In other words, compared to a convex curve,a straight line is relatively more concave.Likewise, compared to a concave curve,a straight line is relatively more convex.You may recall with the example of the tortoise stock and the rabbit stock.The tortoise stock trajectory look like a straight line,whereas the rabbits stock was concave.So, relative to the rabbit stocks trajectory,the tortoise stocks trajectory was more convex andalso more predictive of higher future returns under this hypothesis.

### 10. M4 L3b 08 Winners And Losers Approximating Curves With Polynomials V4-Nw6v2EeECt0.en

Now that we have a sense for how accelerated ordecelerated gains and losses look like visually,how do we represent their relative convexity or concavity in numbers?One way to approximate a curve,is with a formula that looks like y equals x squared.Or more accurately, y equals ax plus bx squared.The authors use t as the independent variable name representing time,such as the number of days from the start of the stocks trajectory.They also use beta as the coefficient for t,and gamma as the coefficient for t squared.To make the discussion easier to follow,let's just give descriptive names to the coefficients.The coefficient for t will be called gain,and the coefficient for t squared will be called accelerate.You'll see why we chose these names in a minute.Let's see what a positive or negative direction coefficient might look like.If we just ignore the t squared term,a positive gain coefficient,will make a line that slopes upward which could represent a stock price that is gaining.Conversely, a negative gain coefficient,makes a line that slopes downward which could represent a stock price that is losing.Next, we can look atthe accelerate coefficient and how it affects the shape of the curve.If the gain coefficient is positive,and the accelerate coefficient is also positive,both the t term and the t squared term are pointing up and to the right,so the curve is convex and looks like an accelerated gain.If the gain coefficient is still positive but the accelerate coefficient is negative,the t term is being counteracted by the t squared term,so the curve looks like a decelerated gain.Let's look at two more cases when the gain coefficient is negative.If the accelerate coefficient is also negative,then both terms are pushing the trajectory downward,so the curve looks like an accelerated loss.Lastly, if the gain coefficient is negative but the accelerate coefficient is positive,the two terms counteract each other and the curve looks like a decelerated loss.Okay, now for the fun part.Based on whether the gain coefficient andthe accelerated coefficient are positive or negative,can you decide which scenarios you would rather go long, or short?

### 12. M4 L3b 09 Winners And Losers Creating A Joint Factor V3-xmW05ii8Vxs.en

Going back to the question I asked earlier,based on whether the gain coefficient andthe accelerate coefficient are positive or negative,can you decide which scenarios we would rather go long or short?We'll take a look at how the coefficients ofthe stock price approximation formula can informour decision as to which stock we should go long or short.This in turn allows us to create an alpha factor based on these two coefficients.Let's take an example of two stocks, A and B.Stock A has a gain coefficient of 10 and an accelerate coefficient of two.Stock B also has a gain coefficient of 10 and accelerate coefficient of negative two.If you were going long both stocks,which one would you put more weight on?Since stock A has the trajectory of an accelerated gain,whereas stock B looks like a decelerated gain,we would put a larger long wait on stock A, the accelerated gain.Let's look at another example with two downward trending stocks.Let's say stock C has a gain coefficient of negative10 and accelerate coefficient of two.Stock D has a gain coefficient of negative 10,as well as an accelerate coefficient of negative two.If you were to go short both stocks,which one would you put a larger short position on?In this case, stock C has the trajectory of a decelerated loss,and stock D has the look of an accelerated loss.So, you'd prefer to put a larger shortwave on stock D, the accelerated loss.Now, you may have noticed something interesting withthe signs of the gain and the accelerate coefficients.When the product of gain times accelerate is positive and large,the signal generally means to take a larger long position.When both are negative,the signal means to take a larger short position.If we were to convert the gain and accelerate coefficientsinto ranks then multiply them together,the product of the gain times accelerate would represent an alpha factor.When the rank of gain and the rank of accelerate are both small,the product is small,and this is a signal to take a larger short position.If the rank of gain and the rank of accelerate are both large,then the product is also large,and this is a signal to take a larger long wait.This is our first conditional factor.The key idea is that when you see and think conditional.Here, we have momentum and convexity.So, to summarize, we can usea multiple regression where the independent variables are time and time squared,and the dependent variable is the stock price.This regression gives us estimates for the coefficients gain and accelerate.We create our factor by first converting gain and accelerate into ranks,and then multiplying the ranks together to create a joint factor.

### 14. M4 L3b 10 Skewness And Momentum Attentional Bias V3-3ZkFRBUmSQ0.en

The next alpha factor that I'd like to share with you is based on the paper titled,Expected Skewness and Momentum.Before we get into the formal definition of skewness,I'd like to give you a hypothetical example that hopefully will guideyour understanding as we discuss the conditional skewness and momentum vector.Why conditional? What word do you see in the title?"And", as a practitioner,it's often helpful to think of the alpha factors in termsof market mechanics and or behavioral psychology.If you look at which stocks are in the news,some stocks get more of press coverage and social media posts than others.Let's hypothesize that stocks that tend to get higher thanaverage media attention tend to also become overpriced.What might cause that mispricing?When individual investors are flooded with news about a high flying company,there's a natural fear of missing out or "Fomo" as the kids are calling it these days.This sets up investors for a situation referred to as attentional bias.Attentional bias is a phenomenon where people focus on some aspect at the expense offocusing on the big picture and are more likely tonotice evidence that supports their prior existing beliefs.So, when a stock that is getting lots ofnews coverage has a high one day return in the past month,this may trigger individual investors tosee this as a sign that the stock will keep going up.Their fear of missing out may kick in,so they jump in and buy as well pushing the stock further up.Since this increase may not be based on fundamentals,the market as a whole both institutional investors andindividual investors may realize that the stock is over bought and sell,thus starting to push the stock price back down to earth.If this is indeed true,then the maximum one day return in a historical window can be used asa reversal factor which can also be combined withmomentum to decide that the stock maybe over bought or oversold.The maximum one day return overa month is the way that we can measure skewness of returns.We'll explain skew in the next section.Before we go to the next section though,let's pause and talk generally about alpha factors again.Many of you might be skeptical of this approach as many newcomers to call financer.Surely in the context of this specific factor,you can think of a few occasions in recent memory wherethe idea behind this factor was wrong, maybe spectacularly wrong.Meaning you can surely think of ideas wherea stock went up a lot and then continued going up.Alpha factors though are trying to capture a mispricing,often imperceptible to humans across one,many stocks two, on a relative basis,and three, persistent across time.We are not trying to get very high conviction in any one specific stock.As we discussed in the section about the fundamental law of active management,we can expect that our skill on any one stock will be low,even imperceptibly difficult to distinguish from noise by a human.Skill just has to be marginally better than 50-50 at the single trait level, and then,we can apply it across many stocks to geta complete alpha factor that hopefully exhibits good Sharpe ratio.

### 15. M4 L3b 11 Skewness And Momentum Defining Skew V2-6PgqIpmJBJ8.en

I hinted previously, that we could use skewness as a signal thatstocks may be over bought in the short-term and may revert back soon afterwards.To help you understand skewness,I'll first introduce a visual notion of skew.Then the common academic definition of skew,followed by the proxy for skewness that we will use when creating this Alpha factor.Skewness refers to the asymmetry in a distribution.When a distribution is negatively skewed,it has a longer tail on the left.When the distribution is positively skewed,it has a longer tail on the right.Skewness also means that the mean of a distribution is different than its medium.Negative skew, indicates that the mean is to the left of the median.You can imagine the small portion of extreme values on the left,pulling the mean to the left.While the median stays closer to the majority of observations.Similarly, a positive skew indicates that the mean is to the right of the median.You can imagine that the extreme values to the right,pull the mean to the right as well while the medianstays closer to the majority of observations.If you learned about skewness in school,you may recall the concept of the first,second, third, and fourth moments of a distribution.The first moment is commonly known as the mean or average.The second moment is known as the variance.The third moment is skewness which we are discussing here.The fourth moment is kurtosis which measures the tails of the distribution.Stock returns generally exhibitexcess kurtosis or fat tails compared to the normal distribution.For now though, we'll focus on skewness.So, those were the visual and traditional interpretations of skewness.Now, let's circle back to the definition of skewness used in this paper,and what we will implement in the Alpha factor.A useful proxy for skewness isthe maximum daily return of a stock over the past 20 trading days.This definition is useful in part,because of how it captures how individual investors may perceive skewness.It's possible to imagine that an investor maylook at a recent stock high of a company as an indicator,upon which they will base their decision to buy or sell the stock.

### 16. M4 L3b 12 Skewness And Momentum Momentum Enhances Or Weakened By Skew V2-S73J_h8DHsE.en

Let's walk through four hypothetical examplesand assume that skewness is a reversal factor,which is another word for mean reversion.We'll be more specific and say thatthe maximum daily return over the trailing month is our measure of skew.Let's also assume that the return over the trailing year is a momentum vector.Notice that it's more precise to callour definition of skew as a measure of positive skew.We aren't defining negative skew with a minimum over the trailing month.Even though this is also a valid approach,for simplicity, we'll just be measuring positive skew.Remember, the Alpha vector is a relative ranking exercise.In our first scenario,let's say momentum is positive and skew is positive.This is the scenario that we saw in the introduction to this factor.We can think of the positive skew as havinga dampening effect on the positive momentum of the stock.The paper calls this weakened momentum.For our second scenario,let's say momentum is still positive,but the skew is less positive.The less positive skew may indicate thatfuture returns maybe stronger than if there were a positive skew.The paper calls this enhanced momentum.For our third scenario,we'll look at negative momentum and positive skew.The paper describes this scenario by suggesting a phenomenon where investors are overlyoptimistic about a downward trending stock when the stock bounces backup temporarily.The hypothesis continues to suggest that when the stock rebounds positively,individual investors may see this as a sign of recovery so they may buy as well.To conclude this scenario,as fundamental still put downward pressure on the stock,the momentum then reasserts itself and the stock continues to slide downward.This scenario is also referred to as enhanced momentum and isthe primary scenario for which the researchersfound more significant effect of skewness on momentum.For our fourth scenario,we'll look at negative momentum and less positive skew.The less positive skew has a dampening effect on the negative momentum,so it can be referred to as weakened momentum.Now that you've seen how momentum and skew interact,can you think of how these can be combined into a conditional factor?

### 17. M4 L3b 13 Skewness And Momentum Conditional Factor V2-cMLTVZFKEm0.en

Let's see how we can create a conditional factor based on momentum and skew.We first might want to convert both momentum and skew to ranks,since ranks are generally more stable and more robust to outliers.Notice though, that skew is a reversal or mean-reversion signal.So, we want to rank the skew in reverse order.Think about this in the context of our definition of enhanced momentum,and work out for yourself a few examples of why this works.That is, if our stock universe had 100 stocks,then the stock with the smallest skew would have the rank of 100,whereas the stock with the largest skew would have a rank of one.The momentum factor would be ranked as usual,where the most negative return is given a rank of one,and the most positive return has a rank of 100.If we multiply the rank of momentum times the reverse rank of skew,we will have the joint factor.The smallest alpha values would translate into the largest short positions,and the largest alpha values would translate into the largest long positions.

### 18. M4 L3b 14 IVol Value And Idiosyncratic Volatility Overview V2-h7vamh2FPMs.en

The last alpha factor that we'll construct is based on the papertitled "Arbitrage asymmetry and the idiosyncratic volatility puzzle."This paper uses a lot of the concepts that we've learned,including a unique example of using some ofthe output of a risk model as part of alpha factor construction.Personally, I enjoy this example because we combined so much of what we havelearned in addition to some more ideas from behavioral finance.Based on this paper,we'll construct a conditional factor,a fundamental factor enhanced by a volatility factor.Let's start with some background information that will helpyou interpret the meaning of the two factors that we'll combine.We'll first remind ourselves of how arbitrage supports efficient markets.Next, we'll look at volatility and how volatility may limit arbitrage activities.Then we'll look at the idiosyncratic portion of volatility and whythis might be more useful as a factor than the total volatility.Then we'll refresh the meaning of a value factor,which is a type of fundamental factor.Lastly, we'll combine the value factor and idiosyncratic volatility into a joined factor.We'll discuss generalizing what we've learned so that you can useyour creativity and create related factors of your own.

### 19. M4 L3b 15 IVol Arbitrage And Efficient Pricing Of Stocks V3-7Fqe5DP6iG8.en

You may recall that overcharge isa process that seeks profits due to mis-pricing of assets.In other words, overcharge looks for inefficiencies inthe market in order to profit from these inefficiencies.Moreover, the act of overcharge actually reduces the mis-pricing of assets,and the individual acts of overcharge by market participantsas a whole add up to support a more efficient market.When we think of profit seeking activitiesin their role in maintaining efficient pricing in markets.It can also include buying stocks that are consideredunder-priced or shorting stocks that are considered over-priced.In other words, overcharge here is thatthe instantaneous buying and selling of perfect substitute assets.Here, we simply mean buying assets with anexpected return greater than that warranted by the risk taken,and similarly shorting assets withan expected negative return that is less than that warranted by the risk taken.Even though only taking a long or onlytaking a short position is not technically overcharge,when market participants seek to profit from mis-priced assets,their actions help to reduce that mis-pricing and make markets more efficient.

### 20. M4 L3b 16 IVol Arbitrage Risk V3-rKtJ3iAYYns.en

Not all arbitrage opportunities are created equal.If you found two similar stocks with similar mispricing,all else being equal,you'd prefer to act on the stock that is less risky.For example, let's imagine we have identifiedthrough a model in which we have high confidence two stocks,A and B, which are each undervalued by 10 percent.However, stock B typically fluctuates two times as much as stock A.If we're limited to our capital deployment,which is how much cash we can invest,and we can only choose one stock,we would likely choose to buy stock A sinceour expected Sharpe ratio will be much higher.Remember, in neither case are we guaranteed to make money,we just have a model in which we have high confidence.Now take this example and think of the universal effect.All savvy arbitragers like us,would prefer stock A,and the opportunity in stock A might get arbitragedaway before we even have a chance to buy the stock.If that's true, then in fact,we may be better off trying to capture the discount instock B as there will be less competition to capture the opportunity presented there.That's the gist of this paper.The risk that acting on an arbitraged opportunitymay actually yield a loss is called arbitrage risk.As you may have guessed from the example,one source of arbitrage risk is the volatility of the stock,defined as the standard deviation of returns.

### 21. M4 L3b 17 IVol Idiosyncratic Volatility V2-B8hOR4G9CJk.en

Remember from our prior discussions,especially the one about risk models,the returns can be broken up into a systematic and idiosyncratic component.Likewise, the volatility of returns can be brokenup into systematic and idiosyncratic components.Idiosyncratic risk maybe a more helpful indicator of arbitrage risk. Why is this?Because market participants who pursue strategies to capture relative mispricingswill seek to eliminate common factor risks leaving them bearing idiosyncratic risks only.As such, when we think about the risks to arbitrage,we likely should consider only the idiosyncratic risks.Recall that the systematic component of a stock's risk canbe attributed to movements in major risk factors.For example, using the CAPM model,a part of a stock's movement can be attributed to the movement of the market as a whole.The rest of the stock's return that cannot be attributed tomajor risk factors is the idiosyncratic component of its return.One way to isolate the idiosyncratic return,is by fitting a multiple regression using the risk factors.In this research paper,the authors use the Fama-French model.The model gives a prediction of the return due to the model factors.Since the model factors do not fully explain the stock's returns,there will be a difference between the actual and modeled returns.This difference is called the residual.If we take the standard deviation of the residual return,we get the idiosyncratic volatility which is also called the idiosyncratic risk.In short, the idiosyncratic volatility oriVol is the volatility that is specific to the stock,and not explained by the risk factors.This is useful for us because when we want to measure a stock's arbitrage risk,what we're interested in is the specific or idiosyncratic risk of that stock,and not the systematic risk that exists broadly among all stocks.

### 22. M4 L3b 18 IVol Value Fundamental Or Discretionary Investing V2-sKAE5Z8e7IM.en

The concept of value in factor modeling is one of the two most widely known concepts,the other being the concept of momentum.The significance of value can be traced to the work of Ben Graham and David Dodd,who wrote about value investing,which seeks to estimate accompaniesintrinsic value than by companies that are priced below that intrinsic value.Some well known followers of Graham invalue investing are Warren Buffett and Charlie Munger.These types of investors are referred to asfundamental investors are also discretionary investors.Given the apparent success of value investing by discretionary investors,quants have worked to replicate the idea by creating quantitative measures of value.Since I mentioned quants being inspired by discretionary investors,I want to tell you about an idea I think is very misunderstood.This is something I am very passionate about, most quants,those who are new to quantfinance and even famous and very experienced quants often think and proclaimloudly and even impolitely thatfundamental investors trade by gut instinct and have no process,whereas quants are the standard bearers ofthe scientific method and they are the only ones with a repeatable process.This is absolutely wrong,I have worked with some of the best fundamental investors in the world inaddition to some of the best quants and both these groups,have had irrational well-articulated, repeatable process.The key difference though between the two are: one,the kind of inputs they use;two, how they process that information;and three, the shape of the portfolios that emerge.Historically, fundamental investors have relied on detective work together inputs.This work includes reading corporate filings,talking to research analysts,and reading analyst reports.Let's think the company conference calls,talking to executives at the companies,and even visiting companies on-site.Fundamental investors then synthesize all this information into a variant view,which is a non-consensus view.The shape of portfolios differ between fundamental investors and quant investors.When I refer to shape,I mean the number of positions and also the concentration of those position.Think of the shape function in the Python library pandas.Great fundamental investors have high IC,the measure of skill.But, because of the style and depth of their research process,they can only achieve low breadth because it's only humanlypossible to research so many companies with that much depth.But, make no mistake,these investors use repeatable inputs,follow a repeatable process and produce high IC bets.Quants on the other hand,have historically used structured data like price and volume data,financial statement data, quantified a research analyst ratings, and the like.As discussed, quants typically have lower IC,but more than make up for that with much higher breadth.

### 23. M4 L3b 19 IVol Quantamental Investing V2-K6Ud6gams-U.en

One of the most exciting developments in investing in the last couple ofyears is the convergence of the quants and fundamental processes.This is called quantamental investing in the popular press,as a combination of the words quant and fundamental.But don't let that goofy name fool you.It's more than just a fad or a marketing term.This quantamental convergence of fundamental and quant is significant and growing.The idea is that the advent of AI methods,and the exponential growth of data generated by the real economy are now allowing quantsto use inputs that have historically beenexclusively in the domain of fundamental investors.On the other hand, the growing acceptance of the value of data science,and the observable successive quants,has led fundamental investors to embracedata-driven research methods and some aspects of the quant workflow,like the use of risk models.This is an exciting time.Let me give you a concrete example of this convergence.A part of the investment process that has been historicallysolely in the domain of fundamental researchers is text analysis,listening to conference calls,reading corporate filings, news, et cetera.There are now effective computational methods to getintelligence from this text information in a systematic way.Note that in term two,we will cover traditional natural language processing techniques,as well as deep learning for NLP.I'm sure you are very excited about that.But please be patient,let's finish term one first.

### 24. M4 L3b 20 IVol Volatility Enhanced Price Earnings Ratio V2-x-1nqTEPGcA.en

Fundamentals such as price to earnings ratio and price to book ratios have often beenused in alpha factors that represent the ratio of market price to intrinsic value.We could use these fundamental factors by themselvesto indicate whether a stock is underpriced or overpriced.One thing to be aware of though when dealing with fundamentals isthat the data change much less frequently than price.Fundamental values come from financial statements which,in the United States,are only updated quarterly.This means that the turnover for fundamental Alphas will be likelymuch lower than it is for price data-driven Alphas.Recall that this can be a positive because lower turnover meansless trading cost but it also could be a negative.We won't be as responsive as the data does not arrive frequently.As a general rule,fundamental Alphas have high capacity but havelower Sharpe ratio than compared to their price data-driven cousins.Some firms embrace this as it allows them to deploy much larger assets for clients.Other firms though can be wary of using fundamentally driven Alphas.One middle ground is to use fundamental data as a conditioning factor along withprice-driven data or to use a fundamental factorwith low weight in a combined Alpha vector that includes more responsive Alphas.In this case, sincehigher idiosyncratic volatility can indicate instances of higher mispricing,we can combine iVol with a fundamental factor.We can think of iVol as a conditioning factor since byitself it doesn't indicate whether to go in the long or short direction.This is an important idea,the conditional information doesn't need to be an alpha factor on its own.The conditioning iVol factor can potentially beused to enhance the signal of any other factor.Perhaps you can try this out in your own work.

### 25. M4 L3b 21 IVol Generalizing The Volatility Factor V2-Lt1JPjKHPmk.en

Now that we've walked through risk factor models and Alpha factors,I hope you'll start to see how you can try out different variationsthat deviate from the specific methods you see in academic papers.For instance, instead of using the Fama French model to extract idiosyncratic volatility,you may try other risk factor models that you learned about in this module,such as a risk factor model based on principle component analysis.Moreover, you can try pairing iVol withother fundamental factors or really any other Alpha factors as well.If you can develop a good habit of reading academic research,proposing and implementing then evaluating potential Alpha vectors,these experiences will serve you well in preparing for roles in quantitative finance.

### 26. M4 L3b 22 Summary V2-Tq8yVPEHxXs.en

You did it. This lesson was pretty advanced,but also very meaningful in that we practice reading academic research,thinking about Alpha ideas based on the papers,and then coding up these factors.I want to emphasize that the value of this lesson isn't that I'm givingyou four Alpha factors that will help you become an instant millionaire.The value that I hope you'll derive from this lesson is an understanding ofthe process and thinking that quants go through to generate Alphas every day on the job.Remember, we're teaching you how to fish forAlpha factors instead of just giving you the Alpha factors.As the saying goes,"Give your students an Alpha factor and they'll have a good Alpha factor fora day or at least until their Alpha factor turns into a common risk factor."Teach your students how to fish for their own Alpha factorsand they'll hopefully have good Alpha factors for a lifetime.

## Part 01-Module 04-Lesson 08_Advanced Portfolio Optimization

### 01. M4 L4 01 Intro V1-9NzZFszX2E4.en

You have come so far.Just think of all the amazing things we've told you in the past few lessons,I bet your head is spinning in a good way.We've been talking about factors,how to think about them,how to find them, how to code them up,how to make them robust,and how to evaluate them.We've told you about risk factors and about the fun ones, alpha factors.But in this lesson,the rubber really hits the road.Now we want to find the optimal weights for our portfolio.So, we're going to plug our Alpha and risk factors into the formalism wediscussed earlier in the course for portfolio optimization.This lesson is going to be critical for finishing the project.I can't wait to tell you about it, so, let's get started.

### 02. M4 L4 02 Setting Up The Problem Alphas V5-6GeyU-thC4U.en

Our goal is to set upa portfolio optimization problem using our alpha factors and risk model.In practice, it's possible that we are doing thisin order to design a portfolio from scratch,but it's also possible that we are trying toguide the evolution of an existing portfolio.So, there may already be a portfolio in production with capital invested ina universe of assets and portfolio weights on the assets from a previous optimization,which may have evolved as the asset values appreciated or depreciated.So, how do we set up the optimization withour new or updated alpha factors and updated data?Thinking back to what we learned before about portfolio optimization,we know we want to do something like maximizereturn and limit risk as measured by variance.This is already great intuition for how to set up the problem.Can you guess where our alpha factors and risk model would go inthe problem formulation following this intuition?Let's make this explicit starting with the alpha factors.Let's say we have just one alpha factor,we know that on a given day,our alpha factor is a vector of values,one value per stock that is hopefully predictive of the future mean return of each stock.But now we want a quantity inthe objective function that represents the predicted portfolio return.Somehow, we need to sum the alpha values over the portfolio.Can you see what we want here?To calculate the portfolio alpha,we just take the dot product of the alpha with a vector of portfolio weights.Let's plot this into the objective function.We make it negative because we want to maximize alpha.

### 03. M4 L4 03 Setting Up The Problem Risk V4-2vcULOlXTzc.en

Okay, now where does the risk model come in?Well, we discussed previously that if we putthe quantity representing the mean in the objective,one option is to place a constraint on the portfolio variance.We already know how to calculate the portfolio covariance matrix using a risk model.All we do to get the portfolio variance isthe same thing we did back when we were calculating the portfolio variance before.We write the portfolio variance asthe weight vector transpose times the covariance matrix times the weight vector.That's great, so let's limit portfolio risk.We'll say it has to be below some value.We're already most of the way there.You might have one very reasonable question at this point which is,how do we know what value to set as our limit on portfolio variance?Well, this value represents the tolerable variance ofyour portfolio returns per time period over which you run the optimization.As a portfolio manager,this is usually given to you,but where does it come from?Well, it represents a business decision.Remember that the investment portfolio we're designing represents a product,which is sold to investors.When investors are shopping around,they typically compare products with different risk return characteristics.What numbers are reasonable?The general stale volatility can be observed from the market itself,for example, from an index like the S&amp;P 500.The annualized standard deviation of the S&amp;P500 over 100 years has been 12 percent or so.So, people benchmark their risk against that.If you look at indices that measure hedge fund industry performance,you see volatility of around four to five percent.So, if I'm running a hedge fund,there's business risk associated with settingmy product's portfolio risk to something dramatically different from those numbers.Over time, there's been evolutionary convergence to the levels of risk,that makes sense given the market for these products.If you work in a really big firm,and you're managing a small portfolio,you will likely receive this number as a mandate,such as you can't lose more than 10 percent.So, you can calibrate your risk so that you are comfortable with where youare currently in relation to that maximum loss threshold.

### 04. M4 L4 04 Regularization V4-fq-CanyDHuw.en

As of right now, we have this as our optimization objective function.But there's one other term that it's a good idea to add to it at this point.This is what's called a regularization term.Here we use the L2 norm of the portfolio weights as the regularization term.In other types of problems,you might use a different term.What this do? Well, as you'll recall,the L2 norm of a vector is its length.So, in this case, the term is basically the length of the vector of portfolio weights.This entire quantity will get very large if the weight on any single asset gets large.So, the effect is to penalize this behaviorand enforce the spreading of weight amongst assets.The parameter here controls the balance between maximizingthe portfolio alpha and enforcing the spreading of the weights across assets.To further clarify this,let's imagine two possible extreme states of the world.In the first, you feel maximally confident in your alpha factors.You know for sure which stocks are going to go up and down in the future and by how much.In this situation you'd put all your long money in the stocks that will goup the most and all your short money in the stocks that will go down the most.In the second extreme state,you are absolutely sure that you know whichstocks are going to go up and which are going to go down,but you don't know by how much.You only know the sign of the change.So, in this case,the optimal portfolio is equal weight on all the stocks you take long positions on,and equal weight on all the stocks you take short positions on.You can think of the regularization parameter as a dial that you can use totune between these two extreme versions of your prior knowledge of the world.This is the Bayesian interpretation of this technique.If you set the parameter equal to zero,that's the state where you have full confidence in the alpha vector.As the parameter approaches infinity,the output weights approach equal weighting.So, you can think of the regularization parameter as your conviction dial.

### 05. M4 L4 06 Standard Constraints V4-OPBKsNQPr6I.en

But we're not quite done.As you know, we can impose lots of different constraints on the problem.These usually have to do withreal-world constraints on the way we want our portfolio to behave.Sometimes they just make sure our results aren't really wacky.Let's discuss some common constraints.First off, we need to think about whether our portfolio is longonly or whether we are allowed to take short positions.We'd usually know this from limitations in our trading environment.If we needed to enforce the constraint that our portfolio be long only,we'd require that each of our portfolio weights to be positive.If we can take long or short positions,we'd place no such constraint.It's very common to require that our portfolio be market neutral.What do we mean by that?In effect, it means that across the board,we short as much capital as we long.This means that we lose as much as we gain from movements that affect the entire market.In other words, when a portfolio is designed to be market neutral,the portfolio's returns are hopefully less affected by movement in the entire market.We mentioned making the portfolio market neutral when we discussed alpha factors.The difference here is that we place this asa constraint on the portfolio weights themselves,so that the constraint will be enforced by the optimization.Hedge funds usually require that the sum of the weights be zero.However, an alternative is to control the balance of long toshort positions by requiring that the sum of the weights remain in some range.

### 06. M4 L4 07 Leverage Constraint V5-zJ9gon4rFQc.en

Another important constraint is the leverage constraints.With this constraint, we limit the leverage ratio which, as you recall,is the sum of the absolute dollar value of all positions,long and short, divided bywhatever actual capital we devote to supporting those positions.In our portfolio optimization problem,we work with portfolio weights which are percentages of total invested capital,so the leverage ratio is just the sum ofthe absolute value of the weights, long and short.Again, we discussed this when we discussed alpha factors,but the basic idea here is that when you short stocks,you borrow the stocks and sell them on the open market.You can use the cash you get to take long positions on other stocks.In general, any investment position is said tobe leveraged if it is financed by a debt position.Whether it's financed by borrowed cash or borrowed stocks.However, it would be much more risky to takevery large short positions in order to finance very large long positions.These are large bets and shorting can be complicated.If the leverage ratio is allowed to be greater than one,it means the total magnitude of all year long and shortpositions is greater than the amount of initial capital that you have to work with.This means you're borrowing money either by shorting more stocks or by borrowing cash.You have to borrow from somebody and they'll have a maximum there willing to lend to you.So, you place a constraint on your leverage ratio which is the sum ofthe absolute values of all your positions to control the scale of this borrowing.This will keep you from ending up in a very extreme leverage position.If you're using an objective function likethis and you are not using a risk constraint like this,you need a constraint likethe leverage constraint to keep your weights from growing to infinity.You can try to control leverage by just controlling risk,but the leverage constraint is usuallya hard constraint that will also be necessary to enforce.

### 07. M4 L4 08 Factor Exposure And Position Constraints V3-wMY4zI5zLSM.en

Another common type of constraint limits the extent towhich you are exposed to any individual factor.This is where you might impose specific limitationson your exposure to common risk model factors.Individual sectors, momentum, value, or company size.Remember that the factor exposure matrix has dimensions of assets and factors.When you multiply the transposed factor exposure matrix by the weight vector,which has dimensions of assets,you get a vector that has dimensions of factors.Thus, this product is still a vector.When we impose a constraint like this,where this is a single scalar,it means that the constraint applies to each element of the vector.That is, we apply the same constraint for every factor.Again, where do we get these constraint numbers from?They come from business and risk mandates.Finally, it's common to place some ultimate constraints on the individual weightsthemselves just to ensure that you don't end up invery extreme positions in individual assets.Again, remember that the weight vector is a vector with dimensions of assets,so placing a constraint like this applies to each asset weight in the vector.You can think of this constraint as a kind of insurance.In theory, the risk model should help limit portfolio risk.But in case there's a problem with a risk model,setting limits on how much weight to put on any single stockalso helps to protect the portfolio from concentrated risk exposure.This constraint is often impose so thatthe hedge fund can clearly communicate to investors thatthere is an absolute limit on the extent to whichthe portfolio is invested in any one asset.This value could be quoted in fund prospectus documentation.

### 09. M4 L4 10 Estimation Error V4-WdrMIRhScN0.en

Let's think for a moment why what we've come up with now issuperior to the way we were doing optimization before.Remember how before we hadthis large covariance matrix of assets that we were using to estimate portfolio variance.Well, now we have a smaller number of factors.Common commercial risk models have around 70 factors.So, this matrix is now a 70 by 70 matrix,as opposed to the potentially several thousand byseveral thousand matrix we would have had ifwe were using a covariance matrix of the assets.There are literally just fewer elements in this matrix.This means that we have reduced the number of quantities we are trying to estimate.How many fewer elements are there?Well, let's count them.Let's say the dimension of our asset covariance matrix isn. Remember that covariance matrices are symmetric.There are n elements on the diagonal.If we look at the remaining elements,we see we have a matrix of dimension n minus 1 byn. But each element is in this matrix exactly twice.So, the number of unique elements is n times n minus 1 divided by 2.So, we have n elements along the diagonal and a halfof n times n minus 1 number of elements that are not along the diagonal.We'll rearrange the formula a bit to get n times n plus 1 all divided by 2.So, in total, we have n times n plus 1 divided by 2 quantities to estimate.Let's say n is 3,000.This is then around 4.5 million quantities.If n is instead 70,then this is more like 2.5 thousand quantities.There's a big difference in scale there.There are many fewer opportunities to introduce estimation error.The fact that we are now estimating many fewer parameters isa good thing and an important reason why we use factor models of risk.Another thing to remember is that each of the elements in this matrix isan estimate of variance or covariance of random variables.In practice, we estimate variances and covariances using time series of data.If you have n assets,and you want to estimate the covariance matrix of those n assets,then the number of days of return that you need,t, has to be greater than n,and ideally much greater than n. There's more weneed to talk about to explain why the number of data points needsto be much greater than the number of variablesn. But one problem with insufficient data isthat there won't be enough observations to accuratelyestimate all of the variances and covariances,which means that the covariance matrix,based on the sample,would be significantly different from the population covariance matrix.Also, PCA, using such a covariance matrix,would not be able to produce meaningful principal components.If you have 3,000 securities,you need at least 3,000 days of data,which is about 12 years.But we also know that variance and covariance probably change over time.So, does using 12 years of data topredict the variance for the next month even make sense?Another reason people usethe risk factor model formulation ofthe covariance matrix is that you get around that problem.N is a smaller number,and so t can be a smaller number.

### 10. M4 L4 12 Infeasible Problems V4-ljg25Rj511Q.en

Remember how earlier we talked about the possibility of setting yourself a problem thatdoesn't have a solution because there is no answer that satisfies all the constraints?Well, that can happen in practice.When you're working with two or three dimensional problemswhere you can graph the functions and observe what's going on,it might be easier to see when the problem becomes infeasible.However, in practice you're working ina high-dimensional space where it's hard to visualize what's going on.In general learning what makes the problem infeasible comes with experience.So, much of this might sound rather abstract,but we want to have a short discussion here to give you an intuition.One possible situation where this might arise,is if you fix the weight on a particular stock.For example, sometimes for some reason the compliance group atyour company may restrict trading company wide on a particular stock.So, there may be some stock that you can'ttrade or there may be a particular stock that's difficult to short,so again, you'd like to constrain the weight on that stock.However, if you put a hard constraint on the weight on that particular stock,this might cause you to reach an infeasible solution in your optimization problem.Let's just say first that it's often dangerous to putequality constraints in an optimization problembecause the problem can get infeasible very easily.Remember that inequality constraint isa much stronger condition than an inequality constraint.It's the difference between requiring that they answerlie on a line or below a line, for example.Infeasibility is an issue with your constraints.One possible solution is to moveone or more constraints to penalty terms in the objective function.This changes the problem from placing a hard limiton some quantity to seeking to minimize that quantity,but balanced against the other terms in the objective.So, what are general rules of thumb orguidelines for avoiding these situations in practice?In production you're inheriting a live portfolio.Your job in optimization is to transition that portfolio to a new state using new data,new Alpha factors, and keeping in mind thatmarket movements have caused the old weights to change slightly.The first thing to check is whetherthe starting portfolio produces an infeasible problem or not.In general start with fewer terms in the objective function.If that problem becomes infeasible,try to figure out which constraints are the ones causing the infeasibility,see if you can change or override any of those constraints.Finally, consider moving constraints to penalty terms in the objective function.

### 11. M4 L4 14 Transaction Costs V3-yxwqTvbJhhc.en

As you already know,transaction costs are a hugely important thing to think about.Can you do anything to mitigate transaction costs and the optimization problem itself?Well, you know you want to minimize transaction costs.Can you just plop some quantity that measurestransaction costs somehow into the objective function?That sounds like a seemingly satisfying solution.The problem with this is that in practice,it's extremely difficult to know how to represent those costs.To know the transaction costs,you must know what the trade is,but the whole reason to do the optimization is to know what the traders.In general, transaction cost is a function of trade size.But the problem is even an infinitesimally small tradewill incur non-trivial transaction costs.That is, transaction cost is a discontinuous function of trade size.This is because there are lower bounds on bid ask spreads and Trade Commission's.This means that a transaction cost term can'tbe included in the objective function easily.One possibility is to impose a turnover constraint.Again, simply limiting the net change in weighton each asset relative to the weights in the previous portfolio.In general, turnover is directly proportional to transaction cost.So, you might limit turnover to say 20 percent.The problem with doing this is you can easily get to an infeasible situation.It's very possible that you can have a situation where through market movements,the portfolio gets pushed into a certain scenariowhere to have all the constraints satisfied,the turnover constraint must be violated.For example, market movements may have pushed the size ofa stock's position beyond the maximum allowed by the individual weight constraint,but the trade required to reduce that weightmay be more than allowed by the turnover constraint.There are two potential solutions.The first is to put that turnover constraint intoa penalty term in the objective function so that instead of a hard cap,you have a scaling term that penalizes turnover.The second solution is to put the whole optimization problem inside a loop.You can impose a hard turnover constraint,and if the problem is infeasible,you can progressively relax the turnover constraintand rerun the optimization until the problem becomes feasible.

### 13. M4 L4 17 Path Dependency 1 V3-ok9rKYRtZLE.en

Why will two portfolios employing the same underlying strategy run on the same as atuniverse during the same period yielda different portfolio if started at different points in time?The reason is that transitions between different setsof portfolio weights can be more or less costly.Consider two portfolios trying to make the same transition to the same ideal portfolio.A starting portfolio close to this portfolio might be directed here,but one starting from farther away might be directed elsewhere.For example; say I start my strategy on January 1 and the signal says,go long Apple 10 percent.So, I enter this 10 percent long position,then later on June 1,the ideal position is to go short Apple 10 percent.If I've imposed a turnover constraint or penalty in the problem,the optimizer may not conclude that it's actually ideal to enterthis 10 percent short position becauseit's too far away from the position I'm already in.However, if my way on Apple was previously zero,it may tell me that the best next step is to go 10 percent short on Apple.This could even happen for portfolio started at the same time butholding different amounts of capital if they have a liquidity constraint.A liquidity constraint means that we restrict our positions to bebelow some percent of each asset's daily trade volume.If two portfolios have different capital amounts thanthe smaller capital portfolio is less constrained by a liquidity constraint.For example, if you have some small stock that trades one million dollars per day,and you have a small portfolio that wants to takea 10 percent or 100k position in that stock,which is 10 percent of its volume, that would be okay.But if you had a $100 million portfolio and the same Alpha vector,then that portfolio would want to take a $10 million position in the stock,which would be 10 times its daily volume,and so you'd only be able to take a very tiny position.So, ultimately, due to these trading constraints,the final portfolio weights of these portfolios would be very different.In practice, you may see this effect if you run the same strategy for different clients.Their portfolios may net different returns.This is the surprising but very common feature.It also affects backtesting.We've talked about backtesting only a little,but you'll recall that it involves rigorously simulating,trading a portfolio by testing on historical data.This type of simulation involves a lot of data and a lot of steps.You may want to speed it up by running parts ofthe computation in parallel across many computers.However, this is difficult because each step of the computation where we calculate whatthe portfolio would be at each point intime is in practice dependent on the previous stage.We call this path dependency.However, for portfolios of the same size over long periods of time,like years, you would expect the paths of the portfolios to rejoin each other.People have attempted creative solutions to this difficulty.They may run simulations of 18-month windows of time that havesix months of overlap on different machines with the whole simulation spanning 10 years.Then, they chop off three months oneither end of each simulation and stitch these together.

### 14. M4 L4 19 What Is Optimization Doing To OUr Alphas V3-6Yqb91Xahvg.en

Let's take a moment to have a final discussion about what is going on here.We started with our alpha vector,which is a vector of numbers that we think will be proportional to future returns.We want to maximize our alpha times our weights andminimize risk as modeled by our risk model,and we use optimization to achieve this.We also apply several other constraints during optimization,such as a constraint that we are market neutral, for example.But if we apply the market neutral constraint now,why did we subtract the mean from each individual alphato make them market neutral back when we were calculating our alphas?Isn't this just duplicating alpha?Why not just wait for the optimizer to make the portfolio market neutral?We are using optimization so that we can control risk,but it creates a challenge when the optimizationhas a significant effect on the alpha vector.If your resulting portfolio is massively different than your alpha vector,then all your research and evaluation of your alpha factors up to that point might beinvalidated if the final portfolio weights given bythe optimizer don't look anything like the original alpha factor.So, you don't want the optimization to change your alpha vector too much.If you find some alphas and the optimization yieldsa very different portfolio than what you had when you started,then it's hard to figure out how to adjust the parameters.It can be hard to know whether to adjustthe risk model or the constraints and by how much.Your alpha research is your pure expression of expected return.Any deviation from that is a sub-optimal deviation.But you're willing to accept that to trade off against risk,and you're willing to change the alpha vector to optimize on risk.But that only goes so far.If you end up with a portfolio that is completely far away from the alpha vector,then this isn't helpful either.You've evaluated the alpha factors and have some sense of how good they are,but the portfolio may no longer be following the signals of those alpha factors.So, what that means from a practical perspective is you want tointroduce risk control as early in the process as possible.If you know that you'll use your alpha in a portfolio that is sector neutral,then you should make the alpha factor sector neutral.That way, if the alpha factor already looks good given the sector neutral weights,it has a better chance of translating well from theoretical alpha to resulting portfolio.

### 15. M4 L4 20 Outro V1-c3J8t6q2BGo.en

Wow. Just wow.I am really proud of you.You made it through factor models and all the portfolio optimization content.This is tough stuff, no bones about it.This is a huge accomplishment on your part,and I congratulate you.Maybe, take a few minutes and think back toeverything you have learned since you started this Nanodegree.Back in the beginning, we were talking about stock prices,adjusting for corporate actions,Panda's time series operations but since then,you've created several trading strategies,learned a number of modeling techniques,learned about portfolio optimization, and now, finally,you're seeing how to put it all together intoa real professional level institutional trading strategy,drawing in novel ideas for Alphas.This is a huge progression.Onwards and upwards, time to put your knowledge to work.This project is truly top notch and I think you'll learn a lot by getting through it.I'm excited for you. I hope you enjoyed it.

## Part 01-Module 04-Lesson 09_Project 4 Alpha Research and Factor Modeling

### 01. M4 01 Intro To Project 4 V1-7goOG7CdUjU.en

Now that you've learned a lot of the theory behind alpha and risk factors,and had a chance to work through some of the important ideas and exercises,you have the opportunity to put all these ideas intopractice in your final project of term one.Personally, it's my favorite project of the term in part, because I designed it.In this project, we code up and test several alpha factors.First, we evaluate the factors to see which ones may bepromising candidates to put together into a combined alpha factor.We examine them initially to check whether the alphashave predictive power in the cross-section of stocks.We calculate their Sharpe ratios,and we tried to get a sense of how much trading they would incur by a turnover analysis.Then we throw the alphas into an optimizer in orderto calculate optimal portfolio weights bymaximizing our predicted return via our alphas andsimultaneously attempting to neutralize exposure to common risk factors,which are sources of return variance.I created this project as a fully production-ready prototype,so that you could see how industry practitioners do this in the real world.It was especially important to me that you seehow ideas for alphas turned into code and howboth alpha and risk factor models playeddistinct and equally important roles in portfolio construction.I also utilized industry-ready Python librariesthat make common operations simpler and faster,so you have a chance to get familiar with these packages.I'm really excited to share this methodology with you,and I hope you enjoy the project.

### 04. M4 03 Coming In Term II V1-2jF5J8MIdqc.en

Hi there. Congratulations again on completingterm one of the AI for Trading nanodegree program.You've built a solid foundation in quant research over these last few months.In term two, you'll build upon this foundation with an emphasison advanced techniques using natural language processing and deep neural nets.You'll use these AI methods to process alternative data and generate trading signals.After, you'll learn and practice back testing,which is a crucial step in research and which isnot often taught in academic or online courses.Gaining concrete experience in Thoreau backtesting is what will enable you tofind actionable signals and differentiate the real signals from the noise.Finally, you'll also learnadvanced techniques for Alpha combination using machine learning.Alpha combination is also a key step in the quant workflow,as real-world portfolios, use a combination of signals from multiple Alphas.Deciding not only which Alphas to include but how to combinethem is a key skill set to have as a quant researcher.We're really excited about what's coming up in term two and I hope you are too.

## Part 02-Module 01-Lesson 01_Welcome To Term II

## Part 02-Module 01-Lesson 02_Intro to Natural Language Processing

### 01. Welcome to NLP-g-AlFF61p0I.en

Welcome to Natural Language Processing.Language is an important medium for human communication.It allows us to convey information,express our ideas, and give instructions to others.Some philosophers argue that it enablesus to form complex thoughts and reason about them.It may turn out to be a critical component of human intelligence.Now consider the various artificial systems we interact with every day,phones, cars, websites, coffee machines.It's natural to expect them to be able to process and understand human language, right?Yet, computers are still lagging behind.No doubt, we have madesome incredible progress in the field of natural language processing,but there is still a long way to go.And that's what makes this an exciting and dynamic area of study.In this lesson you will not only get to knowmore about the applications and challenges in NLP,you will learn how to design an intelligent application thatuses NLP techniques and deploy it on a scalable platform.Sounds fun? Let's get started.

### 02. Structured Languages-NsmqUIHlk6U.en

What makes it so hard for computers to understand us?One drawback of human languages,or feature depending on how you look at it,is the lack of a precisely defined structure.To understand how that makes things difficult let'sfirst take a look at some languages that are more structured.Mathematics, for instance, uses a structured language.When I write y equals 2x plus 5 there is no ambiguity in what I want to convey.I'm saying that the variable y is related to the variable x as two times x plus five.Formal logic also uses a structure language.For example, consider the expression parent(x,y) and parent(x,z) implies sibling(y, z).This statement is asserting that if x is a parent of y and x is a parent of z,then y and z are siblings.A set of structure languages that may be morefamiliar to you are scripting and programming languages.Consider this SQL statement.SELECT name, email FROM users WHERE name LIKE A%.We are asking the database to return the names ande-mail addresses of all users whose names begin with an A.These languages are designed to be asunambiguous as possible and are suitable for computers to process.

### 03. Grammar-Jw3dA7xmoQ4.en

Structured languages are easy to parse and understand forcomputers because they are defined by a strict set of rules or grammar.There are standard forms of expressing such grammars and algorithms,that can parse properly formed statements to understand exactly what is meant.When a statement doesn't match the prescribed grammar,a typical computer doesn't try to guess the meaning, it simply gives up.Such violations of grammatical rules are reported as syntax errors.

### 04. Unstructured Text-OmwSdaec5vU.en

The languages we use to communicate with each other also have defined grammatical rules.And indeed, in some situations we usesimple structured sentences but forthe most part human discourse is complex and unstructured.Despite that, we seem to be really good at understandingeach other and even ambiguities are welcome to a certain extent.So, what can computers do to make sense of unstructured text?Here are some preliminary ideas.Computers can do some level of processing with words and phrases,trying to identify key words,parts of speech, named entities, dates, quantities, etc.Using this information they can also try to parse sentences,at least ones that are relatively more structured.This can help extract the relevant parts of statements, questions, or instructions.At a higher level computers can analyze documents to find frequent and rare words,assess the overall tone or sentiment being expressed,and even cluster or group similar documents together.You can imagine that building on top of these ideas,computers can do a whole lot withunstructured text even if they cannot understand it like us.

### 06. Context-J-4pfu2w1C0.en

So what is stopping computers from becoming as capableas humans in understanding natural language?Part of the problem lies in the variability and complexity of our sentences.Consider this excerpt from a movie review."I was lured to see this on the promise ofa smart witty slice of old fashioned fun and intrigue. I was conned. "Although it starts withsome potentially positive words it turns out to be a strongly negative review.Sentences like this might be somewhat entertaining forus but computers tend to make mistakes when trying to analyze them.But there is a bigger challenge that makes NLP harder than you think.Take a look at this sentence."The sofa didn't fit through the door because it was too narrow."What does "it" refer to?Clearly "it" refers to the door.Now consider a slight variation of this sentence."The sofa didn't fit through the door because it was too wide."What does "it" refer to in this case?Here it's the sofa. Think about it.To understand the proper meaning or semantics ofthe sentence you implicitly applied your knowledge about the physical world,that wide things don't fit through narrow things.You may have experienced a similar situation before.You can imagine that there are countless other scenarios in which some knowledge orcontext is indispensable for correctly understanding what is being said.

### 07. Natural Language Processing-UQBxJzoCp-I.en

Natural language processing is one of the fastest growing fields in the world.NLP Is making its way into a number of products and services that we use every day.Let's begin with an overview of how to design an end-to-end NLP pipeline.Not that kind of pipeline;a natural language processing pipeline,where you start with raw text,in whatever form it is available, process it,extract relevant features, and build models to accomplish various NLP tasks.Now that I think about it,that is kind of like refining crude oil.Anyways, you'll learn how these different stages in the pipeline depend on each other.You'll also learn how to make design decisions,how to choose existing libraries,and tools to perform each step.

### 08. NLP M1-L1 01 NLP Pipeline-vJx6oKlu_MM.en

Let's look at a common NLP pipeline.It consists of three stages,text processing, feature extraction and modeling.Each stage transforms text in some way and produces a result that the next stage needs.For example, the goal of text processing is to take raw input text,clean it, normalize it,and convert it into a form that is suitable for feature extraction.Similarly, the next stage needs to extract and produce feature representations that areappropriate for that type of model you're planning touse and the NLP task you're trying to accomplish.When you're building such a pipeline,your workflow may not be perfectly linear.Let's say, you spend some time implementing text processing functions,then make some simple feature extractors,and then design a baseline statistical model.But then, maybe you are not happy with the results.So you go back and rethink what features you need,and that in turn,can make you change your processing routines.Keep in mind that this is a very simplified view of natural language processing.Your application may require additional steps.

### 09. Text Processing-pqheVyctkNQ.en

Let's take a closer look at text processing.The first question that comes to mind is,why do we need to process text?Why can we not feed it in directly?To understand that, think about where we get this text to begin with.Websites are a common source of textual information.Here's a portion of a sample web page from Wikipedia and the corresponding HTML markup,which serves as our raw input.For the purpose of natural language processing,you would typically want to get rid of all or most of the HTML tags,and retain only plain text.You can also remove or set aside any URLs or other items not relevant to your task.The Web is probably the most common and fastest growing source of textual content.But you may also need to consume PDFs,Word documents or other file formats.Or your raw input may even come froma speech recognition system or from a book scan using OCR.Some knowledge of the source medium can help you properly handle the input.In the end, your goal is to extract plain text that is free ofany source specific markers or constructs that are not relevant to your task.Once you have obtained plain text,some further processing may be necessary.For instance, capitalization doesn't usually change the meaning of a word.We can convert all the words to the same case so that they're not treated differently.Punctuation marks that we use to indicate pauses, etc.can also be removed.Some common words in a language often help provide structure,but don't add much meaning.For example, a, and,the, of, are, and so on.Sometimes it's best to remove them if that helpsreduce the complexity of the procedures you want to apply later.

### 10. Feature Extraction-UgENzCmfFWE.en

Okay. We now have clean normalized text.Can we feed this into a statistical or a machine learning model?Not quite. Let's see why.Text data is represented on modern computers using an encodingsuch as ASCII or Unicode that maps every character to a number.Computer store and transmit these values as binary, zeros and ones.These numbers also have an implicit ordering.65 is less than 66 which is less than 67.But does that mean A is less than B,and B is less and C?No. In fact, that would be an incorrect assumption tomake and might mislead our natural language processing algorithms.Moreover, individual characters don't carry much meaning at all.It is words that we should be concerned with,but computers don't have a standard representation for words.Yes, internally they are just sequences of ASCII or Unicodevalues but they don't quite capture the meanings or relationships between words.Compare this with how an image is represented in computer memory.Each pixel value contains the relative intensity of light at that spot in the image.For a color image,we keep one value per primary color;red, green, and blue.These values carry relevant information.Two pixels with similar values are perceptually similar.Therefore, it makes sense to directly use pixel values in a numerical model.Yes, some feature engineering may be necessary such as edge detection or filtering,but pixels are a good starting point.So the question is,how do we come up with a similar representation fortext data that we can use as features for modeling?The answer again depends on what kind of model you'reusing and what task you're trying to accomplish.If you want to use a graph based model to extract insights,you may want to represent your words assymbolic nodes with relationships between them like WordNet.For statistical models however,you need some sort of numerical representation.Even then, you have to think about the end goal.If you're trying to perform a document level task,such as spam detection or sentiment analysis,you may want to use a per document representations such as bag-of-words or doc2vec.If you want to work with individual words and phrasessuch as for text generation or machine translation,you'll need a word level representation such as word2vec or glove.There are many ways of representing textual information,and it is only through practice that you can learn what you need for each problem.

### 11. Modeling-P4w_2rkxBvE.en

The final stage in this process is what I like to call modeling.This includes designing a model,usually a statistical or a machine learning model,fitting its parameters to training data using an optimization procedure,and then using it to make predictions about unseen data.The nice thing about working with numerical features is thatit allows you to utilize pretty much any machine learning model.This includes support vector machines, decision trees,neural networks, or any custom model of your choice.You could even combine multiple models to get better performance.How you utilize the model is up to you.You can deploy it as a web-based application,package it up into a handy mobile app,integrate it with other products,services, and so on.The possibilities are endless.

## Part 02-Module 01-Lesson 03_Text Processing

### 01. Text Processing-6LO6I5M18PQ.en

In this lesson, you'll learn how to read text data fromdifferent sources and prepare it for feature extraction.You'll begin by cleaning it to remove irrelevant items,such as HTML tags.You will then normalize text by converting it into all lowercase,removing punctuations and extra spaces.Next, you will split the text into words or tokens and remove words that are too common,also known as stop words.Finally, you will learn how to identify different parts of speech,named entities, and convert words into canonical forms using stemming and lemmatization.After going through all these processing steps,your text may look very different,but it captures the essence of what was beingconveyed in a form that is easier to work with.

### 03. Capturing Text Data-Z4mnMN1ApG4.en

The processing stage begins with reading text data.Depending on your application,that can be from one of several sources.The simplest source is a plain text file on your local machine.We can read it in using Python's built in file input mechanism.Text data may also be included as part of a larger database or table.Here, we have a CSV file containing information about some news articles.We can read this in using pandas very easily.Pandas includes several useful string manipulation methodsthat can be applied to an entire column at once.For instance, converting all values to lowercase.Sometimes, you may have to fetch data from an online resource,such as a web service or API.In this example, we use the requests library inPython to obtain a quote of the day from a simple API,but you could also obtain tweets,reviews, comments, whatever you would like to analyze.Most APIs return JSON or XML data,so you need to be aware of the structure in order to pull out the fields that you need.Many data sets you will encounter have likely beenfetched and prepared by someone else using a similar procedure.

### 04. Cleaning-qawXp9DPV6I.en

Text data, especially from online sources,is almost never clean.Let's look at the Udacity course catalog as an example.Say you want to extract the title and description of each course or Nanodegree.Sounds simple, right?Let's jump into Python and give it a shot.You can follow along by downloading and launching the text processing notebook.We can fetch the web page like any other online resource using the requests library.It looks like we got back the entire HTML source.This is what the browser needs to render the web page.But most of this is useless for our purposes.We need a way to extract all the plain text as visible on the website.How about using regular expressions?Let's define a pattern to matchall HTML tags and remove them by replacing with a blank string.Okay, that did something.We can see that the page title has been extracted successfully,but there is a lot of JavaScript and a number of other items that we don't need.In fact, this regular expression somehow didn't match some tags.Maybe they were nested inside other tags.Maybe we need to account for tags spread across lines.Anyway, this doesn't seem like the best approach for this job.What we really need is a way to parse the HTML,just like a web browser,and pull out the relevant elements.Introducing BeautifulSoup.It's a nice Python library meant to do exactly that.You just pass in the raw web page text which in this casecontains HTML to create a soup object,and then you can extract the plain text,leaving behind any HTML tags using a symbol called to the get-text method.This takes care of nested tags,tags that are broken across lines,and a multitude of other edge cases that make HTML parsing a pain.It also forgives some small errors in HTML just like browsers,making it more robust. Let's see.That's better.I don't see any HTML tags,but there are still a bunch of JavaScript and a lot of spaces.What else can we do? Let's take a look at how the HTML source is structured.The easiest way to do this is to right click on an element of your choice,here, this course title,and choose Inspect or View Page Source.Now look at where the title is placed and what isthe most distinct way of finding it in the HTML documentary.Here, we have a parent div with a class of course summary card.That sounds promising. Let's use it.BeautifulSoup is actually very powerful.It enables you to walk the tree or dorm in many different ways.Here we are asking the library to find all divs with a class of course summary card.The result returned is a list of all such divs in the document.Let's store this in a variable and look at one of the divs.Okay. Scrolling through this,I see that the title is stored in this a-tag which is contained in this H3 tag.How do we extract this title?One way to get to it is using a CSS selector.And now we can fetch the plain text content just like we did before.Great. One last thing.Let's strip out the extra whitespace from both ends. There you go.Now, let's look back at the HTML to see how wecan grab the description text. There it is.It's a div with an attribute called data-course-short-summary,but no value or any other attribute.Again, there is a way to select such tags using CSS.Specify the tag name, here, div,followed by the attribute name in square brackets. Looks good.Let's extract the text and clean it up.All right. We can now repeat this over all course summaries.To do this, we can use a simple for loop.Looks spot on to me.Let's store this data so we can use it later.Here, we are simply keeping the data in a list called courses.What we did just now is called scraping a web page.Although it sounds a little violent,trust me, it's not.In fact, scraping is very common.Google News is a prime example.It pulls out the title and first sentence or two from news articles and displays them.Google probably uses a combination of rules andmachine learning to identify what portion ofthe HTML contains the title andthe beginning of the article text that it can use as a preview.It works great most of the time,but sometimes, it does fail.Here, for this article on quantum entanglement,the preview doesn't seem to match the title at all.It looks more like a caption for this image.What likely happened is that the caption was the first piece of text onthe web page and Google's algorithm picked that up as if it was part of the main article.This just goes to show how seemingly routine tasks andtext processing are still not solved all the way.Okay, let's look back at what we just achieved.We started by fetching a single web page,the Udacity course catalog.Then we tried a couple of methods to remove HTML tags.We finally settled on using BeautifulSoup to parse the entire HTML source,find all course summaries,and extract the title and description for each course.And then we saved them all in a list.Depending on what you're planning to do next,you may continue to treat these chunks as part ofa single document or consider each to be a separate document.The latter is useful,for instance, if you want to group related courses.The problem then reduces to document clustering.

### 05. Normalization-eOV2UUY8vtM.en

Plain text is great but it's stillhuman language with all its variations and bells and whistles.Next, we'll try to reduce some of that complexity.In the English language,the starting letter of the first word in any sentence is usually capitalized.All caps are sometimes used for emphasis and for stylistic reasons.While this is convenient fora human reader from the standpoint of a machine learning algorithm,it does not make sense to differentiate between Car,car, and CAR, they all mean the same thing.Therefore, we usually convert every letter in our text to a common case,usually lowercase, so that each word is represented by a unique token.Here's some sample text,a review for the movie, The Second Renaissance,a story about intelligent robots that get into a fight with humans over their rights.Yup, the way we treat robots these days.Anyway, if we have the reviews stored in a variable called text,converting it to lowercase is a simple call to the lore method in Python.Here's what it looks like after a conversion.Note all the letters that were changed.Other languages may or may not have a caseequivalent but similar principles may apply depending on your NLP task,you may want to remove special characters like periods, question marks,and exclamation points from the text and onlykeep letters of the alphabet and maybe numbers.This is especially useful when we are lookingat text documents as a whole in applicationslike document classification andclustering where the low level details do not matter a lot.Here, we can use a regular expression that matcheseverything that is not a lowercase A to Z,uppercase A is Z,or digits zero to nine,and replaces them with a space.This approach avoids having to specify all punctuation characters,but you can use other regular expressions as well.Lowercase conversion and punctuation removalare the two most common text normalization steps.Whether you need to apply them and at what stagedepends on your end goal and the way you design your pipeline.

### 06. Tokenization-4Ieotbeh4u8.en

Token is a fancy term for a symbol.Usually, one that holds some meaning and is not typically split up any further.In case of natural language processing,our tokens are usually individual words.So tokenization is simply splitting each sentence into a sequence of words.The simplest way to do this is using the split method which returns a list of words.Note that it splits on whitespace characters by default,which includes regular spaces but also tabs,new lines, et cetera.It's also smart about ignoring two or more whitespace characters in a sequence,so it doesn't return blank strings.But you can control all this using optional parameters.So far, we've only been using Python's built-in functionality,but some of these operations are much easier to perform using a library like NLTK,which stands for natural language toolkit.The most common approach for splitting up texting NLTK is touse the word tokenized function from nltk.tokenize.This performs the same task as split but is a little smarter.Try passing in some raw text that has not been normalized.You'll notice that the punctuations are treated differently based on their position.Here, the period after the title Doctor hasbeen retained along with Dr as a single token.As you can imagine,NLTK is using some rules or patterns to decide what to do with each punctuation.Sometimes, you may need to split text into sentences.For instance, if you want to translate it.You can achieve this with NLTK using sent tokenize.Then you can split each sentence into words if needed.NLTK provide several other tokenizers,including a regular expression base tokenizer that you can use toremove punctuation and perform tokenization in a single step,and also a tweet tokenizer that is aware of twitter handles,hash tags, and emoticons.Check out the nltk.tokenize package for more details.

### 07. Stop Word Removal-WAU_Ij0GJbw.en

Stop words are uninformative words like, is, our,the, in, at, et cetera that do not add a lot of meaning to a sentence.They are typically very commonly occurring words,and we may want to remove them to reduce the vocabulary we have todeal with and hence the complexity of later procedures.Notice that even without our and the in the sentence above,we can still infer it's positive sentiment toward dogs.You can see for yourself which words NLTK considers to be stop words in English.Note that this is based on a specific corpus or collection of text.Different corpora may have different stop words.Also, a word maybe a stop word in one application,but a useful word in another.To remove stop words from a piece of text,you can use a Python list comprehension with a filtering condition.Here, we apply stop word removal to the movie review after normalizing and tokenizing it.The result is a little hard to read,but notice how it has helped reduce the size of the input,at the same time important words have been retained.

### 08. Part-of-Speech Tagging-WFEu8bXI5OA.en

Remember parts of speech from school?Nouns, pronouns, verbs, adverbs, et cetera.Identifying how words are being used ina sentence can help us better understand what is being said.It can also point out relationships between words and recognize cross references.NLTK, again, makes things pretty easy for us.You can pass in tokens or words to the POS tag functionwhich returns a tag for each word identifying different parts of speech.Notice how it has correctly labelled the first utterance of "lie" as a verb,while marking the second one as a noun.Refer to the NLTK documentation for more details on what each tag means.One of the cool applications of part of speech tagging is parsing sentences.Here's an example from the NLTK book that usesa custom grammar to parse an ambiguous sentence.Notice how the parser returns both interpretations that are valid.It is much easier to see the difference when we visualize the parse trees.I shot an elephant in my pajamas,versus, I shot an elephant and the elephant was in my pajamas.How he got into my pajamas? I don't know.

### 09. Named Entity Recognition-QUQu2nsE7vE.en

Named entities are typically noun phrases thatrefer to some specific object, person, or place.You can use the ne_chunk function to label named entities in text.Note that you have to first tokenize and tag parts of speech.This is a very simple example,but notice how the different entity types are also recognized: person,organization, and GPE, which stands for geopolitical entity.Also note how it identified the two words,Udacity and Inc, together as a single entity.Out in the wild, performance is not alwaysgreat but training on a large corpus definitely helps.Named entity recognition is often used to index and search for news articles,for example, on companies of interest.

### 10. Stemming And Lemmatization-7Gjf81u5hmw.en

In order to further simplify text data,let's look at some ways to normalize different variations and modifications of words.Stemming is the process of reducing a word to its stem or root form.For instance, branching, branched,branches et cetera, can all be reduced to branch.After all, they conveyed the idea of somethingseparating into multiple paths or branches.Again, this helps reduce complexity while retainingthe essence of meaning that is carried by words.Stemming is meant to be a fast and crude operation carriedout by applying very simple search and replace style rules.For example, the suffixes 'ing' and 'ed' can be dropped off,'ies' can be replaced by 'y' et cetera.This may result in stem words that are not complete words,but that's okay, as long as all forms of that word are reduced to the same stem.Thus, capturing the common underlying idea.NLTK has a few different stemmers for you to choose from,including PorterStemmer that we use here,SnowballStemmer, and other language-specific stemmers.You simply need to pass in one word at a time.Note that here, we have already removed stop words.Some of the conversions are actually pretty good,like started, reduced to start.Others, like people, losing the 'e' at the end area result of applying very simplistic rules.Lemmatization is another technique used to reduce words to a normalized form,but in this case,the transformation actually uses a dictionaryto map different variants of a word back to its root.With this approach, we are able to reduce non-trivial inflections such as is,was, were, back to the root 'be'.The default lemmatizer in NLTK usesthe Wordnet database to reduce words to the root form. Let's try it out.Just like in stemming,you initialize an instance of WordNetLemmatizerand pass in individual words to its lemmatize method.What happened here?It seems that only the word ones got reduced to one,all the others are unchanged.If you read the words carefully,you'll see that ones is the only plural noun here.In fact, that's exactly why it got transformed.A lemmatizer needs to know or make an assumptionabout the part of speech for each word it's trying to transform.In this case, WordNetLemmatizer defaults to nouns,but we can override that by specifying the PoS parameter.Let's pass in 'v' for verbs.This time, the two verb forms 'boring' and 'started' got converted.Great. Note that there are other verbs,but they are already in the root form.Also, note how we passed in the output from the previous noun lemmatization step.This way of chaining procedures is very common. Let's recap.As we saw in the previous examples,stemming sometimes results in stems that are not complete words in English.Lemmatization is similar to stemming with one difference,the final form is also a meaningful word.That said, stemming does not need a dictionary like lemmatization does.So depending on the constraints you have,stemming maybe a less memory intensive option for you to consider.

### 11. Summary-zKYEvRd2XmI.en

We have covered a number of text processing steps.Let's summarize what a typical workflow looks like.Starting with a plain text sentence,you first normalize it by converting to lowercase and removing punctuation,and then you split it up into words using a tokenizer.Next, you can remove stop words to reduce the vocabulary you have to deal with.Depending on your application,you may then choose to apply a combination of stemming andlemmatization to reduce words to the root or stem form.It is common to apply both,lemmatization first, and then stemming.This procedure converts a natural language sentence intoa sequence of normalized tokens which you can use for further analysis.

## Part 02-Module 01-Lesson 04_Feature Extraction

### 01. Feature Extraction-Bd6TJB8eVLQ.en

Once we have our text ready in a clean and normalized form,we need to transform it into features that can be used for modeling.For instance, treating each document like a bag ofwords allows us to compute some simple statistics that characterize it.These statistics can be improved by assigningappropriate weights towards using a TF-IDF Scheme.This enables a more accurate comparison between documents.For certain applications, we may need to findnumerical representations of individual words,and for that, we can use word embeddings,which are a very efficient and powerful method.In this lesson, you will learnall these techniques for extracting relevant features from text data.

### 02. Bag Of Words-A7M1z8yLl0w.en

The first feature representation we'll look at is called Bag of Words.The Bag of Words model treats each document as an un-ordered collection or bag of words.Here, a document is the unit of text that you want to analyze.For instance, if you want to compare essayssubmitted by students to check for plagiarism,each essay would be a document.If you want to analyze the sentiment conveyed by tweets,then each tweet would be a document.To obtain a bag of words from a piece of raw text,you need to simply apply appropriate text processing steps: cleaning,normalizing, splitting into words,stemming, lemmatization, et cetera.And then treat the resulting tokens as an un-ordered collection or set.So, each document in your data set will produce a set of words.But keeping these as separate sets is very inefficient.They're of different sizes,may contain different words,and are hard to compare.Also, whatever word occurs multiple times in a document?Is there a better representation you can think of?A more useful approach is to turn each document into a vector of numbers,representing how many times each word occurs in a document.A set of documents is known as a corpus,and this gives the context for the vectors to be calculated.First, collect all the unique words present in your corpus to form your vocabulary.Arrange these words in some order,and let them form the vector element positions or columns of a table,and assume each document is a row.Then count the number of occurrences of each word ineach document and enter the value in the respective column.At this stage, it is easier to think of this as a Document-Term Matrix,illustrating the relationship between documents in rows,and words or terms in columns.Each element can be interpreted as a term frequency.How frequently does that term occur in this document?Now, consider what you can do with this representation.One possibility is to compare two documents based onhow many words they have in common or how similar their term frequencies are.A more mathematical way of expressing that is tocompute the dot product between the two row vectors,which is the sum of the products of corresponding elements.Greater the dot product,more similar the two vectors are.The dot product has one flaw,it only captures the portions of overlap.It is not affected by other values that are not uncommon.So, pairs that are very different can end upwith the same product as ones that are identical.A better measure is cosine similarity,where we divide the dot product of two vectors bythe product of their magnitudes or Euclidean norms.If you think of these vectors as arrows in some n-dimensional space,then this is equal to the cosine of the angle theta between them.Identical vectors have cosine equals one.Orthogonal vectors have cosine equal zero.And for vectors that are exactly opposite, it is minus one.So, the values always range nicely between one for most similar,to minus one, most dissimilar.

### 03. TF-IDF-XZBiBIRcACE.en

One limitation of the bag-of-words approach isthat it treats every word as being equally important,whereas intuitively, we know that some words occur frequently within a corpus.For example, when looking at financial documents,cost or price may be a pretty common term.We can compensate for this by counting the number of documents in which each word occurs,this can be called document frequency,and then dividing the term frequencies by the document frequency of that term.This gives us a metric that is proportional tothe frequency of occurrence of a term in a document,but inversely proportional to the number of documents it appears in.It highlights the words that are more unique to a document,and thus better for characterizing it.You may have heard of, or used,the TF-IDF transform before.It's simply the product of two words,very similar to what we've seen so far,a term frequency and an inverse document frequency.The most commonly used form of TF-IDF defines term frequency as the raw count of a term,t, in a document, d,divided by the total number of terms in d,and inverse document frequency asthe logarithm of the total number of documents in the collection,d, divided by the number of documents where t is present.Several variations exist that try to normalize,or smooth the resulting values,or prevent edge cases such as divide-by-zero errors.Overall, TF-IDF is an innovative approach to assigningweights to words that signify their relevance in documents.

### 04. One-Hot Encoding-a0j1CDXFYZI.en

So far, we've looked at representations that tried tocharacterize an entire document or collection of words as one unit.As a result, the kinds of inferences we can make are also typically at a document level,mixture of topics in the document,documents similarity, documents sentiment, et cetera.For a deeper analysis of text,we need to come up with a numerical representation for each word.If you've dealt with categorical variables fordata analysis or tried to perform multi-class classification,you may have come across this term, One-Hot Encoding.That is one way of representing words,treat each word like a class,assign it a vector that has one ina single pre-determined position for that word and zero everywhere else.Looks familiar?Yeah, it's just like the bag of words idea,only that we keep a single word in each bag and build a vector for it.

### 05. Word Embeddings-4mM_S9L2_JQ.en

One-hot encoding usually works in some situationsbut breaks down when we have a large vocabulary to deal with,because the size of our ward representation grows with the number of words.What we need as a way to control the size ofour word representation by limiting it to a fixed-size vector.In other words, we want to find an embedding for each word insome vector space and we wanted to exhibit some desired properties.For example, if two words are similar in meaning,they should be closer to each other compared to words that are not.And if two pairs of words have a similar difference in their meanings,they should be approximately equally separated in the embedded space.We could use such a representation for a variety ofpurposes like finding synonyms and analogies,identifying concepts around which words are clustered,classifying words as positive,negative, neutral, et cetera.By combining word vectors,we can come up with another way of representing documents as well.

### 06. Word2Vec-7jjappzGRe0.en

Word2Vec is perhaps one of the most popular examples of word embeddings used in practice.As the name Word2Vec indicates,it transforms words to vectors.But what the name doesn't give away is how that transformation is performed.The core idea behind Word2Vec is this,a model that is able to predict a given word,given neighboring words, or vice versa,predict neighboring words for a given word islikely to capture the contextual meanings of words very well.And these are, in fact,two flavors of Word2Vec models,one where you are given neighboring words called continuous bag of words,and the other where you are given the middle word called Skip-gram.In the Skip-gram model,you pick any word from a sentence,convert it into a one-hot encoded vector and feed it into a neural network orsome other probabilistic model that is designed topredict a few surrounding words, its context.Using a suitable loss function,optimize the weights or parameters of the model and repeat thistill it learns to predict context words as best as it can.Now, take an intermediate representation like a hidden layer in a neural network.The outputs of that layer for a given word become the corresponding word vector.The Continuous Bag of Words variation also uses a similar strategy.This yields a very robust representation of wordsbecause the meaning of each word is distributed throughout the vector.The size of the word vector is up to you,how you want to tune performance versus complexity.It remains constant no matter how many words you train on,unlike Bag of Words, for instance,where the size grows with the number of unique words.And once you pre-train a large set of word vectors,you can use them efficiently without having to transform again and again,just store them in a lookup table.Finally, it is ready to be used in deep learning architectures.For example, it can be used as the input vector for recurrent neural nets.It is also possible to use RNNs to learn even better word embeddings.Some other optimizations are possible that further reduce the model andtraining complexity such as representing the output words using Hierarchical Softmax,computing loss using Sparse Cross Entropy, et cetera.

### 07. GloVe-KK3PMIiIn8o.en

Word2vec is just one type of forward embedding.Recently, several other related approaches have been proposed that are really promising.GloVe or global vectors for word representation is one such approach that tries todirectly optimize the vector representation ofeach word just using co- occurrence statistics,unlike word2vec which sets up an ancillary prediction task.First, the probably that word j appears in the context of word i is computed,pj given i for all word pairs ij in a given corpus.What do we mean by j appears in context of i?Simply that word j is present in the vicinity of word i,either right next to it,or a few words away.We count all such occurrences of i and j in our text collection,and then normalize account to get a probability.Then, a random vector is initialized for each word, actually two vectors.One for the word when it is acting as a context,and one when it is acting as the target. So far, so good.Now, for any pair of words, ij,we want the dot product of their word vectors,w_i times w_j, to be equal to their co-occurrence probability.Using this as our goal and a suitable last function,we can iteratively optimize these word vectors.The result should be a set of vectors that capturethe similarities and differences between individual words.If you look at it from another point of view,we are essentially factorizingthe co-occurrence probability matrix into two smaller matrices.This is the basic idea behind GloVe.All that sounds good,but why co-occurrence probabilities?Consider two context words,say ice and steam,and two target words, solid and water.You would come across solid more often in the context of ice than steam, right?But water could occur in either context with roughly equal probability.At least, that's what we would expect.Surprise. That's exactly what co-occurrence probabilities reflect.Given a large corpus,you'll find that the ratio of P solid givenice to P solid given steam is much greater than one,while the ratio of P water given ice and P water given steam is close to one.Thus, we see that co-occurrence probabilitiesalready exhibit some of the properties we want to capture.In fact, one refinement over usingraw probability values is to optimize for the ratio of probabilities.Now, there are a lot of subtleties here,not the least of which is the fact that the co-occurence probability matrix is huge.At the same time,co-occurrence probability values are typically very low,so it makes sense to work with the log of these values.I encourage you to read the original paper that introducedGloVe to get a better understanding of this technique.

### 08. Embeddings For Deep Learning-gj8u1KG0H2w.en

Where the embeddings are fast becoming the de facto choice for representing words,especially for use and deep neural networks.But why do these techniques work so well?Doesn't it seem almost magical that you can actually do arithmetic with words,like woman minus man plus king equals queen?The answer might lie in the distributional hypothesis,which states that words that occur in the same contexts tend to have similar meanings.For example, consider this sentence.Would you like to have a cup of blank?Okay. How about, I like my blank black.One more, I need my morning blank before I can do anything.What are you thinking?Tea? Coffee? What give you the hint?Cup? Black? Morning? But it could be either of the two, right?And that's the point.In these contexts, tea and coffee are actually similar.Therefore, when a large collection of sentences is used to learn in embedding,words with common context words tend to get pulled closer and closer together.Of course, there could also be contexts in which tea and coffee are dissimilar.For example, blank grounds are great for composting.Or, I prefer loose leaf blank.Here we are clearly talking about coffee grounds,and loose leaf tea.How do we capture these similarities and differences in the same embedding?By adding another dimension.Let's see how.Words can be close along one dimension.Here, tea and coffee are both beverages,but separated along some other dimension.Maybe this dimension captures all the variability among beverages.In a human language,there are many more dimensions along which word meanings can vary.And the more dimensions you can capture in your word vector,the more expressive that representation will be.But how many dimensions do you really need?Consider a typical neural network architecturedesigned for an NLP task, say word prediction.It's common to use a word embedding layer thatproduces a vector with a few hundred dimensions,but that's significantly small compared to using one heart encodings directly,which are as large as the size of the vocabulary,sometimes in tens of thousands of words.Also, if you learn the embedding as part of the model training process,you can obtain a representation that capturesthe dimensions that are most relevant for your task.This often adds complexity.So unless you're building a model fora very narrow application like one that deals with medical terminology,you can use a pre-trained embedding as a look-up.For example, work to veck or glove.Then you only need to train the layer specific to your task.Compare this with the network architecture for a computer vision task, say,image classification, the raw input here is also very high dimensional.For example, even 128 by 128 Image contains over 16 thousand pixels.We typically use convolutional layers to exploitthe spatial relationships and image data and reduce this dimensionality.Early stages and visual processing are often transferable across tasks,so it is common to use some pre-trained layers from an existing network,like Alex nad or BTG 16 and only learn the later layers.Come to think of it, using an embedding look up for NLPis not on like using pre-treated layers for computer vision.Both are great examples of transfer learning.

### 09. T-SNE-xxcK8oZ6_WE.en

Word embeddings need to have high dimensionality inorder to capture sufficient variations in natural language,which makes them super hard to visualize.T-SNE, which stands for t-Distributed Stochastic Neighbor Embedding,is a dimensionality reduction technique that can maphigh dimensional vectors to a lower dimensional space.It's kind of like PCA,Principle Component Analysis, but with one amazing property.When performing the transformation,it tries to maintain relative distances between objects,so that similar ones stay closer together while dissimilar objects stay further apart.This makes t-SNE a great choice for visualizing word embeddings.It effectively preserves the linear substructuresand relationships that have been learned by the embedding model.If we look at the larger vector space,we can discover meaningful groups of related words.Sometimes, that takes a while to realize why certain clusters are formed,but most of the groupings are very intuitive.T-SNE also works on other kinds of data, such as images.Here, we see pictures from the Caltech 101 datasetorganized into clusters that roughly correspond to class labels,including airplanes with blue sky being the common theme,sailboats of different shapes and sizes, and human faces.This is a very useful tool for better understanding the representation thata network learns and for identifying any bugs or other issues.

### 10. NLP Summary-B9ul8fsQYOA.en

Congratulations on completing the lesson.But remember, this is only the beginning ofa long and exciting journey into a world with limitless possibilities.It was a pleasure helping you take your first few stepsand I'm looking forward to seeing what systems you will build,what new problems you will solve,and how you'll advance the field of natural language processing.Good luck on your adventure.

## Part 02-Module 01-Lesson 05_Financial Statements

## Part 02-Module 01-Lesson 06_Project 5 NLP on Financial Statements

## Part 02-Module 02-Lesson 01_Introduction to Neural Networks

### 02. Introduction-tn-CrUTkCUc.en

So let's start with two questions,what is deep learning, and what is it used for?The answer to the second question is pretty much everywhere.Recent applications include things such as beatinghumans in games such as Go, or even jeopardy,detecting spam in emails, forecasting stock prices,recognizing images in a picture,and even diagnosing illnesses sometimes with more precision than doctors.And of course, one ofthe most celebrated applications of deep learning is in self-driving cars.And what is at the heart of deep learning?This wonderful object called neural networks.Neural networks vaguely mimic the process of how the brain operates,with neurons that fire bits of information.It sounds pretty scary, right?As a matter of fact, the first time I heard of a neural network,this is the image that came into my head,some scary robot with artificial brain.But then, I got to learn a bit more about neural networks andI realized that there are actually a lot scarier than that.This is how a neural network looks.As a matter of fact, this one here is a deep neural network.Has lots of nodes, lots of edges,lots of layers, information coming through the nodes and leaving, it's quite complicated.But after looking at neural networks for a while,I realized that they're actually a lot simpler than that.When I think of a neural network,this is actually the image that comes to my mind.There is a child playing in the sand,with some red and blue shells and we are the child.Can you draw a line that separates the red and the blue shells?And the child draws this line.That's it. That's what a neural network does.Given some data in the form of blue or red points,the neural network will look for the best line that separates them.And if the data is a bit more complicated like this one over here,then we'll need a more complicated algorithm.Here, a deep neural network will do the job andfind a more complex boundary that separates the points.So with that image in mind,let's dive in and learn about neural networks.

### 03. Exemplo de classificao-Dh625piH7Z0.en

So, let's start with one classification example.Let's say we are the admissions office ata university and our job is to accept or reject students.So, in order to evaluate students,we have two pieces of information,the results of a test and their grades in school.So, let's take a look at some sample students.We'll start with Student 1 who got 9 out of 10 in the test and 8 out of 10 in the grades.That student did quite well and got accepted.Then we have Student 2 who got 3 out of 10 in the test and 4 out of 10 in the grades,and that student got rejected.And now, we have a new Student 3 who got 7 out of10 in the test and 6 out of 10 in the grades,and we're wondering if the student gets accepted or not.So, our first way to find this out is to plot students in a graph withthe horizontal axis corresponding to the score onthe test and the vertical axis corresponding to the grades,and the students would fit here.The students who got three and four gets located in the point with coordinates (3,4),and the student who got nine and eight gets located in the point with coordinates (9,8).And now we'll do what we do in most of our algorithms,which is to look at the previous data.This is how the previous data looks.These are all the previous students who got accepted or rejected.The blue points correspond to students that got accepted,and the red points to students that got rejected.So we can see in this diagram that the students would didwell in the test and grades are more likely to get accepted,and the students who did poorly in both are more likely to get rejected.So let's start with a quiz.The quiz says, does the Student 3 get accepted or rejected?What do you think? Enter your answer below.

### 04.  2 -46PywnGa_cQ.en

Correct. Well, it seems that this data can benicely separated by a line which is this line over here,and it seems that most students over the line getaccepted and most students under the line get rejected.So this line is going to be our model.The model makes a couple of mistakes since there area few blue points that are under the line and a few red points over the line.But we're not going to care about those.I will say that it's safe to predict that if a point is over the linethe student gets accepted and if it's under the line then the student gets rejected.So based on this model we'll look at the new student that we seethat they are over here at the point 7:6 which is above the line.So we can assume with some confidence that the student gets accepted.So if you answered yes, that's the correct answer.And now a question arises.The question is, how do we find this line?So we can kind of eyeball it.But the computer can't.We'll dedicate the rest of the session to show you algorithms that will find this line,not only for this example,but for much more general and complicated cases.

### 05. Linear Boundaries-X-uMlsBi07k.en

So, first let's add some math.We're going to label the horizontal axis corresponding to the test by the variable x1,and the vertical axis corresponding to the grades by the variable x2.So this boundary line that separates the blueand the red points is going to have a linear equation.The one drawn has equation 2x1+x2-18=0.What does this mean?This means that our method for accepting or rejecting studentssimply says the following: take this equation as our score,the score is 2xtest+grades-18.Now when the student comes in, we check their score.If their score is a positive number,then we accept the student and if the score isa negative number then we reject the student.This is called a prediction.We can say by convention that if the score is 0,we'll accept a student although this won't matter much at the end.And that's it. That linear equation is our model.In the more general case, our boundary will be an equation of the following wx1+w2x2+b=0.We'll abbreviate this equation in vector notation as wx+b=0,where w is the vector w1w2 and x is the vector x1x2.And we simply take the product of the two vectors.We'll refer to x as the input,to w as the weights and b as the bias.Now, for a student coordinates x1x2,we'll denote a label as Y and the label is what we're trying to predict.So if the student gets accepted,namely the point is blue,then the label is Y+1.And if the student gets rejected,namely the point is red and then the label is Y=0.Thus, each point is in the formx1x2Y or Y is 1 for the blue points and 0 for the red points.And finally, our prediction is going to be called Y-hat and it willbe what the algorithm predicts that the label will be.In this case, Y-hat is one of the algorithm predicts that the student gets accepted,which means the point lies over the line.And, Y-hat is 0 if the algorithm predicts that this didn't get rejected,which means the point is under the line.In math terms, this means that the prediction Y-hat is 1 if wx+bis greater than or equal to zero and 0 if wx+b is less than 0.So, to summarize, the points above the line haveY hat=1 and the points below the line have Y-hat=0.And, the blue points have Y=1 and the red points have Y=0.And, the goal of the algorithm is to have Y-hat resembling Y as closely as possible,which is exactly equivalent to finding the boundary line that keepsmost of the blue points above it and most of the red points below it.

### 06. 09 Higher Dimensions-eBHunImDmWw.en

Now, you may be wondering what happens if we havemore data columns so not just testing grades,but maybe something else like the ranking of the student in the class.How do we fit three columns of data?Well the only difference is that now,we won't be working in two dimensions,we'll be working in three.So now, we have three axis: x_1 for the test,x_2 for the grades and x_3 for the class ranking.And our data will look like this,like a bunch of blue and red points flying around in 3D.On our equation won't be a line in two dimension,but a plane in three dimensions with a similar equation as before.Now, the equation would be w_1_x_1 plus w_2_x_2 plus w_3_x_3 plus b equals zero,which will separate this space into two regions.This equation can still be abbreviated by Wx plus b equals zero,except our vectors will now have three entries instead of two.And our prediction will still be y head equals one ifWx plus b is greater than or equal to zero,and zero if Wx plus b is less than zero.And what if we have many columns like say n of them?Well, it's the same thing. Now, our data just leaps in n-dimensional space.Now, I have trouble picturing things in more than three dimensions.But if we can imagine that the points are just things with n coordinates called x_1, x_2,x_3 all the way up to x_n with our labels being y,then our boundaries just an n minus one dimensional hyperplane,which is a high dimensional equivalent of a line in 2D or a plane in 3D.And the equation of this n minusone dimensional hyperplane is going to be w_1_x_1 plus w_2_x_2plus all the way to w_n_x_n plus b equals zero,which we can still abbreviate to Wx plus b equals zero,where our vectors now have n entries.And our prediction is still the same as before.It is y head equals one if Wx plus b is greater than or equal tozero and y head equals zero if Wx plus b is less than zero.

### 07. DL 06 Perceptron Definition Fix V2-hImSxZyRiOw.en

So let's recap.We have our data which is all these students.The blue ones have been accepted and the red ones have been rejected.And we have our model which consists of the equation two times test plus grades minus 18,which gives rise to this boundary whichthe point where the score is zero and a prediction.The prediction says that the student gets accepted of the score is positive or zero,and rejected if the score is negative.So now we'll introduce the notion of a preceptron,which is the building block of neural networks,and it's just an encoding of our equation into a small graph.The way we've build it is the following.Here we have our data and our boundary line and we fit it inside a node.And now we add small nodes for the inputs which,in this case, they are the test and the grades.Here we can see an example where test equals seven and grades equals six.And what the perceptron does is it blocks the points seven,six and checks if the point is in the positive or negative area.If the point is in the positive area,then it returns a yes.And if it is in the negative area, it returns and no.So let's recall that our equation is score equals twotimes test plus one times grade minus 18,and that our prediction consists of acceptingthe student if the score is positive or zero,and rejecting them if the score is negative.These weights two, one, and minus 18,are what define the linear equation,and so we'll use them as labels in the graph.The two and the one will label the edges coming from X1 and X 2 respectively,and the bias unit minus 18 will label the node.Thus, when we see a node with these labels,we can think of the linear equation they generate.Another way to grab this node is to consider the bias as part of the input.Now since W1 gets multiplied by X1 and W2 by X2,It's natural to think that B gets multiplied by a one.So we'll have the B labeling and and edge coming from a one.Then what the node does is it multiplies the values coming fromthe incoming nodes by the values and the corresponding edges.Then it adds them and finally,it checks if the result is greater that are equal to zero.If it is, then the node returns a yes or a value of one,and if it isn't then the node returns a no or a value of zero.We'll be using both notations throughoutthis class although the second one will be used more often.In the general case,this is how the nodes look.We will have our node over here then end inputs comingin with values X1 up to Xn and one,and edges with weights W1 up to Wn,and B corresponding to the bias unit.And then the node calculates the linear equation Wx plus B,which is a summation from I equals one to n,of WIXI plus B.This node then checks if the value is zero or bigger, and if it is,then the node returns a value of one for yes and if not,then it returns a value of zero for no.Note that we're using an implicit function,here, which is called a step function.What the step function does is it returns a one if the input is positive or zero,and a zero if the input is negative.So in reality, these perceptrons can be seen as a combination of nodes,where the first node calculates a linear equation and the inputs on the weights,and the second node applies the step function to the result.These can be graphed as follows:the summation sign represents a linear function in the first node,and the drawing represents a step function in the second node.In the future, we will use different step functions.So this is why it's useful to specify it in the node.So as we've seen there are two ways to represent perceptions.The one on the left has a bias unit coming from an input node with a value of one,and the one in the right has the bias inside the node.

### 08. -zAkzOZntK6Y.en

So you may be wondering why are these objects called neural networks.Well, the reason why they're called neural networks isbecause perceptions kind of look like neurons in the brain.In the left we have a perception with four inputs.The number is one, zero,four, and minus two.And what the perception does,it calculates some equations on the input and decides to return a one or a zero.In a similar way neurons in the brain take inputs coming from the dendrites.These inputs are nervous impulses.So what the neuron does is it does something with the nervous impulsesand then it decides if it outputs a nervous impulse or not through the axon.The way we'll create neural networks later in this lessonis by concatenating these perceptions so we'll be mimickingthe way the brain connects neurons by taking the output fromone and turning it into the input for another one.

### 10. 07 Perceptron Algorithm Trick-lif_qPmXvWA.en

Now, let me show you a trick that will make a line go closer to a point.Let's say we have our linear equation for example,3x1 + 4x2 -10.And that linear equation gives us a line which isthe points where the equation is zero and two regions.The positive region drawn in blue where 3x1 + 4x2 - 10 is positive,and the negative region drawn in red with 3x1 + 4x2 - 10 is negative.So here we have our lonely misclassified point, the 0.4,5 which is a red point in the blue area,and the point has to come closer.So how do we get that point to come closer to the line?Well, the idea is we're going to take the four and five and use them to modifythe equation of the line in order to get the line to move closer to the point.So here are parameters of the line 3,4 and -10 and the coordinates of the point are 4 and 5,and let's also add a one here for the bias unit.So what we'll do is subtract these numbers from the parameters of the line to get 3 - 4,4 - 5, and -10 -1.The new line will have parameters -1, -1, -11.And this line will move drastically towards the point,possibly even going over it and placing it in the correct area.Now, since we have a lot of other points,we don't want to make any drastic moves since we mayaccidentally misclassify all our other points.We want the line to make a small move towards that point and for this,we need to take small steps towards the point.So here's where we introduce the learning rate,the learning rate is a small number for example,0.1 and what we'll do is instead of subtracting four,five and one from the coordinates of the line,we'll multiply these numbers by 0.1 and then subtract them from the equation of the line.This means we'll be subtracting 0.4,0.5, and 0.1 from the equation of the line.Obtaining a new equation of 2.6x1 + 3.5x 2 - 10.1 = 0.This new line will actually move closer to the point.In the same way, if we have a blue point in the red area, for example,the point 1,1 is a positively labeled point in the negative area.This point is also misclassified and it says, come closer.So what do we do here is the same thing,except now instead of subtractingthe coordinates to the parameters of the line, we add them.Again, we multiply by the learning rate in order to make small steps.So here we take the coordinates of the point 1,1 and putan extra one for the constant term and now,we multiply them by the learning rates 0.1.Now, we add them to the parameters of the line and we get a new line withequation 3.1x1 + 4.1x2 - 9.9.And magic, this line is closer to the point.So that's the trick we're going to use repeatedly for the Perceptron Algorithm.

### 10. DL 10 S  Perceptron Algorithm-fATmrG2hQzI.en

Well, consider this. If you're in the wrong area,you would like the line to go over you,in order to be in the right area.Thus, the points just come closer!So the line can move towards it and eventually classify it correctly.

### 10. Perceptron Algorithm--zhTROHtscQ.en

So we had a question we're trying to answer and the question is,how do we find this line that separatesthe blue points from the red points in the best possible way?Let's answer this question by first looking ata small example with three blue points and three red points.And we're going to describe an algorithm that will findthe line that splits these points properly.So the computer doesn't know where to start.It might as well start at a random place by picking a random linear equation.This equation will define a line anda positive and negative area given in blue and red respectively.What we're going to do is to look at how badly this lineis doing and then move it around to try to get better and better.Now the question is,how do we find how badly this line is doing?So let's ask all the points.Here we have four points that are correctly classified.They are these two blue points in the blue area and these two red points in the red area.And these points are correctly classified,so they say, "I'm good."And then we have these two points that are incorrectly classified.That's this red point in the blue area and this blue point in the red area.We want to get as much information from them so we want themto tell us something so that we can improve this line.So what is it that they can tell us?So here we have a misclassified point,this red point in the blue area.Now think about this.If you were this point,what would you tell the line to do?Would you like it to come closer to you or farther from you?That's our quiz.Will the misclassified point want the line to come closer to it or farther from it?

### 11. Perceptron Agorithm Pseudocode-p8Q3yu9YqYk.en

Now, we finally have all the tools for describing the perceptron algorithm.We start with the random equation,which will determine some line,and two regions, the positive and the negative region.Now, we'll move this line around to get a better and better fit.So, we ask all the points how they're doing.The four correctly classified points say, "I'm good."And the two incorrectly classified points say, "Come closer."So, let's listen to the point in the right,and apply the trick to make the line closer to this point.So, here it is. Now, this point is good.Now, let's listen to the point in the left.The points says, "Come closer."We apply the trick,and now the line goes closer to it,and it actually goes over it classifying correctly.Now, every point is correctly classified and happy.So, let's actually write the pseudocode for this perceptron algorithm.We start with random weights,w1 up to wn and b.This gives us the question wx plus b,the line, and the positive and negative areas.Now, for every misclassified point with coordinates x1 up to xn,we do the following.If the prediction was zero,which means the point is a positive point in the negative area,then we'll update the weights as follows: for i equals 1 to n,we change wi, to wi plus alpha times xi,where alpha is the learning rate.In this case, we're using 0.1.Sometimes, we use 0.01 etc.It depends. Then we also change the bi as unit to b plus alpha.That moves the line closer to the misclassified point.Now, if the prediction was one,which means a point is a negative point in the positive area,then we'll update the weights in a similar way,except we subtract instead of adding.This means for i equals 1, change wi,to wi minus alpha xi,and change the bi as unit b to b minus alpha.And now, the line moves closer to our misclassified point.And now, we just repeat this step until we get no errors,or until we have a number of error that is small.Or simply we can just say,do the step a thousand times and stop.We'll see what are our options later in the class.

### 12. Non-Linear Regions-B8UrWnHh1Wc.en

Okay, so let's look more carefully at this model for accepting and rejecting students.Let's say we have this student four,who got nine in the test,but only one on the grades.According to our model this student gets accepted since it'splaced over here in the positive region of this line.But let's say we don't want that since we'll say,"If your grades were terrible,no matter what you got on the test, you won't get accepted".So our data should look more like this instead.This model is much more realistic but now we have a problemwhich is the data can no longer be separated by just a line.So what is the next thing after a line?Maybe a circle. A circle would work.Maybe two lines. That could work, too.Or maybe a curve like this.That would also work. So let's go with that.Let's go with the curve.Now, unfortunately, the perceptron algorithm won't work for us this time.We'll have to come up with something more complex and actually the solution will be,we need to redefine our perceptron algorithm fora line in a way that it'll generalize to other types of curves.

### 13. Error Functions-YfUUunxWIJw.en

So the way we'll solve our problems from now on is with the help of an error function.An error function is simply something that tells us how far we are from the solution.For example, if I'm here and my goal is to get to this plant,an error function will just tell me the distance from the plant.My approach would then be to look around myself,check in which direction I can take a step to get closer to the plant,take that step and then repeat.Here the error is simply the distance from the plant.

### 14. Error Functions-jfKShxGAbok.en

Here is obvious realization of the error function.We're standing on top a mountain,Mount Errorest and I want to descendbut it's not that easy because it's cloudy and the mountain is very big,so we can't really see the big picture.What we'll do to go down is we'll look around us and weconsider all the possible directions in which we can walk.Then we pick a direction that makes us descend the most.Let's say it's this one over here.So we take a step in that direction.Thus, we've decreased the height.Once we take the step and we start the process again and again always decreasingthe height until we go all the way down the mountain, minimizing the height.In this case the key metric that we use to solve the problem is the height.We'll call the height the error.The error is what's telling us how badly we're doing atthe moment and how far we are from an ideal solution.And if we constantly take steps to decrease the error thenwe'll eventually solve our problem, descending from Mt.Errorest.Some of you may be thinking,wait, that doesn't necessarily solve the problem.What if I get stuck in a valley,a local minimum, but that's not the bottom of the mountain.This happens a lot in machine learning and we'llsee different ways to solve it later in this Nanodegree.It's also worth noting that many timesa local minimum will give us a pretty good solution to a problem.This method, which we'll study in more detail later,is called gradient descent.So let's try that approach to solve a problem.What would be a good error function here?What would be a good way to tell the computer how badly it's doing?Well, here's our line with our positive and negative area.And the question is how do we tell the computer how far it is from a perfect solution?Well, maybe we can count the number mistakes.There are two mistakes here.So that's our height. That's our error.So just as we did to descend from the mountain,we look around all the directions in which we can movethe line in order to decrease our error.So let's say we move in this direction.We'll decrease the number of errors to one and then if we're moving in that direction,we'll decrease the number of errors to zero.And then we're done, right? Well, almost.There's a small problem with that approach.In our algorithms we'll be taking very small steps and the reason for that is calculus,because our tiny steps will be calculated by derivatives.So what happens if we take very small steps here?We start with two errors and then move a tiny amount and we're still at two errors.Then move a tiny amount again and we're still two errors.Another tiny amount and we're still at two and again and again.So not much we can do here.This is equivalent to using gradient descent to try todescend from an Aztec pyramid with flat steps.If we're standing here in the second floor,for the two errors and we look around ourselves,we'll always see two errors and we'll get confused and not know what to do.On the other hand in Mt.Errorest we can detect very small variations in height and we canfigure out in what direction it can decrease the most.In math terms this means that in order for us to do gradient descentour error function can not be discrete, it should be continuous.Mt. Errorest is continuous sincesmall variations in our position will translate to small variations inthe height but the Aztec pyramid does notsince the high jumps from two to one and then from one to zero.As a matter of fact, our error function needsto be differentiable, but we'll see that later.So, what we need to do here is to constructan error function that is continuous and we'll do this as follows.So here are six points with four of them correctly classified,that's two blue and two red,and two of them incorrectly classified,that is this red point at the very left and this blue point at the very right.The error function is going to assign a large penalty tothe two incorrectly classified points andsmall penalties to the four correctly classified points.Here we are representing the size of the point as the penalty.The penalty is roughly the distance from the boundary when the point ismisclassified and almost zero when the point is correctly classified.We'll learn the formula for the error later in the class.So, now we obtain the total error by adding all the errors from the corresponding points.Here we have a large number so it istwo misclassified points add a large amount to the error.And the idea now is to move the line around in order to decrease these error.But now we can do it because we can make very tiny changes to the parameters ofthe line which will amount to very tiny changes in the error function.So, if you move the line,say, in this direction,we can see that some errors decrease, some slightly increase,but in general when we consider the sum,the sum gets smaller and we can see that because we've nowcorrectly classified the two points that were misclassified before.So once we are able to build an error function with this property,we can now use gradient descent to solve our problem.So here's the full picture.Here we are at the summit of Mt.Errorest. We're quite high up because our error is large.As you can see the error is the height which is the sum of the blue and red areas.We explore around to see what direction brings us down the most, or equivalently,what direction can we move the line to reduce the error the most,and we take a step in that direction.So in the mountain we go down one step and in the graph we've reduced the error abit by correctly classifying one of the points. And now we do it again.We calculate the error,we look around ourselves to see in what direction we descend the most,we take a step in that direction and that brings us down the mountain.So on the left we have reduced the height and successfullydescended from the mountain and on the right we havereduced the error to its minimum possible value and successfully classified our points.Now the question is, how do we define this error function?That's what we'll do next.

### 15. Discrete vs Continuous-rdP-RPDFkl0.en

In the last section we pointed out the difference between a discrete anda continuous error function and discovered that in order forus to use gradient descent we need a continuous error function.In order to do this we also need to movefrom discrete predictions to continuous predictions.Let me show you what I mean by that.

### 15. Discrete vs. Continuous-Rm2KxFaPiJg.en

The prediction is basically the answer we get from the algorithm.A discreet answer will be of the form yes, no.Whereas a continued answer will be a number,normally between zero and one which we'll consider a probability.In the running example,here we have our students where blue is accepted and red is rejected.And the discrete algorithm will tell us if a student is accepted or rejectedby typing a zero for rejected students and a one for accepted students.On the other hand,the farther our point is from the black line,the more drastic these probabilities are.Points that are well into the blue area get very high probabilities,such as this point with an 85% probability of being blue.And points that are well into the red region are given very low probabilities,such as this point on the bottom that is given a 20% probability of being blue.The points over the line are all given a 50% probability of being blue.As you can see the probability is a function of the distance from the line.The way we move from discrete predictions to continuous,is to simply change your activation function from the step function in the left,to the sigmoid function on the right.The sigmoid function is simply a function which forlarge positive numbers will give us values very close to one.For large negative numbers will give us values very close to zero.And for numbers that are close to zero,it'll give you values that are close to point five.The formula is sigmoid effects equals (x) = 1/(1 + exp(-x))So, before our model consisted of a line with a positive region and a negative region.Now it consists of an entire probability space or for each point in the planewe are given the probability that the label of the point is one for the blue points,and zero for the red points.For example, for this point the probability ofbeing blue is 50% and of being red is 50%.For this point, the probabilities are 40% for being blue,and 60% for being red.For this one over here it's 30% for blue,and 70% for red.And for this point all over here is 80% for beingblue and 25 percent for being red.The way we obtain this probability space is very simple.We just combine the linear function WX + b with the sigmoid function.So in the left we have the lines that represent the points for which WX + b iszero, one, two, minus one, minus two, etc.And once we apply the sigmoid function to each of these values in the plane,we then obtain numbers from zero to one for each point.These numbers are just the probabilities of the point being blue.The probability of the point being blue is a prediction ofthe model Y hat to sigmoid of W x plus b.Here we can see the lines for which the prediction is point five,point six, point seven,point four, point three, et cetera.As you can see, as we get more into the blue area,(Wx + b) gets closer and closer to one.And as we move into the red area,(Wx + b) gets closer and closer to zero.When we're over the main line,W x plus b is zero,which means sigmoid of W s plus b is exactly zero point five.So here on the left we have our old perceptron withthe activation function as a step function.And on the right we have our new perceptron,where the activation function is the sigmoid function.What our new perceptron does,it takes the inputs,multiplies them by the weights in the edges and adds the results,then applies the sigmoid function.So instead of returning one and zero like before it returnsvalues between zero and one such as 0.99 or 0.67 etc.Before it used to say the student got accepted or not,and now it says the probability of the student got accepted is this much.

### 16. DL 18 Q Softmax V2-RC_A9Tu99y4.en

Let's switch to a different example for a moment.Let's say we have a model that will predict if you receive a gift or not.So, the model use predictions in the following way.It says, the probability that you get a gift is 0.8,which automatically implies that the probability that you don't receive a gift is 0.2.And what does the model do?What the model does is take some inputs.For example, is it your birthday or have it been good all year?And based on those inputs,it calculates a linear model which would be the score.Then, the probability that you get the gift or notis simply the sigmoid function applied to that score.Now, what if you had more options than just getting a gift or not a gift?Let's say we have a model that just tell us what animal we just saw,and the options are a duck,a beaver and a walrus.We want a model that tells an answer along the lines of,the probability of a duck is 0.67,the probability of a beaver is 0.24,and the probability of a walrus is 0.09.Notice that the probabilities need to add to one.Let's say we have a linear model based on some inputs.The inputs could be, does it have a beak or not?Number of teeth. Number of feathers.Hair, no hair. Does it live in the water? Does it fly?Etc. We calculate linear function based on those inputs,and let's say we get some scores.So, the duck gets a score of two,and the beaver gets a score of one,and the walrus gets a score of zero.And now the question is,how do we turn these scores into probabilities?The first thing we need to satisfy with probabilities is as we said,they need to add to one.So the two, the one,and the zero do not add to one.The second thing we need to satisfy is,since the duck had a higher score than the beaverand the beaver had a higher score than the walrus,then we want the probability of the duck to be higher than the probability of the beaver,and the probability of the beaver to be higher than the probability of the walrus.Here's a simple way of doing it.Let's take each score and divide it by the sum of all the scores.The two becomes two divided by two plus one plus zero,the one becomes one divided by two plus one plus zero,and the zero becomes zero divided by two plus one plus zero.This kind of works because the probabilities we obtain are two thirds for the duck,one third for the beaver,and zero for the walrus.That works but there's a little problem. Let's think about it.What could this problem be?The problem is the following.What happens if our scores are negative?This is completely plausible since the scores arelinear function which could give negative values.What if we had, say, scores of 1, 0 and (-1)?Then, one of the probabilities would turn into one divided byone plus zero plus minus one which is zero,and we know very well that we cannot divide by zero.This unfortunately won't work,but the idea is good.How can we turn this idea into one that works all the time even for negative numbers?Well, it's almost like we need to turn these scores into positive scores.How do we do this?Is there a function that can help us?This is the quiz. Let's look at some options.There's sine, cosine, logarithm, and exponential.Quiz. Which one of these functions will turn every number into a positive number?Enter your answer below.

### 16. DL 18 S Softmax-n8S-v_LCTms.en

So, if you said exponential, you are correct.Because this is a function that returns a positive number for every input.E to the X is always a positive number.So, what we're going to do is exactly what we did before,except, applying it to the X to the scores.So, instead of 2,1, 0,we have E to the 2,E to the 1 and E to the 0.So, that 2 becomes E to the 2 divided by E to the two plus E to the 1 plus E to the 0.And, similarly for 1 and 0.So, the probabilities we obtain now are as 0.67, 0.24 and 0.09.This clearly add to 1.And, also notice that since the exponential function is increasing,then the duck has a higher probability than the beaver.And this one has a higher probability than the walrus.This function is called the Softmax function and it's defined formally like this.Let's say we have N classes and a linear model that gives us the following scores.Z1, Z2, up to ZN.Each score for each of the classes.What we do to turn them into probabilities is to saythe probability that the object is in class I is going to beE to the power of the ZI divided bythe sum of E to the power of Z1 plus all the way to E to the power ZN.That's how we turn scores into probabilities.So, here's a question for you.When we had two classes,we applied the sigmoid function to the scores.Now, that we have more classes we apply the softmax function to the scores.The question is, is the softmax functionfor N equals to the same as the sigmoid function?I'll let you think about it. The answer is actually,yes, but it's not super trivial why.And, it's a nice thing to remember.

### 16. Quiz - Softmax-NNoezNnAMTY.en

So far we have models that give us an answer ofyes/no or the probability of a label being positive or negative.What if we have more classes?What if we want our model to tell us if something is red,blue, yellow or dog, cat, bird?In this video I'll show you what to do.

### 17. One-Hot Encoding-AePvjhyvsBo.en

So, as we've seen so far,all our algorithms are numerical.This means we need to input numbers,such as a score in a test or the grades,but the input data will not always look like numbers.Sometimes it looks like this.Let's say the module receives as an inputthe fact that you got a gift or didn't get a gift.How do we turn that into numbers? Well, that's easy.If you've got a gift, we'll just say that the input variable is 1.And, if you didn't get a gift,we'll just say that the input variable is 0.But, what if we have more classes as before or,let's say, our classes are Duck, Beaver and Walrus?What variable do we input in the algorithm?Maybe, we can input a 0 or 1 and a 2,but that would not work because it would assume dependencies betweenthe classes that we can't have. So, this is what we do.What we do is, we come up with one variable for each of the classes.So, our table becomes like this.That's one variable for Duck,one for Beaver and one for Walrus.And, each one has its corresponding column.Now, if the input is a duck then the variable for duck is1 and the variables for beaver and walrus are 0.Similarly for the beaver and the walrus.We may have more columns of data but at least there are no unnecessary dependencies.This process is called The One-Hot Encodingand it will be used a lot for processing data.

### 18. Maximum Likelihood 1-1yJx-QtlvNI.en

So we're still in our quest for an algorithm that will helpus pick the best model that separates our data.Well, since we're dealing with probabilities then let's use them in our favor.Let's say I'm a student and I have two models.One that tells me that my probability of getting accepted is80% and one that tells me the probability is 55%.Which model looks more accurate?Well, if I got accepted then I'd saythe better model is probably the one that says 80%.What if I didn't get accepted?Then the more accurate model is more likely the one that says 55 percent.But I'm just one person. What if it was me and a friend?Well, the best model would more likely be the one thatgives the higher probabilities to the events that happened to us,whether it's acceptance or rejection.This sounds pretty intuitive.The method is called maximum likelihood.What we do is we pick the model that gives the existing labels the highest probability.Thus, by maximizing the probability,we can pick the best possible model.

### 18. Maximum Likelihood 2-6nUUeQ9AeUA.en

So let me be more specific.Let's look at the following four points: two blue and two redand two models that classify them,the one on the left and the one on the right.Quick. Which model looks better? You are correct.The model on the right is much better since itclassifies the four points correctly whereasthe model in the left gets two points correctly and two points incorrectly.But let's see why the model in the right is better from the probability perspective.And by that, we'll show you that the arrangement in the right ismuch more likely to happen than the one in the left.So let's recall that our prediction is  = (Wx+b) and that thatis precisely the probability of a point being labeled positive which means blue.So for the points in the figure,let's say the model tells you that the probability of being blue are 0.9,0.6, 0.3, and 0.2.Notice that the points in the blue region are much more likely to beblue and the points in the red region are much less likely to be blue.Now obviously, the probability of being red is one minus the probability of being blue.So in this case, the probability of some of the points being red are 0.1,0.4, 0.7 and 0.8.Now what we want to do is we want to calculate the probability ofthe four points are of the colors that they actually are.This means the probability that the two red pointsare red and that the two blue points are blue.Now if we assume that the colors of the points are independent events thenthe probability for the whole arrangement isthe product of the probabilities of the four points.This is equal to 0.1  0.6  0.7  0.2 = 0.0084. This is very small.It's less than 1%.What we mean by this is that if the model is given by these probability spaces,then the probability that the points are of these colors is 0.0084.Now let's do this for both models.As we saw the model on the left tells us that the probabilities ofthese points being of those colors is 0.0084.If we do the same thing for the model on the right.Let's say we get that the probabilities of the two points inthe right being blue are 0.7 and0.9 and of the two points in the left being red are 0.8 and 0.6.When we multiply these we get 0.3024 which is around 30%.This is much higher than 0.0084.Thus, we confirm that the model on the right is better because it makesthe arrangement of the points much more likely to have those colors.So now, what we do is the following?We start from the bad modeling,calculate the probability that the points are those colors,multiply them and we obtain the total probability is 0.0084.Now if we just had a way to maximize this probability we can increaseit all the way to 0.3024.Thus, our new goal becomes precisely that,to maximize this probability.This method, as we stated before,is called maximum likelihood.

### 19. Quiz - Cross 1--xxrisIvD0E.en

Well we're getting somewhere now.We've concluded that the probability is important.And that the better model will give us a better probability.Now the question is,how we maximize the probability.Also, if remember correctly we're talking about an error function and howminimizing this error function will take us to the best possible solution.Could these two things be connected?Could we obtain an error function from the probability?Could it be that maximizing the probability is equivalentto minimizing the error function? Maybe.

### 19. Quiz Cross Entropy-njq6bYrPqSU.en

So a quick recap. We have two models,the bad one on the left and the good one on the right.And the way to tell they're bad or good is to calculatethe probability of each point being the color it is according to the model.Multiply these probabilities in order to obtain the probability ofthe whole arrangement and then check that the model onthe right gives us a much higher probability than the model on the left.Now all we need to do is to maximize this probability.But probability is a product of numbers and products are hard.Maybe this product of four numbers doesn't look so scary.But what if we have thousands of datapoints?That would correspond to a product of thousands of numbers,all of them between zero and one.This product would be very tiny,something like 0.0000 something and we definitely want to stay away from those numbers.Also, if I have a product of thousands of numbers and I change one of them,the product will change drastically.In summary, we really want to stay away from products.And what's better than products?Well, let's ask our friend here.Products are bad, but sums are good. Let's do sums.So let's try to turn these products into sums.We need to find a function that will help us turn products into sums.What would this function be?It sounds like it's time for a quiz.Quiz. Which function will help us out here?Sine, cosine, logarithm or the exponential function?Enter your answer below.

### 20. Cross Entropy 1-iREoPUrpXvE.en

Correct. The answer is logarithm,because logarithm has this very nice identity that says that the logarithm ofthe product A times B is the sum of the logarithms of A and B.So this is what we do.We take our products and we take the logarithms,so now we get a sum of the logarithms of the factors.So the ln(0.6*0.2*0.1*0.7) is equal toln(0.6) + ln(0.2) + ln(0.1) + ln(0.7) etc. Now from now until the end of class,we'll be taking the natural logarithm which is base e instead of 10.Nothing different happens with base 10.Everything works the same as everything gets scaled by the same factor.So it's just more for convention.We can calculate those values and get minus 0.51, minus 1.61,minus 0.23 etc. Notice that they are all negative numbers and that actually makes sense.This is because the logarithm of a number between 0 and 1 is alwaysa negative number since the logarithm of one is zero.So it actually makes sense to think of the negative ofthe logarithm of the probabilities and we'll get positive numbers.So that's what we'll do. We'll take the negative of the logarithm of the probabilities.That sums up negatives of logarithms of the probabilities,we'll call the cross entropy which is a very important concept in the class.If we calculate the cross entropies,we see that the bad model on left has a cross entropy 4.8 which is high.Whereas the good model on the right has a cross entropy of 1.2 which is low.This actually happens all the time.A good model will give usa low cross entropy and a bad model will give us a high cross entropy.The reason for this is simply thata good model gives us a high probability and the negativeof the logarithm of a large number is a small number and vice versa.This method is actually much more powerful than we think.If we calculate the probabilities and pair the points with the corresponding logarithms,we actually get an error for each point.So again, here we have probabilities for both models and the products of them.Now, we take the negative of the logarithms which gives us sum oflogarithms and if we pair each logarithm with the point where it came from,we actually get a value for each point.And if we calculate the values,we get this. Check it out.If we look carefully at the values we can see thatthe points that are mis-classified has likevalues like 2.3 for this point or 1.6 one for this point,whereas the points that are correctly classified have small values.And the reason for this is again is thata correctly classified point will have a probability that as close to 1,which when we take the negative of the logarithm,we'll get a small value.Thus we can think of the negatives of these logarithms as errors at each point.Points that are correctly classified will havesmall errors and points that are mis-classified will have large errors.And now we've concluded that our cross entropy will tell us if a model is good or bad.So now our goal has changed from maximizing a probability to minimizinga cross entropy in order to get from the model in left to the model in the right.And that error function that we're looking for,that was precisely the cross entropy.

### 21. CrossEntropy V1-1BnhC6e0TFw.en

Let's look a bit closer into Cross-Entropy by switching to a different example.Let's say we have three doors.And no this is not the Monty Hall problem.We have the green door, the red door,and the blue door, and behind each door we could have a gift or not have a gift.And the probabilities of there being a gift behind each door is 0.8 for the first one,0.7 for the second one,0.1 for the third one.So for example behind the green doorthere is an 80 percent probability of there being a gift,and a 20 percent probability of there not being a gift.So we can put the information in this table wherethe probabilities of there being a gift are given in the top row,and the probabilities of there not being a gift are given in the bottom row.So let's say we want to make a bet on the outcomes.So we want to try to figure out what is the most likely scenario here.And for that we'll assume they're independent events.In this case, the most likely scenario is justobtained by picking the largest probability in each column.So for the first door is more likely to have a gift than not have a gift.So we'll say there's a gift behind the first door.For the second door, it's also more likely that there's a gift.So we'll say there's a gift behind the second door.And for the third door it's much more likely that there's no gift,so we'll say there's no gift behind the third door.And as the events are independent,the probability for this whole arrangement isthe product of the three probabilities which is 0.8,times 0.7, times 0.9,which ends up being 0.504,which is roughly 50 percent.So let's look at all the possible scenarios in the table.Here's a table with all the possible scenarios for each doorand there are eight scenarios since each door gives us two possibilities each,and there are three doors.So we do as before to obtain the probability ofeach arrangement by multiplying the three independent probabilities to get these numbers.You can check that these numbers add to one.And from last video we learned that the negativeof the logarithm of the probabilities across entropy.So let's go ahead and calculate the cross-entropy.And notice that the events with high probability havelow cross-entropy and the events with low probability have high cross-entropy.For example, the second row which has probability of0.504 gives a small cross-entropy of 0.69,and the second to last row which is very very unlikely has a probability of0.006 gives a cross entropy a 5.12.So let's actually calculate a formula for the cross-entropy.Here we have our three doors,and our sample scenario said that there is a gift behind the first and second doors,and no gift behind the third door.Recall that the probabilities of these events happeningare 0.8 for a gift behind the first door,0.7 for a gift behind the second door,and 0.9 for no gift behind the third door.So when we calculate the cross-entropy,we get the negative of the logarithm of the product,which is a sum of the negatives of the logarithms of the factors,which is negative logarithm of 0.8 minus logarithm of 0.7 minus logarithm 0.9.And in order to drive the formula we'll have some variables.So let's call P1 the probability that there's a gift behind the first door,P2 the probability there's a gift behind the second door,and P3 the probability there's a gift behind the third door.So this 0.8 here is P1,this 0.7 here is P2,and this 0.9 here is one minus P3.So it's a probability of there not beinga gift is one minus the probability of there being a gift.Let's have another variable called Yi,which will be one of there's a present behind the ith door,and zero there's no present.So Yi is technically a number of presents behind the ith door.In this case Y1 equals one,Y2 equals one, and Y3 equals zero.So we can put all this together and derive a formulafor the cross-entropy and it's this sum.Now let's look at the formula inside the summation.Noted that if there is a present behind the ith door,then Yi equals one.So the first term is logarithm of the Pi.And the second term is zero.Likewise, if there is no present behind the ith door,then Yi is zero.So this first term is zero.And this term is precisely logarithm of one minus Pi.Therefore, this formula really encompasses the sums of thenegative of logarithms which is precisely the cross-entropy.So the cross-entropy really tells us when two vectors are similar or different.For example, if you calculate the cross entropy of the pair one one zero,and 0.8, 0.7, 0.1, we get 0.69.And that is low because one one zero is a similar vector to 0.8, 0.7, 0.1.Which means that the arrangement of gifts given by the first set ofnumbers is likely to happen basedon the probabilities given by the second set of numbers.But on the other hand if we calculate the cross-entropy of the pairs zero zero one,and 0.8, 0.7, 0.1,that is 5.12 which is very high.This is because the arrangement of gifts being given by the first set of numbers isvery unlikely to happen from the probabilities given by the second set of numbers.

### 21. Formula For Cross 1-qvr_ego_d6w.en

So this cross entropy, it looks like kind of a big deal.Cross entropy really says the following.If I have a bunch of events and a bunch of probabilities,how likely is it that those events happen based on the probabilities?If it's very likely,then we have a small cross entropy.If it's unlikely, then we have a large cross entropy. Let's elaborate.

### 22. DL 27 Multi-Class Cross Entropy 2 Fix-keDswcqkees.en

Now that was when we had two classes namely receiving a gift or not receiving a gift.What happens if we have more classes? Let's take a look.So we have a similar problem.We still have three doors.And this problem is still not the Monty Hall problem.Behind each door there can be an animal,and the animal can be of three types.It can be a duck, it can be a beaver,or it can be a walrus.So let's look at this table of probabilities.According to the first column on the table,behind the first door,the probability of finding a duck is 0.7,the probability of finding a beaver is 0.2,and the probability of finding a walrus is 0.1.Notice that the numbers in each column need to add toone because there is some animal behind door one.The numbers in the rows do not need to add to one as you can see.It could easly be that we have a duck behind every door and that's okay.So let's look at a sample scenario.Let's say we have our three doors,and behind the first door, there's a duck,behind the second door there's a walrus,and behind the third door there's also a walrus.Recall that the probabilities are again by the table.So a duck behind the first door is 0.7 likely,a walrus behind the second door is 0.3 likely,and a walrus behind the third door is 0.4 likely.So the probability of obtaining this three animals is the product ofthe probabilities of the three events since they are independent events,which in this case it's 0.084.And as we learn,that cross entropy here is given bythe sums of the negatives of the logarithms of the probabilities.So the first one is negative logarithm of 0.7.The second one is negative logarithm of 0.3.And the third one is negative logarithm of 0.4.The Cross entropy's and the sum of these three which is actually 2.48.But we want a formula, so let's put some variables here.So P11 is the probability of finding a duck behind door one.P12 is the probability of finding a duck behind door two etc.And let's have the indicator variables Y1j D1 if there'sa duck behind door J. Y2j B1 if there's a beaver behind door J,and Y3j B1 if there's a walrus behind door J.And these variables are zero otherwise.And so, the formula for the cross entropy issimply the negative of the summation from i_ equals_ one to n,up to summation from y_ equals_ j to m of Yij_ times_ the logarithm of Pij.In this case, m is a number of classes.This formula works because Yij being zero one,makes sure that we're only adding the logarithmsof the probabilities of the events that actually have occurred.And voila, this is the formula for the cross entropy in more classes.Now I'm going to leave this equestion.Given that we have a formula for cross entropy for two classes and one for m classes.These formulas look different but are they the same for m_ equals_ two?Obviously the answer is yes,but it's a cool exercise to actually write them down andconvince yourself that they are actually the same.

### 23. DL 29 Logistic Regression-Minimizing The Error Function-KayqiYijlzc.en

Okay. So now our goal is to minimize the error function and we'll do it as follows.We started some random weights,which will give us the predictions (Wx+b).As we saw, that also gives us a error function given by this formula.Remember that the summands are also error functions for each point.So each point will give us a larger function ifit's mis-classified and a smaller one if it's correctly classified.And the way we're going to minimize this function,is to use gradient decent.So here's Mt. Errorest and this is us,and we're going to try to jiggle the line around tosee how we can decrease the error function.Now, the error function is the height which is E(W,b),where W and b are the weights.Now what we'll do, is we'll use gradient decent in order toget to the bottom of the mountain at a much smaller height,which gives us a smaller error function E of W', b'.This will give rise to new weights,W' and b' which will give us a much better prediction.Namely,  (W'x+b').

### 23. Error Function-V5kkHldUlVU.en

So this is a good time for a quick recap of the last couple of lessons.Here we have two models.The bad model on the left and the good model on the right.And for each one of those we calculate the cross entropy which is the sum ofthe negatives of the logarithms off the probabilities of the points being their colors.And we conclude that the one on the right is betterbecause a cross entropy is much smaller.So let's actually calculate the formula for the error function.Let's split into two cases.The first case being when y=1.So when the point is blue to begin with,the model tells us that the probability of being blue is the prediction y_hat.So for these two points the probabilities are 0.6 and 0.2.As we can see the point in the blue area hasmore probability of being blue than the point in the red area.And our error is simply the negative logarithm of this probability.So it's precisely minus logarithm of y_hat.In the figure it's minus logarithm of 0.6. and minus logarithm of 0.2.Now if y=0, so when the point is red,then we need to calculate the probability of the point being red.The probability of the point being red is one minus the probability of the point beingblue which is precisely 1 minus the prediction y_hat.So the error is precisely the negative logarithm ofthis probability which is negative logarithm of 1 - y_hat.In this case we get negative logarithm 0.1 and negative logarithm 0.7.So we conclude that the error is a negative logarithm of y_hat if the point is blue.And negative logarithm of one - y_hat the point is red.We can summarize these two formulas into this one.Error = - (1-y)(ln( 1- y_hat)) - y ln(y_hat).Why does this formula work?Well because if the point is blue,then y=1 which means 1-y=0 which makes the first term0 and the second term is simply logarithm of y_hat.Similarly, if the point is red then y=0.So the second term of the formula is 0 and the first one is logarithm of 1- y_hat.Now the formula for the error function is simply the sum overall the error functions of points which is precisely the summation here.That's going to be this 4.8 we have over here.Now by convention we'll actually consider the average,not the sum which is where we are dividing by n over here.This will turn the 4.8 into a 1.2.From now on we'll use this formula as our error function.And now since y_hat is given by the sigmoid of the linear function wx + b,then the total formula for the error is actually in termsof w and b which are the weights of the model.And it's simply the summation we see here.In this case y_i is just the label of the point x_superscript_i.So now that we've calculated it our goal is to minimize it.And that's what we'll do next.And just a small aside,what we did is for binary classification problems.If we have a multiclass classification problem thenthe error is now given by the multiclass entropy.This formula is given here where for every data point we take the productof the label times the logarithm of the prediction and then we average all these values.And again it's a nice exercise to convince yourself thatthe two are the same when there are just two classes.

### 24. Gradient Descent-rhVIF-nigrY.en

So let's study gradient descent in more mathematical detail.Our function is a function of the weights and it can be graph like this.It's got a mathematical structure so it's not Mt.Everest anymore, it's more of a mount Math-Er-Horn.So we're standing somewhere in Mount Math-Er-Horn and we need to go down.So now the inputs of the functions are W1 and W2 and the error function is given byE. Then the gradient of E is given bythe vector sum of the partial derivatives of E with respect to W1 and W2.This gradient actually tells us the direction we want tomove if we want to increase the error function the most.Thus, if we take the negative of the gradient,this will tell us how to decrease the error function the most.And this is precisely what we'll do.At the point we're standing,we'll take the negative of the gradient of the error function at that point.Then we take a step in that direction.Once we take a step,we'll be in a lower position.So we do it again, and again,and again, until we are able to get to the bottom of the mountain.So this is how we calculate the gradient.We start with our initial prediction Y had equals sigmoid of W Expo's B.And let's say this prediction is bad becausethe error is large since we're high up in the mountain.The prediction looks like this,Y had equal sigmoid of W 1 x 1 plus all the way to WnXn plus b.Now the error function is given by the formula we saw before.But what matters here is the gradient of the error function.The gradient of the error function is precisely the vector formed bythe partial derivative of the error function with respect to the weights and the bias.Now, we take a step in the direction of the negative of the gradient.As before, we don't want to make any dramatic changes,so we'll introduce a smaller learning rate alpha.For example, 0.1.And we'll multiply the gradient by that number.Now taking the step is exactly the same thing asupdating the weights and the bias as follows.The weight Wi will now become Wi prime.Given by Wi minus alpha times the partial derivative of the error,with respect to Wi.And the bias will now become b prime given by b minusalpha times partial derivative of the error with respect to b.Now this will take us to a prediction with a lower error function.So, we can conclude that the prediction we have now with weights W prime b prime,is better than the one we had before with weights W and b.This is precisely the gradient descent step.

### 25. Gradient Descent Algorithm-snxmBgi_GeU.en

And now we finally have the tools to writethe pseudocode for the grading descent algorithm,and it goes like this.Step one, start with random weights w_one up to w_n and b which will give us a line,and not just a line, but the whole probability function given by sigmoid of w x plus b.Now for every point we'll calculate the error,and as we can see the error is high formisclassified points and small for correctly classified points.Now for every point with coordinates x_one up to x_n,we update w_i by adding the learning ratealpha times the partial derivative of the error function with respect to w_i.We also update b by adding alpha timesthe partial derivative of the error function with respect to be.This gives us new weights,w_i_prime and then new bias b_prime.Now we've already calculated these partial derivatives and weknow that they are y_hat minus y timesx_i for the derivative with respect to w_iand y_hat minus y for the derivative with respect to b.So that's how we'll update the weights.Now repeat this process until the error is small,or we can repeat it a fixed number of times.The number of times is called the epochs and we'll learn them later.Now this looks familiar,have we seen something like that before?Well, we look at the points and what each point is doing isit's adding a multiple of itself into the weights ofthe line in order to get the line to move closer towards it if it's misclassified.That's pretty much what the Perceptron algorithm is doing.So in the next video, we'll look atthe similarities because it's a bit suspicious how similar they are.

### 28. Gradient Descent Vs Perceptron Algorithm-uL5LuRPivTA.en

So let's compare the Perceptron algorithm and the Gradient Descent algorithm.In the Gradient Descent algorithm,we take the weights and change them from Wi toWi_ plus_ alpha_ times_ Y hat_ minus_ Y_ times_ Xi.In the Perceptron algorithm,not every point changes weights,only the misclassified ones.Here, if X is misclassified,we'll change the weights by adding Xi to Wi if the point label is positive,and subtracting if negative.Now the question is, are these two things the same?Well, let's remember that in that Perceptron algorithm,the labels are one and zero.And the predictions Y-hat are also one and zero.So, if the point is correct, classified,then Y_ minus_ Y-hat is zero because Y is equal to Y-hat.Now, if the point is labeled blue,then Y_ equals_ one.And if it's misclassified,then the prediction must be Y-hat_ equals_ zero.So Y-hat_ minus_ Y is minus one.Similarly, with the points labeled red,then Y_ equals_ zero and Y-hat_ equals_ one.So, Y-hat_ minus_ Y_ equals_ one.This may not be super clear right away.But if you stare at the screen for long enough,you'll realize that the right and the left are exactly the same thing.The only difference is that in the left,Y-hat can take any number between zero and one,whereas in the right,Y-hat can take only the values zero or one.It's pretty fascinating, isn't it?But let's study Gradient Descent even more carefully.Both in the Perceptron algorithm and the Gradient Descent algorithm,a point that is misclassified tells a line to come closer because eventually,it wants the line to surpass it so it can be in the correct side.Now, what happens if the point is correctly classified?Well, the Perceptron algorithm says do absolutely nothing.In the Gradient Descent algorithm,you are changing the weights.But what is it doing?Well, if we look carefully,what the point is telling the line,is to go farther away.And this makes sense, right?Because if you're correctly classified,say, if you're a blue point in the blue region,you'd like to be even more into the blue region,so your prediction is even closer to one,and your error is even smaller.Similarly, for a red point in the red region.So it makes sense that the point tells the line to go farther away.And that's precisely what the Gradient Descent algorithm does.The misclassified points asks the line to come closer andthe correctly classified points asks the line to go farther away.The line listens to all the points and takes steps insuch a way that it eventually arrives to a pretty good solution.

### 29. Continuous Perceptrons-07-JJ-aGEfM.en

So, this is just a small recap video that will get us ready for what's coming.Recall that if we have our data in the form of these points overhere and the linear model like this one, for example,with equation 2x1 + 7x2 - 4 = 0,this will give rise to a probability function that looks like this.Where the points on the blue or positive region have more chance of beingblue and the points in the red or negative region have more chance of being red.And this will give rise to this perception where we labelthe edges by the weights and the node by the bias.So, what the perception does,it takes to point (x1, x2),plots it in the graph and then it returns a probability that the point is blue.In this case, it returns a 0.9and this mimics the neurons in the brain because they receive nervous impulses,do something inside and return a nervous impulse.

### 30. Non-Linear Data-F7ZiE8PQiSc.en

Now we've been dealing a lot with data sets that can be separated by a line,like this one over here.But as you can imagine the real world is much more complex than that.This is where neural networks can show their full potential.In the next few videos we'll see how to deal withmore complicated data sets that requirehighly non-linear boundaries such as this one over here.

### 31. Non-Linear Models-HWuBKCZsCo8.en

So, let's go back to this example of where we sawsome data that is not linearly separable.So a line can not divide these red and blue points and we looked at some solutions,and if you remember, the one we considered more seriously was this curve over here.So what I'll teach you now is to find this curve and it's very similar than before.We'll still use grading dissent.In a nutshell, what we're going to do is forthese data which is not separable with a line,we're going to create a probability function where the points in the blue region are morelikely to be blue and the points in the red region are more likely to be red.And this curve here that separates them isa set of points which are equally likely to be blue or red.Everything will be the same as before except this equationwon't be linear and that's where neural networks come into play.

### 32. 29 Neural Network Architecture 2-FWN3Sw5fFoM.en

So in the previous session we learn that we canadd to linear models to obtain a third model.As a matter of fact, we did even more.We can take a linear combination of two models.So, the first model times a constant plus the second model times aconstant plus a bias and that gives us a non-linear model.That looks a lot like perceptrons where we can take a value times a constant plusanother value times a constant plus a bias and get a new value.And that's no coincidence.That's actually the building block of Neural Networks.So, let's look at an example.Let's say, we have this linear model where the linear equation is 5x1 minus 2x2 plus 8.That's represented by this perceptron.And we have another linear model with equations 7x1 minus3x2 minus 1 which is represented by this perceptron over here.Let's draw them nicely in here and let's use another perceptronto combine these two models using the Linear Equation,seven times the first model plus five times the second model minus six.And now the magic happens when we join these together and we get a Neural Network.We clean it up a bit and we obtain this. All the weights are there.The weights on the left,tell us what equations the linear models have.And the weights on the right,tell us what the linear combination is ofthe two models to obtain the curve non-linear model in the right.So, whenever you see a Neural Network like the one on the left,think of what could be the nonlinear boundary defined by the Neural Network.Now, note that this was drawn using the notation that puts a bias inside the node.This can also be drawn using the notation that keeps the bias as a separate node.Here, what we do is, in every layer we havea bias unit coming from a node with a one on it.So for example, the minus eight on the top nodebecomes an edge labelled minus eight coming from the bias node.We can see that this Neural Network usesa Sigmoid Activation Function and the Perceptrons.

### 32. Combinando modelos-Boy3zHVrWB4.en

Now I'm going to show you how to create these nonlinear models.What we're going to do is a very simple trick.We're going to combine two linear models into a nonlinear model as follows.Visually it looks like this.The two models over imposed creating the model on the right.It's almost like we're doing arithmetic on models.It's like saying "This line plus this line equals that curve."Let me show you how to do this mathematically.So a linear model as we know is a whole probability space.This means that for every point it gives us the probability of the point being blue.So, for example, this point over here is inthe blue region so its probability of being blue is 0.7.The same point given by the second probability space isalso in the blue region so it's probability of being blue is 0.8.Now the question is,how do we combine these two?Well, the simplest way to combine two numbers is to add them, right?So 0.8 plus 0.7 is 1.5.But now, this doesn't look like a probability anymore since it's bigger than one.And probabilities need to be between 0 and 1. So what can we do?How do we turn this number that is larger than 1 into something between 0 and 1?Well, we've been in this situation before and we have a pretty good tool thatturns every number into something between 0 and 1.That's just a sigmoid function.So that's what we're going to do.We applied the sigmoid function to 1.5 to get the value0.82 and that's the probability ofthis point being blue in the resulting probability space.So now we've managed to create a probability function forevery single point in the plane and that's how we combined two models.We calculate the probability for one of them,the probability for the other,then add them and then we apply the sigmoid function.Now, what if we wanted to weight this sum?What, if say, we wanted the model in the top to havemore of a saying the resulting probability than the second?So something like this where the resulting model looks a lot more like the one inthe top then like the one in the bottom. Well, we can add weights.For example, we can say "I want seven times the first model plus the second one."Actually, I can add the weights since I want.For example, I can say "Seven times the first one plus five times the second one."And when I do get the combine the model is I take the first probability,multiply it by seven,then take the second one and multiply it by five and I can even add a bias if I want.Say, the bias is minus 6,then we add it to the whole equation.So we'll have seven times this plus five times this minus six,which gives us 2.9.We then apply the sigmoid function and that gives us 0.95.So it's almost like we had before, isn't it?Before we had a line that is a linear combinationof the input values times the weight plus a bias.Now we have that this model is a linear combination ofthe two previous model times the weights plus some bias.So it's almost the same thing.It's almost like this curved model in the right.It's a linear combination of the two linear models beforeor we can even think of it as the line between the two models.This is no coincidence.This is at the heart of how neural networks get built.Of course, we can imagine that we can keep doing this always obtainingmore new complex models out of linear combinations of the existing ones.And this is what we're going to do to build our neural networks.

### 32. Layers-pg99FkXYK0M.en

Neural networks have a certain special architecture with layers.The first layer is called the input layer,which contains the inputs,in this case, x1 and x2.The next layer is called the hidden layer,which is a set of linear models created with this first input layer.And then the final layer is called the output layer,where the linear models get combined to obtain a nonlinear model.You can have different architectures.For example, here's one with a larger hidden layer.Now we're combining three linear models toobtain the triangular boundary in the output layer.Now what happens if the input layer has more nodes?For example, this neural network has three nodes in its input layer.Well, that just means we're not living in two-dimensional space anymore.We're living in three-dimensional space,and now our hidden layer,the one with the linear models,just gives us a bunch of planes in three space,and the output layer bounds a nonlinear region in three space.In general, if we have n nodes in our input layer,then we're thinking of data living in n-dimensional space.Now what if our output layer has more nodes?Then we just have more outputs.In that case, we just have a multiclass classification model.So if our model is telling us if an image is a cat or dog or a bird,then we simply have each node inthe output layer output a score for each one of the classes: one for the cat,one for the dog, and one for the bird.And finally, and here's where things get pretty cool,what if we have more layers?Then we have what's called a deep neural network.Now what happens here is our linear models combine to createnonlinear models and then these combine to create even more nonlinear models.In general, we can do this many times and obtainhighly complex models with lots of hidden layers.This is where the magic of neural networks happens.Many of the models in real life,for self-driving cars or for game-playing agents,have many, many hidden layers.That neural network will just splitthe n-dimensional space with a highly nonlinear boundary,such as maybe the one on the right.

### 32. Multiclass Classification-uNTtvxwfox0.en

We briefly mentioned multi-class classificationin the last video but let me be more specific.It seems that neural networks work really well whenthe problem consist on classifying two classes.For example, if the model predicts a probability of receivinga gift or not then the answer just comes as the output of the neural network.But what happens if we have more classes?Say, we want the model to tell us if an image is a duck,a beaver, or a walrus.Well, one thing we can do is create a neural network to predict if the image is a duck,then another neural network to predict if the image is a beaver,and a third neural network to predict if the image is a walrus.Then we can just use SoftMax or pick the answer that gives us the highest probability.But this seems like overkill, right?The first layers of the neural network should be enough to tell us things aboutthe image and maybe just the last layer should tell us which animal it is.As a matter of fact, as you'll see in the CNN section,this is exactly the case.So what we need here is to add more nodes in the output layer and each one ofthe nodes will give us the probability that the image is each of the animals.Now, we take the scores and apply the SoftMax function that was previouslydefined to obtain well-defined probabilities.This is how we get neural networks to do multi-class classification.

### 33. DL 41 Feedforward FIX V2-hVCuvMGOfyY.en

So now that we have defined what neural networks are,we need to learn how to train them.Training them really means what parameters should theyhave on the edges in order to model our data well.So in order to learn how to train them,we need to look carefully at how they process the input to obtain an output.So let's look at our simplest neural network, a perceptron.This perceptron receives a data point of the form x1,x2 where the label is Y=1.This means that the point is blue.Now the perceptron is defined by a linear equation say w1, x1 plus w2,x2 plus B, where w1 and w2 are the weights in the edges and B is the bias in the note.Here, w1 is bigger than w2,so we'll denote that by drawing the edge labelled w1much thicker than the edge labelled w2.Now, what the perceptron does is it plots the point x1,x2 and it outputs the probability that the point is blue.Here is the point is in the red area and then the output is a small number,since the point is not very likely to be blue.This process is known as feedforward.We can see that this is a bad model because the point is actually blue.Given that the third coordinate,the Y is one.Now if we have a more complicated neural network,then the process is the same.Here, we have thick edges corresponding to large weights andthin edges corresponding to small weights and the neural network plotsthe point in the top graph and also inthe bottom graph and the outputs coming out will be a small number from the top model.The point lies in the red area which means it has a small probability of beingblue and a large number from the second model,since the point lies in the blue area which meansit has a large probability of being blue.Now, as the two models get combined into this nonlinear model andthe output layer just plotsthe point and it tells the probability that the point is blue.As you can see, this is a bad model because itputs the point in the red area and the point is blue.Again, this process called feedforward and we'll look at it more carefully.Here, we have our neural network and the other notations so the bias is in the outside.Now we have a matrix of weights.The matrix w superscript one denoting the first layer and the entries are the weights w1,1 up to w3, 2.Notice that the biases have now been written as w3,1 and w3, 2 this is just for convenience.Now in the next layer,we also have a matrix this one is w superscript two for the second layer.This layer contains the weights that tell us how to combinethe linear models in the first layer to obtain the nonlinear model in the second layer.Now what happens is some math.We have the input in the form x1, x2,1 where the one comes from the bias unit.Now we multiply it by the matrix w1 to get these outputs.Then, we apply the sigmoid function to turn the outputs into values between zero and one.Then the vector format these values gets a one attatched forthe bias unit and multiplied by the second matrix.This returns an output that now gets thrown into a sigmoid function toobtain the final output which is y-hat.Y-hat is the prediction or the probability that the point is labeled blue.So this is what neural networks do.They take the input vector and then applya sequence of linear models and sigmoid functions.These maps when combined become a highly non-linear map.And the final formula is simply y-hat equals sigmoid ofw2 combined with sigmoid of w1 applied to x.Just for redundance, we do this again on a multi-layer perceptron or neural network.To calculate our prediction y-hat,we start with the unit vector x,then we apply the first matrix anda sigmoid function to get the values in the second layer.Then, we apply the second matrix and another sigmoid function to get the values onthe third layer and so on and so forth until we get our final prediction, y-hat.And this is the feedforward process that the neural networksuse to obtain the prediction from the input vector.

### 33. DL 42 Neural Network Error Function (1)-SC1wEW7TtKs.en

So, our goal is to train our neural network.In order to do this,we have to define the error function.So, let's look again at what the error function was for perceptrons.So, here's our perceptron.In the left, we have our input vector withentries x_1 up to x_n, and one for the bias unit.And the edges with weights W_1 up to W_n,and b for the bias unit.Finally, we can see that this perceptor uses a sigmoid function.And the prediction is defined as y-hat equals sigmoid of Wx plus b.And as we saw, this function gives us a measure ofthe error of how badly each point is being classified.Roughly, this is a very small number if the point is correctly classified,and a measure of how far the point is fromthe line and the point is incorrectly classified.So, what are we going to do to define the error function in a multilayer perceptron?Well, as we saw, our prediction is simplya combination of matrix multiplications and sigmoid functions.But the error function can be the exact same thing, right?It can be the exact same formula,except now, y-hat is just a bit more complicated.And still, this function will tell us how badly a point gets misclassified.Except now, it's looking at a more complicated boundary.

### 34. Backpropagation V2-1SmY3TZTyUk.en

So now we're finally ready to get our hands into training a neural network.So let's quickly recall feedforward.We have our perceptron with a point coming in labeled positive.And our equation w1x1 + w2x2 + b,where w1 and w2 are the weights and b is the bias.Now, what the perceptron does is,it plots a point and returns a probability that the point is blue.Which in this case is small since the point is in the red area.Thus, this is a bad perceptron since itpredicts that the point is red when the point is really blue.And now let's recall what we did in the gradient descent algorithm.We did this thing called Backpropagation.We went in the opposite direction.We asked the point, "What do you want the model to do for you?"And the point says, "Well,I am misclassified so I want this boundary to come closer to me."And we saw that the line got closer to it by updating the weights.Namely, in this case,let's say that it tells the weight w1 to go lower and the weight w2 to go higher.And this is just an illustration,it's not meant to be exact.So we obtain new weights,w1' and w2' which define a new line which is now closer to the point.So what we're doing is like descending fromMt. Errorest, right?The height is going to be the error function E(W) and we calculate the gradientof the error function which is exactlylike asking the point what does is it want the model to do.And as we take the step down the direction of the negative of the gradient,we decrease the error to come down the mountain.This gives us a new error,E(W') and a new model W' with a smaller error,which means we get a new line closer to the point.We continue doing this process in order to minimize the error.So that was for a single perceptron.Now, what do we do for multi-layer perceptrons?Well, we still do the same process of reducing the error by descending from the mountain,except now, since the error function is more complicated then it's notMt. Errorest, now it'sMt. Kilimanjerror. But same thing,we calculate the error function and its gradient.We then walk in the direction of the negative of the gradient in order to finda new model W' with a smaller errorE(W') which will give us a better prediction.And we continue doing this process in order to minimize the error.So let's look again at what feedforward does in a multi-layer perceptron.The point comes in with coordinates (x1, x2) and label y = 1.It gets plotted in the linear models corresponding to the hidden layer.And then, as this layer gets combined the point getsplotted in the resulting non-linear model in the output layer.And the probability that the point is blue is obtained bythe position of this point in the final model.Now, pay close attention because this isthe key for training neural networks, it's Backpropagation.We'll do as before, we'll check the error.So this model is not good because it predicts thatthe point will be red when in reality the point is blue.So we'll ask the point,"What do you want this model to do in order for you to be better classified?"And the point says, "I kind of want this blue region to come closer to me."Now, what does it mean for the region to come closer to it?Well, let's look at the two linear models in the hidden layer.Which one of these two models is doing better?Well, it seems like the top one is badly misclassifyingthe point whereas the bottom one is classifying it correctly.So we kind of want to listen to the bottom one more and to the top one less.So what we want to do is to reduce the weight coming fromthe top model and increase the weight coming from the bottom model.So now our final model will look a lotmore like the bottom model than like the top model.But we can do even more.We can actually go to the linear models and ask the point,"What can these models do to classify you better?"And the point will say, "Well,the top model is misclassifying me,so I kind of want this line to move closer to me.And the second model is correctly classifying me,so I want this line to move farther away from me."And so this change in the model will actually update the weights.Let's say, it'll increase these two and decrease these two.So now after we update all the weights we have better predictions atall the models in the hidden layer andalso a better prediction at the model in the output layer.Notice that in this video we intentionally left the bias unit away for clarity.In reality, when you update the weights we're also updating the bias unit.If you're the kind of person who likes formality,don't worry, we'll calculate these gradients in detail soon.

### 34. Calculating The Gradient 1 -tVuZDbUrzzI.en

Okay. So, now we'll do the same thing as we did before,painting our weights in the neural network to better classify our points.But we're going to do it formally,so fasten your seat belts because math is coming.On your left, you have a single perceptron with the input vector,the weights and the bias and the sigmoid function inside the node.And on the right, we have a formula for the prediction,which is the sigmoid function of the linear function of the input.And below, we have a formula for the error,which is the average of all points ofthe blue term for the blue points and the red term for the red points.And in order to descend from Mount Errorest,we calculate the gradient.And the gradient is simply the vector formed by all the partial derivatives ofthe error function with respect to the weights w1 up to wn and and the bias b.They correspond to these edges over here,and what do we do in a multilayer perceptron?Well, this time it's a little more complicated but it's pretty much the same thing.We have our prediction,which is simply a composition of functions namely matrix multiplications and sigmoids.And the error function is pretty much the same,except the  is a bit more complicated.And the gradient is pretty much the same thing,it's just much, much longer.It's a huge vector where each entry isa partial derivative of the error with respect to each of the weights.And these just correspond to all the edges.If we want to write this more formally,we recall that the prediction is a composition of sigmoids and matrix multiplications,where these are the matrices andthe gradient is just going to be formed by all these partial derivatives.Here, it looks like a matrix but in reality,it's just a long vector.And the gradient descent is going to do the following;we take each weight,w_i_j super k and we update it by adding a small number,the learning rate times the partial derivative of E with respect to that same weight.This is the gradient descent step,so it will give us new updated weight w_i_j super k prime.That step is going to give us a whole new modelwith new weights that will classify the point much better.

### 34. Chain Rule-YAhIBOnbt54.en

So before we start calculating derivatives,let's do a refresher on the chain rule whichis the main technique we'll use to calculate them.The chain rule says, if you have a variable x on a function f that youapply to x to get f of x, which we're gonna call A,and then another function g,which you apply to f of x to get g of f of x,which we're gonna call B, the chain rule says,if you want to find the partial derivative of B with respect to x,that's just a partial derivative of B with respect toA times the partial derivative of A with respect to x.So it literally says,when composing functions, that derivatives just multiply,and that's gonna be super useful for us becausefeed forwarding is literally composing a bunch of functions,and back propagation is literally taking the derivative at each piece,and since taking the derivative of a compositionis the same as multiplying the partial derivatives,then all we're gonna do is multiply a bunch ofpartial derivatives to get what we want. Pretty simple, right?

### 34. DL 46 Calculating The Gradient 2 V2 (2)-7lidiTGIlN4.en

So, let us go back to our neural network with our weights and our input.And recall that the weights with superscript 1 belong to the first layer,and the weights with superscript 2 belong to the second layer.Also, recall that the bias is not called b anymore.Now, it is called W31,W32 etc. for convenience,so that we can have everything in matrix notation.And now what happens with the input?So, let us do the feedforward process.In the first layer,we take the input and multiply it by the weights and that gives us h1,which is a linear function of the input and the weights.Same thing with h2,given by this formula over here.Now, in the second layer,we would take this h1 and h2 and the new bias,apply the sigmoid function,and then apply a linear function to them by multiplying them bythe weights and adding them to get a value of h. And finally,in the third layer,we just take a sigmoid function of h to getour prediction or probability between 0 and 1, which is .And we can read this in more condensed notation by saying thatthe matrix corresponding to the first layer is W superscript 1,the matrix corresponding to the second layer is W superscript 2,and then the prediction we had is just going to bethe sigmoid of W superscript 2 combined withthe sigmoid of W superscript 1 applied to the input x and that is feedforward.Now, we are going to develop backpropagation,which is precisely the reverse of feedforward.So, we are going to calculate the derivative ofthis error function with respect to each of the weights inthe labels by using the chain rule.So, let us recall that our error function is this formula over here,which is a function of the prediction .But, since the prediction is a function of all the weights wij,then the error function can be seen as the function on all the wij.Therefore, the gradient is simply the vector formed byall the partial derivatives of the error function E with respect to each of the weights.So, let us calculate one of these derivatives.Let us calculate derivative of E with respect to W11 superscript 1.So, since the prediction is simply a composition of functions and by the chain rule,we know that the derivative with respect to thisis the product of all the partial derivatives.In this case, the derivative E with respectto W11 is the derivative of either respect to  timesthe derivative  with respect to htimes the derivative h with respect to h1 times the derivative h1 with respect to W11.This may seem complicated,but the fact that we can calculate a derivative ofsuch a complicated composition function by justmultiplying 4 partial derivatives is remarkable.Now, we have already calculated the first one,the derivative of E with respect to .And if you remember, we got  minus y.So, let us calculate the other ones.Let us zoom in a bit and look at just one piece of our multi-layer perceptron.The inputs are some values h1 and h2,which are values coming in from before.And once we apply the sigmoid and a linear functionon h1 and h2 and 1 corresponding to the biased unit,we get a result h. So,now what is the derivative of h with respect to h1?Well, h is a sum of three things and only one of them contains h1.So, the second and the third summon just give us a derivative of 0.The first summon gives us W11 superscript 2 because that is a constant,and that times the derivative of the sigmoid function with respect to h1.This is something that we calculated below in the instructor comments,which is that the sigmoid function has a beautiful derivative,namely the derivative of sigmoid of h isprecisely sigmoid of h times 1 minus sigmoid of h. Again,you can see this development underneath in the instructor comments.You also have the chance to code this in the quiz because at the end of the day,we just code these formulas and then use them forever, and that is it.That is how you train a neural network.

### img

## Part 02-Module 02-Lesson 02_Deep Learning with PyTorch

### 04. PyTorch V2 Part 1 V1-6Z7WntXays8.en

Hello everyone and welcome to this lesson on deep learning with PyTorch.So, in this lesson I'm going to be showing you how we canbuild neural networks with pyTorch and train them.By working through all these notebooks I built,you'll be writing the actual code yourself for building these networks.By the end of the lesson,you will have built your own state of the art image classifier.But first we're going to start with basics,so how do you build just a simple neural network in pyTorch?So, as a reminder of how neural networks work,in general we have some input values so here x1,x2, and we multiply them by some weights w and bias.So, this b is this bias we just multiply it by one then you sum all these upand you get some value h. Then we have what's called an activation function.So, here f of h and passingthese input values h through this activation function gets you output y.This is the basis of neural networks.You have these inputs,you multiply it by some waves, take the sum,pass it through some activation function and you get an output.You can stack these up so that the output of these units,of these neurons go to another layer like another set of weights.So, mathematically this what it looks like, y,our output is equal to this linear combination of the weights andthe input values w's and x's plus your bias valueb passes through your activation function f and you get y.You could also write it with this sum.So, sum of wi times xi and plus b, your bias term.That gives you y.So, what's nice about this is that you can actually think of the x's,your input features, your values,as a vector, and your weights as another vector.So, your multiplication and sum is the same as a dot or inner product of two vectors.So, if you consider your input as a vector and your weights as a vector,if you take the dot product of these two,then you get your value h and then you passh through your activation function and that gets you your output y.So, now if we start thinking of our weights and our input values as vectors,so vectors are an instance of a tensor.So, a tensor is just a generalization of vectors and matrices.So, when you have these like regular structured arrangements ofvalues and so a tensor with only one dimension is a vector.So, we just have this single one-dimensional array of values.So, in this case characters T-E-N-S-O-R. A matrix like this isa two-dimensional tensor and so we have values going intwo directions from left to right and from top tobottom and so that we have individual rows and columns.So, you can do operations across the columns likealong a row or you can do it across the rows like going down a column.You also have three-dimensional tensors so you can think ofan image like an RGB color image as a three-dimensional tensor.So, for every pixel,there's some value for all the red and the greenand the blue channels and so for every individual pixel,in a two-dimensional image,you have three values.So, that is a three-dimensional tensor.Like I said before, tensors are a generalization ofthis so you can actually have four-dimensional,five-dimensional, six-dimensional, and so on like tensors.It's just the ones that we normally work with areone and two-dimensional, three-dimensional tensors.So, these tensors are the base data structure that you usean pyTorch and other neural network frameworks.So, TensorFlow is named after tensors.So, these are the base data structures thatyou'll be using so you pretty much need to understandthem really well to be able to usepretty much any framework that you'll be using for deep learning.So, let's get started. I'm going to show you how to actually createsome tensors and use them to build a simple neural network.So, first we're going to import pyTorch and so just import torch here.Here I am creating activation function,so this is the Sigmoid activation function.It's the nice s shape that kind of squeezes the input values between zero and one.It's really useful for providing a probability.So, probabilities are these values that can only be between zero and one.So, you're Sigmoid activation if you wantthe output of your neural network to be a probability,then the sigmoid activation is what you want to use.So, here I'm going to create some fake data, I'm generating some data,I'm generating some weights and biases and with these you're actually going todo the computations to get the output of a simple neural network.So, here I'm just creating a manual seeds.So, I'm setting the seed for the random number generation that I'llbe using and here I'm creating features.So, features are like the input features of the input data for your network.Here we see torch.randn.So, randn is going to create a tensor of normal variables.So, random normal variables as samples from a normal distribution.You give it a tuple of the size that you want.So, in this case I want the features to be a matrix,a 2-dimensional tensor of one row and five columns.So, you can think of this as a row vector that has five elements.For the weights, we're going to create another matrixof random normal variables and this time I'm using randn_like.So, what this does is it takesanother tensor and it looks at the shape of this tensor and then it creates it,it creates another tensor with the same shape.So, that's what this this like means.So, I'm going to create a tensor ofrandom normal variables with the same shape as features. So, it gives me my weights.Then I'm going to create a bias term.So, this is again just a random normal variable.Now I'm just creating one value.So, this is one row and one column.Here I'm going to leave this exercise up to you.So, what you're going to be doing is taking the features, weights,and the bias tensors and you're going tocalculate the output of this simple neural network.So, remember with features and weights you want to takethe inner product or you want to multiply the features bythe weights and sum them up and then add the bias and then pass it throughthe activation function and from that you should get the output of your network.So, if you want to see how I did this,checkout my solution notebook or watchthe next video which I'll show you my solution for this exercise.

### 05. PyTorch V2 Part 1 Solution V1-mNJ8CujTtpo.en

So now, this is my solution for this exerciseon calculating the output of this small simple neural network.So, remember that what we want to do is multiply our features by our weights,so features times weights.So these tensors, they work basically the same as NumPy arrays,if you've used NumPy before.So, when you multiply features times weights,it'll just take the first element from each one, multiply them together,take the second element and multiply them together and soon and give you back a new tensor,where there's element by element multiplication.So, from that we can do torch.sum to sum it all up into one value,add our bias term and then pass it through the activation function and then we get Y.So, we can also do this where we do features times weights again,and this creates another tensor,but tensors have a method.sum,where you just take a tensor do.sum and then it sums up all the values in that tensor.So, we can either do it this way or we do torch.sum,or we can just take this method,this sum method of a tensor and some upper values that way.Again, pass it through our our activation function.So, here what we're doing, we're doingthis element wise multiplication and taking the sum in two separate operations.We're doing this multiplication and then we're doing the sum.But you can actually do this in the same operation using matrix multiplication.So, in general, you're going to be wanting touse matrix multiplications most of the time,since they're the more efficient andthese linear algebra operations have been accelerated using modern libraries,such as CUDA that run on GPUs.To do matrix multiplication in PyTorch with our two tensors features and weights,we can use one of two methods.So, either torch.mm or torch.matmul.So, torch.mm, so matrix multiplication is moresimple and more strict about the tensors that you pass in.So, torch.matmul, it actually supports broadcasting.So, if you put in tensors that have weird sizes,weird shapes, then you could get an output that you're not expecting.So, what I tend to use torch.mm more often,so that it does what I expect basically,and then if I get something wrong it's going throw an error instead of justdoing it and continuing the calculations.So, however, if we actually try to usetorch.mm with features and weights, we'll get an error.So, here we see RuntimeError, size mismatch.So, what this means is that we passed in our two tensors to torch.mm,but there's a mismatch in the sizes and it can't actually dothe matrix multiplication and it lists out the sizes here.So, the first tensor,M1 is one by five and the second tensor is one by five also.So, if you remember from your linear algebra classes or if you studied it recently,when you're doing matrix multiplication,the first matrix has to have a number ofcolumns that's equal to the number of rows in the second matrix.So, really what we need is we need our weights tensor,our weights matrix to be five by one instead of one by five.To checkout the shape of tensors,as you're building your networks,you want to use tensor.shape.So, this is something you're going to be using all the time in PyTorch,but also in TensorFlow and in other deep learning frameworks So,most of the errors you're going to see when you're building networks andjust a lot of the difficulty when it comes todesigning the architecture of neural networks is gettingthe shapes of your tensors to work right together.So, what that means is that a large part of debugging,you're actually going to be trying to look atthe shape of your tensors as they're going through your network.So, remember this, tensor.shape.So, for reshaping tensors,there are three, in general,three different options to choose from.So, we have these methods;reshape, resize, and view.The way these all work, in general,is that you take your tensorweights.reshape and then pass in the new shape that you want.So, in this case,you want to change our weights to be a five by one matrix,so we'd say.reshape and then five comma one.So, reshape here, what it will do is it's going toreturn a new tensor with the same data as weights.So, the same data that's sitting in memory at those addresses in memory.So, it's going to basically just createa new tensor that has the shape that you requested,but the actual data in memory isn't being changed.But that's only sometimes.Sometimes it does return a clone and what that means is that it actuallycopies the data to another part of memory and then returnsyou a tensor on top of that part of the memory.As you can imagine when it actually does that,when it's copying the data that's less efficient than if you hadjust changed the shape of your tensor without cloning the data.To do something like that,we can use resize, where there's underscore at the end.The underscore means that this method is an in-place operation.So, when it's in-place,that basically means that you're just not touching the dataat all and all you're doing is changingthe tensor that's sitting on top of that addressed data in memory.The problem with the resize method is that if you requesta shape that has more or less elements than the original tensor,then you can actually cut off,you can actually lose some of the data that you had or you cancreate this spurious data from uninitialized memory.So instead, what you typically want isthat you want a method that's going to return an errorif you changed the shape fromthe original number of elements to a different number of elements.So, we can actually do that with.view.So.view is the one that I use the most,and basically what it does it just returns a new tensorwith the same data in memory as weights.This is just all the time, 100 percent of the time,all it's going to do is return a new tensor withoutmessing with any of the data in memory.If you tried to get a new size,a new shape for your tensor with a different number of elements,it'll return an error.So, you are basically using.view,you're ensuring that you will always getthe same number of elements when you change the shape of your weights.So, this is why I typically use when I'm reshaping tensors.So, with all that out of the way,if you want to reshape weights to have five rows and one column,then you can use something like weights.view (5, 1), right.So, now, that you have seen how you can changethe shape of a tensor and also do matrix multiplication,so this time I want you to calculate the output ofthis little neural network using matrix multiplication.

### 06. PyTorch V2 Part 1 Solution 2 V1-QLaGMz8Ca3E.en

Welcome to my solution for this exercise.So, for here, I had you calculate the output of our network using matrix multiplication.So remember, we wanted to use matrix multiplication because it's moreefficient than doing these two separate operations of the multiplication and the sum.But to do the matrix multiplication,we actually needed to change the size of our weights tensor.So, to do that, just do weights.view 5, 1,and so this will change the shape of our weights tensor to be five rows and one column.If you remember, our features has the shape of one row and five columns,so we can do this matrix multiplication.So, there's just one operation that does the multiplication and the sum and just one go,and then we again add our bias term,pass it through the activation,and we get our output.So, as I mentioned before, you could actually stack upthese simple neural networks into a multi-layer neural network,and this basically givesyour network greater power to capture patterns and correlations in your data.Now, instead of a simple vector for our weights,we actually need to use a matrix.So, in this case, we have our input vector and our input data x_1, x_2, x_3.You think of this as a vector of just x, which our features.Then we have weights that connect our input to one hidden unit in this middle layers,usually called the hidden layer, hidden units,and we have two units in this hidden layer.So then, if we have our features,our inputs as a row vector,if we multiply it by this first column,then we're going to get the output,we're going to get this value of h_1.Then if we take our features and multiply it by the second column,then we're going to get the value for h_2.So again, mathematically looking at this with matrices and vectors and linear algebra,we see that to get the values for this hidden layer thatwe do a matrix multiplication between our feature vector,this x_1 to x_n,and our weight matrix.Then as before with these values,we're going to pass them throughsome activation function or maybe not an activation function,maybe we just want the row output of our network.So here, I'm generating some random data, some features,and some random weight matrices and bias terms that you'llbe using to calculate the output of a multi-layer network.So, what I've built is basically we have three input features,two hidden units and one output unit.So, you can see that I've listed it here.So, our features we're going to create three features and this features vector here,and then we have an input equals three,so the shape is this, and two hidden units, one output unit.So, these weight matrices are created using these values. All right.I'll leave it up to you to calculate the output for this multi-layer network.Again, feel free to use the activation function definedearlier for the output of your network and the hidden layer. Cheers.

### 07. PyTorch V2 Part 1 Solution 3 V1-iMIo9p5iSbE.en

All right. So, here's my solution for this exercise.So, here, I had you calculate the output ofthis multi-layer network using the weights and features that we've defined up here.So, it was really similar to what we did before withour single layer simple neural network.So, it's basically just taking the features and our weight matrix,our first weight matrix,and calculating a matrix multiplication.So, here's the torch.mm plus B1,and then that gives us values for our hidden layer H. Now,we can use the values H as the input for the next layer of our network.So, we just do, again,a matrix multiplication of these hidden values H,with our second weight matrix W2,and adding on our bias terms, and then we get the output.So, my favorite features of PyTorches is being able toconvert between Numpy arrays and Torch tensors,in a very nice and easy manner.So, this is really useful because a lot of the times,you'll be preparing your data and to do some preprocessing using Numpy,and then you want to move it into your network,and so, you have to bridge these Numpy arrays,what you're using for your data,and then the Torch tensors that you're using for your network.So, actually, to do this,we can actually get a tensor from a Numpy array using torch.fromnumpy.So, here I've just created a random array, a four-by-three array,and then we can create a Torch tensor fromthis array just by doing.from Numpy, and passing an array.So, this creates a nice tensor for us.So, this is a tensor in PyTorch,we can use with all of our Torch methods and eventually,use it in a neural network.Then, we can go backwards,so we can take a tensor such as B here.This is our Torch tensor and we can go back to a Numpy array doing b.numpy.So, this gives us back our Numpy array.So, one thing to remember when you're doing this,is that the memory is actually shared between the Numpy array and this Torch tensor.So, what this means, is that if you do any operations inplace on either the Numpy array or the tensor,then you're going to change the values for the other one.So, for example, if we do this in-place operation of multiplying by two,which means that we're actually changing the values in memory,and not creating a new tensor,then we will actually change the values in the Numpy array.So, you see here, we have our Numpy array.So initially, it's like this,convert it to a Torch tensor, and here,I'm doing this in-place multiplication,and we've changed our values for this tensor.Then, if you look back at the Numpy array,the values have changed.So, that's just something to keep in mind as you're doing this,so you're not caught off guard when you're seeing your arrays,your Numpy arrays, being changed because of operations you're doing on the tensor.See you in the next video, cheers.

### 08. PyTorch V2 Part 2 V1-CSQOdOb2mlg.en

Hello everyone and welcome back.So, in this notebook and series of videos,I'm going to be showing you a more powerful way to build neural networks and PyTorch.So, in the last notebook, you saw how you can calculate the output fornetwork using tensors and matrix multiplication.But PyTorch has this nice module, nn,that has a lot of my classes and methods andfunctions that allow us to build large neural networks in a very efficient way.So, to show you how this works,we're going to be using a dataset called MNIST.So, MNIST it's a whole bunch of grayscale handwritten digits.So ,0, 1, 2, 3,4 and so on through nine.Each of these images is 28 by 28 pixels and the goal isto actually identify what the number is in these images.So, that dataset consists of each of these images andit's labeled with the digit that is in that image.So, ones are labeled one,twos are labeled two and so on.So, what we can do is we can actually showour network and image and the correct label andthen it learns how to actually determine what the number and the image is.This dataset is available through the torchvision package.So, this is a package that sits alongside PyTorch,that provides a lot of nice utilities likedatasets and models for doing computer vision problems.We can run this cell to download and load the MNIST dataset.What it does is it gives us back an object which I'm calling trainloader.So, with this trainloader we can turn into an iterator with iter and thenthis will allow us to start getting good atit or we can actually just use this in a loop,in a for loop and so we can get our images and labels outof this generator with four image,comma label and trainloader.One thing to notice is that when I created the trainloader,I set the batch size to 64.So, what that means and every time we get a set of images and labels out,we're actually getting 64 images out from our data loader.So, then if you look at the shape and the size of these images,we'll see that they are 64 by one by 28 by 28.So, 64 images and then one color channels so it's grayscale,and then it's 28 by 28 pixels is the shape of these images and so we can see that here.Then our labels have a shape of 64 so it's just a vector that's 64 elements which witha label for each of our images and we can see whatone of these images looks like this is a nice number four.So, we're going to do here is builda multi-layer neural network using the methods that we saw before.By that I mean you're going to initialize some weight matrices andsome bias vectors and use those to calculate the output of this multi-layer network.Specifically, we want to build this network with 784 input units,256 hidden units, and 10 output units.So, 10 output units,one for each of our classes.So, the 784 input units,this comes from the fact that with this type of network iscalled a fully connected network or a dense network.We want to think of our inputs as just one vector.So, our images are actually this 28 by 28 image,but we want to put a vector into our network and so what we need to do isactually convert this 28 by 28 image into a vector and so,784 is 28 times 28.When we actually take this 28 by 28 image and flatten it intoa vector then it's going to be 784 elements long.So, now what we need to do is take each of our batcheswhich is 64 by one by 28 by 28 and thenconvert it into a shape that is to another tensor which shapes 64 by 784.This is going to be the tensor that's the input to our network.So, go and give this a shot.So again, build the networks 784 input units,256 hidden units and 10 output units and you're going to begenerating your own random initial weight and bias matrices. Cheers.

### 09. PyTorch V2 Part 2 Solution V1-zym36ihtOMY.en

Here is my solution forthis multi-layer neural network forclassifying handwritten digits from the MNIST dataset.So, here I've defined our activation function like before, so,again this is the sigmoid function and here I'm flattening the images.So, remember how to reshape your tensors.So, here I'm using.view.So, I'm just grabbing the batch size.So, images.shape.The first element zero here,gives you the number of batches in your images tensor.So, I want to keep the number of batches the same,but I want to flatten the rest of the dimensions.So, to do this, you actually can just put in negative one.So, I could type in 784 here but a kind ofa shortcut way to do this is to put in negative one.So, basically what this does is it takes 64 as your batch size here and then when youput a negative one it sees this and then it just choosesthe appropriate size to get the total number of elements.So, it'll work out on its own that it needs to make the second dimension,784 so that the number of elementsafter reshaping matches the number elements before reshaping.So, this is just a kind of quick way to flattena tensor without having to know what the second dimension used to be.Then here I'm just creating our weight and bias parameters.So, we know that we want an input of 784 units and we want 256 hidden units.So, our first weight matrix is going to be 784 by 256.Then, we need a bias term for each of our hidden units.So we have 256 bias terms here in b1.Then, for our second weight's going from the hidden layer tothe output layer we want 256 inputs to 10 outputs.Then again 10 elements in our bias.Before we can do a matrix multiplication of our inputs with the first set of weights,our first weight parameters,add in the bias terms and passesthrough our activation functions so that gives us the output of our hidden layer.Then we can use that as the input to our output layer, and again,a matrix multiplication with a second set of weights and the second set of bias terms.This gives us the output of our network. All right.So, if we look at the output of this network,we see that we get those 64.So, first let me print the shape just to make sure we did that right.So, 64 rows for one of each of our sort of input examples and then 10 values,so, basically it's a value that's trying tosay this image belongs to this class like this digit.So, we can inspect our output tensor and see what's going on here.So, we see these values are just sort of all over the place.So, you got like six and negative 11 and so on.But we really want is we want our network to kind of tellus the probability of our different classes given some image.So, kind of we want to pass in an image to our network and then the output should bea probability distribution that tells us which arethe most likely classes or digits that belong to this image.So, if it's the image of a six,then we want a probability distribution where most ofthe probability is in the sixth class.So, it's telling us that it's a number six.So, we want it to look something like this.This is like a class probability.So, it's telling us the probabilities ofthe different classes given this image that we're passing in.So, you can see that the probability foreach of these different classes is roughly the same,and so it's a uniform distribution.This represents an untrained network,so it's a uniform probability distribution.It's because it hasn't seen any data yet,so it hasn't learned anything about these images.So, whenever you give an image to it,it doesn't know what it is so it's just going to give an equal probability to each class,regardless of the image that you pass in.So, what we want is we want the output of our network to bea probability distribution that gives us the probabilitythat the image belongs to any one of our classes.So for this, we use the softmax function.So what this looks like is the exponential.So,you pass in your 10 values.So, for each of those values,we calculate the exponential of that value divided bythe sum of exponentials of all the values.So, what this does is it actually kind of squishes eachof the input values x between zero and one,and then also normalizes all the values so thatthe sum of each of the probabilities is one.So, the entire thing sums up to one.So, this gives you a proper probability distribution.What I want you to do here is actually implementa function called softmax that performs this calculation.So, what you're going to be doing is taking the output fromthis simple neural network and has shaped 64 by 10 and pass it througha dysfunction softmax and make sure it calculatesthe probability distribution for each of the different examples that we passed in.Right? Good luck.

### 10. PyTorch V2 Part 2 Solution 2 V1-8KRX7HvqfP0.en

Welcome back. Here is my solution for the softmax function.Here in the numerator,we know we want to take the exponential,so it's pretty straight forward with torch.exp.So we're going to use the exponential of x,which is our input tensor.In the denominator, we know we want to do something like,again take exponentials so torch.exp,and then take the sum across all those values.So, one thing we need to remember is that we want the sum across one single row.So, each of the columns in one single row for each example.So, for one example, we want to sum up those values.So, for here in torch.sum,we're going to use dimension equals one.So, this is basically going to take the sum across the columns.What this does, torch.sum here,is going to actually going to give us a tensor,that is just a vector of 64 elements.So, the problem with this is that,if this is 64 by 10,and this is just a 64-long vector,it's going to try to divide every element in this tensor by all 64 of these values.So, it's going give us a 64 by 64 tensor, and that's not what we want.We want our output to be 64 by 10.So, what you actually need to do is reshape this tensor here to have 64 rows,but only one value for each of those rows.So, what that's going do,it's going look at for each row in this tensor,is going to look at the equivalent row in this tensor.So, since each row in this tensor only has one value,it's going to divide this exponential by the one value in this denominator tensor.This can be really tricky,but it's also super important to understand how broadcasting works in PyTorch,and how to actually fit all these tensors together withthe correct shape and the correct operations to get everything out right.So, if we do this, it look what we have,we pass our output through the softmax function,and then we get our probabilities,and we can look the shape and it is 64 by 10,and if you take the sum across each of the rows,then it adds up to one,like it should with a proper probability distribution.So, now, we're going to look at how you use this nn module to build neural networks.So, you'll find that it's actually in a lot of ways simpler and more powerful.You'll be able to build larger and larger neural networks using the same framework.The way this works in general,is that we're going to create a new class,and you can call it networking,you can call it whatever you want,you can call it classifier,you can call it MNIST.It doesn't really matter so much what you call it,but you need to subclass it from nn.module.Then, in the init method, it's __init method.You need to call it super and run the init method of nn.module.So, you need to do this because then,PyTorch will know to registerall the different layers andoperations that you're going to be putting into this network.If you don't do this part then,it won't be able to track the things thatyou're adding to your network, and it just won't work.So, here, we can create our hidden layers using nn.Linear.So, what this does,is it creates a operation for the linear transformation.So, when we take our inputs x and then multiply it by weights and add your bias terms,that's a linear transformation.So, what this does is calling NN.Linear,it creates an object that itself has createdparameters for the weights and parameters for the bias and then,when you pass a tensor through this hidden layer,this object, it's going to automatically calculate the linear transformation for you.So, all you really need to do is tell it what's the size of the inputs,and then what are the size of the output.So, 784 by 256,we're going to use 256 outputs for this.So, it's kind of rebuilding the network that we saw before.Similarly, we want another linear transformation between our hidden units and our output.So, again, we have 256 hidden units,and we have 10 outputs, 10 output units,so we're going to create a output layer called self.output,and create this linear transformation operation.We also want to create a sigmoid operation for the activation and then,softmax for the output,so we get this probability distribution.Now, we're going to create a forward method and so,forward is basically going to be,as we pass a tensor in to the network.It's gonna go through all these operations,and eventually give us our output.So, here, x, the argument is going to be the input tensor and then,we're going to pass it through our hidden layer.So, this is again, like this linear transformation that we defined up here,and it's going to go through a sigmoid activation,and then through our output layer or output linear transformation, we have here,and then through the sigmoid function,and then finally return the output of our softmax.so we can create this.Then, if we kind of look at it, so it'll print it out,and it'll tell us the operations, and not necessarily the order,but at least it tells us the operations that we have defined for this network.You can also use some functional definitions for things like sigmoid and softmax,and it kind of makes the class the way you write the code a little bit cleaner.We can get that from torch.nn.functional.Most of the time, you'll see is like import torch.nn.functional as capital F. So,there's kind of that convention in PyTorch code.So, again, we define our linear transformations,self.hidden, self.output but now in our forward method.So, we can call self.hidden to get like our values for hidden layer, but then,we pass it through the sigmoid function,f.sigmoid, and the same thing with the output layers.So, we have our output linear transformations of the output,and we pass it through this softmax operation.So, the reason we can do this because,when we create these linear transformations,it's creating the weights and bias matrices on its own.But for sigmoid and softmax,it's just an element wise operation,so it doesn't have to create any extra parametersor extra matrices to do these operations,and so we can have these be purely functionalwithout having to create any sort of object or classes.However, they are equivalent.So this way to build the network is equivalent to this way up here,but it's a little bit more succinct when you'redoing it with these kind of functional pattern.So far, we've only been using the sigmoid function as an activation function,but there are, of course,a lot of different ones you want to use.Really the only requirement is that,these activation functions should typically be non-linear.So, if you want your network to be able to learn non-linear correlations and patterns,and we want the output to be non-linear,then you need to use non-linear activation functions in your hidden layers.So, a sigmoid is one example.The hyperbolic tangent is another.One that is pretty much used all the time,like almost exclusively as activation function and hidden layers,is the ReLU, so the rectified linear unit.This is basically the simplest non-linear function that you can use,and it turns out that networks tend to train a lot fasterwhen using ReLU as compared to sigmoid and hyperbolic tangent,so ReLU was what we typically use.Okay. So, here, you're going to build your own neural network, that's larger.So, this time, it's going to have two hidden layers,and you'll be using the ReLU activation function for this on your hidden layers.So using this object-oriented class method within a.module,go ahead and build a network that looks like this,with 784 input units,a 128 units in the first hidden layer,64 units and the second hidden layer,and then 10 output units. All right. Cheers.

### 11. PyTorch V2 Part 3 V1-9ILiZwbi9dA.en

Hello, everyone, and welcome back.So, in this video and in this notebook,I'll be showing you how to actually train neural networks in PyTorch.So, previously, we saw how to define neural networks in PyTorch using the nn module,but now we're going to see how we actually takeone of these networks that we defined and train it.So, what I mean by training is that we're going to useour neural networks as a universal function approximator.What that means is that,for basically any function,we have some desired input for example,an image of the number four,and then we have some desired output of this function.In this case a probability distribution thatis telling us the probabilities of the various digits.So, in this case,if we passed it in image four,we want to get out a probability distribution where there'sa lot of probability in the digit four.So, the cool thing about neural networks is that ifyou use non-linear activations and thenyou have the correct dataset of these images that are labeled with the correct ones,then basically you pass in an image and the correct output,the correct label or class,and eventually your neural network will build to approximate this function that isconverting these images into this probability distribution, and that's our goal here.So, basically we want to see how in PyTorch,we can build a neural network and then we're going to give it the inputs and outputs,and then adjust the weights of that network so that it approximates this function.So, the first thing that we need for that is what is called a loss function.So, it's sometimes also called the cost,and what this is it's a measure of our prediction error.So, we pass in the image of a four and thenour network predicts something else that's an error.So, we want to measure how far away our networks prediction is from the correct label,and we do that using loss function.So, in this case, it's the mean squared error.So, a lot of times you'll use this in regression problems,but use other loss functions and classification problems like this one here.So, the loss depends onthe output of our network or the predictions our network is making.The output of a network depends on the weight.So, like the network parameters.So, we can actually adjust our weights such that this loss is minimized,and once the loss is minimized,then we know that our network is making as good predictions as it can.So, this is the whole goal to adjust our network parameters to minimize our loss,and we do this by using a process called gradient descent.So, the gradient is the slope of the loss function with respect to our perimeters.The gradient always points in the direction of fastest change.So, for example if you have a mountain,the gradient is going to always point up the mountain.So, you can imagine our loss function being likethis mountain where we have a high loss up here and we have a low loss down here.So, we know that we want to get to the minimum of our loss when we minimize our loss,and so, we want to go downwards.So, basically, the gradient points upwards and so,we just go the opposite direction.So, we go in the direction of the negative gradient,and then if we keep following this down,then eventually we get to the bottom of this mountain, the lowest loss.With multilayered neural networks,we use an algorithm called backpropagation to do this.Backpropagation is really just an application of the chain rule from calculus.So, if you think about it when we pass in some data,some input into our network,it goes through this forward pass through the network to calculate our loss.So, we pass in some data,some feature input x and then it goes throughthis linear transformation which depends on our weights and biases,and then through some activation function like a sigmoid,through another linear transformation with some more weights and biases,and then that goes in,from that we can calculate our loss.So, if we make a small change in our weights here, W1,it's going to propagate through the network andend up like results in a small change in our loss.So, you can think of this as a chain of changes.So, if we change here, this is going to change.Even that's going to propagate through here,it's going to propagate through here, it's going to propagate through here.So, with backpropagation, we actually use these same changes,but we go in the opposite direction.So, for each of these operations like the loss andthe linear transformation into the sigmoid activation function,there's always going to be some derivative,some gradient between the outputs and inputs, and so,what we do is we take each of the gradients forthese operations and we pass them backwards through the network.Each step we multiply the incoming gradient with the gradient of the operation itself.So, for example just starting at the end with the loss.So, we pass this gradient or the loss dldL2.So, this is the gradient of the loss with respect to the second linear transformation,and then we pass that backwards again and if we multiply it by the loss of this L2.So, this is the linear transformation with respect tothe outputs of our activation function,that gives us the gradient for this operation.If you multiply this gradient by the gradient coming from the loss,then we get the total gradient for both of these parts,and this gradient can be passed back to this softmax function.So, as the general process for backpropagation, we take our gradients,we pass it backwards to the previous operation,multiply it by the gradient there,and then pass that total gradient backwards.So, we just keep doing that through each of the operations in our network,and eventually we'll get back to our weights.What this does is it allows us to calculatethe gradient of the loss with respect to these weights.Like I was saying before,the gradient points in the direction of fastest change in our loss,so, to maximize it.So, if we want to minimize our loss,we can subtract the gradient off from our weights,and so, what this will do is it'll give us a new set of weightsthat will in general result in a smaller loss.So, the way that backpropagation algorithm works is that it willmake a forward pass through a network, calculate the loss,and then once we have the loss, we can gobackwards through our network and calculate the gradient,and get the gradient for a weights.Then we'll update the weights.Do another forward pass,calculate the loss, do another backward pass, update the weights,and so on and so on and so on,until we get sufficiently minimized loss.So, once we have the gradient and like I was saying before,we can subtract it off from our weights,but we also use this term Alpha which is called the learning rate.This is basically just a way to scale our gradients so that we're nottaking too large steps in this iterative process.So, what can happen if you're update steps are too large,you can bounce around in this trough aroundthe minimum and never actually settle in the minimum of the loss.So, let's see how we can actually calculate losses in PyTorch.Again using the nn module,PyTorch provides us a lot of different losses including the cross-entropy loss.So, this loss is what we'll typically use when we're doing classification problems.In PyTorch, the convention is to assign our loss to its variable criterion.So, if we wanted to use cross-entropy,we just say criterion equals nn.crossEntropyLoss and create that class.So, one thing to note is that,if you look at the documentation for cross-entropy loss,you'll see that it actually wants the scoreslike the logits of our network as the input to the cross-entropy loss.So, you'll be using this with an output such as softmax,which gives us this nice probability distribution.But for computational reasons,then it's generally better to use the logits which arethe input to the softmax function as the input to this loss.So, the input is expected to be the scoresfor each class and not the probabilities themselves.So, first I'm going to import the necessary moduleshere and also download our data and create it in,like you've seen before, as a trainloader,and so, we can get our data out of here.So, here I'm defining a model.So, I'm using nn.Sequential, and if you haven't seen this,checkout the end of the previous notebook.So, the end of part two,will show you how to use nn.Sequential.It's just a somewhat more concise way to define simple feed-forward networks, and so,you'll notice here that I'm actually only returning the logits,the scores of our output function and not the softmax output itself.Then here we can define our loss.So, criterions equal to nn.crossEntropyLoss.We get our data with images and labels,flatten it, pass it through our model to get the logits,and then we can get the actual loss by bypassing in our logits and the true labels,and so, again we get the labels from our trainloader.So, if we do this, we see we have calculated the loss.So, my experience, it's more convenient to build your modelusing a log-softmax output instead of just normal softmax.So, with a log-softmax output to get the actual probabilities,you just pass it through torch.exp. So, the exponential.With a log-softmax output,you'll want to use the negative log-likelihood loss or nn.NLLLoss.So, what I want you to do here is builda model that returns the log-softmax as the output,and calculate the loss using the negative log-likelihood loss.When you're using log-softmax,make sure you pay attention to the dim keyword argument.You want to make sure you set it right so that the output is what you want.So, go and try this and feel free to check out my solution.It's in the notebook and also in the next video,if you're having problems. Cheers.

### 12. PyTorch V2 Part 3 Solution V2-zBWlOeX2sQM.en

Hi and welcome back. Here's my solution for this model that uses a LogSoftmax output.It is a pretty similar to what I built before with an index sequential.So, we just use a linear transformation,ReLU, linear transformation, ReLU,another linear transformation for output and then we can pass thisto our LogSoftmax module.So, what I'm doing here is I'm making sure I setthe dimension to one for LogSoftmax and thismakes it so that it calculates the function across the columns instead of the rows.So, if you remember, the rows correspond to our examples.So, we have a batch of examples that we're passingto our network and each row is one of those examples.So, we want to make sure that we're coveringthe softmax function across each of our examples andnot across each individual feature in our batches.Here, I'm just defining our loss or criterion asthe negative log likelihood loss andagain get our images and labels from our train loader,flatten them, pass it through our model to get the logits.So, this is actually not the largest anymore,this is like a log probability,so we call it like logps,and then you do that.There you go. You see we get our nice loss.Now, we know how to calculate a loss,but how do we actually use it to perform backpropagation?So, PyTorch towards actually has this really great module calledAutograd that automatically calculates the gradients of our tensors.So, the way it works is that,PyTorch will keep track of all the operations you do ona tensor and then when you can tell it to do a backwards pass,to go backwards through each ofthose operations and calculate the gradients with respect to the input parameters.In general, you need to tell PyTorch that you want to use autograd on a specific tensor.So, in this case, you would create some tensor like x equals torch.zeros,just to make it a scalar,say one and then give it requires grad equals true.So, this tells PyTorch to track the operations on this tensor x,so that if you want to get the gradient then it will calculate it for you.So, in general, if you're creating a tensor andyou don't want to calculate the gradient for it,you want to make sure this is set to false.You can also use this context torch.no grad to make sureall the gradients are shut off for all ofthe operations that you're doing while you're in this context.Then, you can also turn on or off gradients globally withtorch.set grad enabled and give it true or false, depending on what you want to do.So, the way this works in PyTorch is that you basically create your tensor and again,you set requires grad equals true and then you just perform some operations on it.Then, once you are done with those operations, you type in.backwards.So, if you use x, this tensor x,then calculate some other tensor z then if you do z.backward,it'll go backwards through your operations and calculate the total gradient for x.So, for example, if I just create this random tensor,random two-by-two tensor, and then I can square it like this.What it does, you can actually see if you look at y,so y is our secondary or squared tensor.If you look at y.grad function,then it actually shows us that this grad function is a power.So, PyTorch just track this and itknows that the last operation done was a power operation.So, now, we can take the mean of y and get another tensor z.So, now this is just a scalar tensor, we've reduced y,y is a two-by-two matrix,two by two array and then we take in the mean of it to get z.Ingredients for tensor show up in this attribute grad,so we can actually look at what's the gradient of our tensor x right now,and we've only done this forward pass,we haven't actually calculated the gradient yet and so it's just none.So, now if we do z.backward,it's going to go backwards through this tiny little set of operations that we've done.So, we did a power and then a mean and let's gobackwards through this and calculate the gradient for x.So, if you actually work out the math,you find out that the gradient of z with respect to x shouldbe x over two and if we look at the gradient,then we can also look at x divided by two then they are the same.So, our gradient is equal to what it should be mathematically,and this is the general process for working with gradients,and autograd, and PyTorch.Why this is useful,is because we can use this to get our gradients when we calculate the loss.So, if remember, our loss depends on our weight and bias parameters.We need the gradients of our weights to do gradient descent.So, what we can do is we can set up our weights astensors that require gradients and then do a forward pass to calculate our loss.With the loss, you do a backwards pass which calculates the gradients for your weights,and then with those gradients, you can do your gradient descent step.Now, I'll show you how that looks in code.So, here, I'm defining our model like I did before with LogSoftmax output,then using the negative log-likelihood loss,get our images and labels from our train loader, flatten it,and then we can get our log probabilities fromour model and then pass that into our criterion,which gives us the actual loss.So, now, if we look at our models weights,so model zero gives us the parameters for this first linear transformation.So, we can look at the weight and then we can look at the gradient,then we'll do our backwards pass starting fromthe loss and then we can look at the weight gradients again.So, we see before the backward pass,we don't have any because we haven't actuallycalculated it yet but then after the backwards pass,we have calculated our gradients.So, we can use these gradients in gradient descent to train our network. All right.So, now you know how to calculate losses and you knowhow to use those losses to calculate gradients.So, there's one piece left before we can start training.So, you need to see how to use those gradients to actually update our weights,and for that we use optimizers and these come from PyTorch's Optim package.So, for example, we can use stochastic gradient descent with optim.SGD.The way this is defined is we importthis module optim from PyTorch and then we'd say optim.SGD,we give it our model parameters.So, these are the parameters that we wantthis optimizer to actually update and then we give it a learning rate,and this creates our optimizer for us.So, the training pass consists of four different steps.So, first, we're going to make a forward pass through the networkthen we're going to use that network output to calculate the loss,then we'll perform a backwards pass throughthe network with loss.backwards and this will calculate the gradients.Then, we'll make a step with our optimizer that updates the weights.I'll show you how this works with one training step and then you're goingto write it up for real and a loop that is going to train a network.So, first, we're going to start by getting our imagesand labels like we normally do fromour train loader and then we're going to flatten them.Then next, what we want to do is actually clear the gradients.So, PyTorch by default accumulates gradients.That means that if you actually domultiple passes and multiple backwards like multiple forward passes,multiple backwards passes, and you keep calculating your gradient,it's going to keep summing up those gradients.So, if you don't clear gradients,then you're going to be getting gradients from the previous training step inyour current training step and it's going to end up whereyour network is just not training properly.So, for this in general,you're going to be calling zero grad before every training passes.So, you just say optimizer.zero grad and this will just cleanout all the gradients and all the parameters in your optimizer,and it'll allow you to train appropriately.So, this is one of the things in PyTorch that is easy to forget,but it's really important.So, try your hardest to remember to do this part,and then we do our forward pass,backward pass, then update the weights.So, we get our output,so we do a forward pass through our model with our images,then we calculate the loss using the output of the model and our labels,then we do a backwards pass and then finally we take an optimizer step.So, if we look at our initial weights,so it looks like this and then we can calculate our gradient,and so the gradient looks like and then ifwe take an optimizer step and update our weights,then our weights have changed.So, in general, what has worked is you're going to be loopingthrough your training set and then for each batch out of your training set,you'll do the same training pass.So, you'll get your data,then clear the gradients,pass those images or your input through your network to get your output, from that,in the labels, calculate your loss,and then do a backwards pass on the loss and then update your weights.So, now, it's your turn to implement the training loop for this model.So, the idea here is that we're going to be looping through our data-set,so grabbing images and labels from train loader and then on each of those batches,you'll be doing the training pass,and so you'll do this pass where you calculate the output of the network,calculate the loss, do backwards pass on loss,and then update your weights.Each pass through the entire training set is called anepoch and so here I just have it set for five epochs.So, you can change this number if you want to go more or less.Once you calculate the loss,we can accumulate it to keep track,so we're going to be looking at a loss.So, this is running loss and so we'll beprinting out the training loss as it's going along.So, if it's working,you should see the loss start falling,start dropping as you're going through the data.Try this out yourself and if you need some help,be sure to check out my solution. Cheers.

### 13. PyTorch V2 Part 3 Solution 2 V1-ExyFG2MjsKs.en

Hi again. So, here's my solution for the train pass that I had you implement.So, here we're just defining our model like normal andthen our negative log-likelihood loss usingstochastic gradient descent and pass in our parameters.Then, here is our training pass.So, for each image in labels in trainloader,we're going to flatten it and then zero out the gradients using optimizer. zero_ grad.Pass our images forward through the model and the output and then from that,we can calculate our loss and then doa backward pass and then finally with the gradients,we can do this optimizer step.So, if I run this and we wait a little bit for to train,we can actually see the loss dropping over time, right?So, after five epochs,we see that the first one,it starts out fairly high at 1.9 but after five epochs,continuous drop as we're training and we see it much lower after five epochs.So, if we kept training then our network would learn the databetter and better and the training loss would be even smaller.So, now with our training network,we can actually see what our network thinks it's seen in these images.So, for here, we can pass in an image.In this case, it's the image of a number two andthen this is what our network is predicting now.So, you can see pretty easily that it's putting most of the probability,most of its prediction into the class for the digit two.So we try it again and put in passes in number eight and again, it's predicting eight.So, we've managed to actually trainour network to make accurate predictions for our digits.So next step, you'll write the code for traininga neutral network on a more complex dataset and you'll be doing the whole thing,defining the model, running the training loop, all that. Cheers.

### 14. PyTorch - Part 4-AEJV_RKZ7VU.en

Welcome back. So, in this notebook,you'll be building your own neural network to classify clothing images.So, like I talked about in the last video,MNIST is actually a fairly trivial dataset these days.It's really easy to get really high accuracy with a neural network.So instead, you're going to be using Fashion-MNIST,and this is basically just a drop-in replacement for MNISTso we have 28 by 28 grayscale images,but this time it's clothing.So, you have a lot more variation in the classes,and it just ends up beinga much more difficult problem to classify like there's a t-shirt,there's pants, there's a sweater,there's shoes instead of handwritten digits.So it's a better representation of datasets that you'd use in the real world.So, I've left this up to you to actually build a network and train it.So here you can define your network architecture,then here you will create your network todefine the criterion and optimizer and then write the code for the training pass.Once you have your network built and trained,you can test out your network.So here, you'd want to do a forward pass, get your logits,calculate the class probabilities, maybe output of your network,and then pass in one of these images fromthe test set and check out if your network can actually predict it correctly.If you want to see my solution, it's in the next notebook, part five,and you'll also see it in the next video. Cheers.

### 15. PyTorch V2 Part 4 Solution V1-R6Y4hPLVQWM.en

Again.So, in the last video,I hand you try out buildingyour own neural network to classify this fashion in this dataset.Here is my solution like how I decided to build this.So first, building our network.So, here, I'm going to import our normal modules from PyTorch.So, nn and optim, so,nn is going to allow us to build our network,and optim is going to give us our optimizers.I must going to import this functional modules, so,we can use functions like ReLU and log softmax.I decided to define my network architectures using the class.So, in nn.modules subclassing from this,and it's called a classifier.Then I created four different linear transformations.So, in this case, it's three hidden layers and then one output layer.Our first hidden layer has 256 units.The second hidden layer has a 128,one after that has 64.Then our output has 10 units.So, in the forward pass, I did something a little different.So, I made sure here that the input tensor is actually flattened.So now, you don't have to flatten your input tensors in the training loop,it'll just do it in the forward pass itself.So, to do this is do x.view,which is going to change our shape.So, x.shape zero is going to give us our batch size.Then the negative one here is going to basically fill outthe the second dimension with as many elements as itneeds to keep the same total number of elements.So, what this does is it basically gives us another tensor,that is the flattened version of our input tensor.It doing a pass these through our linear transformations,and then ReLU activation functions.Then finally, we use a log softmax with a dimension set to one,as our output, and return that from our forward function.With the model defined,I can do model equals classifiers.So, this actually creates our model.Then we define our criterion with the negative log likelihood loss.So, I'm using log softmax as the output my model.So, I want to use the NLLLoss as the criterion.Then, here I'm using the Adam optimizer.So, this is basically the same as stochastic gradient descent,but it has some nice properties where it usesmomentum which speeds up the actual fitting process.It also adjust the learning rate for each of the individual parameters in your model.Here, I wrote my training loop.So, again I'm using five epochs.So, for e in range epoch, so,this is going to basically loop through our dataset five times,I'm tracking the loss with running loss,and just kind of instantiated it here.Then, getting our images.So, from images labels in train loader,so I get our log probabilities by passing in the images to a model.So, one thing to note, you can kind of do a little shortcut.If you just pass these in to model as if it was a function,then it will run the forward method.So, this is just a kind of a shorter way to run the forward pass through your model.Then with the log probabilities and the labels,I can calculate the loss.Then here, I am zeroing the gradients.Now I'm doing the lost up backwards to calculating our gradients,and then with the gradients, I can do our optimizer step.If we tried it, we can see at least forthese first five epochs that are loss actually drops.Now the network is trained,we can actually test it out.So, we pass in data to our model, calculate the probabilities.So, here, doing the forward pass through the model toget our actual log probabilities and with the log probabilities,you can take the exponential to get the actual probabilities.Then with that, we can pass it into this nice little view classify function that I wrote,and it shows us,if we pass an image of a shirt,it tells us that it's a shirt.So, our network seems to have learned fairly well,what this dataset is showing us.

### 16. PyTorch V2 Part 5 V1 (1)-XACXlkIdS7Y.en

Hey there. So now,we're going to start talking about inference and validation.So, when you have your trained network,you typically want to use it for making predictions.This is called inference,it's a term borrowed from statistics.However, neural networks have a tendency to perform too well onyour training data and they aren't able togeneralize the data that your network hasn't seen before.This is called overfitting.This happens because as you're training more and more and more on your training set,your network starts to pick up correlations andpatterns that are in your training set but they aren'tin the more general dataset of all possible handwritten digits.So, to test for overfitting,we measure the performance of the network on data that isn't in the training set.This data is usually called the validation set or the test set.So, while we measure the performance on the validation set,we also tried to reduce overfitting through regularization such as dropout.So, in this notebook,I'll show you how we can both look atour validation set and also use dropout to reduce overfitting.So, to get the training set for your data like from PyTorch,then we say train equals true and for fashionMNIST.To get our test set,we're actually going to set train equals false here.Here, I'm just defining the model like we did before.So, the goal of validation is to measureour model's performance on data that is not part of our training set.But what we mean by performance is up to you,up to the developer, the person who's writing the code.A lot of times, it'll just be the accuracy.So, like how many correct classifications didour model make compared to all of the predictions?And other options for metrics are precision and recall,and the top five error rate.So here, I'll show you how to actually measure the accuracy on the validation set.So first, I'm going to do a forward pass that is one batch from the test set.So, see in our test set we get our probabilities.So, just 64 examples in a batch.Then 10 columns like one for each of the classes.So, the accuracy, we want to see if our model madethe correct prediction of the class given the image.The prediction we can consider it to be whichever class has the highest probability.So, for this, we can use this top-k method on our tensors.This returns the k highest values.So, if we pass in one,then this is going to give us the one highest value.This one highest value is the most likely class that our network is predicting.So, for the first ten examples,and this batch of test data that I grabbed,we see that the class four and class five are what are being predicted for these.So, remember that this network actually hasn't been trained yet,and so it's just making these guesses randomly becauseit doesn't really know anything about the data yet.So, top-k actually returns a tuple with two tensors.So, the first tensor is the actual probability values,and the second tensor are the class indices themselves.So typically, we just want this top class here.So, I'm calling top-k here and I'm separating out the probabilities in the classes.So, we'll just use this top class going forward.So, now that we have the predicted classes from our network,we can compare that with the true labels.So, we say, we can say like top class equals equals labels.The only trick here is that we need to makesure our top class tensor and the labels tensor has the same shape.So, this equality actually operates appropriately like we expect.So, labels from the test loader is actually a 1D tensor with 64 elements,but top class itself is a 2D tensor, 64 by one.So here, I'm just like changing the shape of labels to match the shape of top class.This gives us this equals tensor.We can actually see it looks like.So, it gives us a bunch of zeros and ones.So, zeros are where they don't match,and then ones are where they do match.Now, we have this tensor that's all just a bunch of zeros and ones.So, if we want to know the accuracy, right?We can just sum up all the correct things,all the correct predictions,and then divide by the total number of predictions.If you're tensor is all zeros and ones,that's actually equivalent to taking the mean.So for that, we can do torch.mean,but the problem is that equals is actually a byte tensor,and torch.mean won't work on byte tensors.So, we actually need to convert equals until a float tensor.If we do that, then we can actually see our accuracy forthis one particular batch is 15.6 percent.So, this is roughly what we expect.So, our network hasn't been trained yet.It's making pretty much random guesses.That means that we should see our accuracy be about one in ten forany particular image because it's just uniformly guessing one of the classes, okay?So here, I'm going to have you actually implement this validation loop,where you'll pass in data fromthe test set through the network and calculate the loss and the accuracy.So, one thing to note, I think I mentioned this before.For the validation paths, we're not actually going to be doing any training.So, we don't need the gradients.So, you can actually speed up your code a little bit if you turn off the gradients.So, using this context,so with torch.no_grad, then you can put your validation pass in here.So, for images and labels in your test loader and then do the validation pass here.So, I've basically built a classifier for you, set all this up.Here's the training pass, and then it's up to you to implement the validation pass,and then print out the accuracy.All right. Good luck,and if you get stuck or want any help,be sure to check out my solution.

### 17. PyTorch V2 Part 5 Solution V1-AjrXltxqsK4.en

Welcome back. So, here's my solution for the validation pass.So, here, our model has been defined,our loss, and our optimizer, and all this stuff.I've set to 30 epochs so we can see like how this trains or how the training loss drops,and how the validation loss changes over time.The way this works is that after each pass,after each epoch, after each pass through the training set,then we're going to do a validation pass.That's what this else here means.So, basically, four of this stuff and thenelse basically says after this four loop completes,then run this code.That's what this else here means.As seen before, we want to turn off our gradients so with torch.no_grad,and then we're going to get our images and labels from a test set,pass it into the images into our model to get our log probabilities, calculate our loss.So, here, I am going to just be updating our test_loss.So, test_loss is just an integer that's going to countup our loss on our test set as we're training,as we're doing more of these validation passes.So, this way, we can actually trackthe test loss over all the epochs that we're training.So, from the law of probabilities,I can get our actual probability distributions using torch.exponential.Again, topk1 gives us our predicted classes and we can measure,we can calculate the equalities.So, here, we can get our probabilities from our log probabilities using torch.exp,taking the exponential of a log gives you back into the probabilities.From that, we do ps.topk, so one,and this gives us the top_class or predicted class from the network.Then, using checking for equality,we can see where our predicted classes match with the true classes from labels.Again, measure our calculator accuracy.So, using torch.mean and changing equals into a FloatTensor.So, I'm going to run this and then let it run for a while,and then we can see what the actual trainingand validation losses look like as we were training this network.Now, the network is trained, we can see howthe validation loss and the training loss actuallychanged over time like as we continue training on more and more data.So, we see is the training loss drops butthe validation loss actually starts going up over time.There's actually a clear sign of overfittingso our network is getting better and better and better on the training data,but it's actually starting to get worse on the validation data.This is because as it's learning the training data,it's failing to generalize the data outside of that.Okay. So, this is what the phenomenon of overfitting looks like.The way that we combine it,the way that we try to avoid this and prevent it is byusing regularization and specifically, dropout.So, deep behind dropout is that we randomly drop input units between our layers.What this does is it forces the network to share information between the weights,and so this increases this ability to generalize to new data.PyTorch adding dropout is pretty straightforward.We just use this nn.Dropout module.So, we can basically create our classifier like we had beforeusing the linear transformations to do our hidden layers,and then we just add self.dropout,nn.Dropout, and then you give it some drop probability.In this case, this is 20 percent,so this is the probability that you'll drop a unit.In the forward method, it's pretty similar.So, we just pass in x,which is our input tensor,we're going to make sure it's flattened,and then we pass this tensor through each ofour fully connected layers into an activation,relu activation and then through dropout.Our last layer is the output layer so we're not going to use dropout here.There's one more thing to note about this.So, when we're actually doing inference,if we're trying to make predictions with our network,we want to have all of our units available, right?So, in this case,we want to turn off dropout when we're doing validation,testing, when we're trying to make predictions.So, to do that, we do model.eval.So, model.eval will turn off dropout and this will allow us to get the most power,the highest performance out of our network when we're doing inference.Then, to go back in the train mode, use model.train.So, then, the validation pass looks like this now.So, first, we're going to turn off our gradients.So, with torch.no_grad, and then we set our model to evaluation mode,and then we do our validation pass through the test data.Then, after all this,we want to make sure the model is set back to train mode so we do model.train.Okay. So, now, I'm going to leave it up to you to create your new model.Try adding dropout to it and then try training your model with dropout.Then, again, checkout the training progress of validation using dropout. Cheers.

### 18. PyTorch V2 Part 5 Solution 2 V1-3Py2SbtZLbc.en

Hi. Here's my solution for your building and training this network using dropout now.Just like I showed you before,we can define our dropout module asself.dropout and then nn.dropout to give it some drop probability.So, in this case, 20 percent,and then just adding it to our forward method now on each of our hidden layers.Now, our validation code looks basically the same asbefore except now we're using model.eval.So, again, this turns our model intoevaluation or inference mode which turns off dropout.Then, like the same way before,we just go through our data and the test say,calculate the losses and accuracy and after all that,we do model.train to set the model back into train mode,turn dropout back on,and then continue on in train smart.So, now, we're using dropout and if you look atagain the training loss and the validation loss over these epochs that we're training,you actually see that the validation loss sticks alot closer to the train loss as we train.So, here, with dropout, we've managed to at least reduce overfitting.So, the validation losses isn't as low as we got without dropout being is still,you can see that it's still dropping.So, if we kept training for longer,we would most likely manage to get our validation loss lower than without dropout.

### 19. PyTorch - Part 6-3ZJfo2bR-uw.en

Welcome back. So, in this video,I'm going to be showing you how to save and load models that you've trained with PyTorch.Again, this is important because most of the time you'll wantto train your network on some data and then just save it to disk,and then you can later load it up and train moreor use it for inference, like making predictions.So I've already gone ahead and ran most of this codes,trained my network on fashion hymnist.Again, we see we get about an accuracy of 84-85 percent.Now that the network is trained,we actually don't want to take these weights,never really learned this problem,and we want to save them so that we can in the future load them back up and use it again.These weights, these parameters,are actually stored in model.state_dict.So we print this out, so we can see our network here.So we have two hidden layers.So, this one goes to 500, 500 to 100,and then this goes to our output layer of 10 output units,and then we have drop out.So this is what our network looks like.If we look at the state_dict keys,this is showing us we have hidden layers at zero weight. So that's here.Hidden layers gives us a bias,the second hidden layer gives us the weightand also the weights and bias for our output layer.So that the tensors that for our weights andbiases are actually stored and model.state_dict.So this is what typically you want to save.So to save the state_dict, it's pretty simple.So use the torch.save,and then pass in your model.state_dict and then call it checkpoint.pth.There you go. It's saved.So once it's saved, we can then load in the state_dict.So state_dict is equal to torch.load.Let's give it the same thing, checkpoint.pth,and then one p to state_dict,then we can print this out just see.keys.So it gives us the same layers that we saw up here, so in layers.0.weight.So, we all see the same thing.But the state_dict, it's self loaded.How do we actually load this into a model?So we say model.load_state_dict and we pass in our state_dict.So notice that we need to have our model existing already.We need to have it already created,and it's going to be created with randomly initialize the weights and biases.The parameters are going to be randomly initialized.Basically we just pass in our state dict and thenthose random parameters are replaced by the ones that we trained previously.This seems pretty straightforward,you just save your state_dict and you can load it backup into a model,but it's not actually all that simple,and I'll show you why.So, if I create a new network,784 at normal, 10 output units,but this time I'm going to use three hidden layers with 400,200 and 100 units each.Okay. So now, I have this pre-trained State_dict, right?So it's like, okay well I created this new model,I'm going to try to load this state_dict, and it gives me an error.So basically it says,inconsistent tensor size, expected tensor,this size but the sources is dense.Right? So basically what that means is that when Itrained this state_dict with this checkpoint,I had a different network architecture.So that in the original network I had 500 units in the first layer,and now I have 400 units in the first layer.So, the thing is that when you build your network and you train it,when you load the state_dict back up,it has to go to a model with exactly the same architecture.So if you have code that can generate networks withdifferent architectures when you load your your state_dict back up,you also have to include information about the architecture.So, for instance, here with this network since I havean arbitrary number of hidden layers,I need to include this information about the hidden layers in my checkpoint that I saved.To rebuild this model exactly as it was trained,I'm going to store the state_dict and all the information about the model architecture.Here I can say let's say, create a dictionary,given it my input size it's 784,my output size 10.So if I have a model,then I can do up features for each model.So remember here that model.hidden_layers is a list of these linear operations.So basically hidden_layers is a list of the layers in the network.So, to get the actual valuesI had set for the hidden_layers depending on the number of units,I just call out features on each of those layers.So this is basically just going through each of the layers,in this list of hidden layers and then gettingthe number of features or units in that layer.Then I can save the state_dict.Then once I have that dictionary,then I can simply save the dictionary.Checkpoint, checkpoint.pth again. There you go.Now our checkpoint has all the necessary information we need to rebuild our model.So what I'd like to do is createa function just called like load_checkpoint and give it a file path,and then we can load our checkpoint.So our checkpoint is torch.load(file path) then we can create our model.So our model is network,then we get our parameters for this model from our checkpoint.So we can put in the input size,we can put in the output size,we pass in the hidden layers.So what this does is passing inall the necessary arguments parameters to create our model.So, once we have our model,then we can load the state_dict from uh,I want to return the model.Then we can call the checkpoint, passed in checkpoint.pth.So then we load the model and we successfully loadthe state_dict and everything is present.So in general that's how you're going to save and load your networks.So, it's important to remember that everyparameter that you use for building your network,you have to also include that in your checkpoints.So you can reconstruct your model exactly the same way it was when you saved it.Right. See you in the next video. Cheers

### 20. PyTorch - Part 7-hFu7GTfRWks.en

In this video, I'll be showing you how to load image data.This is really useful for what you'll be doing in real projects.So previously, we used MNIST.Fashion-MNIST were just toy datasets for testing your networks,but you'll be using full-size images like you'd get from smartphone cameras andyour actual projects that you'll be doing with deep learning networks.So with this, we'll be using a dataset of cat and dog photos, super cute.That come from Kaggle. So, if you want to learn more about it,you can just click on this link.So, you can see our images are now much larger,much higher resolution and they're coming indifferent shapes and sizes than what we saw with MNIST and fashion-MNIST.So, the first step to using these is to actually load them in with PyTorch.Then once you have them in,you can train a network using these things.So, the easiest way to load in our image data is with datasets.ImageFolder.This is from torchvision, that datasets module.So basically, you just pass in a path to your dataset,so into the folder where your data issitting into image folder and give us some transforms,which we talked about before.I'll go into some more detail about transforms next.So, the image folder,it expects your files and directories to look like this,where you have some root directory that's where all your data.Then each of the different classes has their own folder.So in this case, we have two classes.We have dog and cat.So, we have these two folders, dog and cat.Get more classes like for MNIST,now you have ten classes.There will be one folder for each of the different digits, right?Those are our classes or labels.Then within each of the specific class folders,you have your images that belong to those classes.So, in your dog folder are going to be all ofyour dog pictures and the cat folder are going to be all of your cat pictures.So, if you're working in a workspace,then the data should already be there,but if you're working on your local computer,you can get the data by clicking here.I've also already split this into a training set and test set for you.When you load in the image folder,you need to define some transforms.So, what I mean by this is you'll want to resize it, you can crop it,you can do a lot of things like typically you'll want to convert it toa PyTorch tensor and it is loaded in as a pillow image.So, you need to change the image into a tensor.Then you combine these transforms intoa pipeline of transforms, using transforms.compose.So, if you want to resize your image to be 255 by 255,then you say transforms.resize 255 and then you take just the center portion,you just crop that out with a size of 224 by 224.Then you can convert it to a tensor.So, these are the transforms that you'll use and you pass this intoImageFolder to define the transforms that you're performing on your images.Once you have your dataset from your image folder,defining your transforms and then you pass that to dataloader.From here, you can define your batch size,so it's the number of images you get per batch like per loop throughthis dataloader and then you can also do things like set shuffle to true.So basically, what shuffle does is itrandomly shuffles your data every time you start a new epoch.This is useful because when you're training your network,we prefer it the second time it goes through to see your images in a different order,the third time it goes through you see your images in a different order.Rather than just learning in the same order every time because then this couldintroduce weird artifacts in how your network is learning from your data.So, the thing to remember is thatthis dataloader that you get from this class dataloader,the actual dataloader object itself, is a generator.So, this means to get data out of it you actually have to loop through itlike in a for loop or you need to call iter on it,to turn into an iterator.Then call next to get the data out of it.Really what's happening here in this for loop,this for images comma labels in dataloader is actually turning this into an iterator.Every time you go through a loop, it calls next.So basically, this for loop is an automatic way of doing this.Okay. So, I'm going to leave up to you is to define some transforms,create your image folder and then pass that image folder to create a dataloader.Then if you do everything right,you should see an image that looks like this.So, that's the basic way of loading in your data.You can also do what's called data augmentation.So, what this is is you want to introduce randomness into your data itself.What this can do is you can imagine if you have images,you can translate where a cat shows up and you can rotate the cat,you can scale the cat,you can crop different parts of things,you can mirror it horizontally and vertically.What this does is it helps your network generalizedbecause it's seen these images in different scales,at different orientations and so on.This really helps your network train and willeventually lead to better accuracy on your validation tests.Here, I'll let you define some transforms for training data.So here, you want to do the data augmentation thing,where you're randomly cropping and resizing and rotatingyour images and also define transforms for the test dataset.So, one thing to remember is that for testing when you're doing your validation,you don't want to do any of this data augmentation.So basically, you just want to just do a resize and center crop of your images.This is because you want your validation to be similarto the eventual like in state of your model.Once you train your data,you're going to be sending in pictures of cats and dogs.So, you want your validation set to look pretty muchexactly like what your eventual input images will look like.If you do all that correctly,you should see training examples are like this.So, you can see how these are rotated.Then you're testing examples should look like this,where they are scaled proportionally and they're not rotated.Once you've loaded this data,you should try to build a network based on what you've alreadylearned that can then classify cats and dogs from this dataset.I should warn you this is actually a pretty tough challenge and it probably won't work.So, don't try too hard at it.Before you used MNIST and fashion-IMNIST.Those are very simple images, right?So, there are 20 by 28.They only have grayscale colors.But now these cat and dog images, they're much larger.Their colors, so you have those three channels.Just in general, it's going to be very difficult to builda classifier that can do this just using this fully connected network.The next part, I'll show you how to use a pre-trained network to build a model thatcan actually classify these cat and dog images. Cheers.

### 21. PyTorch V2 Part 7 Solution V1-d_NhvI1yEf0.en

Hello, and welcome back.So, here are my solutions for the exercises I had you do on loading image data.Here, I had you define some transforms and then load the actual dataset withimage folder and then turn that into a data loaderusing this torch utils data loader class.So, here, I chose a couple transforms.So, first, I'm resizing the images to be 255 by 255 squares.So, basically, even if your image is actually a rectangle,then this will resize it to be square with 255 pixels on each size.The first transform I used was resize.So, this resizes your images to be squares with 255 pixels on each side.So, even if your original image is a rectangle,this will change it into a square.Then, I did a center crop with 224 pixels.So, this crops a square out of the center of the image with 224 pixels on each side.Then, I convert it into a tensor which we can then use in our networks.With the transform defined,we can pass that into this image folder and alongwith the path to our dataset and that creates a dataset object.Then, with the dataset object,we can pass that to data loader.So, this will give us back a generator were we actually can get our images and labels.So, here, I just chose a batch size of 32 and this shuffle set to true.So, basically, every time you loop through the generator again like multiple times,every time you do that,it'll randomly shuffle the images and labels.So, that loaded, here's what it looks like.We have a nice little dogs now here.So, here, I had you define transforms for our training data and our testing data.So, like I was saying before,with training data, you typically want to do data augmentation.So, that means rotating it,resizing it, flipping it, et cetera,to create this simulated dataset of more images than we actually have.Firstly, it just gives you more data to actually train with.But secondly, it helps the network generalize to images that aren't in the training set.So, my transformations here,I first chose to do a random rotation with 30 degrees.So, this is going to rotate in either direction up to 30 degrees.Then, I did a random resize crop.So, this is going to randomly resize the image and then takea crop from the center of 224 pixels square.Then, after that crop, then it do a random horizontal flip.So, it's going to mirror it horizontally and change it to a tensor.Then, with the test transforms kind of the same as before resizeit to 255 pixels and then do a center crop 224,and it finally change it to a tensor.Then, here with the train data and test data,we can pass our data directories and our transforms through this image folder.I should actually load the data,and then give our loaded data to our data loaders to actually getour load our datasets so that we can see data from the train loader,it looks like this, and we can see data from our test loader, so like that.

### 24. PyTorch - Part 8-S9F7MtJ5jls.en

Hello everyone, welcome back.So in this network, we will be using a pre-trained network to solvethis challenging problem of creating a classifier for your cat and dog images.These pre-trained networks were trained on ImageNet which isa massive dataset of over one million labeled images from 1,000 different categories.These are available from torchvision and this module, torchvision.models.And so, we see we have six different architectures that we can use,and here's a nice breakdown of the performance of each of these different models.So, AlexNet gives us the top one error and the top five error.So basically, as you see,some of these networks and these numbers here,19, 11, 34, and so on,they usually indicate the number of layers in this model.So, the larger this number,the larger the model is.And accordingly, the larger the model is,you get better accuracy,you get lower errors.At the same time,again, the larger the model is,the longer it's going to take to compute your predictions and to train and all that.So when you're using these,you need to think about the tradeoff between accuracy and speed.So, all these networks use an architecture called convolutional layers.What these do, they exploit patterns and regularities in images.I'm not going to get into the details but if you want to learn more about them,you can watch this video.So we're saying, these deep learning networks are typically very deep.So that means, they have dozens or even hundreds of different layers,and they were trained on this massive ImageNet dataset.It turns out that they were astonishingly wellas future detectors for images that they weren't trained on.So using a pre-trained network like this ona training set that it hasn't seen before is called transfer learning.So basically, what's learned fromthe ImageNet dataset is being transferred to your dataset.So here, we're going to use transfer learning to trainour own network to classify our cat and dog photos.What you'll see is you'll get really good performance with very little work on our side.So again, you can download these models from torchvision.models,this model here, so we can include this in our imports, right here.Most of these pre-trained models require a 224 by 224 image as the input.You'll also need to match a normalizationused when these models were trained on ImageNet.So when they train these models,each color channel and images were normalized separately.And you can see the means here and the standard deviations here.So, I'm going to leave it up to you to definethe transformations for the training data and the testing data now.And if you're done, we can get to a new one.Now, let's see how we can actually load in one of these models.So here, I'm going to use the Densenet-121 model.So you see, it has very high accuracy onthe ImageNet dataset and it's one 121 tells us that it has 121 layers.To load this in our code and use it,so we just say model models.densenet121 and then we say pretrained equals true.So this is going to download the pre-trained network,the weights, the parameters themselves,and then load it into our model.So now, we can do that and then we can look at what the architecture of this model.And this is what our DenseNet architecture looks like.So, you'll notice that we have this features part here and then a bunch of these layer.So this is like a convolutional layer which again I'm not going to talk abouthere but you don't really need to understand it to be able to actually use this thing.There's two main parts that we're interested in.So firstly, again, this features part,but then if we scroll all the way to the bottom,we also see this classifier part.So we see here is that we have the classifier.This has been defined as a linear combination layer,it's a fully connected dense layer,and it has 1,024 input features and then 1,000 output features.So again, the ImageNet dataset has 1,000 different classes.And so, the the number of outputs ofthis network should be 1,000 for each of those classes.So, the thing to know is that this whole thing was trained on ImageNet.Now, the features will work forother datasets but the classifier itself has been trained for ImageNet.So this is the part that we need to retrain, the classifier.We want to keep the feature part static.We don't want to update that,but we just need to update the classifier part.So then, the first thing we need to do is freeze our feature parameters.To do that, we go through our parameters in our model.And then, we just say, requires_grad equals false.So what this will do is that when we run our tensors through the model,it's not going to calculate the gradients.It's not going to keep track of all these operations.So firstly, this is going to ensure thatour our feature parameters don't get updated but italso will speed up training becausewe're not keeping track of these operations for the features.Now, we need to replace the classifier with our own classifier.So here, I'm going to use a couple of new things.I'm going to use the sequential module available from PyTorch.And so, what this does, you basically just give ita list of different operations you want todo and then it will automatically pass a tensor through them sequentially.So, you can pass in an ordered dict to name each of these layers.So I'll show you how this works.So we want a fully connected layer,so I'll just name it FC1,and then that is a fully connected layer coming from1,024 inputs and I'm going to say 500 for this hidden layer.And then we want to pass this through ReLu activation andthen this should go throughanother fully connected layer and this will be our output layer.So, 500 to two,so we have cat and dog,so we want two outputs here.And finally, our output is going to be the LogSoftmax like before.Okay, and that is how we define the classifier.So now, we can take this classifier,just a classifier built from fully connected layers,and we can attach it to our model.classifier.So now, the new classifier that we built that isuntrained is attached to our model and this model also has the features parts.The features parts are going to remain frozen.We're not going to update those weights but we need to train our new classifier.Now, if we want to train our network that we're using,this Densenet-121 is really deep and it has 121 layers.So, if we can try to train this on the CPU like normal,it's going to take pretty much forever.So instead, what we can do is use the GPU.GPUs are built specifically for doing a bunch of linear algebra computations inparallel and our neural networks arebasically just a bunch of linear algebra computations.So if we run these on the GPU,they're done in parallel and we get something like 100 times increase speeds.In PyTorch, it's pretty straightforward to use the GPU.If you have your model, so model,the idea is that your model has all these parameters in theretensors that are sitting in your memory on your computer,but we can move them over to our GPU by saying model.cuda.So what this does is it moves the parameters for your model to the GPUand then all of the computations and the processing and are going to be done on the GPU.Similarly, if you have a tensor like your images, select images,if you want to run your images through your model,you have to make sure that the tensors that you're puttingthrough your model or on the GPU if your model's on the GPU.So you just have to make those match up.So to do that, to move a tensor from computer to the GPU,you just, again, say images.cuda.So that will move a tensor,that's images, to the GPU.Then oftentimes, you'll want to move your model and your tensors backfrom the GPU to your local memory and CPU, and so, to do that,you just say like model.cpu or images.cpu,so this'll bring your tensorsback from the GPU to your local computer to run on your CPU.Now, I'm going to give you a demonstration of how this all worksand the amazing increased speed we get by using the GPU.So here, I'm just going to do for cuda and false, true.So this way, I'm going to be able to basically like loopthrough and try it once where we're not using the GPU,and once where we are using the GPU.So let's define my criterion which is going to benatural log_loss like we'd normally do, define our optimizer.So again, here, remember that we only want to update the parameters for the classifier.So we're just going to pass in model.classifier.parameters.This will work and that it's going to update the premise for our classifier butit's going to lead the parameters for the feature detector part of the model static.So I typically do is, say like, if cuda,then we want to move our model to the GPU.Otherwise, let's leave it on the CPU.And then I'm going to write a little training loop.We'll get our inputs and our labels,changes into variables like normal, then again,if we have cuda enabled,so if we have GPUs,then we can do inputs, labels,and we'll just move these over to the GPU.We're using the GPU now and we're also using this pre-trained network, but in general,you're going to do the training loop exactly the same wayyou have been doing it with these feed forward networks that you've been building.So first, I'm actually going to define a start time just so I can time things,then you just do your training pass like normal.So, you just do a forward pass through your model and you can calculate the loss,do your backward pass.Finally, update your weights with your optimizer.So I'm going to do here, I'm going to breakthis training loop after the first three iterations.So I want to time the difference between using a GPU and not using the GPU.What happens is the very first batch to gothrough the training loop tends to take longer than the other batches,so I'm just going to take the first three or four and then average over thosejust so we get a better sense of how long it actually takes to process one batch.So, that will just print out our training times.So we can see that if we're not using the GPU,then each batch takes five and a half seconds to actually go through this training step.Whereas, with the GPU,it only takes 0.012 seconds.So, I mean, this is a speedup of over 100 times.So here, I basically set cuda manually but you can also checkif a GPU is available so you say torch.cuda is available,and this will give you back true or false depending if you havea GPU available that can use cuda.Okay, so from here,I'm going to let you finish training this model.So you can either continue with a DenseNet model that is alreadyloaded or you can try ResNet which is also a good model to try out.I also really like VGGNet,I think that one's pretty good.It's really up to you. Cheers.

### 25. PyTorch V2 Part 8 Solution V1-4n6T93hKRD4.en

Hi everyone, here is my solution for the transfer learning exercise that I had to do.So, this one's going to be a little different.I'm going to be typing it out as I do it so you can understandmy that process is kind of the combination of everything you've learned in this lesson.So, the first thing I'm going do is,if I have a GPU available,I'm going to write this code in agnostic way so that I can use the GPU.So, what I'm going to say, device = torch.device and then this is going to be cuda.So, it's going to run on our GPU if torch.cuda is available, else CPU.So, what this will do is,if our GPU is available,then this will be true and then we'll return cuda here and then otherwise, it'll be CPU.So, now we can just pass device to all the tensors and models and then itwill just automatically go to the GPU if we have it available.So, next, I'm going to get our pre-trained model.So, here I'm actually going to use ResNet.So, to do this, model dot models.So, we already imported models fromtorch vision then we can kind of like took out all the ones they have.So, there's ResNet there.So, I'm just going to use a fairly small one,ResNet 50 and then we want pre-trained true and that should get us our model.So, now if we look,so we can just print it out like this and itwill tell us all the different operations and layers and everything that's going on.So, if we scroll down, we can see that at the end here has fc.So, this is the last layer,this fully connected layer that's acting like a classifier.So, we can see that it has,it expects 2,048 inputs to this layer and then the out features are 1,000.So, remember that this was trained on ImageNet and so ImageNetis typically trained with 1,000 different classes of images.But here we're only using cat and dog,so we just need to output features and in our classifier.So, we can load the model like that and now I'm going to make surethat our models' perimeters are frozenso that when we're training they don't get updated.So, I'll just run this make sure it works.So, now, we can load the model and we can turn off gradients.Turn off gradients for our model.So, then, the next step is we want todefine our new classifier which we will be training.So, here, we can make it pretty simple.So, models= nn.sequential.You can define this in a lot of different ways,so I'm just using an industrial sequential here.So, our first layer, so linear,so remember we needed 248 inputs and then let's say,let's drop it down to 512 at a ReLu layer, a dropout.Now our output layer,512 to two and then we're going to do log softmax.I should change this to be a classifier.Okay. So, that's to finding our classifier and now we can attach it to our model,so to say, model.fc= classifier.Now, if we look at our model again,so we can scroll down to the bottom here.So, we see now this fully-connected module layerhere is a sequential classifier linear operation ReLu,dropout, another linear transformation and then log softmax.So, the next thing we do is define my loss, my criterion.So, this is going to be the negative log like we had loss.Then, define our optimizer, optim.Adam.So, we want to use the parameters fromour classifier which is fc here and then set our learning rate.The final thing to do is to move our model to whichever device we have available.So, now we have the model all set up and it's time to train it.Here's is the first thing I'm going to do is definesome variables that we're going to be using during the training.So, for example, I'm going to set our epochs, so I'm going to set to one.I'll be tracking the number of train steps we do,so set that to zero.I'll be tracking our loss,so also set this to zero, and finally,we want to kind of set a loop forhow many steps we're going to go before we print out the validation loss.So, now we want to loop through our epochs.So, for epoch and range epochs.Now, we're going to loop through our data for images,labels in trainloader, cumulate steps.So, basically, every time we go through one of these batches,we're going to increment steps here.So, now that we have our images and our labels,we're going to want to move them over to the GPU, if that's available.So, we're just going to do is images.to (device), labels.to(device).Now we're just going to write out our training loop.So, the first thing we need to do is zero our gradients.So, it's very important, don't forget to do this.Then, get our log probabilities from our model,model passed in the images with the log probabilities,we can get our loss from the criterion in the labels.Then do a backwards pass and then finally with our optimizer we take a step.Here, we can increment are running loss like so.So, this way we can keep track of our training loss as we aregoing through more and more data. All right.So, that is the training loop.So, now every once in a while so which is set by this like print every variable.We actually want to drop out of the train loop and testour network's accuracy and loss on our test dataset.So, for step modulo print_every,this is equal to zero, then we're going to go into our validation loop.So, what we need to do first is set model.eval.So, this'll turn our model into evaluation inference mode which turns off dropout.So, we can actually accurately use our network for makingpredictions instead a test loss and accuracy.So, now we'll get our images and labels from our test data.Now we'll do our validation loop.So, with our model so we'll pass in the images.So, these are the images from our test set.So, we're going to get our logps from our test set so again,get the loss with our criterion and keep track of our loss to test loss plus+= loss.item.So, this will allow us to keeptrack of our test loss as we're going through these validation rules.So, next we want to calculate our accuracy.So, probabilities= torch.exponential(logps).So, remember that our model is returning log softmax,so it's the log probabilities of our classes and to get the actual probabilities,we're going to use torch.exponential.So we get our top probabilities and top classes from ps.topk(1).So, that's going to give us our first largest value in our probabilities.Here, we need to make sure we set dimension to one to make sure it's actually likelooking for the top probabilities along the columns.Go to the top classes,now we can check for equality with our labels and then with the equality tensor,we can update our accuracy.So, here remember we can calculate our accuracy from equality.Once we change it to a FloatTensor then we can dotorch.mean and get our accuracy and so again justkind of incremented accumulated into this accuracy variable. All right.Now, we are in this loop here so this four step every print_every.So basically, now we have a running loss of our training loss and we havea test loss that we passed our test data throughour model and measured the loss in accuracy.So now we can print all this outand I'm just going to copy and paste this because it's a lot to type.So, basically here, we're just printing out our epochs.So, we can keep track and know where we are and keep track of that.So, running_loss divided by print_every so basically we'retaking the average of our training loss.So every time we print it out,we're just going to take the average.Then, test_loss over length the testloader.So basically length test loader tells us how many batchesare actually in our test dataset that we're getting from testloader.So, since we're we're summing up all the losses for each of our batches,if we take the total loss and divide bythe number of batches and that gives us our average loss,we do the same thing with accuracy.So, we're summing up the accuracy for each batch here and then we just divideby the total number of batches and that gives us our average accuracy for the test set.Then at the end,we can set our running loss back tozero and then we also want to put our model back into training mode.Great. So, that should be the training code and we'll see if it works.Now, this should be an if instead of a for.So, here I forgot this happens a lot.I forgot to transfer my tensors over to the GPU.So, hopefully this will work. All right.So, even like pretty quickly,we see that we can actually get our test accuracy on this above 95 percent.So, this is, remember that we're printing this outevery five steps and so this is a total of 15 batches,training batches that were updated in the model.So, we're able to easily fine tune these classifiers ontop and get greater than a 95 percent accuracy on our dataset.

### img

## Part 02-Module 02-Lesson 03_Recurrent Neural Networks

### 02. RNN Vs LSTM-70MgF-IwAr8.en

Okay so, let's say we have a regular neural network whichrecognizes images and we fitted this image.And the neural neural network guesses that the image is most likelya dog with a small chance of being a wolf and an even smaller chance of being a goldfish.But, what if this image is actually a wolf?How would the neural network know?So, let's say we're watching a TV show about nature and the previous imagebefore the wolf was a bear and the previous one was a fox.So, in this case, we want to use this information to hintto us that the last image is a wolf and not a dog.So, what we do is analyze each image with the same copy of a neural network.But, we use the output of the neural network as a part of the input of the next one.And, that will actually improve our results.Mathematically, this is simple.We just combine the vectors in a linear function,which will then be squished with an activation function,which could be sigmoid or hyperbolic tan.This way, we can use previous information andthe final neural network will know that the show is about wild animals inthe forest and actually use this information to correctlypredict that the image is of a wolf and not a dog.And, this is basically how recurrent neural networks work.However, this has some drawbacks.Let's say the bear appeared a while ago andthe two recent images are a tree and a squirrel.Based on those two,we don't really know if the new image is a dog or a wolf.Since trees and squirrels are just as associated todomestic animals as they are with forest animals.So, the information about being in the forest comes all the way back from the bear.But, as we've already experienced,information coming in gets repeatedlysquished by sigmoid functions and even worse than that,training a network using back propagation all the way back,will lead to problems such as the vanishing gradient problem etc.So, by this point pretty much all the bear information has been lost.That's a problem with recurring neural networks;that the memory that is stored is normally short term memory.RNNs, have a hard time storing long term memory and this is whereLSTMs or long short term memory networks will come to the rescue.So, as a small summary, an RNN works as follows;memory comes in and merges with a current eventand the output comes out as a prediction of what the input is.And also, as part of the input for the next iteration of the neural network.And in a similar way, an LSTM works as follows;it keeps track not just of memory but of long term memory,which comes in and comes out.And also, short term memory,which also comes in and comes out.And in every stage,the long and short term memory in the event get merged.And from there, we get a new long term memory,short term memory and a prediction.In here, we protect old information more.If we deem it necessary,the network can remember things from long time ago.So, in the next few videos, I will show you the architecture of LSTMs and how they work.

### 03. LSTM Basics-gjb68a4XsqE.en

So let's recap. We have the following problem: we are watching a TV show and we havea long term memory which is that the show is about natureand science and lots of forest animal have appeared.We also have a short term memory which is what we haverecently seen which is squirrels and trees.And we have a current event which is what we just saw,the image of a dog which could also be a wolf.And we want these three things to combine to form a prediction of what our image is.In this case, the long term memory which says that the show is aboutforest animals will give us a hint that the picture is of a wolf and not a dog.We also want the three pieces of information, long term memory,short term memory, and the event,to help us update the long term memory.So let's say we keep the fact that the show is aboutnature and we forget that it's about science.And we also remember that the show is aboutforest animals and trees since we recently saw a tree.So we add a bit and remove a bit to the long term memory.And finally we also want to usethese three pieces of information to help us update the short term memory.So let's say in our short term memory you want toforget that the show has trees and rememberthat it has wolves since the trees happened a few images ago and we just saw a wolf.So basically we have an architecture like this and weuse even more animals to represent our stages of memory.The long term memory is represented by an elephant since elephants have long term memory.The short term memory will be represented bya forgetful fish and the event will still be represented by the Wolf we just saw.So LSTM works as follows: the three pieces of information go insidethe node and then some math happens andthen the new pieces of information get updated and come out.There is a long term memory,a short term memory and the prediction of the event.More specifically the architecture of the LSTM contains a few gates.It contains a forget gate,a learn gate, a remember gate, and a use gate.And here's basically how they work.So the long term memory goes tothe forget gate where it forgets everything that it doesn't consider useful.The short term memory and the event are joined together in the learn gate,containing the information that we've recentlylearned and it removes any unnecessary information.Now the long term memory that we haven't forgotten yet plus the newinformation that we've learned get joined together in the remember gate.This gate puts these two together and since it's called remember gate,what it does is it outputs an updated long term memory.So this is what we'll remember for the future.And finally, the use gate is the one that decideswhat information we use from what we previouslyknow plus what we just learned to makea prediction so it also takes those inputs the long term memory,and the new information joins them and decides what to output.The output becomes both the prediction and the new short term memory.And so the big unfolded picture that we have is asfollows: we have the long term memory andthe short term memory coming in which we call LTM and STM.And then an event and an output are coming in and out of the LSTM.And then this passes to the next node,and so on and so forth.So in general at time t we label everything withan underscore t as we can see information passes from time t -1 to time t.

### 04. LSTM Architecture-ycwthhdx8ws.en

So in order to study the architecture of an LSTM,let's quickly recall the architecture of an RNN.Basically what we do is we take our event E_t and our memory M_t-1,coming from the previous point in time,and we apply a simple tanh orsigmoid activation function to obtain the output and then your memory M_t.So to be more specific,we join these two vectors and multiply them by a matrix W and add a bias b,and then squish this with the tanh function,and that gives us the output M_t.This output is a prediction and also the memory that we carry to the next node.The LSTM architecture is very similar,except with a lot more nodes inside and with two inputsand outputs since it keeps track of the long- and short-term memories.And as I said, the short-term memory is,again, the output or prediction.Don't get scared. These are actually not as complicated as they look.We'll break them down in the next few videos.

### 05. Learn Gate-aVHVI7ovbHY.en

So, let's keep this our base case.We have a long term memory which is atthe show we're watching it's about nature and science.We also have a short term memory which is what we've recently seen,a squirrel and a tree.And finally, we have our current event which is a picture wejust saw that looks like a dog but it could also be a wolf.So let's study the learn gate.What the learn gate does is the following.It takes a short term memory and the event and it joins it. Actually, it does a bit more.It takes the short term memory and the event and it combinesthem and then it ignores a bit of it keeping the important part of it.So here it forgets the fact that there's a tree and itremembers how we recently saw a squirrel and a dog/wolf.And how does this work mathematically?Well, it works like this. We have the short term memory STMt minusone and the event Et and it combines them by putting them througha linear function which consists of joining the vectors multiplying bya matrix adding a bias and finally squishing the result with a tanh activation function.Then the new information Nt has this form over here.Now, how do we ignore part of it?Well, by multiplying by an ignore factor, I-T.The ignore factor, I-T,is actually a vector but it multiplies element wise.And how do we calculate I-T?Well, we use our previous information of the short term memory and the event.So again, we create a small neural networkwhose inputs are the short term memory and the event.We'll pass them through a small linear function with a new matrix anda new bias and squish them with the sigmoid function to keep it between zero and one.So that's it. That's how the learn gate works.

### 06. Forget Gate-iWxpfxLUPSU.en

Now, we go to the Forget Gate,this one works as follows: It takesa long term memory and it decides what parts to keep and to forget.In this case, the show is about nature and science and the forget gate decides toforget that the show is about science and keep the fact that it's about nature.How does the Forget Gate work mathematically?Very simple. The long-term memory (LTM) from time T minus 1 comes in,and it gets multiplied bya Forget Factor ft. And how does the forget factor ft get calculated?Well, simple. We'll usea short term memory STM and the event information to calculate ft.So, just as before,we run a small one layer neural network with a linear function combined withthe sigmoid function to calculatethis Forget Factor and that's how the Forget Gate works.

### 07. Remember Gate-0qlm86HaXuU.en

And now we're going to learn the Remember Gate. This one is the simplest.It take the long-term memory coming out of the Forget Gate andthe short-term memory coming out of the Learn Gate and simply combines them together.And how does this work mathematically? Again, very simple.We just take the outputs coming from the Forget Gateand from the Learn Gate and we just add them.That's it, that's all we do.And that's how the Remember Gate works.

### 08. LSTM 7 Use Gate-5Ifolm1jTdY.en

And finally, we come to the use gate or output gate.This is the one that uses the long term memory that just came outof the forget gate and the short term memory that just came out of the learned gate,to come up with a new short term memory and an output.These are the same thing.In this case, we'll take what's useful fromthe long term memory which is this bear over here,and what's useful from the short term memory which is these dark wolf,and the squirrel, and that's what's going to be our new short term memory.So our output basically says,your image is most likely a wolf butit also carry some of the other animals that I've seen recently.And mathematically what this does is the following: it appliesa small neural network on the output ofthe forget gate using the tanh activation function,and it applies to another small neural network on the short term memoryand the events using the sigmoid activation function.And as a final step,it multiplies these two in order to get the new output.The output also worth of the new short term memory.And that's how they use gate works.

### 09. Putting It All Together-IF8FlKW-Zo0.en

So here we go. As we've seen before,here is the architecture for an LSTM with the four gates.There is the forget gate,which takes the long-term memory and forgets part of it.The learn gate puts the short-term memory together withthe event as the information we've recently learned.The remember gate joins the long-term memory that we haven't yet forgotten plusthe new information we've learned in order to update our long-term memory and output it.And finally, the use gate also takes the information we justlearned together with long-term memory we haven't yet forgotten,and it uses it to make a prediction and update the short-term memory.So this is how it looks all put together.It's not so complicated after all, isn't it?Now you may be thinking, wait a minute,this looks too arbitrary.Why use tanh sometimes and sigmoid other times?Why multiply sometimes and add other times,and other times apply a more complicated linear function?You can probably think of different architectures thatmake more sense or that are simpler,and you are absolutely right.This is an arbitrary construction.And as many things in machine learning,the reason why it is like this is because it works.And in the following section,we'll see some other architectures which can be simpleror more complex and that also do the job.But you're welcome to look for others and experiment.This is an area very much under development so if you come upwith a different architecture and it works, that is wonderful.

### 10. Other Architectures-MsxFDuYlTuQ.en

In this video, I will show you a pair of similar architectures that also work well,but there are many variations to LSTMs and we encourage you to study them further.Here's a simple architecture which also works well.It's called the gated recurring unit or GRU for short.It combines the forget and the learn gate intoan update gate and then runs this through a combine gate.It only returns one working memory instead of a pair of long- and short-term memories,but it actually seems to work in practice very well too.I won't go much into details,but in the instructor comments I'll recommendsome very good reference to learn more about gated recurrent units.Here's another observation.Let's remember the forget gate.The forget factor f_t was calculating using asinput a combination of the short-term memory and the event.But what about the long term memory?It seems like we left it away from the decision.Why does a long-term memory not have a say into whichthings get remembered or not? Well let's fix that.Let's also connect the long-term memory intothe neural network that calculates the forget factor.Mathematically, this just means the input matrix is larger sincewe're also concatenating it with the long-term memory matrix.This is called a peephole connection since now the long-term memoryhas more access into the decisions made inside the LSTM.We can do this for every one of the forget-type nodes,and this is what we get: an LSTM with peephole connections.

### 12. 02 Time Series Prediction V2-xV5jHLFfJbQ.en

To introduce you to RNNs in PyTorch,I've created a notebook that will show you how to dosimple time series prediction with an RNN.Specifically, we'll look at some data and see if we can createan RNN to accurately predict the next data point given a current data point,and this is really easiest to see in an example.So, let's get started.I'm importing our usual resources,and then I'm actually going to create some simple input and target training data.A classic example is to use a sine wave as inputbecause it has enough variance and shape to be an interesting task,but it's also very predictable.So, I want to create a sample input and target sequence of data points of length 20,which I specify here as sequence length.Recall that RNNs are meant to work with sequential data,and so the sequence length isjust the length of a sequence that it will look at as input.Often, the sequence length will indicate the number of words ina sentence or just some length of numerical data as is the case here.So, in these two lines,I'm just going to generate the start ofa sine wave in a range from zero to Pi time steps.At first, I'm going to create a number of points that sequence length 20 plus 1,then I'm going to reshape my sine wave data to give it one extra dimension,the input size, which is just going to be one.Then, to create an input and target sequence of the length I want,I'm going to say an input X is equal to all but the last point in data,and the target Y is equal to all but the first point.So, X and Y should contain 20 data points and have an input size of one.Finally, I'm going to display this data using the same x-axis.You can see the input X is in red and the target Y is shifted over by one in blue.So, if we look at this point as an example at the same time step,Y is basically X shifted one time step in the future,and that's exactly what we want.So, now we have our training data andthe next step is defining an RNN to learn from this data.We can define an RNN as usual,which is to say as a class using PyTorche's NN library.The syntax will look similar to how we've defined CNNs in the past.Let's actually click on the RNN documentation to readabout the parameters that our recurrent layer takes in as input.So, here's the documentation for an RNN layer.We can see that this layer is responsible forcalculating a hidden state based on its inputs.Now, to define a layer like this,we have these parameters: an input size,a hidden size, a number of layers and a few other arguments.The input size is just the number of input features,and in our specific case we're going to have inputs that are 20 valuesin sequence and one in input size features.This is like when we thought about the depth of an input image when we made CNN's.Next, we have a hidden size that defines how manyfeatures the output of an RNN will have and its hidden state.We also have a number of layers,which if it's greater than one,just means we're going to stack two RNNs on top of each other.Lastly, I want you to pay attention to this batch first parameter.If it is true, that means the input and output tensors that weprovide are going to have the batch size as the first dimension,which in most cases that we go through will be true.So, this is how you define an RNN layer,and later in the forward function we'll see that it takesin an input and an initial hidden state,and it produces an output and a new hidden state.Back to our notebook. Here, I'm defining an RNN layer, self- doubt RNN.This RNN is taking in an input size and a hidden dimensionthat defines how many features the output of this RNN will have.Then it takes in a number of layers which allows you to create a stacked RNN ifyou want and this is typically a value kept between one and three layers.Finally, I'm setting batch first to true because I'mshaping the input such that the batch size will be the first dimension.Okay. Then to complete this model I have to addone more layer which is a final fully-connected layer.This layer is responsible for producing the number of outputs,output size that I want given the output of the RNN.So, all of these parameters are just going to be passed into our RNN when we create it.You'll also note that I'm storing the value ofour hidden dimension so I can use it later in our forward function.In the forward function, I'm going to specify how a batchof input sequences will pass through this model.Note that this forward takes in an input X and the hidden state.The first thing I'm doing is grabbingthe batch size of our input calling X dot size of 0.Then I'm passing my initial input and hidden state into the RNN layer.This produces the RNN output and a new hidden state.Then I'm going to call view on the RNN output to shape it into the size I want.In this case that's going to be batch size, times sequence lengthrows and the hidden dimension number of columns.This is a flattening step where I'mpreparing the output to be fed into a fully-connected layer.So, I'll pass this shaped output to the final fully-connected layer,and return my final output here and my hidden state generated from the RNN.Now, as a last step here,I'm going to actually create some text data and totest RNN and see if it's working as I expect.The most common error I get when programmingRNNs is that I've messed up the data dimension somewhere.So, I'm just going to check that there as I expect.So, here I'm just creating a test RNN with an input and output size of one,a hidden dimension of 10, and the number of layers equal to two,and you can change the hidden dimension and the number of layers.I basically just want to see that this is making the shape of outputs I expect.So, here I'm creating some test data that are sequence length along.I'm converting that data into a tensor datatype,and I'm squeezing the first dimension to give ita batch size of one as a first dimension.Then I'm going to print out this input size and I'll pass it into our test RNN as input.Recall that this takes an initial hidden state,and an initial one here is just going to be none.Then this should return an output and a hidden state,and I'm going to print out those sizes as well.Okay. So, our input size is a 3D tensor which is exactly what I expect.If first dimension is one our batch size,then 20 our sequence length,and finally our input number of features.Which is just one as we specified here.The output size is a 2D tensor.This is because in the forward function of our model definition actuallysmooshed the batch size and sequence length into one parameter.So, batch size times sequence length is 20 and then we have an output size of one.Finally we have our hidden state.Now, the first dimension here is our number oflayers that I specified in the model definition two.Next we have the value one which is just the batch size of our input here.Finally, the last dimension here is 10 which is just our hidden dimension.So, all of these look pretty good and as I expect and I can proceed.Next I'll show you how to train a model like this.

### 13. 03 Training Memory V1-sx7T_KP5v9I.en

Last time, we defined a model, and next,I want to actually instantiate it and train it using our training data.First, I'll specify my model hyperparameters.The input and output will just be one,it's just one sequence at a time that we're processing and outputting,then I'll specify a hidden dimension which is just the numberof features expect to generate with the RNN layer.I'll set this to 32, but for a small data set like this,I may even be able to go smaller.I'll set n_layers to one for now.So, I'm not stacking any RNN layers.I'll create this RNN and printed out.I should see the variables that I expect.My RNN layer with an input size and hidden dimension,and a linear layer with an input number of features and output number.Before training, I'm defining my loss and optimization functions.Now in this case, we're training our model to generatedata points that are going to be basically coordinate values.So to compare a predicted and ground truth point like this,we'll use a regression loss because this isjust a quantity rather than something like a class probability.So for the loss function,I'm going to use mean squared error loss which willjust measure the distance between two points.I'll use an Adam optimizer which is standard for recurrent models,passing in my parameters and our learning rate.Next, I have a function train that's going to takein and RNN a number of steps to train for,and the parameter that will determine when it will print out law statistics.Now, at the very start of this function,I'm initializing my hidden state.At first, this is going to be nothing and it will default to a hidden state of all zeros.Then let's take a look at our batch loop.Now, this is a little unconventional,but I'm just generating data on the flyhere according to how many steps we will train for.So, in these lines, I'm just generating a sequence of 20 sine wave values at a time.As we saw when I generated data at the start.Here, I'm getting my input x and a targety that's just shifted by one time step in the future.Here, I'm converting this data into tensors andsqueezing the first dimension of our x_tensor to give it a batch size of one.Then I can pass my input tensor into my RNN model.So this is taking in my x input tensor and my initial hidden state at first.It produces a predicted output and a new hidden state.Next is an important part.I want to feed this new hidden state into the RNN asinput at the next time step when we loop around once more.So I'm just copying the values from this produced hidden state into a new variable.This essentially detaches the hidden state from its history and I willnot have to backpropagate through a series of accumulated hidden states.So this is what's going to be passed as input tothe RNN at the next time step or next point in our sequence.So then, I have the usual training commands,I zero out any accumulated gradients.Calculate the loss, and perform a backpropagation in optimization step.Down here, I have some code to print out the lossand show what our input and predicted outputs are.Finally, this function returns a trained RNNwhich will be useful if you want to save a model for example.So, let's run this code.I'll choose to train our RNN,and that we defined above for 75 steps.I'll print out the result every 15 steps.We can see the mean squared error loss here and thedifference between our red input in our blue output values.Recall that we want the blue output values to beone times step in the future when compared to the red ones.So it starts out pretty incorrect.Then we can see the loss decreases quite a lot after the first 15 steps.Our blue line is getting closer to our red one.As we train the blue predicted line gets closer to what we know our target is,at the end of 75 steps,our loss is pretty low.Our blue line looks very similar to what we know or output should be.If we look at the same time step for a red input dot,and a blue input dot,we we shouldn't see that the blue input isone time-step shifted in the future. It's pretty close.You could imagine getting even better performance after training formore steps or if you wanted to add more layers to your RNN.So, in this video,I wanted to demonstrate the basic structure of a simple RNN and show you howto keep track of the hidden state and represent memory over time as you train.You could imagine doing something very similar with data aboutworld temperature or stock prices which are a little bit more complicated than this.But it will be really interesting to see if you couldpredict the future given that kind of data.Okay, so this is just an example,you can check out this code in our program GitHub,which is linked to below.I encourage you to play around withthese model parameters until you have a good handle onthe dimensions of an RNN input andoutput and how hyperparameters might change, how this model trains.Next, Matt and I will go over an exercise in generating text.

### 14. Character-Wise RNN-dXl3eWCGLdU.en

Coming up in this lesson you'll implement a character-wise RNN.That is, the network will learn about some textone character at a time and then generate new text one character at a time.Let's say, we want to generate new Shakespeare plays.As an example, to be or not to be.We'd pass the sequence into our RNN one character at a time.Once trained the network will generate new text by predictingthe next character based on the characters it's already seen.So then to train this network we wantedto predict the next character in the input sequence.In this way the network will learn to produce a sequence ofcharacters that look like the original text.Let's consider what the architecture of this network will look like.First, let's unroll the RNN so we can see how this all works as a sequence.Here, we have our input layer where we'llpass in the characters as one hot encoded vectors.These vectors go to the hidden layer.The hidden layer is built with LSTM cells wherethe hidden state and cell state pass from one cell to the next in the sequence.In practice, we'll actually use multiple layers of LSTM cells.You just stack them up like this.The output of these cells go to the output layer.The output layer is used to predict to the next character.We want the probabilities for each character the same wayyou did image classification with the cabinet.That means that we want a Softmax activation on the output.Our target here will be the input sequence but shifted over oneso that each character is predicting the next character in the sequence.Again, we'll use cross entropy loss for training with gradient descent.When this network is trained up we can pass in one characterand get out a probability distribution for the likely next character.Then we can sample from that distribution to get the next character.Then we can take that character,pass it in and get another one.We keep doing this and eventually we'll build up some completely new text.We'll be training this network on the text from Anna Karenina,one of my favorite books.It's in the public domain so it's free to use however you want.Also, it's an amazing novel.

### 15. Sequence-Batching-Z4OiyU0Cldg.en

One of the most difficult parts of building networks for me is getting the batches right.It's more of a programming challenge than anything deep learning specific.So here I'm going to walk you through how batching works for RNN.With RNNs we're training on sequences of data like text,stock values, audio etc.By taking a sequence and splitting it into multiple shorter sequences,we can take advantage of matrix operations to make training more efficient.In the fact, the RNN is training on multiple sequences in parallel.Let's look at a simple example,a sequence of numbers from 1 to 12.We can pass these into an RNN as one sequence.What's better. We could split it in half and pass in two sequences.The batch size corresponds to the number of sequences we're using.So here we'd say the batch size is 2.Along with the batch size we also choosethe length of the sequences we feed to the network.For example, let's consider using a sequence length of 3.Then the first batch of data we pass intothe network are the first 3 values in each mini sequence.The next batch contains the next three values and so on until we run out of data.We can retain the hidden state from one batch and use it at the start of the next batch.This way the sequence information is transferred across batches for each mini sequence.Next up you'll see how to actually build a recurrent network. Cheers.

### 17. 04 Implementing CharRNN V2-MMtgZXzFB10.en

This is a notebook where you'll be building a characterwise RNN.You're going to train this on the text of Anna Karenina,which is a really great but also quite sad a book.The general idea behind this,is that we're going to be passing one character ata time into a recurrent neural network.We're going to do this for a whole bunch of text,and at the end what's going to happen,is that our network is going to be able to generate new text,one character at a time.This is the general structure.We have our input characters and we want to one-hot encode them.This one-hot vector, will be fed into a hidden recurrent layer,and then the hidden layer has two outputs.First, it produces some RNN output,and it produces a hidden state,which will continue to change and be fed tothis hidden layer at the next time step in the sequence.We saw something similar in the last code example.So, our recurrent layer keeps track of our hidden state,and its output goes to a final fully connected output layer.Our linear output layer,will produce a series of character class scores.So, this output will be as long as our input vector,and we can apply a Softmax function to geta probability distribution for the most likely next character.So, this network is based off of Andrej Karpathy's post on RNNs, which you can find here.It's a really good post and so you can check out these links to read more about RNNs.[inaudible] notebook is broken into a small series of exercises that you can implement yourself.For each exercise, I'm also going to provide a solution to consult.I recommend that you open the exercise notebook inone window and watch videos in another.That way you can work alongside me.Okay, so first things first,I'm loading in and taking a look at our text data.Here, I'm loading inthe Anna Karenina text file and I'm printing out the first 100 characters.The characters are everything from letters, to spaces,to newline characters, and we can see the classic first-line,"Happy families are all alike.Every unhappy family is unhappy in its own way."Then, I'll actually want to turn our text into numerical tokens.This is because our network can only learn from numerical data,and so we want to map every character in the text to a unique index.So, first off, with the text,we can just create a unique vocabulary as a set.Sets, are a built in python data structure,and what this will do, is look at every character in the past in the text.Separate it out as a string and get rid of any duplicates.So, chars, is going to be a set of all our unique characters.This is also sometimes referred to as a vocabulary.Then, I'm creating a dictionary from a vocabulary of all our characters,that maps the actual character to a unique integer.So, it's just giving a numerical value to each of our unique characters,and putting it in a dictionary int2char.Then I'm doing this the other way,where we have a dictionary that goes from integers to characters.Recall that any dictionary is made of a set of key and value pairs.In the int2char case,the keys are going to be integers and the values are going to be string characters.In the char2int case,our keys are going to be the characters andour values are going to be their unique integers.So, these basically give us a way to encode text as numbers.Here, I am doing just that.I'm encoding each character in the text as an integer.This creates an encoded text,and just like I printed the first 100 characters before,I can print the first 100 encoded values.If you look at the length of our unique characters,you'll see that we have 83 unique characters in the text.So, our encoded values will fall in this range.You can also see some repeating values here like 82,82, 82 and 19,19.If we scroll back up to our actual text,we can surmise that the repeated 82s are probably this new line character,and 19 is maybe a p. Okay,so are encodings are working,and now what we want to do,is turn these encodings into one-hot vectors,that our RNN can take in as input,just like in our initial diagram.Here, I've actually written a function that takes in an encoded array,and turns it into a one-hot vector of some specified length.I can show you what this does with an example below.I've made a short test sequence three, five,one and a vector length that I specify, eight.So, I'm passing this test sequence and the number oflabels that I expect into our one-hot function.I can see that the result is an array of three one-hot vectors.All of these vectors are of length eight and the index three,five, and one are on for their respective encodings.Now, for our vocabulary of 83 characters,these are just going to be much longer vectors.Cool. So, we have our preprocessing functions and data in place,and now your first task will be to take our encoded characters,and actually turn them into mini batches that we can feed into our network.So, as Matt mentioned before,the idea is that we actually want to runmultiple sequences through our network at a time.Where one mini batch of data contains multiple sequences.So, here's an example starting sequence.If we say we want a batch size of two,we're going to split this data into two batches.Then, we'll have these sequence length windows thatspecify how big we want our sequences to be.In this case, we have a sequence length of three,and so our window will be three in width.For a batch size of two and sequence length ofthree these values will make up our first mini-batch.We'll just slide this window over by three to get the next mini-batch.So, each mini-batch is going to have the dimensions batch size by sequence length.In this case, we have a two by three windowon are encoded array that we pass into our network.If you scroll down, I have more specific instructions.The first thing you're going to be doing is taking in an encoded array,and you'll want to discard any values that don't fit into completely full mini-batches.Then, you want to reshape this array into batch size number of rows.Finally, once you have that batch data,you're going to want to create a window that iteratesover the batches a sequence length at a time,to get your mini batches.So, here's the skeleton code.Your array is going to be some encoded data,then you have a batch size and sequence length.Basically, you want to create an input x that should bea sequence length or number of timesteps wide and a batch size tall.This will make up our input data and you'll also want to provide targets.The targets y, for this network are going to be just like the input characters x,only shifted over by one.That's because we want our network to predictthe most likely next character more some input sequence.So, you'll have your input sequence x and our targets y shifted over by one.Then finally, when we [inaudible] batches,we're going to create a generator that iterates throughour array and returns x and y with this yield command.Okay, I'll leave implementing this batching function up to you.You can find more information about how you could do this in the notebook.There's some code for testing out your implementation below.In fact, this is what your batches should look like when you run this code.If you need any help or you just want to see my solution,go ahead and check out the solution video next.

### 18. 05 Batching Data V1-9Eg0wf3eW-k.en

So, this is my complete get_batches code that generates mini-batches of data.So, the first thing I wanted to do here is getthe total number of complete batches that we can make in batches.To do that, I first calculated how many characters were in a complete mini-batch.So, in one mini-batch,there's going to be batch size times sequence length number of characters.Then, the number of complete batches that we can make isjust the length of the array divided by the total number of characters in a mini-batch.This double slash is an integer divisionwhich we'll just round down any decimal leftover from this division.With that, we have the number of completely full batches that we can make.Then, we get our array and we take all the characters inthe array up to n_batches times this total character size for a mini-batch.So here, we're making sure that we're keepingonly enough characters to make full batches,and we may lose some characters here.But in general, you're going to have enough data that getting ridof a last unfold batch is not really going to matter.Next, with reshaping, we can take our array and wecan make the number of rows equal to our batch size,and that's just how many sequences we want to include in a mini-batch.So, we just say we want the number of rowsto be batch size and then we put this negative one.Negative one here is kind of a dimension placeholder,and it'll just automatically fill up the second dimensionto whatever size it needs to be to accommodate all the data.Then finally, I'm iterating over my batch data using a window of length sequence length.So here, I'm taking in a reshaped complete array and then looking at all our rows,all our batches, and the columns are in a range from n to n plus sequence length,which makes our sequence length window.This completes x our input mini-batch.Then, what I did here for the target y is I justinitialized an array of all zeros that's the same shape as x,and I just kind of fill it up with values from our x array shifted by one.From the start to the end,I just shifted over x by one.Then in the case of reaching the very end of our array,I'm going to make the last element of y equal to the first element in our array.I'm not super sure why most people do it this way,wrapping our array around so that the last element of y is the first element of x.But I've seen this many times,and so I did it in the cyclical way and itseems like the network trains perfectly fine doing this.So, it does not seem to be a problem.The main thing is we want x and y to be the same size.So, if you did this right and you want to test your implementation,you should have gotten batches that looks something like this.Right here, we have a batch size of eight,so you have eight rows here,and then we're just printing out the first 10 items in a sequence.So, you should see 10 items here.The important thing to note here is that you want to make sure that the elements in x,like the actual encoded values,are shifted over by one in y.So, we have 51 as the first item here and as the zeroth item here,then 23 and 23.Likewise, 55 is here and 55 is here in y. Ibasically want to make sure that everything is shiftedover correctly, and this looks good.So now that we have our batch data,the next step we'll talk about is actually building the network.

### 19. 06 Defining Model V2-_LWzyqq4hCY.en

All right. So, we have our mini batches of data and now it's time to define our model.This is a little diagram of what the model will look like.We'll have our character's put into our input layer and then a stack of LSTM cells.These LSTM cells make up our hidden recurrent layer and when they look at a mini batch ofdata as input they'll look at one character ata time and produce an output and a hidden state.So, we will pass an input character intoour first LSTM cell which produces a hidden state.Then at the next time step,we'll look at the next character in our sequence and pass that intothis LSTM cell which will see the previous hidden state as input.You have so far seen this behavior in a one layer RNN but in this case weplan on using a two-layer model that has stacked LSTM layers andthat means that the output of this LSTM layer is going to go to the next one asinput and each of these cells is sharingits hidden state with the next cell in the unrolled series.Finally, the output of the last LSTM layer will includesome character class scores that will be the length of our vocabulary.We'll put this through a Softmax activation function which we'll use to getthe probability distribution for predicting the most likely next character.So, to start you off on this task,you've been given some skeleton code for creating a model.First, we're going to check to see if a GPU isavailable for training then you'll see this class character RNN.You can see that this character RNN class hasour usual init and forward functions and lateryou've been given some code to initialize the hidden state ofan LSTM layer and I'll go over this in a moment.You can definitely take a look at this given code and how we'recreating our initial character dictionaries but you won't need to change it.We also have several parameters that are going to be passed in when a characterRNN is instantiated and I've saved some of these as class variables.So, using these input parameters and variables,it will be up to you to create our model layers and complete the forward function.You'll first create an LSTM layer which you can read about in the documentation here.We can see that an LSTM layer is created using our usual parameters;an input size, hidden size,number of layers, and a batch first parameter.We'll also add a dropout value.This introduces a dropout layer in between the outputsof LSTM layers if you've decided to stack multiple layers.So, after you define an LSTM layer,I'll ask you to define two more layers;one dropout layer and a final fully-connected layer for getting our desired output size.Once you've defined these layers,you'll move on to define the forward function.This takes in an input x and hidden state.You'll pass this input through the layers ofthe model and return a final output and hidden state.You'll have to make sure to shape the LSTM outputso that it can be fed into the last fully connected layer.Okay. Then at the bottom here,you'll see this function for initializing the hidden state of an LSTM.An LSTM has a hidden and a cell state that are saved as a tuple hidden.The shape of the hidden and cell state isdefined first by the number of layers in our model,the batch size of our input,and then the hidden dimension that we specified in model creation.In this function, we're initializing the hidden weights all tozero and moving them to GPU if it's available.Okay, so all the code that you see you don't need to change,you just need to define the model layers and feedforward behavior.If you've implemented this correctly,you should be able to set your model hyperparametersand proceed with training and generating some sample text.Try this out on your own then next,check out my solution.

### 20. 07 CharRNN Solution V1-ed33qePHrJM.en

We wanted to define a character RNN with a two layer LSTM.Here in my solution, I am running this code onGPU and here's my code for defining our character level RNN.First, I defined an LSTM layer, self.lstm.This takes in an input size,which is going to be the length ofa one-hot encoded input character andthat's just the length of all of my unique characters.Then, it takes a hidden dimension a number oflayers and a dropout probability that we've specified.Remember that this will create a dropout layer in between multiple LSTM layers,and all of these are parameters that are going to be passedin as input to our RNN when it's constructed.Then, I've set batch first to true because when we created our batch data,the first dimension is the batch size,rather than the sequence length.Okay. Next, I've defined a dropout layer to goin-between my LSTM and a final linear layer.Then, I have FC, my final fully connected linear layer.This takes in our LSTM outputs,which are going to be dimension and hidden.It's going to output our character class scores for the most likely next character.So, these are the class scores for each possible next character.This output size is the same size as our input,the length of our character vocabulary.Then, I move to the forward function.I'm passing my input X and a hidden state to my LSTM layer here.This produces my LSTM output and a new hidden state.I'm going to pass the LSTM output throughthe dropout layer that I defined here to get a new output.Then, I'm making sure to reshape this output,so that the last dimension is our hidden dim.This negative one basically means I'm going to be stacking up the outputs of the LSTM.Finally, I'm passing this V-shaped output to the final fully connected layer.Then, I'm returning this final output andthe hidden state that was generated by our LSTM.These two functions in addition to the init hidden function complete my model.Next, it's time to train and let's take a look at the training loop that was provided.This function takes in a model to train some data,and the number of epics to train for,and a batch size,and sequence length that define our mini batch size.It also takes in a few more training parameters.First in here, I've defined my optimizer and my loss function.The optimizer is a standard Adam optimizer witha learning rate set to the past and learning rate up here.The last function is cross entropy loss,which is useful for when we're outputting character class scores.Here, you'll see some details about creatingsome validation data and moving our model to GPU if it's available.Here, you can see the start of our epic loop.At the start of each epic,I'm initializing the hidden state of our LSTM.Recall that this takes in the batch size of our data to define the size ofthe hidden state and it returns a hidden in cell state that are all zeros.Then, inside this epic loop,I have my batch loop.This is getting our X and Y mini batches from our get batches generator.Remember that this function basically iterates through are encoded data,and returns batches of inputs X and targetsY. I'm then converting the input into a one-hot encoded representation,and I'm converting both X and Y areinputs and targets into Tensors that can be seen by our model.If GPU's available, I'm moving those inputs and targets to our GPU device.The next thing that you see is making sure that we detachany past in hidden state from its history.Recall that the hidden state of an LSTM layer is a Tuple,and so here, we are getting the data as a tuple.Then, we proceed with back propagation as usual.We zero out any accumulated gradients and pass in our input Tensors to our model.We also pass in the latest hidden state here.In this returns of final output and a new hidden state,then we calculate the loss by looking at the predicted output and the targets.Recall that in the forward function of our model,I smashed the batch size and sequence length of our LSTM outputs into one dimension,and so I'm doing the same thing for our targets here.Then, we're performing back propagation and movingone step in the right direction updating the weights of our network.Now before the optimization step,I've added one line of code that may look unfamiliar.I'm calling clip grad norm.Now, this kind of LSTM model has one main problem with gradients.They can explode and get really, really big.So, what we do is we can clip the gradients,we just set some clip threshold,and then if the gradient is larger than that threshold,we set it to that clip threshold,and encode we do this by just passing inthe parameters and the value that we want to clip the gradients at.In this case, this value is passed in,in our train function as a value five.Okay. So, we take a backwards step,then we clip our gradients,and we perform an optimization step.At the end here, I'm doing something very similar for processingour validation data except not performing the back propagation step.Then, I'm printing out some statistics about our loss.Now with this train function defined,I can go about instantiating and training a model.In the exercise notebook,I've left these hyper parameters for you to define.I've set our hidden dimension to the value of 512 and a number of layers up two,which we talked about before.Then, I have instantiated our model, and printed it out,and we can see that we have 83 unique characters as input,512 as a hidden dimension,and two layers in our LSTM.For a dropout layer,we have the default dropout value of 0.5 and for our last fully connected layer,we have our Input features,which is the same as this hidden dimension and our output features,the number of characters.Then, there are more hyper parameters that defineour batch size sequence length and number of epics to train for.Here, I've set the sequence length to 100,which is a lot of characters,but it gives our model a great deal of context to learn from.I also want to note that the hidden dimension isbasically the number of features that your model can detect.Larger values basically allow a network to learn more text features.There's some more information below in this notebook about defining hyper parameters.In general, I'll try to start out with a pretty big model like this,multiple LSTM layers and a large hidden dimension.Then, I'll basically take a look at the loss asthis model trains and if it's decreasing, I'll keep going.But if it's not decreasing as I expect,then I'll probably change some hyper parameters.Our text data is pretty large and here,I've trained our entire model for 20 epics on GPU.I can see the training and validation loss over time decreasing.Around epic 15, I'm seeing the lost slow down a bit.But it actually looks like the validation andtraining loss are still decreasing even after epic 20.I could have stood to train for an even longer amount of time.I encourage you to read this information about settingthe hyper parameters of a model and really getting the best model.Then, after you've trained a model like I've just done,you can save it by name and then there's one last step,which is using that model to make predictions and generate some new text,which I'll go over next.

### 21. 08 Making Predictions V3-BhrpV3kwATo.en

Now, the goal of this model is to train it so that it can take inone character and produce a next character and that's what this next step,Making Predictions is all about.We basically want to create functions that can take ina character and have our network predict the next character.Then, we want to take that character,pass it back in, and get more and more predicted next characters.We'll keep doing this until we generate a bunch of text.So, you've been given this predict function which will help with this.This function takes in a model andoccurring character and its job is to basically give usback the encoded value ofthe predictive next character and the hidden state that's produced by our model.So, let's see what it's actually doing step-by-step.It's taking in our input character and converting it into it's encoded integer value.Then, as part of pre-processing,we're turning that intoa one-hot encoded representation and then converting these inputs into a tensor.These inputs we can then pass to our model,and then you'll see a couple of steps that are reallysimilar to what we saw in our training loop.We put our inputs on a GPU if it'savailable and we detach our hidden state from its history here.Then, we pass in the inputs and the hidden state toour model which returns an output and a new hidden state.Next, we're processing the output a little more.We're applying a softmax function to get p probabilities for the likely next character.So, p is a probability distribution overall the possible mixed characters given the input character x.Now, we can generate more sensible characters byonly considering the k most probable characters.So, here we're giving you a couple of lines of code to use top k sampling,which finds us the k most likely next characters.Then, here we're adding an element of randomness,something that selects from among those top likely next characters.So, then we have a most likely next character and we're actually returningthe encoded value of that character and the hidden state produced by our model,but we'll basically want to call the predict function several times,generating one character's output,then passing that in as input and predicting the next and next characters.That brings me to our next function sample.Sample will take in our trained model and the size of text that we want to generate.It will also take in prime,which is going to be a set of characters that we want to start our model off with.Lastly, we will take in a value for top k which willjust return our k most probable characters in our predict function.So, in here, we're starting off by moving our model to GPU if it's available,and here we're also initializing the hidden state with a batch size of one because,for one character that we're inputting at a time,the batch size will be one.In this way, prediction is quite different than training a model.Then, you'll see that we're getting each character in our prime word.The prime word basically helps us answer the question,how do we start to generate text?We shouldn't just start out randomly.So, what is usually done is to provide a prime word or a set of characters.Here the default prime set is just the,T-H-E, but you can pass in any set of characters that you want as the prime.The sample function first processesthese characters in sequence adding them to a list of characters.It then calls predict on these characters passing in our model,each character and hidden state and this returnsthe next character after our prime sequence and the hidden state.So, here we have all our prime characters in the default case.This is going to be T, H,and E and then we're going to append the next most likely character.So, we're basically building up a list of characters here,then we're going to generate more and more characters.In this loop, we're passing in our model and the last character in our character list.This returns the next character and the hidden state.This character is appended to our list and the cycle starts all over again.So, predict is generating a next likely character which isappended to our list and then that goes back as input into our predict function.The effect is that we're getting next and next andnext characters and adding them to our characters list,that is until we reach our desired text length.Finally, we join all these characters together to return a sample text,and here I've generated a couple samples.You can see that I've passed in my model that was trained for 20 epochs, and I said,generate a text that's 1,000 characters long starting with the prime word Anna.I've also passed in a value for top k equal to five.You can see that this starts with the prime word and generateswhat might be thought of as a paragraph of text in a book.Even with just a few prime characters,our model is definitely making complete and real words that make sense.The structure and spelling looks pretty goodeven if the content itself is a little confusing,and here's another example where I've loaded in a model byname and I'm using this loaded model to generate a longer piece of text,starting with the prime words, "And Levin said."So, this is pretty cool.A well-trained model can actually generate some text that makes some sense.It learned just from looking at long sequences ofcharacters what characters were likely to come next.Then in our sampling and prediction code,we used top-k sampling and some randomness in selecting the best likely next character.You can train a model like this on any other text data.For example, you could try it on generatingShakespeare sonnets or another text of your choice.Great job on getting this far.You've really learned a lot about implementing RNNs in PyTorch.

### img

## Part 02-Module 02-Lesson 04_Training Neural Networks

### 02. Training Optimization-UiGKhx9pUYc.en

So by now we've learned how to builda deep neural network and how to train it to fit our data.Sometimes however, we go out there and train onourselves and find out that nothing works as planned.Why? Because there are many things that can fail.Our architecture can be poorly chosen,our data can be noisy,our model could maybe be taking years to run and we need it to run faster.We need to learn ways to optimizethe training of our models and this is what we'll do next.

### 03. Testing-EeBZpb-PSac.en

So let's look at the following data form by blue and red points,and the following two classification modelswhich separates the blue points from the red points.The question is which of these two models is better?Well, it seems like the one on the left is simpler since it'sa line and the one on the right is more complicated since it's a complex curve.Now the one in the right makes no mistakes.It correctly separates all the points,on the other hand, the one in the left does make some mistakes.So we're inclined to think that the one in the right is better.In order to really find out which one is better,we introduce the concept of training and testing sets.We'll denote them as follows: the solid color points are the training set,and the points with the white inside are the testing set.And what we'll do is we'll train our models inthe training set without looking at the testing set,and then we'll evaluate the results on that testing to see how we did.So according to this,we trained the linear model andthe complex model on the training set to obtain these two boundaries.Now we reintroduce the testing set and we can see that the model inthe left made one mistake while the model in the right made two mistakes.So in the end, the simple model was better.Does that match our intuition?.Well, it does, because in machine learning that's what we're going to do.Whenever we can choose between a simple model that doesthe job and a complicated model that may do the job a little bit better,we always try to go for the simpler model.

### 04. Underfitting And Overfitting-xj4PlXMsN-Y.en

So, let's talk about life.In life, there are two mistakes one can make.One is to try to kill Godzilla using a flyswatter.The other one is to try to kill a fly using a bazooka.What's the problem with trying to kill Godzilla with a flyswatter?That we're oversimplifying the problem.We're trying a solution that is too simple and won't do the job.In machine learning, this is called underfitting.And what's the problem with trying to kill a fly with a bazooka?It's overly complicated and it will lead to bad solutions andextra complexity when we can use a much simpler solution instead.In machine learning, this is called overfitting Let's look at howoverfitting and underfitting can occur in a classification problem.Let's say we have the following data,and we need to classify it.So what is the rule that will do the job here?Seems like an easy problem, right?The ones in the right are dogs while the ones in the left are anything but dogs.Now what if we use the following rule?We say that the ones in the right areanimals and the ones in the left are anything but animals.Well, that solution is not too good, right?What is the problem?It's too simple. It doesn't even get the whole data set right.See? It misclassified this cat over here since the cat is an animal.This is underfitting.It's like trying to kill Godzilla with a flyswatter.Sometimes, we'll refer to it as error due to bias.Now, what about the following rule?We'll say that the ones in the right are dogs that are yellow, orange,or grey, and the ones in the left are anything but dogs that are yellow, orange, or grey.Well, technically, this is correct as it classifies the data correctly.There is a feeling that we went too specific sincejust saying dogs and not dogs would have done the job.But this problem is more conceptual, right?How can we see the problem here?Well, one way to see this is by introducing a testing set.If our testing set is this dog over here,then we'd imagine that a good classifier would put it on the right with the other dogs.But this classifier will put it on the left since the dog is not yellow, orange, or grey.So, the problem here, as we said,is that the classifier is too specific.It will fit the data well but it will fail to generalize.This is overfitting.It's like trying to kill a fly with a bazooka.Sometimes, we'll refer to overfitting as error due to variance.The way I like to picture underfitting and overfitting is when studying for an exam.Underfitting, it's like not studying enough and failing.A good model is like studying well and doing well in the exam.Overfitting is like instead of studying,we memorize the entire textbook word by word.We may be able to regurgitate any questions in the textbook but we won'tbe able to generalize properly and answer the questions in the test.But now, let's see how this would look like in neural networks.So let's say this data where, again,the blue points are labeled positive and the red points are labeled negative.And here, we have the three little bears.In the middle, we have a good model which fits the data well.On the left, we have a model that underfits since it's too simple.It tries to fit the data with the line but the data is more complicated than that.And on the right, we have a model that overfits since ittries to fit the data with an overly complicated curve.Notice that the model in the right fits the data really well since it makes no mistakes,whereas the one in the middle makes this mistake over here.But we can see that the model in the middle will probably generalize better.The model in the middle looks at this point as noisewhile the one in the right gets confused by it and tries to feed it too well.Now the model in the middle will probably bea neural network with a slightly complex architecture like this one.The one in the left will probably be an overly simplistic architecture.Here, for example, the entire neural network isjust one preceptors since the model is linear.The model in the right is probablya highly complex neural network with more layers and weights than we need.Now here's the bad news.It's really hard to find the right architecture for a neural network.We're always going to end either withan overly simplistic architecture like the one inthe left or an overly complicated one like the one in the right.Now the question is, what do we do?Well, this is like trying to fit in a pair of pants.If we can't find our size,do we go for bigger pants or smaller pants?Well, it seems like it's less bad to go fora slightly bigger pants and then try to geta belt or something that will make them fit better,and that's what we're going to do.We'll err on the side ofan overly complicated models and then we'llapply certain techniques to prevent overfitting on it.

### 05. Model Complexity Graph-NnS0FJyVcDQ.en

So, let's start from where we left off, which is,we have a complicated network architecture which would bemore complicated than we need but we need to live with it.So, let's look at the process of training.We start with random weights in her first epoch and we get a model like this one,which makes lots of mistakes.Now as we train, let's say for 20 epochs we get a pretty good model.But then, let's say we keep going for a 100 epochs,we'll get something that fits the data much better,but we can see that this is starting to over-fit.If we go for even more,say 600 epochs, then the model heavily over-fits.We can see that the blue region is pretty much a bunch of circles around the blue points.This fits the training data really well,but it will generalize horribly.Imagine a new blue point in the blue area.This point will most likely be classified as red unless it's super close to a blue point.So, let's try to evaluate these models by adding a testing set such as these points.Let's make a plot of the error in the training setand the testing set with respect to each epoch.For the first epoch,since the model is completely random,then it badly misclassifies both the training and the testing sets.So, both the training error and the testing error are large.We can plot them over here.For the 20 epoch,we have a much better model which fit the training data pretty well,and it also does well in the testing set.So, both errors are relatively small and we'll plot them over here.For the 100 epoch,we see that we're starting to over-fit.The model fits the data very well but it starts making mistakes in the testing data.We realize that the training error keeps decreasing,but the testing error starts increasing,so, we plot them over here.Now, for the 600 epoch, we're badly over-fitting.We can see that the training error is very tiny because the data fits the training setreally well but the model makes tons of mistakes in the testing data.So, the testing error is large.We plot them over here.Now, we draw the curves that connect the training and testing errors.So, in this plot,it is quite clear when we stop under-fitting and start over-fitting,the training curve is always decreasing since as we train the model,we keep fitting the training data better and better.The testing error is large when we're under-fitting because the model is not exact.Then it decreases as the model generalizes well until it getsto a minimum point - the Goldilocks spot.And finally, once we pass that spot,the model starts over-fitting again since it stopsgeneralizing and just starts memorizing the training data.This plot is called the model complexity graph.In the Y-axis, we have a measure ofthe error and in the X-axis we have a measure of the complexity of the model.In this case, it's the number of epochs.And as you can see,in the left we have high testing and training error, so we're under-fitting.In the right, we have a high testing error and low training error, so we're over-fitting.And somewhere in the middle,we have our happy Goldilocks point.So, this determines the number of epochs we'll be using.So, in summary, what we do is,we degrade in descent until the testing error stops decreasing and starts to increase.At that moment, we stop.This algorithm is called Early Stopping and is widely used to train neural networks.

### 06. DL 53 Q Regularization-KxROxcRsHL8.en

Now let me show you a subtle way of overfitting a model.Let's look at the simplest data set in the world, two points,the point one one which is blue and the point minus one minus one which is red.Now we want to separate them with a line.I'll give you two equations and you tell me which onegives a smaller error and that's going to be the quiz.Equation one is x1 plus x2.That means w1 equals w2 is one and the bias b is zero.And then equation two is 10x1 plus 10x2.So that means w1 equals w2 equals 10 and the bias b equals zero.Now the question is,which prediction gives a smaller error?This is not an easy question but I want you to think aboutit and maybe make some calculations, if necessary.

### 07. Regularization-ndYnUrx8xvs.en

Well the first observation is that both equations give us the same line,the line with equation X1+X2=0.And the reason for this is that solutiontwo is really just a scalar multiple of solution one. So let's see.Recall that the prediction is a sigmoid of the linear function.So in the first case, for the 0.11,it would be sigmoid of 1+1,which is sigmoid of 2, which is 0.88.This is not bad since the point is blue,so it has a label of one.For the point (-1, -1),the prediction is sigmoid of -1+-1,which is sigmoid of -2, which is 0.12.It's also not best since a point label has a label of zero since it's red.Now let's see what happens with the second model.The point (1, 1) has a prediction sigmoidof 10 times 1 plus 10 times 1 which is sigmoid of 20.This is a 0.9999999979,which is really close to 1,so it's a great prediction.And the point (-1, -1) hasprediction sigmoid of 10 times negative one plus 10 times negative one,which is sigmoid of minus 20,and that is 0.0000000021.That's a really, really close to zero so it's a great prediction.So the answer to the quiz is the second model,the second model is super accurate.This means it's better, right?Well after the last section you may be a bitreluctant since this hint's a bit towards overfitting.And your hunch is correct.The problem is overfitting but in a subtle way.Here's what's happening and here's why the first model isbetter even if it gives a larger error.When we apply sigmoid to small values such as X1+X2,we get the function on the left which has a nice slope to the gradient descent.When we multiply the linear function by 10 and take sigmoid of 10X1+10X2,our predictions are much better since they're closer to zero and one.But the function becomes much steeper and it's much harder to do great descent here.Since the derivatives are mostly close tozero and then very large when we get to the middle of the curve.Therefore, in order to do gradient descent properly,we want a model like the one in the left more than a model like the one in the right.In a conceptual way,the model in the right is toocertain and it gives little room for applying gradient descent.Also as we can imagine,the points that are classified incorrectly in the model in the right,will generate large errors and it will be hard to tune the model to correct them.These can be summarized in the quote bythe famous philosopher and mathematician BertrAIND Russell.The whole problem with artificial intelligence,is that bad models are so certain of themselves,and good models are so full of doubts.Now the question is,how do we prevent this type of overfitting from happening?This seems to not be easy since the bad model gives smaller errors.Well, all we have to do is we have to tweak the error function a bit.Basically we want to punish high coefficients.So what we do is we takethe old error function and add a term which is big when the weights are big.There are two ways to do this.One way is to add the sums of absolute values of the weights times a constant lambda.The other one is to add the sum of the squares of the weights times that same constant.As you can see, these two are large if the weights are large.The lambda parameter will tell us how much we want to penalize the coefficients.If lambda is large,we penalized them a lot.And if lambda is small then we don't penalize them much.And finally, if we decide to go for the absolute values,we're doing L1 regularization,and if we decide to go for the squares,then we're doing L2 regularization.Both are very popular,and depending on our goals or application,we'll be applying one or the other.Here are some general guidelines for deciding between L1 and L2 regularization.When we apply L1, we tend to end up with sparse vectors.That means, small weights will tend to go to zero.So if we want to reduce the number of weights and end up with a small set,we can use L1.This is also good for feature selections andsometimes we have a problem with hundreds of features,and L1 regularization will help us select which ones are important,and it will turn the rest into zeroes.L2 on the other hand,tends not to favor sparse vectors since ittries to maintain all the weights homogeneously small.This one normally gives better results for training models soit's the one we'll use the most. Now let's think a bit.Why would L1 regularization produce vectors with sparse weights,and L2 regularization will produce vectors with small homogeneous weights?Well, here's an idea of why.If we take the vector (1, 0),the sums of the absolute values of the weights are one,and the sums of the squares of the weights are also one.But if we take the vector (0.5, 0.5),the sums of the absolute values of the weights is still one,but the sums of the squares is 0.25+0.25, which is 0.5.Thus, L2 regularization will prefer the vector point (0.5,0.5) over the vector (1, 0),since this one produces a smaller sum of squares.And in turn, a smaller function.

### 08. Dropout-Ty6K6YiGdBs.en

Here's another way to prevent overfitting.So, let's say this is you,and one day you decide to practice sports.So, on Monday you play tennis,on Tuesday you lift weights,on Wednesday you play American football,on Thursday you play baseball,on Friday you play basketball,and on Saturday you play ping pong.Now, after a week you've kind ofnoticed that you've done most of them with your dominant hand.So, you're developing a large muscle on that arm but not on the other arm.This is disappointing.So, what can you do?Well, let's spice it up on the next week.What we'll do is on Monday we'll tieour right hand behind our back and try to play tennis with the left hand.On Tuesday, we'll tie our left handbehind your back and try to lift weights with the right hand.Then on Wednesday again,we'll tie our right hand and play American football with the left one.On Thursday we'll take it easy and play baseball with both hands, that's fine.Then, on Friday we'll tie both hands behind our back and try to play basketball.That won't work out too well.But it's OK. It's the training process.And then on Saturday again,we tie our left hand behind our back and play ping pong with the right.After a week, we see that we've developed both of our biceps. Pretty good job.This is something that happens a lot when we train neural networks.Sometimes one part of the network hasvery large weights and it ends up dominating all the training,while another part of the network doesn'treally play much of a role so it doesn't get trained.So, what we'll do to solve this is sometimes during training,we'll turn this part off and let the rest of the network train.More thoroughly, what we do is as we go through the epochs,we randomly turn off some of the nodes and say,you shall not pass through here.In that case, the other nodes have to pick upthe slack and take more part in the training.So, for example, in the first epoch we're not allowed to use this node.So, we do our feat forward and our back propagation passes without using it.In the second epoch,we can't use these two nodes.Again, we do our feet forward and back prop.And in the third epoch we can't use these nodes over here.So, again, we do forward and back prop.And finally in last epoch,we can't use these two nodes over here.So, we continue like that.What we'll do to drop the nodes is we'll give the algorithm a parameter.This parameter is the probability that each node gets dropped at a particular epoch.For example, if we give it a 0.2 it means each epoch,each node gets turned off with a probability of 20 percent.Notice that some nodes may get turned off more thanothers and some others may never get turned off.And this is OK since we're doing it over and over and over.On average each node will get the same treatment.This method is called dropout and it's reallyreally common and useful to train neural networks.

### 09. Local Minima-gF_sW_nY-xw.en

So let's recall a gradient descent does.What it does is it looks at the direction where you descend themost and then it takes a step in that direction.But in Mt. Everest, everything was nice and prettysince that was going to help us go down the mountain.But now, what if we try to do it here in this complicated mountain range.The Himalayans lowest point that we want to go is around here,but if we do gradient descent,we get all the way here.And once we're here,we look around ourselves and there's no direction wherewe can descend more since we're at a local minimum.We're stuck. In here,gradient descent itself doesn't help us.We need something else.

### 10. Random Restart-idyBBCzXiqg.en

One way to solve this is to use random restarts,and this is just very simple.We start from a few different random places and do gradient descend from all of them.This increases the probability that we'll get to the global minimum,or at least a pretty good local minimum.

### 11. Vanishing Gradient-W_JJm_5syFw.en

Here's another problem that can occur.Let's take a look at the sigmoid function.The curve gets pretty flat on the sides.So, if we calculate the derivative at a point way at the right or way at the left,this derivative is almost zero.This is not good cause a derivative is what tells us in what direction to move.This gets even worse in most linear perceptrons. Check this out.We call that the derivative of the error function with respect to a weight wasthe product of all the derivatives calculatedat the nodes in the corresponding path to the output.All these derivatives are derivatives as a sigmoid function,so they're small and the product of a bunch of small numbers is tiny.This makes the training difficult because basically grading the [inaudible] gives us very,very tiny changes to make on the weights,which means, we make very tiny steps and we'll never be able to descend Mount Everest.So how do we fix it? Well, there are some ways.

### 12. Other Activation Functions-kA-1vUt6cvQ.en

The best way to fix this is to change the activation function.Here's another one, the Hyperbolic Tangent,is given by this formula underneath,e to the x minus e to the minus x divided by e to the x plus e to the minus x.This one is similar to sigmoid,but since our range is between minus one and one,the derivatives are larger.This small difference actually led togreat advances in neural networks, believe it or not.Another very popular activation function is the Rectified Linear Unit or ReLU.This is a very simple function.It only says, if you're positive,I'll return the same value,and if your negative, I'll return zero.Another way of seeing it is as the maximum between x and zero.This function is used a lot instead of the sigmoid and it can improvethe training significantly without sacrificing much accuracy,since the derivative is one if the number is positive.It's fascinating that this function which barely breakslinearity can lead to such complex non-linear solutions.So now, with better activation functions,when we multiply derivatives to obtain the derivative to any sort of weight,the products will be made ofslightly larger numbers which will make the derivative less small,and will allow us to do gradient descent.We'll represent the ReLU unit by the drawing of it's function.Here's an example of a Multi-layer Perceptron with a bunch of ReLU activation units.Note that the last unit is a sigmoid,since our final output still needs to be a probability between zero and one.However, if we let the final unit be a ReLU,we can actually end up with regression models, the predictive value.This will be of use in the recurring neural network section of the Nanodegree.

### 13. Batch vs Stochastic Gradient Descent-2p58rVgqsgo.en

First, let's look at what the gradient descent algorithm is doing.So, recall that we're up here in the top of Mount Everest and we need to go down.In order to go down,we take a bunch of steps following the negative of the gradient of the height,which is the error function.Each step is called an epoch.So, when we refer to the number of steps,we refer to the number of epochs.Now, let's see what happens in each epoch.In each epoch, we take our input,namely all of our data and run it through the entire neural network.Then we find our predictions,we calculate the error, namely,how far they are from where their actual labels.And finally, we back-propagate this error in orderto update the weights in the neural network.This will give us a better boundary for predicting our data.Now this is done for all the data.If we have many, many data points,which is normally the case,then these are huge matrix computations,I'd use tons and tons of memory and all that just for a single step.If we had to do many steps,you can imagine how this would take a long time and lots of computing power.Is there anything we can do to expedite this?Well, here's a question: do we need to plug in all our data every time we take a step?If the data is well distributed,it's almost like a small subset of it would give usa pretty good idea of what the gradient would be.Maybe it's not the best estimate for the gradient but it's quick,and since we're iterating,it may be a good idea.This is where stochastic gradient descent comes into play.The idea behind stochastic gradient descent is simply that we take small subsets of data,run them through the neural network,calculate the gradient of the error function based onthose points and then move one step in that direction.Now, we still want to use all our data, so,what we do is the following;we split the data into several batches.In this example, we have 24 points.We'll split them into four batches of six points.Now we take the points in the first batch and run them through the neural network,calculate the error and its gradient and back-propagate to update the weights.This will give us new weights,which will define a better boundary region as you can see on the left.Now, we take the points in the second batch and we do the same thing.This will again give us better weights and a better boundary region.Now, we do the same thing for the third batch.And finally, we do it for the fourth batch and we're done.Notice that with the data,we took four steps whereas,when we did normal gradient descent,we took only one step with all the data.Of course, the four steps we took were less accurate but in the practice,it's much better to take a bunch of slightly inaccurate steps than to take one good one.Later in this nanodegree, you'll have the chance to applystochastic gradient descents and really see the benefits of it.

### 14. Learning Rate-TwJ8aSZoh2U.en

The question of what learning rate to use is pretty mucha research question itself but here's a general rule.If your learning rate is too big then you're taking huge steps which could be fast atthe beginning but you may miss the minimum and keepgoing which will make your model pretty chaotic.If you have a small learning rate you will makesteady steps and have a better chance of arriving to your local minimum.This may make your model very slow, but in general,a good rule of thumb is if your model's not working,decrease the learning rate.The best learning rates are those whichdecrease as the model is getting closer to a solution.We'll see that Keras has some options to let us do this.

### 15. Momentum-r-rYz_PEWC8.en

So, here's another way to solve a local minimum problem.The idea is to walk a bit fast with momentum anddetermination in a way that if you get stuck in a local minimum,you can, sort of, power through and get over the hump to look for a lower minimum.So let's look at what normal gradient descent does.It gets us all the way here. No problem.Now, we want to go over the hump but by now the gradient is zero or too small,so it won't give us a good step.What if we look at the previous ones?What about say the average of the last few steps.If we take the average, this will takes us indirection and push us a bit towards the hump.Now the average seems a bit drastic since the step we made 10steps ago is much less relevant than the step we last made.So, we can say, for example,the average of the last three or four steps.Even better, we can weight each step so thatthe previous step matters a lot and the steps before that matter less and less.Here is where we introduce momentum.Momentum is a constant beta between 0 and 1 thatattaches to the steps as follows: the previous step gets multiplied by 1,the one before, by beta, the one before,by beta squared, the one before,by beta cubed, etc.In this way, the steps that happened a long timeago will matter less than the ones that happened recently.We can see that that gets us over the hump.But now, once we get to the global minimum,it'll still be pushing us away a bit but not as much.This may seem vague, but the algorithms thatuse momentum seem to work really well in practice.

### 16. Error Functions Around the World-34AAcTECu2A.en

So, in this nano degree,we covered a few error functions,but there are a bunch of other error functions around the world that made the shortlist,but we didn't have time to study them.So, here they are. These are the ones you met: thereis Mount Everest and Mount Kilimanjerror.The ones you didn't meet: there is Mt.Reinerror, he's a big Seahawks fan.Straight from Italy, we got Mount Ves-oops-vius,and my favorite, from Iceland,we got the Eyjafvillajokull.This is the famous volcano that stopped all the European flights back in 2012.See what I did there is I took the original name which isEyjafjallajokull and then I changed the word Jalla to Villa,which is Icelandic for error.

### img

## Part 02-Module 02-Lesson 05_Embeddings  Word2Vec

### 01. M4L51 HSA Word Embeddings V3 RENDER V1-ZsLhh1mly9k.en

In this lesson, I want to talk a bit more aboutusing neural networks for natural language processing.We'll be discussing word embedding,which is the collective term for models that learned to map a set of words orphrases in a vocabulary to vectors of numerical values.These vectors are called embeddings,and we can use neural networks to learn to do word embedding.In general this technique is used to reduce the dimensionality of text data.But these embedding models can also learnsome interesting traits about words in a vocabulary.In fact, we'll focus on the Word2Vec embedding model.Which learns to map words to embeddings that contain semantic meaning.For example embeddings can learn the relationshipbetween verbs in the present and past tense.The relationship between the embeddings for walking and walked,should be the same as the relationship between the embeddings for swimming and swam.Similarly embeddings can learn the relationships between words and common genders.Such as between woman and Queen and between man and King.You can think of these embeddings as vectors that have learned tomathematically represent the relationship between words in a vocabulary.A word of caution here. The embeddings are learned from a body of text andso any word associations in that source text will be replicated in the embeddings.If your text contains false information or gender biased associations.These traits will be replicated in your embeddings.In fact debiasing word embeddings isan active area of research and you can read more about it below.In this lesson we'll first talk about how word embedding works in theory,then a walk through a series of notebooks inwhich you'll learn to implement the Word2Vec model.Before we start coding, let's learn more about howembeddings can reduce the dimensionality of text data.

### 02. M4L52 HSA Embedding Weight Matrix V3 RENDER V2-KVCcG5v8fi0.en

We've talked a bit about how neural networks are designed to learn from numerical data.In our case, word embedding is really all aboutimproving the ability of networks to learn from texted data.The idea is this, embeddings can greatlyimprove the ability of networks to learn from text data,by representing that data as lower-dimensional vectors.Let's think about this in an example.Usually, when you're dealing with text and you split things up into words,you tend to have tens of thousands of different words in a large data set.When you're using these words as input to a network like an R&amp;N,we've seen that we can one-hot encode them.What that means is that you have these giant vectors that are like 50,000 units long,and only one of them is set to one,and all the others are set to zero.Then, you pass this long vector as input to some hidden layer in the network.The output of this hidden layer is calculated by multiplyingthat input vector by some matrix of learned weights.The result is a huge matrix of values.Most of which are zero, because of the initial one-hot vector.So, all these computing resources are used on values that do not hold any information,and this is really computationally inefficient.To solve this problem, we can use embeddings,which basically provide a shortcut for doing this matrix multiplication.To learn word embeddings,we use a fully-connected linear layer like you've seen before.We'll call this layer the embedding layer,and its weights are the embedding weights.These weights will be values that are learned during training this embedding model,and they make up a useful weight matrix.With this matrix, we can skip the big multiplication step from before,by instead grabbing the values for the output ofour hidden layer directly from a row in our weight matrix.We can do this because the multiplication ofa one-hot encoded vector with a weight matrix,returns only the row of the matrix that corresponds tothe index of the one or the on input unit.So, instead of doing matrix multiplication,we can use the embedding weight matrix as a lookup table.Instead of representing words as one-hot vectors,we can encode each word as a unique integer.As an example, say we have the word heart encoded as the integer 958.Then, to get the hidden layer values for heart,we just take the 958th row of the embedding weight matrix.This process is called an embedding lookup,and the number of hidden units is the embedding dimension.So, the embedding lookup table is just a weight matrix,and the embedding layer is just a hidden layer.It's important to know that the lookup table holds weights that arelearned during training just like any weight matrix.So, this is the basic idea behind how embedding works.In the next few sections,you'll see how word "the vec" uses the embedding layer to findvector representations of words that contain semantic meaning.

### 03. 3 Word2Vec Notebook V2-4cWzv3YiF_w.en

So in this notebook, I'll be leading you through a Word2Vec implementation in PyTorch.Now, you've just learned about the idea behind embeddings in general.For any dataset with lots of classes or input dimensions like a large word vocabulary,we're basically skipping the one-hot encoding step,which would result in extremely long input vectors of mostly zeros.We're taking advantage of the fact that whenone-hot vectors are multiplied by a weight matrix,we will just get one row of values back.For example, if we have a one-hot vector that has its fourth index on,and we multiply this by a weight matrix,we'll get the fourth row of weights back as a result.So, what we can do,is actually just have input numbers instead of one-hot vectors,and then we can use an embedding weight matrix to look up the correct output.In this case, we see the word heart is encoded as the integer 958,and we can look up the embedding vector for this word inthe 958 row of an embedding weight matrix,this is also often called a lookup table.In text analysis, this is great.Because we already know that we can convert a vocabulary of words into integer tokens.So, each unique word will have a corresponding integer value.If we have a vocabulary of 10,000 words,we'll have a 10,000 row embedded weight matrix,where we can look up the correct output values.These output values which will just be rows inthis weight matrix will be a vector representation of that input word.These representations are called embeddings,and they have as many values as is weight matrix has columns.This width is called the embedding dimension,it's usually some value in the hundreds.Now, Word2Vec is a special algorithm that basically says,"Any words that appear in the same context ina given text should have similar vector representations."So context in this case,basically means the words that come before and after a word of interest.Here are a couple of examples.In a body of text, you'll find a variety of sentences,and these ones all involve drinking some beverage.In some of these cases, even if I removed a word of interests,you may be able to guess what goes in there,just based on the context words surrounding it.So here it says, "I often drink coffee in the mornings.When I'm thirsty, I drink water,and I drink tea before I go to sleep."The mention of I drink before these words,makes these contexts similar.So, we'll expect these words coffee, water,and tea to have similar word embeddings.You can imagine that if we look at a large enough text,we may also see that coffee is more closely associated with morning time and so on.So, just by looking at a word of interest and some context words that surround it,Word2Vec can find similarities between words and relationships between them.In fact, for such similar words,Word2Vec should produce vectors that are very close in vector space,different words will be some distance away from each other.In this way, we're actually able to do vector arithmetic,and that's how Word2Vec confined mappings betweenwords in the past and present tense for example.So, mapping the verb drink to drinking,and swam to swimming,is going to be the same transformation in vector space.In practice, Word2Vec is implemented in one of two ways.The first option is to basically give our model the context,so several words surrounding a word of interest,and have it tried to predict the missing word.So, context words in and a single word out,this is called the continuous bag of words or a CBOW model.The second option is the reverse,to input our word of interests and have our model tried to predict the context.So, one word n and a few you context words out.This is the skip-gram model,and we'll be implementing Word2Vec inthis way because it's been shown to work a bit better.You'll notice that for either of these models,we'll also have to formalize the idea of context to be a window of a specified size.So, something like two words before and two words after a word of interest.Here, for an input word w at time t,we have context words from t minus two to t plus two,that is minus two words in the past and plus two in the future.Notice that the context does not include the original word of interest.So, now that you've been introduced to this notebook and the Word2Vec skip-gram model.Next, I'll show you the data that we'll be working with and give you your first exercise.

### 06. 4 Data Subsampling V1-7SJXv2BQzZA.en

Okay, let's get started with implementing the skip-gram word2vec model.The first thing you want to do is load in the necessary data.In this example, I'm using a large body of textthat was scraped from Wikipedia articles by Matt Mahoney.If you're working locally,you'll actually need to click this link to download this data as a zip file,and we'll move it into our data directory, and unzip it.You should then be left with a file just called text8 in our data directory.So, I've already put that data in the data directory, and here,I'm loading that file in my name and printing out the first 100 characters.It looks like the first section of text is about anarchism and the working class.So I loaded that in correctly,and then I want to do some preprocessing.Essentially, I want to break this text up intoa giant list of words so that I can build up a vocabulary.So here, I'm going to do that using a function in theprovided utils.py file called preprocess.Let's actually take a look at this code.So here's our utils.py file and our preprocess function.This function takes in some text and you can see that it does a few things.First, in all of these lines,it converts any punctuation into tokens.So a period is changed to a bracketed period token and so on.Next, we see that it stores the number of timescertain words appear in the text using a Counter.A Counter is a collection that will basicallyreturn a dictionary of words and their frequency of occurrence.Here, we're creating a list of trimmed words that basically cutsall words that show up five or fewer times in this dataset.This should greatly reduce issues due to noise in the data,and it should improve the quality of the vector representations.Then, finally, it returns those trimmed words.So back to our notebook,I'm going to say words equal utils.preprocess text,and I'll print out the first trim 30 words.This may take a few moments to run since our text data is quite big.Then you should see an output like this.Pretty much the same text that we saw above,only the words are split into a list.Here, I'm going to print out some statistics about this data.I'm printing out the length of the text so a word count of our data,and I'll print out the number of unique words.To get the number of unique words,I'm using the built-in Python data type set,which if you recall from the last lesson,will get rid of any duplicate words.So, we have a set of only unique words in this text.So, you can see that we have over 16 million words in this text,and over 60 thousand unique words,and these numbers will be useful to keep in mind as we continue processing.Next, I'm creating two dictionaries to convertwords to integers and back again, integers to words.This is our usual tokenization step.This is again done with a function in the utils.py file, create lookup tables.So, let's take a look at what this function is doing.So, this function takes in a list of words in a text and it returnstwo dictionaries that map from our vocabulary to integer values and back.You may notice an interesting use of counter here.First, this is creating a sorted vocabulary.So this is a list of words from most to leastfrequent according to the word counts returned by counter.Then integers are assigned in descending frequency order.So the most frequent word like B is given the integer 0,and the next most frequent is 1 and so on.So in our notebook, this function returns are two dictionaries.Once we have those, the words are then converted tointegers and stored in the list into words.I'll print out the first 30 tokenized words here just to check that they make sense.So, if we look at these values and back to our list of words above,we'll be able to see that 'the' and 'of'are some of the most common words in our dictionary.We can see that 'the' is tokenized as the integer 0,and it looks like 'of' is the next most frequent word tokenized as 1.We have over 60,000 words in our vocabulary,so all of these token value should be integer values in that range.Now, our goal is to implement word2vec,which relies on looking at the context around a word of interest.We want to define our context very carefully,basically looking at a window of the most relevant words around a word of interests.There are some words that are almost nevergoing to be relevant because they're so common,words that show up anywhere and really often such as the, of, and for.These don't provide much context to other nearby words.So if we discard some of these common words,we can remove some noise from our data, and in return,get faster training and better vector representations.This process is called subsampling.This will be your first task.Subsampling works like this.For each word wi in our training set,you want to discard it with a probability given by this equation.The probability of discarding a word w is equal to1 minus the square root of t over that words frequency,and t is the threshold value that we set.So say, we're thinking of discarding the word 'the', word index 0.Let's say, it occurs one million times in our 16-million long dataset.These are approximations but this is one million over16 million here the frequency of occurrence.The numerator is a threshold I've set,which is 1 times 10 to the negative fifth.So, if I just run these values through our equation,I'm going to get a probability of getting rid of this word 98.7 percent of the time.Even after discarding the majority of these inner text willstill leave over 12,000 of the original one million these inner text.The idea with subsampling is really to just get rid ofa lot of these frequently occurring words so that they're not alwaysaffecting the context of other words whilesimultaneously keeping enough examples to learn a word embedding for that word.So, the subsampling equation says the probability thatwe discard a word is going to be higher if that word's frequency is higher.Here I provided some code,a threshold to start you out,and a dictionary of word counts.This is using the counter collection which takes in our list of encoded words,and returns how many times they appear in that list,and I can print out the first key value pair in this list.So here, I can see that the word token 5233 appears 303 times in our text.I want you to use this information to calculatethe discard probability for each word in our vocabulary,then use that to create a new set of data train words,which will basically be our original list ofint words only with some of our most frequent words discarded.This is more of a programming challenge rather thana deep learning task but preparing data is an important skill to have,so try to solve this task,and next, I'll show you my solution.

### 07. 5 Subsampling Solution V1-YXruURuFD7g.en

Here is my solution for creating a new list of train words.First, I calculated the frequency of occurrence for each word in our vocabulary.So, I stored the total length of our text in a variable, total_count.Then, I created a dictionary of frequencies.For each word token and count in the word counter dictionary that was given,I added an item to this dictionary,where the word was the key and the value was the count ofthat word over the total number of words in our text, the frequency.Then, I calculated the discard probability as p_drop.This is another dictionary that maps words to the drop probability.Here, I'm just using the subsampling equation to get that,which is 1 minus the square root of our threshold over that word's frequency.Finally, I created a new list of train words.For each word in our list of int_words,I said I'll keep this word with some probability.So, I generated a random value between zero and one,and I checked if that value was less than 1 minus the drop probability for that word.This is saying, okay,I want to keep this word with a probability of 1 minus p_drop.So, if I have a drop probability of 0.98,then the keyboard probability is 1 minus this p_drop,which will be 0.02.If I generate a value less than 0.02,which is unlikely, only then will I keep this word in my list of train words.There are other ways to solve this problem,but I like to frame this as a which words do I keep task.Okay. Then I'm printing out the first 30 words of this train data.This should look similar to the first 30 tokens in our int_words list.Only you'll notice that most of the zeros and ones are gone.These were our most common words from before,and so this is looking as I expect,and I can move on to the next step,which will be defining a context window and batching our data.

### 08. 6 Defining Context Targets V1-DJN9MzD7ctY.en

Now that our data is in good shape,we need to get it into the proper form to pass it into our network.With the skip-gram architecture for each word in the text,we want to define a surrounding context and grab all the words ina window around that word with size C. When I talk about a window,I mean a window in time.So, like two words in the past and two words in the future from our given input word.More generally than two words in the past and future,I'm going to say we want to define a window of size C. Here,I have some text from the Mikolov paper on Word2Vec,"Since the more distant words are usually lessrelated to the current word than those close to it,we give less weight to the distant words by samplingless from those words in our training examples.If we choose C equals five,for each training word,we'll randomly select a number R in range one to C,and then use R words from history and R wordsfrom the future of the current word as correct labels."So, this is saying that we don't want to choose too big ofa window because too big of a window will give us irrelevant context.In other words, good context words are usuallythe ones closest to the current word rather than farther away,and we want to include some randomness in how we define our context.If we define a context window of size C equals five,then we'll create a range R that's going to be a random integer between one or five.So, say we get an R equal to two as an example,then we'll define the context around a given word to bethe two words that appear right before and after our word of interest.I have an example here.Say we're interested in the word at the second index in this list, 741.If we randomly generate an R equal to two,we'll be interested in the two tokens before and after this word.I want you to write a function that will return context words in a list like this.This will be the function get_target,which takes in a list of word IDs,and index of interests,and a context window size.So, the effect of getting words within a random rage R instead of a consistentlarger range C is that you're morelikely to get words that are right next to your current word,and less likely to get words that are further away from your current word.So, what you're really doing is going to be training on context words that arecloser to your word of interests and likely more relevant, more often.So here, I've left this function for you to fill out.Now, there are some special cases.If the index that's passed in is zero oryour range cannot go back in the past as far as you want,then you can start your context at the start of the past and list of words.You can test out your implementation in this cell below.Next, we'll use this function to actually batch the data,and so it's important that this is implemented correctly.As usual, if you're stuck or want to see my solution,checkout my solution video next.

### 09. 7 Batching Data Solution V1-nu2rjLzt1HI.en

Here's how I'm defining the context targets around a given word index.First, according to the excerpt from the paper,I'm going to define a range R. R is going to bea random integer in the range one to c, the window size.randint takes in a range that is not inclusive of the last number.So, that's why I have a plus one here.Then I define the start and stop indices of my context window.The start will be a range of words in the past.That is the index of my word of interest minus my range R.This will only happen as long as that doesn't get us to a negative index.If this operation does give us a negative value,then I just set my start index to the startup my list of words index zero.Then my stop index is where my feature words end.So, my word of interests index,plus our range R. Finally,I do not want my return target context to include the word at the past in index.So, I'm defining my target words as the wordsbehind my index of interest from start to idx,plus the words in front,idx plus one to stop plus one.Then I'm returning these words as a list.Then when I go to test this out on a test set of word tokens,and I can run this a couple of times,I see that I get a variable length of words around my past an index of five.I can see the target does not include my index of interest.These line up just because I've created some input data,that's the integer zero through nine.If you run the cell multiple times,you will see a different target based on a differentrandomly generated R. So, this looks good.Right below this function,I've defined a generator function.This function we'll use our get_target function that we've just defined.get_batches takes in a list of word tokens a batch_size and a window_size.It makes sure that we can make complete batches of data.In this four loop, I'm iterating over our words one batch length at a time.I get a batch of words then for each word in a batch I'm calling get_target.This should return a batch of target words in a window around the given batch word.I'm calling extend here so that each batch x and y will be one row of values.Here, I'm making x the same length as y.Finally, it returns this list of input words x and target context words why using yield,which makes this a generator function.Then in the blue cell,we can test this batching out to see what itlooks like when applied to some fake data here.So, I'm getting an x and y batch of data by calling next on our generator function.Here, I've passed in some int_text,a batch_size of four, and a window_size of five.When I run this cell,this output might look a little weird because everything's been extended into one row.But I can see that I've made my desired four batchesbecause I have four different input x values;zero, one, two, and three.If we take a look at the first input zero,we see is length three.So, the target must have also been length three.The corresponding context is one, two, three.All the targets in the future window that surroundthe input index zero, which is what I expect.For the other input, output batches,I can see that I'm generating targets that surround the input values one, two, and three.So, we have our batch inputs,and our target context.Now, we can get to defining and training a word to vec model on this batch data,which I'll go over next.

### 10. 8 Word2vec Model V2-7BEYWhym8lI.en

Now that we've taken the time to preprocess and batch our data,it's time to actually start building the network.Here, we can see the general structure of the network that we're going to build.So, we have our inputs,which are going to be like batches of our train word tokens,and as we saw when we loaded in a batch,a lot of these values will actually be repeated in this input vector.So, we're going to be parsing in a long list of integers,which are going into this hidden layer, our embedding layer.The embedding layer, is responsible for looking atthese input integers and basically creating a lookup table.So, for each possible integer value,there will be a row in our embedding weight matrix,and the width of the matrix will be the embedding dimension that we define.That dimension will be the size of the embedding layers outputs.Then these embeddings are fed into a final fully connected softmax output layer.Remember, that in the skip gram model,we're parsing in some input words and we're trainingthis whole model to generate target context words.So, for one input value,the targets will be randomly selected context words from a window around the input word.Our output layer, is going to output the probability thata randomly selected context word is going to be the word the,or of, or nine,or any other word in our vocabulary.We're going to be trying to predictour target context words using the outputs of the softmax layer.Basically, looking at the words with the highest probability that they are context words.Then when we train everything, what's going to happen,is that our hidden layer is going to formthese vector representations of the input words.So, each row in the embedding look-up table will be a vector representation for a word.Row zero, will be the embedding for the word the, for example.These vectors contain some semantic meaning,and that's what we're really interested in.We only really care about these embeddings.From these embeddings, we can do some interesting things.Performing vector math to see which of our words are most similar,or we can use these embeddings as input toanother model that works with the same text input data.So, when we're done training,we can actually just get rid of this last softmax layer,because it's just there to help us train this model andcreate correct embeddings in the first place.Okay. So, right before we define the model,I have a function that will help us seewhat kind of word relationships this model is learning.When I introduced the idea of word2vec,I mentioned that representing words as vectors,gives us the ability to mathematically operate on these words in vector space.To see which words are similar,I'm going to calculate how similar vectors are using cosine similarity.Cosine similarity, looks at two vectors a and b and the angle between them,theta. It says, "Okay.The similarity between these two vectors is just the cosine of the angle between them."If you're familiar with vector math,that can also be calculated as the normalized dot product of a and b.You can really just think of it like this.When theta is zero,cosine of theta is equal to one.This is the maximum value that cosine can take.When theta is 90 degrees or rather these vectors are orthogonal to one another,then the cosine is going to be zero.So, the similarity really ends up being a value between zero and one,that indicates how similar two vectors are in vector space.So, let's look at this cosine similarity function.This function takes in an embedding layer,a validation size, and a validation window.In here, I'm getting the embeddings from the pasting layer.These are just the layer weights.Then, I'm doing some math and storing the magnitudes of these embedding vectors.That magnitude is just going to bethe square root of the sum of the embedded vectors squared.Then, I'm randomly selecting some common and uncommon validation word examples.These are just integers in a range, in this case,from zero to 1,000 for common words,and for a higher range for uncommon words.Recall that lower indices indicate that a word appears more frequently.So, I'm generating half of our validation examples from a more common range,and half from a more uncommon range.These are collected in an np array and then converted into a long tensor type.Then I'm passing these validation examples into the embedding layer.In return, I get their vector representations back.So, these validation words are encoded as our vectors, a,and we're going to calculate the similarity between a,and each word vector,b, in the embedding table.We mentioned that the similarity is a dot product of a and b over the magnitude.This dot product is just a matrix multiplication betweenthe validation vectors a and the transpose of the embedded vectors b.Here, I'm dividing by the magnitude,and this is not the exact equation here,but it will give us valid values for similarities,just scaled by a constant.This function returns the validation examples and similarities.This gives us all we need to later print out the validation wordsand the words in our embedding table that are semantically similar to those words.It's going to be a nice way to check that our embedding tableis grouping together words with similar semantic meanings.So, this is a given function,you don't have to change anything about this.Now, on to defining the model.So, we know our model accepts some inputs,then it has an embedding layer,and a final softmax output layer.You'll have to define this using PyTorch's embedding layer,which you can read about here.Here's the documentation.So, the embedding layer is known as a sparse layer type.It takes in a number of input embeddings,which is going to be the number of rows inyour embedding weight lookup matrix and an embedding dimension.This is the size of each embedding vector.The number of columns in your embedding look-up table.These two are the most important inputs when defining this layer.So, after the embedding layer,you'll define a linear layer to go fromour embedding size to our predicted context words.You'll also have to apply a softmax function to the output,so that this model returns word probabilities.So, here's the skeleton code for this model,and when we instantiate this model,we're going to be parsing in input values for n_vocab,the size of our vocabulary,and n_embed, our embedding dimension.So, you should be able to complete the init and forward functions for this model.When you do that, you should be able to proceedwith training using the provided training loops below.I'd really recommend training on GPU.Training this particular model takes quite a while even on GPU.So, I'd start training with maybe just one or two epics for now. All right.So, I'll leave this as an exercise,and next I'll go over one solution for defining a skip-gram model and training it.

### 11. 9 Model Validation Loss V2-GKDCq8J76tM.en

This is what we want our model to look like.It should take in some inputs and then put those through an embedding layer,which produces some embedded vectors that are sent to a final softmax output layer,and here's my model definition.You can see that it's a pretty simple model.First, I'm defining my embedding layer, the self.embed.This takes n the length of my word vocabulary.This means that it will create an embedding weight matrix thathas a row for each of the words in our vocabulary,and this will output vectors of size n_embed, our embedding dimension.Then, I have a fully-connected layer that takes in that embedding dimension as input,and its output size is also the length of our vocabulary.That's because this output is a series of word class scoresthat tells us the likely context word for a given input word,and then I've defined a softmax activation layer here.You could have just done this in the forward function too.This is just one solution.Then at my forward function,I'm passing in my input X into the embedding layer.This returns are embeddings which moves to our fully-connected layer,which returns a series of class scores.Finally, a softmax activation function isapplied and I'll be left with my log probabilities for context words.Below in this training section,I'm actually going to instantiate this model.So here, I've defined an embedding dimension and I've set this to 300,but you're welcome to experiment with larger or smaller values.The embedding dimension can be thought of asthe number of word features that we can detect,like the length, the type of word, and so on.So, this takes in the entire length of our vocabulary and the embedding dimension,and I've moved this to a GPU for training.Here, you'll see that I'm using negative log-likelihood loss,and this is because a softmax in combination withnegative log-likelihood basically equals cross entropy loss.So, this is a great loss for looking at probabilities of context words,and I'm using an Adam optimizer,which is just my go-to and I'm passing in my model parameters and a learning rate.Then I have my training loop and I've decided to train for five epochs.In this training, actually took a few hours even on GPU,so I'd recommend that you train for a shorter amount oftime or wait until I show you how to train more efficiently.So, in my training loop, I'm getting batchesof data by calling the generator function thatwe defined above and passing in my list of chain words and a batch size.I'm getting my inputs and my target context words,and I'm converting them into LongTensor types,and moving these two GPU if it's available,and then I'm performing backpropagation as usual,and passing my inputs into my skip-gram modelto get the log probabilities for the context words.Then I'm applying my loss function to these contexts words and my targets,then performing backpropagation and updating the weights of my model,not forgetting to zero out any accumulated gradients before these two steps.Then I'm printing out some validation examples using my cosine similarity function.Here, I'm passing in my model and a GPU device,and I'm getting back some validation examples and their similarities.Here, I'm actually using topk sampling to getthe top six most similar words to a given example.Here, I'm iterating through my validation examples.I'm printing out the first validation word and thenthe five closest words next to it after a line character,and here are some initial results.I printed a lot of data after training for five epochs.At first, these word associations look pretty random.We have and, returns,liverpudlians, and so on.But as I train, I should see thatthese validation words are getting more and more similar.If I scroll down all the way to the end of my training,I can see that similar words are nicely grouped together.You can see a bunch of number words are grouped together.Here, I have a bunch of animals and mammals grouped in one line,some lines that are related to states and politics,and even lines that are related to a place and a language.So, it looks like my word2vec model is learning,and I can visualize these embeddings in another way too.Another really powerful method for visualization is called t-SNE,which stands for t-distributed stochastic neighbor embeddings.It's a non-linear dimensionality reduction technique that aims to separatedata in a way that cluster similar data close together and separates different data.In this case, it's an algorithm that I'm loading in from the sklearn library.I give it the number of embeddings that I want to visualize,and I get these embeddings from the weights ofour embedded layer which I'm calling by name from our model.So, remember that our embedding layer was just named embed,and I can get the weights by same model.embed.weight.So here, I'm applying t-SNE to 600 of our embeddings,and this is what this t-SNE clustering ends up looking like.We can actually see that similar words are grouped together.Here we have east, west, north, and south.If we look to the right, we can see some musical terms: rock,music, album, band, and song.Lower down, we can see some religious terms,some colors over here,some academic terms: school, university, and college.On the left side here,I can see clusters of the months in the year and it looks like a few integer values here.So, this clustering indicates that my word2vec model has worked.It learns to generate embeddings that hold semantic meaning,and this also gives us a cool way to visualize the relationships between words in space.So, one problem with this model was that it took quite a while to train,and next I'm going to address that challenge.

### 12. 10 NegativeSampling V1-gnCwdegYNsQ.en

Now, the last model took quite a while to train,and there are some ways that we can speed up this process.In this video, I'll talk about one such method which is called negative sampling.So, this is a new notebook,but it contains basically the same info asour previous notebook including this architecture diagram.This is our current architecture where we have a softmax layer on the output,and since we're working with tens of thousands of words,the softmax layer is going to have tens of thousands of units.But with any one input,we're really going to have one true context target.What that means is, when we train,we're going to be making very small changes to the weights betweenthese two layers even though we only have one true output that we care about.So, very few of the weights are actually going to be updated in a meaningful way.Instead what we can do is approximate the loss from the softmax layer,and we do this by only updating a small subset of all the weights at once.We'll update the weights for what we know to be the correct target output,but then we'll only update a small number ofincorrect or noise targets usually around 100 or so as opposed to 60,000.This process is called negative sampling.To implement this, there are two main modifications we need to make to our model.First, since we're not taking the softmax output over all the words,we're really only concerned with one output one at a time.Similar to how we used an embedding layer to mapan input word to a row of embedding weights,we can now use another embedding layer tomap the output words to a row of hidden weights.So, we'll have two embedding layers,one for input words and one for output words.Second, we have to use a modified loss function that only cares aboutthe true target and a small subset of noisy and correct target context words,and that's this big loss function here.It's a little heavy on notation,so I'll go over it one part at a time.Let's take a look at the first term.We can see that this is a negative log operation,and this little loop, this lowercase sigma is a sigmoid activation function.A sigmoid activation function scales any input from a range from zero to one.So, let's look at the input inside the parentheses.UW0 transpose is the embedding vector for our output target word.So, this is the embedding vector that we know asthe correct contexts target for a given input word.This T here is the transpose symbol.Then we have VWI which is the embedding vector for our input word.In general, you will indicate an output embedding and V are input.If you remember from doing cosine similaritya transpose multiplication like this is equivalent to doing a.product operation.So, this whole first term is same that we take the log sigmoid of the.productof our correct output word vector with our input word vector,and this represents our correct target loss.Next, we want to sample our outputs and get some noisy target words,and that's what the second part of this equation is all about.So, let's look at this piece by piece.This capital sigma means we're going to take a sum over all of our words WI.This P and W indicates that these words are drawn from a noise distribution.The noise distribution is our vocabulary ofwords that are not in the context of our input word.In effect, we want to randomly sample words fromour vocabulary to get these noisy irrelevant target words.So P and W isan arbitrary probability distribution whichmeans we can get to decide how to weight the words that we're sampling.This could be a uniform distribution where we sample all words withequal probability or it could be according tothe frequency that each word shows up in our text corpus,the unigram distribution UW.In fact the authors of the negative sampling paper foundthe best distribution to be a unigram distribution raised to the three-fourths.Then we get to this last part which looks very similar to our first term.This takes the log sigmoid of the negated.product between a noise vector UWI,and our input vector from before.To give you an intuition for what this whole loss is doing here,remember that this sigmoid function returns a probability between zero and one.So, the first term in this loss is going to push the probability thatour network will predict the correct context word towards one.In the second term, since we're negating the sigmoid input,we're pushing the summed probabilities that our network willpredict the incorrect noisy words towards zero.Okay. So next, I'll present your task which will be todefine this negative sampling model in code.

### 15. 11 SkipGram Negative V1-e7ZrzpyXNDs.en

All right. So, we have two tasks to complete,to define a more efficient Word2vec skip-gram model.Here, I'm calling this model skip-gram neg to include negative sampling.This model takes in our usual vocab and embedding dimension.It also takes in a noise distribution, if it's provided.Okay. So, first, we want to define two embedding layers,one for input and one for output words.Here, I'm calling those in_embed and out_embed.I want you to define these layers,such that they can accept an input or output target as inputand return an embedding that's a vector of dimension in_embed.I'll also suggest that you initialize the weights of these layersusing a uniform distribution between negative one and one.Now, let's look at our loss function for a moment.When we think about defining a negative sampling loss,we know that this loss will take in a few things as input.It will for sure take in our input word embedding, vwi.It will also take in our correct output word embeddinguw0 and several noisy incorrect embeddings uwi.So, in this model definition,I'm actually going to ask you to definethree different forward functions for creating these embeddings.The first forward input should return our input embeddings,which are just going to be our input words passed through our input embedding layer.Similarly, forward output, which should returnoutput vectors for passed and output words.Finally, a forward noise function, this one is special.It takes in a batch size and a number ofnoise samples to generate for performing negative sampling.This function first gets noisy samples from a passed in noise distribution.If no distribution is passed in,this will default to uniform distribution.Now, it gets a sample of noise words usingtorch.multinomial and gets batch size times n_samples of values.In this line, those words are being moved to a GPU, if available,and what you need to do to complete this function is pass these wordsthrough the output embedding layer to get their respective embeddings.So, you get our noise embeddings and then you shouldreshape these embeddings to be batch size by n_samples,by n_embed in dimension. All right.So, complete these forward functions,making sure to return correct embeddings for each forward function.If you've completed this implementation,you should be able to proceed with training this model.Next, I'll go over one solution for this model and I'llshow you how I defined a custom negative sampling loss.

### 16. 12 CompleteModel CustomLoss V2-7SqNN_eUAdc.en

So, I ran all the cells in my notebook and here'smy solution and definition for the SkipGramNeg Module.First, I've defined my two embedding layers, in-embed and out-embed,and they'll both take in the size ofour word vocabulary and produce embeddings of size and embed.So, mapping from our vocab to our embedding dimension.Here, I'm doing an additional step which is initializingthe embedding look-up tables with uniform weights between negative one and one.I'm doing this for both of our layers and I believethis helps our model reached the best way faster.Then I've defined my three forward functions.Forward input passes our input words through our inputembedding layer and returns input embedding vectors.I do the same thing in forward output only passing thatthrough our output embedding layer to get output vectors.Notice that there are no linear layers or softmax activation functions here.The last forward function is forward noise,which will return a noisy target embeddings.So, this samples noisy words from our noise distribution andreturns the number of samples batch size times N samples.Then we get the embeddings bypassingthose noise words through our output embedding layer.In the same line,I'm reshaping these to be the size I want,which is batch size by N samples by our embedding dimension and I return those vectors.Okay, so this completes the SkipGramNeg Module.Next, I'm defining a custom negative sampling loss.This was carefully defined above in our equations and Ihaven't ever gotten into the details of defining a custom loss,but suffice to say that it is really similar to defining a model class.Only in this case, the init function is leftempty and we're really left with defining the forward function.The forward function should take in some inputs and targetstypically and you can define what it takes in as parameters here.This should return a single value that indicates the average loss over a batch of data.So, in this case, I know I what my loss to look at an input embedded vector,my correct output embedding,and my incorrect noisy vectors.So here, I am getting the batch size andembedding dimension from the shape of my input vector,then I'm shaping my input vector into a shape that is batch first,and I'm doing something similar to my output vector here,only I'm swapping these last two dimensionsone an embed size effectively making this the output vector transpose.This way, I'll be able to calculate the.product betweenthese two vectors by performing batch matrix multiplication on them,and that's just what I'm doing here.First, I'm calculating the loss term betweenmy input vector and my correct target vector.I'm using batch matrix multiplication and then applying a sigmoid and a log function.Here, I'm squeezing the output so that no empty dimensions are left in the output.Next, I'm doing something similar only betweenmy input vector and my negated noise vectors.So, this is the second term in our loss function.I'm using batch matrix multiplication,applying a sigmoid and a log function,and then I'm summing the losses over the sample of noise vectors.Okay finally, I'm adding these two losses up negating them since I kept thempositive during my calculations and taking the mean of this total loss.This way, I'm returning the average negative sample loss over a batch of data.Then I can move on to creating this model and training it.This training loop will look pretty similar to before,but with some key differences.First, I'm creating a unigram noise distribution thatrelates noisy vectors to their frequency of occurrence,and this is a value I calculated earlier in this notebook.So, I'm defining our noise distribution asthe unigram distribution raised to a powerof three-fourths as was specified in the paper.Then, I'm defining my model passing in the length ofour vocabulary and embedding dimension which I left as 300,and this noise distribution that I've just created,and I'm moving this altered GPU.Then I have another key difference,instead of using NLL loss,I'm using my custom negative sampling loss that I defined above.In my training loop, I'll have to pass in three parameters to this loss function.So, I'm training for five epochs again,getting batches of input and target words.Then using my three different forward functions I'm getting my input embedding,my desired output embedding,and my noise embeddings.So, forward input takes in my inputs,forward output takes in my targets and forward noise takes in two parameters.It takes in a batch size and a number of noise vectors to generate.Then to calculate my loss,I'm passing in my input,output and noise embeddings here.Then, I just have the same code as before,performing backpropagation and optimization steps as usual,and I have my validation similarities that I'm going to printout along with the epoch and loss, a little more information.So, note that I chose to define my three different forward functionsjust so I a get the vectors that I needed to calculate my negative sampling loss here.You can try training this yourself just to see how much faster this training is.Then imprinting data less frequently because it's generated quicker.So here, after the first epoch,we see our usual sort of noisy relationships.But by the end of training,we see words grouped together that makes sense.So, we have mathematics, algebra,calculus, we have ocean,islands, Pacific, Atlantic,and some smaller words that all seem to be grouped together as well.Once again, I visualize the word vectors using T-SNE.This time I'm visualizing fewer words and I'm gettingthe embeddings from our input embedding layer only.Then I'm passing these embeddings into our T-SNE model and this is the result I get.I can see some individual integers grouped over here,some educational terms and war and military terms over here.I see some governmental terms and other relationshipsand it's pretty interesting to poke around a visualization like this.The word2vec model always makes me think abouthow a learned vector space can be really interesting.Just think about how you might embed images and find relationships betweencolors and objects or how you might transform words using vector arithmetic.Building and training this model was also quite involved and if you feel comfortable withthis model code and especially manipulating models toadd your own forward functions and custom loss types,you've really learned a lot about the Pythonic nature of PyTorch and model customization.In addition to implementing a very effective word2vec model.So, great job on making it this far,and I hope you're excited to learn even more.

## Part 02-Module 02-Lesson 06_Sentiment Prediction RNN

### 01. 1 SentimentRNN Intro V1-bQWUuaMc9ZI.en

Welcome to this lesson on Sentiment Analysis with an RNN. All right.So, in this notebook, I want to give you one more LSTM example,and in this case, we'll actually be training an RNN tosolve our sentiment analysis task from a few lessons ago.Sentiment analysis is all about taking in some text,in this case movie reviews,and predicting the sentiment of that review.So, whether it's positive or negative.Recall that Andrew Trask showed you how to build a sentiment analysis model from scratch,and he also did some really cool visualizations with this data.What I want to do now is use this as an opportunity to show youhow well an RNN versus just a feedforward network performs on this task.My thinking is that an RNN should work really well becausewe can include information about the sequence of words in a movie review.I've broken this notebook up into a series of exercises,and I'll leave them pretty open-ended.You'll be tasked with pre-processing some text data andbuilding a model that includes both an embedding and LSTM layer.I'll also be showing you a few more things that are useful toknow about batching data and making predictions.Next, you'll be able to access this notebook,and I'd suggest that you keep the exercise notebook open.You can have it open in one tab and watch my exercise and solution videos in another,moving back and forth between absorbing information and practicing what you've learned.

### 04. 3 Data PreProcessing V1-Xw1MWmql7no.en

So, let's get started with sentiment analysis.First, I'm going to load in data from our data directory.In here, there are two files reviews.txt and labels.txt.These are just the text files for our movie reviewsdata and their corresponding labels, positive or negative.So, I'm going to load these in and print out some of their contents.Here, you can see some example review textthat's talking about a comedy called bromwell high.And here you see some of the text and the label's file,which just has lines positive and negative.Actually, this looks like just one review and Iwant to see if I can print out more than one.All right. So here, I've started printing out a second reviewhere and you can see that these two are separated by new line characters,much like positive and negative are separated by new lines.Now, we already know that we need to pre-process this data and to organize all ofthe words in our vocabulary so that we have numerical data to feed to our model later.Since we're using an embedding layer,we'll need to encode each word asan integer and we'll also want to clean up our data a bit.The first pre-processing steps I want to take are turnour text to lowercase and getting rid of extraneous punctuation.Punctuation that, in this case,will not really have any bearing on whetherour review is classified as positive or negative.Okay. So in this cell,I'm converting all my review text tolowercase and I'm getting rid of everything that is punctuation.And I'm using a built-in Python list here,which is from string import punctuation,and I'm going to print out what all is in there.So, punctuation is just a list of all of these punctuation characters.Then for our reviews, I'm looking at every character and ifit's not and the punctuation list, I'm keeping it.This gives me a version of the review text that is all text no punctuation.So, I'm storing that in this variable all_text.Next, I know that my reviews are separated by a new line characters slash n. So,to separate out our reviews,I'm going to split the text into each review using slash n as the delimiter here.Then I can combine all the reviews back together as one big string.Finally, I get to my end goal,which is splitting that text into individual words.So, I'll run the cell and print out the first 30 words,and it looks just as I expect.Essentially, the original text that I printed outonly all the punctuation is removed and we've separated everything into individual words.So, our data is in good shape,and by now you should know what's coming next.We have to take our word data andour label text data and convert this into numerical data.Your first couple of exercises will be to createa dictionary vocab_to_int that can convert any unique word into an integer token.Then using this dictionary,I want you to create a new list of tokenized words,all the words in our data but converted into their integer values.I'd also like it so that our dictionary maps more frequent words to lower integer tokens.One important thing to note here is that later,we're going to pad our input vectors with zeros.So, I actually do not want zero as a word token.I want the tokenized values to start at one.And so, the most common word in our vocabulary should be mapped to the integer value one.So, create that dictionary,use it to tokenize our words,and then store those tokens in a list, reviews_ints.Below this, I provided some code that lets you test your implementation.It'll print the length of your vocabulary and it willprint the first review in your tokenized review list.Your next and similar task is going to be to encode our label text into numerical values.We saw that this text was just lines of positive or negative,and I want you to create an array encoded labelsthat converts the word positive to one and negative to zero.I'm not providing any testing code here,but I encourage you to get in the habit of testingout your own code piece by piece as you build.It's good practice and can be as simple asa few print statements to check that your data isconverted as you expect or that it's the correct size and so on.These checks can really save some time later onbecause these code blocks really build on one another,and it's good to debug early and often.Okay. So, try encoding all of our words and labels on your own.And if you get stuck or want to check your solution,feel free to look at the solution video next.

### 05. 4 EncodingWords Sol V1-4RYyn3zv1Hg.en

First, here's how I went about creating a vocabto int dictionary and encoding our word data,and there are a few ways to do this.I chose to use this important counter to create a dictionary thatmaps the most common words in our reviews text to the smallest integers.So the first thing I'm doing is to get a count of how many times each of our wordsactually appears in our data using counter and passing in our words.Then with these counts, I'm creating assorted vocabulary.This sorts each unique word by its frequency of occurrence.So this vocab should hold all of the unique words thatmake up our word data without any repeats,and it will be sorted by commonality.I also know that I want to start encodingmy words with the integer value of one rather than zero.So the most common word like be or of should actually be encoded as one.I'm making sure that we start our indexing at one by usingenumerate and passing in our vocab and our starting index, one.Enumerate is going to return a numerical value,ii, and a word in our vocabulary. It will do this in order.So our first index is going to be one,and the first word is going to be the most common word in our assorted vocabulary.So to create the dictionary vocab to int,I'm taking each unique word in our vocab andmapping it to an index starting at the value one.Great. Next, I'm using this dictionary to tokenize all of our word data.So here, I'm looking at each individual review.Each of these is one item and review splitfrom before when I separated reviews by the newline character.Then, for each word in a review,I'm using my dictionary to convert that word into its integer value,and I'm appending the token as review to reviews_ints.So the end result will be a list of tokenized reviews.Here in the cells below, I'm printing outthe length of my dictionary and my first sample encoded review.I can see that my dictionary is a bit over 74,000 words long,which means that we have this many unique words that make up our reviews data.Let's take a look at this tokenized review.I'm not seeing any zero values which is good,and these encoded values look as I might expect.So I've successfully encoded the review words,and I'll move on to the next task,which is encoding our labels.So in this case, I want to look atmy label's text data and turn the word positive into one and negative into zero.Now we haven't much processed our labels data,and I know much like the reviews text that a new label is on every new line in this file.So I can get a list of labels, labels_split,by splitting our loaded in data using the newline character as a delimiter.Then I just have a statement that says,for every label in this label_split list,I'm going to add one to my array if it reads as positive,and a zero otherwise.I'm wrapping this in np.array,and that's all I need to do to create an array of encoded labels. All right.This is a good start. There are still a few data clean up andformatting steps that I'll want to take before we get to defining our model.So let's address those tasks next.

### 06. 5 GettingRid ZeroLength V1-Hs6ithuvDJg.en

After encoding all our word and label data as an additional preprocessing step,we want to make sure that our reviews are in good shape for standard processing.That is, our network will expect a standard input text size,and so we'll want to shape our reviews into a consistent specific length.There are two things we'll need to do to approach this task.First, I'm going to take a look at the review data and see do we haveany especially sure or longer views that might mess with our training process.I'll especially look to see if we have any reviews of length zero whichwill not provide any text information and will just act as noisy data.If I find any of those zero length reviews,I'll want to remove them from our data entirely.Then second, I'll look at the remaining reviews,and for really long reviews,I'll actually truncate them at a specific length.I'll do something similar for shortest reviews and make surethat I'm creating a set of reviews that are all the same length.This will be our padding and truncation step,where we basically pad our data with columns of zerosor remove columns until we get our desired input shape.Okay. Before we pad our review text,we should check for reviews of length zero.The way I'm gonna do this is to use a counter.For each review length that's currently in our data,whether that's a length of zero or thousands of words,I'll look at how many reviews are of that length.So this returns a dictionary ofreview lengths and account for how many our reviews fall into those lengths.So, here I'm looking at how many of our reviews arezero length and I'll also print out the longest review length just to see.So, when I run this cell,I can see that I have one review that is zero lengthand that my longest review has over 2,000 words in it.This zero length review is just going to add noise into our dataset.So next, your task will be to create a new list ofreviews_ints and an array of encoded labels,where any reviews of zero length will be removed from this data.So, remove any zero length reviews fromreviews_ints and remove that corresponding label as well.In this particular case, after running this cell,I expect to see that one of our reviews was removed.Try to solve this task on your own, and next,I'll present my solution and introduce you to your next exercise.

### 07. 6 Cleaning And Padding V1-UgPo1_cq-0g.en

So last time, we noticed that we had one review with zero length in our dataset.This review will not contribute any meaningful training information.So here, I'm removing any reviews of zero length from our reviews ends list.I'll do the same thing with their corresponding label.The way I went about this task was I thought, "Okay,I'm going to want to find any reviews of zero length and removethat data from my existing reviews ends and encoded labels data."So, I first identified the indices in our datathat I want to keep which I'll call my non-zero indices.I'm checking the length of each review in our reviews end data.If the length is not equal to zero,that means I want to keep it and I'm recording its index in our list of non-zero indices.Then I'm just getting those indices frommy existing reviews ends list and encoded labels array.I'm just trying the new clean data in these variables of the same name.When I do my length check,I can see that this is effectively removed one review from our data.In this case, there was only one review of zero length, so this looks good.Now, the next thing I want to deal with isvery long review text data and standardizing the length of our reviews in general.We saw that the maximum review length was about2,500 words and that's going to be too many steps for our RNN.In cases like this, I want to truncatethis data to a reasonable size and number of steps.This brings me to the next exercise.To deal with both short and very long reviews,we'll either pad or truncate all reviews to a specific sequence length.For reviews that are shorter than some sequence length,we'll pad it on the left with zeros.For reviews longer than the sequence length,we can truncate them to the first sequence length worth of words.So, here's a padding example.Say, we have a short sequence of words and wespecify that we want a sequence length equal to 10.The resultant padded sequence should be this,padded on the left with seven zeros and the original three-word tokens are at the end.Now in the case of a long review,it would just be cut at the sequence length of 10.This is just a small example and for our movie review data,a good sequence length is going to be around 200.An exercise, I want you to complete this function pad features.This takes in our list of reviews ends and a sequence lengthand it should either pad or truncate every review in the past end list.It should return an array of transformed reviews whichI'll call R features which are our tokenized reviews ofthe same sequence length and you'll often hear transform datalike this referred to as the features or input features for a model.So, at the end, each of the rows in the feature's array will be transformedreview of a standard sequence length that we can then feed into a model as input.So, try to solve this and then the next cell I've included some print statementsand assertions that act as tests on the shape of your feature's array.This will help you check your work and next I'll go over one solution

### 08. 7 PaddedFeatures Sol V1-sYOd1IDmep8.en

So here's my solution for creating an array of features,reviews that have either been padded on the left with zerosuntil their sequence length on or truncated at that length.First, I'm actually creating an array of zeros,that's just the final shape that I know I want.That is, it should have as many rows as I have reviews inthe input reviews_ints data into as many columns as the specified sequence length,and this will just hold all the zero integers for now.Then for each review in my list,I'll put it as a row in my features array.The first review is going to go on the first row,and the second in the second, and so on.I started out thinking of my short review case.I want to keep a left padding of zeros,up until I reach where that review can fill the remaining values.So, I'm looking at filling my features,starting at the index that's at the end of the features row,minus the length of the input review.So, if a reviewer show,this means our features are going to keep the zeros which are padding on the left,and the review tokens will be on the right side.It turns out that I only have to add one more piece tothis line to make this work for a long reviews too.Hear for annual review including those longer than the given sequence length,I'm truncating them at that sequence length,and this should fill the corresponding features row.So, this loop will do this for every review in reviews_ints,and then returns these features.Below I'm running my test code.Here I'm creating features passing in my list ofreviews_ints and a sequence length equal to 200.I don't trigger any of these error messages,so I know my dimensions are correct,and then printing out the first ten values of the first three rows here.And here's what these rows look like.A lot of these start with zeros,which is what I expect for left padding,and others have filled up these rows with various token values.So, this is great. And I'll also add that.In this step, we've actually introduced a new token into our review features.Remember that before, all words in our vocabulary hadn't associated integer value,and we started organizing with the value one.So, in our vocab_to_int dictionary,we had integers from one up to 74,000 or so.And here by adding zero as padding,I've effectively inserted the zero token into our vocabulary.Okay. Now for your next and the last data transformationexercise with our data in nice shape, next,I want you to split the features and encoded labels into three different datasets,training, validation, and test sets.You'll need to create datasets for grouping our features andlabels like train_x and train_y, for example.And we'll use these different sets to train and test our model.So, I've defined a split fraction,split_frac, as the fraction of data to keep in the training set.This is set to 0.8 or 80 percent of data.The 20 percent of the data that's left should be split inhalf to create the validation and testing data respectively.So, I'll leave this as an exercise.And next, I'll go over how I split the data and I'll show yousome PyTorch resources we can use toeffectively batch and iterate through these different datasets.

### 09. 8 TensorDataset Batching V1-Oxuf2QIPjj4.en

In this cell, I've split our features andencoded labels into training test and validation sets.I started by splitting our features and label data according to their split frac.So, I'm reserving 80 percent of my data fortraining and I'm basically getting the index at which I should splitmy features and label data based on this value0.8 and actually I could have just put in this variable here.Then I'm splitting my features first,getting the features up until my 80 percent split index.This makes up my training features train_x then I'm getting the remaining data.So, after the split index and that makes up my remaining_x.Then, I'm doing the exact same thing but for my labels data,splitting it at the 80 percent index to get my training labels andmy remaining data then I'm doing somethingsimilar all over again only with the remaining data.I'm getting an index to split this data in half,so at the 0.5 mark.Then each half of our remaining_x will make up our validation and test sets offeatures and each half of remaining_y will make up our validation and test set of labels.That's it. The last step I'm doing is checkingmy work and printing out the shapes of my features data.I can see that I have the largest number of reviews in my training set witha sequence length of 200 and my validation and test sets are of the same size.If you want, you can do the same thing foryour labels data and you should see the same number of rows here.So, this is 80 percent of my data,10 percent, and 10 percent.After creating training test and validation data,we want to batch this data so that we can train on batches at a time.Typically, you've seen this done witha generator function which we could definitely do here but I want to showyou a really nice way to batch our datasets when we'vesplit up our input features and labels like this.We can actually create data loaders for our data by following a couple of steps.First, we can use pytorch's tensor dataset to wraptensor data into a known format and I can look at the documentation for this here.This dataset basically takes any amount of tensors with the same first dimension,so the same number of rows,and in our case this is our input features and the label tensors and itcreates a dataset that can be processed and batched by pytorch's data loader class.So, once we create our data wrapping it in a tensor dataset,we can then pass that to a data loader as usual.Data loader just takes in some data anda batch size and it returns a data loader that batches our data as we typically might.This is a great alternative to creatinga generator function for batching our data into full batches.The data loader class is going to take care of a lot ofbehind-the-scenes work for us and here's what this looks like in code.First, I'm creating my tensor datasets.To create my training data,I'm passing in the tensor version of my train_x and train_y that I createdabove and torch that from numpy justtakes in numpy arrays and converts them into tensors.So, I'm doing that for my training validation and testdata and if you named your data differently above,you'll have to change those names here.In fact, I could have actually done these steps the other way around.Creating a tensor dataset for all my dataand then splitting the data into different sets. Both approaches work.Then for each tensor dataset that I just created,I'm passing it into pytorch's data loader or I canspecify a batch size parameter equal to 50 in this case.So, without the messiness of loops and yield commands,this defines training validation and test data loadersthat I can use in my train loop to batch data into the size I want.So, this gives me three different iterators and I just want to show you what a sample ofdata from this data loader looks likelooking at our train loader and getting an iterator,then grabbing one batch of data using a call to next.So, this should return some sample input features and some sample labels.Then I'm printing out the size of my input which I can see is the batch size 50and the sequence length 200 and the output label size which is just 50,one label for each review inthe input batch and I see my tokens and the encoded labels as well.So, this is looking really great.Next, we can proceed with defining and training the model on this data.

### 10. 9 DefiningModel V1-SpvIZl1YQRI.en

By now, you've had a lot of practice with data processing and with defining RNNs.So, I'm not going to give you too much guidance here,when it comes to defining this model.Here's what it should look like generally.The model should be able to take in our word tokens,and the first thing that these go through will be an embedding layer.We have about 74,000 different words,and so this layer is going to be responsible for converting our word tokens,our integers into embeddings of a specific size.Now, you could train a Word2Vec model separately,and actually just use the learned word embeddings as input to an LSTM.But it turns out that these embedding layers are still useful even ifthey haven't been trained to learn the semantic relationships between words.So in this case, what we're mainly usingthis embedding layer for is dimensionality reduction.It will learn to look at our large vocabulary and mapeach word into a vector of a specified embedding dimension.Then, after our embedding layer,we have an LSTM layer.This is defined by a hidden state size and number of layers as you know.At each step, these LSTM cells will produce an output and a new hidden state.The hidden state will be passed to the next cell as input,and this is how we represent a memory in this model.The output is going to be fed into a Sigmoid activated fully connected output layer.This layer will be responsible for mappingthe LSTM layer outputs to a desired output size.In this case, this should be the number of our sentiment classes, positive or negative.Then, the Sigmoid activation function is responsible forturning all of those outputs into a value between zero and one.This is the range we expect for our encoded sentiment labels.Zero is a negative and one is a positive review.So, this model is going to look at a sequence of words that make up a review.Here, we're interested in only the last Sigmoid output because this willproduce the one label we're looking for atthe end of processing a sequence of words in a review.So here's a little more explanation and some links to documentation if you need it.Then below, in this first cell,I'm going to check if a GPU is available for training.Then here, I want you to complete this model.It should take in all these parameters: our vocab size,output size, embedding dimension,hidden dimension, number of layers,and an optional dropout probability,and create an entire sentiment RNN model.You'll be responsible for completing the init and forward functions for this model.Remember that the output should just be the last value from our Sigmoid output layer.I'll also ask you to complete the init hidden function for the LSTM layer.This should initialize the hidden and cell state to beall zeros and move them to a GPU, if available.I'd encourage you to look at the documentation when helpful or your other code examples.You should have all the information you need to complete this model on your own.If you're confident in your model definition, later in this code,you'll be able to define your model hyperparameters and train it.Next, I'll show you one solution for defining the sentiment RNN model.But I do think this is a fun task to try out on your own in earnest too.I think it's a great exercise in thinking about howdata is shaped as it moves through a model. So, good luck.

### img

## Part 02-Module 02-Lesson 07_Project 6 Sentiment Analysis with Neural Networks

## Part 02-Module 03-Lesson 01_Coming Soon!

## Part 02-Module 04-Lesson 01_Coming soon!

## Part 03-Module 01-Lesson 01_Why Python Programming

### 02. L1 01 Intro V3-yyNtiUyI5Tw.en

Hi there. I'm Juno and I'm really excited you want to learn Python.Before coming to Udacity,I worked as a data scientist and used Python for data analysis,machine learning, and deep learning projects.Python is one of the most widely used programming languages in industry.It's a powerful, general purpose language withapplications ranging from web development to data science.In this course you'll learn how to apply Pythonand good programming practices to solve practical problems.

### 03. L1 03 Programming In Python V4-O1cTNYAjeeg.en

As you learn Python throughout this course,there are a few things you should keep in mind.First, unlike languages like SQL,Python is case sensitive,meaning these two words mean two different things.Second, spacing is important in Python.Since it isn't very heavy on syntax like other languages,it relies a lot on spacing,which can be confusing at first,but really nice and clean when you get the hang of it.Third, there will be times you get errors in this course.Keep in mind, errors don't make you a bad programmer.It's just a computer telling you it doesn't understand.Use error messages to help you learn how to write codethat can be interpreted the way you wish. Let's get started.

### 04. L1 02 Course Overview V4-vFxXSIV5cHM.en

Whether you're a beginner in programming or have experience in other languages,this course is well-structured to help you develop fluency in Python.Each lesson includes videos,text summaries, quizzes and coding activities for each concept.If you'd like to move through the course faster,you can skip directly to text summaries and quizzes to test your knowledge,and go back to videos if you get stuck.However, if you are new to programming,I strongly recommend watching the videos for a detailed walk-through of each topic.Here's an overview of what you learn.You'll start off learning about the basics of python in lesson two;data types and operators.These are the building blocks you use to write your programs.In lesson three, you'll dive into control flow tools.You learn about conditional statements,loops, built-in functions, and list comprehensions.With control flow tools,you'll be able to write more complex and creative code.In lesson four, you'll learn about function definitions, variable scope,documentation, lambda expressions, iterators, and generators.This is where things can become a little tricky.But these are also some ofthe most important tools you'll need to use in day-to-day practice.Finally, the last lesson will show you how to run your codelocally and scripts that take in raw input and work with files.You'll also learn about error handling and importing different libraries.I know this looks like a lot of content,but there's plenty of review and practicethroughout the course to reinforce what you're learning.

### img

## Part 03-Module 01-Lesson 02_Data Types and Operators

### 01. Introduction-4F7SC0C6tfQ.en

Welcome to this lesson on data types and operators.This is a very important lesson because it's where you'll learn aboutthe building blocks of Python datatypes and operators.Students tend to learn how to program at different rates.Take your time understanding each of the concepts thatfollow and getting practice with activities in this lesson.Having a firm grasp on these building blocks will provide a strong foundationfor future lessons and essential skills to continue programming beyond this course.Throughout this course, we'll go through code examples using these boxes.This first box represents the Code Editor which is where we will input our Python code,and the second box representsthe output window which is where our results will be displayed.You can follow along with the video examples inthe classroom by typing in the code editor below each video.Click the Test Run button to run your code and view the results in the output window.You'll be seeing this print function pretty often.It helps us see what's going on in our code.For example, let's say you wanted to compute three plus five.If we run this line of code,Python would still add three and five but we wouldn't seethe result because we didn't tell Python to do anything with it.Print is a useful built-in function in Python that we can use to display our results.All we have to do is type in Print followed byparentheses around whatever we want printed.You will be using Print often as you get to know the building blocks of Python,its data types and operators.

### 02. Arithmetic Operators-M8TIOK2P2yw.en

In the last video,you saw this line of Python that computes the sum of 3 and 5.The plus sign and this line is an arithmetic operator.Python has several arithmetic operatorsmost of which follow the usual rules of Mathematics.Let's look at the first four,in Python addition and subtraction are performed with the usual symbols, plus and minus.Multiplication uses an asterisk,and division uses a forward slash.Here you can see that multiplication happens before addition.This is because Python followsMathematical Order of Operations which you can get a refresher on in the notes below.If you want addition to come first,you can enclose this part in parentheses.Moving on from those four,here's the operator for exponentiation.You can raise one number to the power of another with two asterisks.For example, this line prints three to the power of two which results in nine.There's another operator that is sometimesmistaken for the exponentiation operator. The caret.This actually performs a more obscure operation called bitwise XOR.This is an arithmetic operator that doesn't follow the usual rules of Mathematics.Bitwise operators are not something you need to know for this course but,if you're interested there's information about this in the notes below.All you need to remember is that if you perform exponentiation,you use two asterisks and not witha caret or you'll accidentally produce very confusing results.Another useful operator is this percent sign which performs the modulo operation.It returns the remainder after you've divided the first number by the second.In this example, nine divided by two is four with a remainder of one.So this line would print one since modulo gives us the remainder.You might also find use for integer division denoted by two forward slashes.It divides one integer by another but rather than giving the exact answer,it rounds down the answer down to an integer.Seven divided by two is three point five which rounds down to three.Notice it rounds down even if the answer is negative.In this case, negative three point five was rounded down to negative four.There are other categories of operators that we'll learn aboutsoon but these are all the arithmetic operators in Python.

### 05. Assignment Operators-p_qfzL-x3Cs.en

We've already set mv_ population to this value.But what if we want to update it now that the population changed?We can just assign this variable again to its new value which we found to be 78,128 orif we got this new value because we knew 4,000 moved to Mountain View and 600 moved away,we could just apply those changes directly to this variable.In this line, the variable mv_population is being assigned toitself plus 4000 minus 600 which results in 78,128.Because this kind of increment and reassign operation is very common,Python actually has special assignment operators for this.Instead of using mv_population twice in one line,we can actually use this plus equals operator to tellPython we are incrementing the variable on the left by the value on the right.Plus equals is one example of another assignment operator in Python,minus equals is another,and there are a bunch more.These are actually just all the arithmetic operators followed by an equal sign.All of these operators just apply the arithmetic operation to the variable onthe left with the value on the right and makes your code more concise which is good.Check out the text below this video for more assignment operators.

### 05. L2 04b Variables II V3-4IJqbP8vi6A.en

Python also has a useful way to assign multiple variables at once.These three assignments can be abbreviated usingmultiple assignments like this: x is still assigned to two,y to three, and z to five.You can use this when you're assigning closely related variables likethe width and height of an object or an object coordinates.In this example, we used x,y, and z as variable names.However, normally, we'd want something more descriptive.x, y, and z would beappropriate variable names for something like coordinates in a 3D space.But for most other situations, we can do better.For example, if you were computing the population density ofMountain View by dividing the population by the land area,this is one way to compute the correct answer.But it's much clear with these variable names,which are actually descriptive of the values they represent.In addition to being descriptive,there are a few things to watch out for when naming variables in Python.First, only use ordinary letters,numbers, or underscores in your variable names.They can't have spaces and need to start with a letter or underscore.Second, you can't use reserved words orbuilt-in identifiers that have important purposes in Python,which you'll learn about throughout this course.There are also links below. Using them as variable names will lead toerrors or problematic situations when you try to use that word for its intended purpose.Third, the pythonic way to name variables is touse all lowercase letters and underscores to separate words.

### 05. Variables-7pxpUot4x0w.en

Understanding how to perform arithmetic in Python is useful.But understanding how to use variables can turn Python into more than just a calculator.Using variables, as opposed to just raw numbers, has many advantages.Let's get started. Creating a new variable in Python is simple.Here's one which stores the population of mountain view.The variable name in this example is mv_population.The equal sign is the assignment operator and the value of the variable is 74,728.Notice, this isn't simply an expression of equality like in Math where x equals y,and y equals x mean the same thing.In Python, the equal sign is an operator that assignsthe value on the right to the variable name on the left.In other words, whatever term is onthe left side is now a name for whatever value is on the right side.Let's see another example.The first line here defines x as 2,and the second line defines y as the value of x which is printed in the third line.Notice we can use a variable's name to access its value.In this line, we only needed the name of the variable x to define y as its value two.Similarly, in this next line,we were able to print the value of y just by using the name y.If you try to access the value of a variablethat was never defined, you'll get this error.It's explained pretty clearly in the error message,name z was not defined.

### 08. Nmeros inteiros e floats-MiJ1vfWp-Ts.en

So far, most of the numbers we've been working with have been whole numbers or integers.However, as you may have noticed other types of numbers do exist in Python.Here, dividing one integer by another gives us a number that isn't an integer 0.75.In Python and computing in general,we represent such a number as a float,which is short for floating point number.A float is a real number that usesa decimal point to allow numbers with fractional values.Even if one integer divides another exactly,the result will be a float.The int and float are actually two kinds of data types.In Python, every object you encounter will have a type.An object's type defines which operators andfunctions will work on that object and how they work.You can check the type of any object directly using the built-in function type.Here you can see that the type of a number withouta decimal and a number with a decimal are different in Python.To make an int,just give a whole number without a decimal point.Here is an int.To make a float, include a decimal point.If the number itself is actually a whole number,that's okay, you don't even have to put anything after the decimal point.These are both floats.An operation involving an int and a float always produces a float.Sometimes, you might need to manually convert one numeric type to another.And you can do that by constructing new objects of those types with int and float.When we convert a float to an int,the part of the number after the decimal pointis cut off which means that no rounding occurs.49.7 is cut to 49.4.0 calculated from 16 divided by 4 is cut to 4.Converting from int to float adds decimal zero to the end of the number.So, we've seen Python's two main numeric types:integers and floating point numbers. What are they good for?There are many times when you might need to count items orneed to rely on the result of a computation being an integer.For example, let's say you want to count how many people showed up at your dinner party.You can't count 0.47 of a person,so you use an integer.If what you're working on isn't necessarily a whole number,a float is the type you're looking for.For example, let's say you made five pies foryour dinner party and you want to keep track of the amount of pie left.People usually take one-sixth of a slice.Once one slice is taken,you're down to 4.83 pies,so you use a float.Floating point numbers are approximations of the numbers they're opposed to represent.This is necessary because floats can represent an enormous range of numbers.So, in order to fit numbers in computer memory,Python must use approximations.This trade off can sometimes have surprising results.Because the float or approximation for 0.1 is actually slightly more than zero 0.1,when add several of them together we can see a differencebetween the mathematically correct answer and the one that Python creates.In most contexts, these small differences areirrelevant but it's important to know that they're there.You can read more about this in the instructor notes.

### 08. Whitespace-UxkIwcOczQQ.en

One thing you might have noticed is that in a single line of Python,whitespace doesn't really affect how your code works.For example, this will give exactly the same output as this, however.That doesn't mean that these lines are equally good lines of code.Learning how to write clear and readable code iscritical for others in your company and future you to understand.Here are some best practices for code style in Python.When you call a function like print,put the opening parentheses straight after the name of the function,like here. Not like this.Don't put extra spaces immediately inside the parentheses either.So, this is good. But this isn't.If you are mixing operators withdifferent priorities like multiplication and subtraction,then you might like to add a space around the lower priority,in this case subtraction,to make the code easier to read.See how this is slightly harder to comprehend for operation order.Don't write extremely long lines of code.They're hard to understand.People commonly limit themselves to lines that are 79 or 99 characters long.If you feel like you need to write longer lines,consider rewriting, simplifying, or separating your code into multiple lines.These conventions come from the Python developers guide,which has a style guide called PEP 8, which is linked below.There's a lot in there.Don't worry, you don't need to digest it all now.Why are these conventions important?Although how you format the code doesn't affect how it runs,following standard style guidelines makes codeeasier to read and consistent among different developers on a team.So, it's a good idea to follow the guidelines,even with one line expressions.It will be useful to refer back to PEP 8 once in a while to get your Python style right.Later, we'll also learn about tools that can check your code for you,and provide suggestions based on PEP 8 guidelines.

### 10. Boolean Comparison and Logical Operators-iNNsUJIDtVU.en

We've seen two kinds of Python data types so far,ints and floats and we've usedarithmetic operators like addition and exponentiation to work with these values.Another type is Bool which is used to represent the values true and false.Bool is an abbreviation of boolean.Boolean Algebra is a branch of algebra dealing withvariables whose values are true or false.Boolean algebra is named for its inventor George Bool.Boolean logic underpins all digital devices existing in almost every line ofcomputer code and has transformed the way we liveour lives which you can read more about in the instructor notes below.We can assign boolean values like this.It's not very useful on its own though.We can use comparison operators like less than andgreater than to compare values and produce a boolean result.Here, 42 is not greater than 43.So printing the result provides the boolean false.Here, you can see a full list of the comparison operators in Python: less than,greater than, less than or equal to,greater than or equal to,equal to, and not equal to.Notice that evaluating equality is performed withtwo equal signs and a not equal uses an exclamation point.This is a bit different than Excel or SQL.In addition to comparison operators,these logical operators are very useful when working withbooleans and evaluates if both sides aretrue or evaluates if at least one side is true and not inverse as a boolean type.Here's an example that evaluates whether age is within the range of a teenager.Here, you can imagine the 14 being placed in these two spots.Then, if both are true,true will be assigned to the variable is_teen.In other words if the person is older than 12 and younger than 20 this person is a teen.And here's not an action inversing the boolean type of same statement.

### 13. Strings-ySZDrs-nNqg.en

Often programming involves more than numbers and arithmetic.There may be situations where you need to work with text.To work with text in Python,you will need to use a string,which is an immutable ordered series of characters.More on the immutable ordered part later.You can create a string by using quotes.Single or double quotes work equally well,although there are some edge cases, which we will work through.In each of these cases I printed the string "hello" and got the output "hello".We can set a variable to be a string the same way we did with numbers.Strings can include any characters,even spaces, punctuation and numbers.However, what do we do when we want quotation marks in our string?Since we use quotation marks to define our strings,this presents a small problem.Here the code doesn't work the way we want it to.Python offers two solutions to this problem.The first is to place the string in single quotes rather than double quotes, like this.You can use either type of quote to define strings,but sometimes you'll need to define a string that includes both single and double quotes.What then? In that case,you can use the backslash to escape quotes.Here the string is delimited by single quotes.The single quote within the string is preceded by a backslash so that Pythonknows that it should be interpreted as part ofthe string rather than the quote that ends the string.Once our strings are defined,there are a few operations that are used forintegers and floats that we can also use for strings.For example, we can use the plus sign to put strings together,and we can use multiplication to repeat strings.Let's look at an example of each.Here our variables are holding two words.We can use the plus sign to concatenate the two strings together and print the result.This is fundamentally different from numeric addition.However, notice the two names have been squished together.We're missing a space.Python is completely literal when working with strings.We need to explicitly include spaces andpunctuation if we want what we write to make sense.This time we got a string that makes sense,putting the two words together with a space in between.Note that previously I said a white space doesn't matter inbetween parentheses in bits of code, like print statements.Here with strings, you can see that spaces do matter in between the quotation marks.Let's try another mathematical operation.Turns out we can use the multiplication operator as well.It repeats the string as many times as you multiply it.Here five times.Although addition and multiplication have different applications for strings,subtraction and division do not.Here we get an error that a string is an unsupported type for the division operator.A useful function that's built into Pythonis Len which can tell us the length of a string.This is just the number of characters in the string.Len is like print in that it's a built-in function thattakes in a value in parentheses to perform an action.Len differs from print in that it returns a value that can be stored in a variable.In this example, the Len function outputs to number seven,which is stored in the Udacity length variable.Built-in just means Python provides these functions for us.Later, we'll learn how to define our own functions.

### 16. Type  Type Conversion-yN6Fam_vZrU.en

Up until now, we've discussed four data types in Python: Int,float, bool, and string.If you recall from our previous video,you can check the type of any object directly using the built-in function type.Using type, we can observe that the same number can be coded in different types,each with their own set of behaviors.As a side note, here,we're calling a function print on the output of another function type.We use parenthesis to define the order in which functions get run.What's contained in one set of parenthesis needs to beevaluated first before being given as input to the next function.Here, the type function is run first and then its output is printed.Different types have different properties.And when you're designing on computer program,you'll need to choose the types for your data based on how you're going to use them.For example, if you want to use a number as a part of a sentence,it'll be easiest if that number is a string.If you want to encode a true-false value,it'll be much easier to manipulate as a boolean than a string.Why is it easier?There are specially designed functions for working with each data type.You'll learn about these soon.You might also have situations where you don't control the type of data that you receive,but you still need to use it.The good news is that you can createnew objects from old and change a type in the process.We went over this in the integers and floats video.For example, here, we create an int from a float and assign it to a variable count.Here, we create a string out ofthe house_number and use that to build a larger address string.First, we have the house_number,as well as the street and town_name.You can see that the house_number is currently an int.We can change it to a string, like this,and use the plus operator to create the full address.You can also build a number from a string like this.Here, we start with a string of 35.But by wrapping it in a float function,we can see the type has now changed.

### 19. String Methods-Bv7CAxVOONs.en

So far, we've seen two distinct ways to processdata with Python, operators and functions.We've used operators like these,which process the two values on either side of the operator.We've also used functions like print and len.Functions are very similar to operators.In fact, the only real difference is in how they look.Function Inputs are put in parentheses rather than being placed next to an operator,and functions have descriptive names rather than short symbols.There is a third technique for operating on values, methods.The best way to learn about methods is with an example.Consider this title method.Methods are related to functions,but unlike functions, methods are associated with specific types of objects.That is, there are different methods depending on the type of data you're working with.In this example, the object is a string with the valueSebastian Thrun and we are calling its title method.The method returns a string in title case,meaning the first letter of each word is capitalized.So methods are functions that belong to an object,an object, for example, being a string.Let's try another string method, islower().The lower method checks one of the characters in a string or lowercase.In this case, the string object is Sebastian Thrun.Is lower returns true since there areno uppercase letters you've probablynoticed that when we call the is lower and title methods,we use parentheses, but we haven't putanything in them like we did when calling functions.Those inputs in the parentheses are called arguments.Since methods or special types of functions that belong to an object,the object is always the first argument to a method.So, is lower and title actually did havean argument although there was nothing in the parentheses.The argument is disguised as the string object.Let's try a method that takes more arguments than just the object, count.Here, the count method returns how many times the substring fish occurs in the string.The object is the string, one fish,two fish, red fish, blue fish,and the method is dot count,and four occurrences of the word fish exist in the string.

### 25. L2 05 Lists Methods V1-WXkPm4rv6ng.en

Let's introduce a new string method that works with lists.Join. Join takes a list as an argument,and returns a string consisting of the list elements,joined by separator string.In this example, we use a string backslash and,as a separator, so that there's a new line between each element.Fore newline, aft newline, starboard newline, port.We can also use other strings as separators would join.Here, we use a hyphen.It's important to remember to separate each ofthe items in the list you're joining, with a comma.Forgetting to do so, will not trigger an error,but will also give you unexpected results.In the example below,omitting a comma between Garcia and Okelly,results in the following: Notice howthe hyphen separator is missing between Garcia and Okelly,and instead, the two strings were appended.This happens because of Python's default string literal appending.If join returns different results than expected,check for missing commas.Also, note that join will trigger an error,if we try to join anything other than strings.We get an error here,because an integer was included in the list.Lastly, a helpful method called append,adds an element to the end of the list.Next, you'll practice working with lists,and explore more list methods.

### 25. L2 06 Lists Methods V1-tz2Ja1Eaeqo.en

Previously, when we created a variable,that held an immutable object,the value of that immutable object was saved in memory.Here, we create a name with value Jim,and assign it to another variable, called Student.It is the value Jim,that is assigned to student.So, when we reassign name,to update it to Tim,this change is not reflected in the value of student.Lists are different from strings,as they are mutable.Here, we create a list of scores,and assign the same list to the variable grades: B,C, A, D, B,A, six of them.When we change, or mutate the score's list,making the fourth grade B instead of D,this affects both scores and grades.Both scores and grades are variable names for the same underlying list,and either name can be used to access and change that lists.The behavior of variables containing mutable and immutable objects,are very different and might even seem surprising at times.Experiment used to print functions and double check your work where you can,to make sure that your programs correctly keep track of their data.While you experiment with lists,there are some useful functions you should get familiar with.Len, retains how many elements are in a list.Max, retains the greatest element of a list.How the greatest element is determined,depends on what type of objects are in your list.The maximum element in a list of numbers,is the largest number.The maximum element in a list of strings,is the element that would occur last,if the list was sorted alphabetically.That's reticulated python for this list.R is the largest letter alphabetically.In other words, greater than B,A, B, and A.This works because the max function,is defined in terms of the greater than comparison operator.The greater than comparison operator,is defined for many non-numeric types.If you're working with objects that can be compared with this,then you can use max on a list of the objects.For strings, the standard comparison is alphabetical.So the maximum of this list,is the element that appears last alphabetically.Although you can create lists,that hold a mix of elements of many types as you see here, integers and texts.The max function is undefined for lists,that contain elements from different incomparable types.Here, you can see it breaks,with this mix of datatypes.Min is the opposite of max,and returns the smallest element in a list.Sorted returns a copy of a list,in order from smallest to largest,leaving the original lists unchanged.Here, for a list called sizes,the order is ascending.You can sort from largest to smallest,by adding the optional argument, reverse equals true.Now, the order is descending.

### 27. L2 04 Tuples V3-33xN-AbTMoc.en

Python provides another useful container, tuples.Tuples are used to store related pieces of information.A tuple is a data structure in Python that is an immutable ordered sequence of elements.Consider this example involving latitude and longitude.Tuples are similar to lists in that they storean ordered collection of objects which can be accessed by their indices.For example, location zero and location one.Unlike lists however, tuples are immutable.You cannot add or remove items from tuples or sort them in place.Why do we have tuples if they're like lists with fewer features?Tuples are useful when you have two or more values that are soclosely related that they will always be used together,like latitude and longitude coordinates.Tuples can also be used to assign multiple variables in a compact way.Notice that the values assigned to the tuple dimensionsaren't surrounded with parentheses as previous examples were.The parentheses are optional when making tuples andprogrammers frequently omit them if parentheses don't clarify the code.In the second line,three variables are assigned from the content of the tuple dimensions,this is called tuple unpacking.You can use tuple unpacking to assign the information from a tupleinto multiple variables without having to access them one by one,and make multiple assignment statements.In this example, if we won't need to use dimensions directly,we could shorten those two lines of code intoa single line that assigns three variables in one go.

### 29. L2 03 Sets V2-eIHNFgTFfnA.en

Imagine that you run a popular search engine,and you've surveyed your users to see where they're browsing from.You've collected the 785 responses and have assembled them into a list of countries.There aren't 785 countries in the world,which means that there are duplicate entries in the country's list.Slicing the list to see the first few elements confirms this.It will be useful to remove the duplicates toproduce a list of all the countries that users browse from.Well, a set in Python does exactly that.Sets our containers of unique elements without any particular ordering.We can create a set from a list like this.Set removes the duplicates and the print function printsthe unique values of which there are 196 countries.Sets support the in operator the same way lists do.India is in this set.You can add elements to sets where youdon't use the append method like you do with lists,instead, sets have the add method.Here, Italy is added.Sets also have a pop method just like lists.When you pop an element from a set,a random element is removed.Remember that sets, unlike lists,are unordered, so there is no last element.Other operations you can perform with sets,include those of mathematical sets.Methods like union, intersection,and difference are easy to perform with sets andare much faster than such operators with other containers.

### 31. L2 02 Dictionaries And Identiy Operators V3-QR8HTxCTWi0.en

Sets, are simple data structures,and they have one main use, collecting unique elements.Our next data structures,dictionaries, are more flexible.Rather than storing single objects like lists and sets do,dictionaries store pairs of elements, keys and values.In this example, we define a dictionary,where the keys are element names,and their values are their corresponding atomic numbers.We can look up values in the dictionary,by using square brackets enclosing a key.We can also insert new values into the dictionary, with square brackets.Here, we are adding lithium,and giving it a value of three.Dictionary keys are similar to list indices.We can select elements from the data structure,by putting the key in square brackets.Unlike lists, dictionaries can have keys of any immutable type, not just integers.The element dictionary uses strings for its keys.However, it's not even necessary for every key to have the same type.We can check whether a value is in a dictionary,the same way we check whether a value is in a list or set,with the in keyword.We can use in to verify,whether a key is in the dictionary, before looking it up.If there's a possibility,that the key is not there.Mithril, was not part of our elements dictionary, so false is printed.Dictionaries have a related method,that's also useful, "get"."Get" looks up values in a dictionary,but unlike square brackets,"get" returns none, or a default value of your choice,if the key is not found.The dilithium is not in our dictionary,so none is returned, and then printed.If you expect lookups to sometimes fail,Get might be a better tool than normal square bracket lookups,because errors can crash your program, which isn't good.You can check if a key return none with the "is" operator,or you can check for the opposite using "is not".These are called identity operators.You will learn more about identity operators,and how they differ from using these equals to,or not equals to, comparison operators in the quizzes that follow.

### 35. L2 01 Compound Data Structures V1-jmQ8IKvQgBU.en

In the elements dictionary we saw earlier,element names which are strings are mapped to their atomic numbers which are integers.But what if we wanted to store more information about each element,like their weight and symbol.We can do that by adjusting this dictionary so that it mapsthe element names to another dictionary that stores that collection of data.We can look up information about an entry inthis nested dictionary in the same ways we did before,with square brackets or the GET method.We can look up specific information from the helium dictionary like this.This code is first looking up the key helium in the elements dictionary,producing the helium dictionary.The second lookup weight,then looks up the weight key in that helium dictionary to find helium's atomic weight.

### 38. Conclusion-LLEZadlXM8A.en

Congratulations on completing this lesson on data types and operators.You started your journey in Python with a great foundation.Next, you'll learn how to piece together the building blocks you justlearned to write cooler and more complex programs. Great job.

### img

## Part 03-Module 01-Lesson 03_Control Flow

### 01. Introduction-eUrvACMMJ5w.en

Welcome to this lesson on Control Flow.Here, you'll learn how to add more functionality toyour code by being able to use conditional statements and loops.You'll learn how to implement decision-making with if statements,repeat code with for and while loops,exit or skip loops with break and continue,use helpful built-in functions like zip and enumerate,and combine some of these concepts in concise list comprehensions.With these tools, you'll be able to bring together the data types and operators youlearned from the previous lesson in more complex and creative ways. Let's get started.

### 02. If Elif and Else-KZubH5XT0eU.en

Now you know how to execute a block of code if a condition is true.But what if you have a different block of code that youwant to execute when that condition is false?You can use the 'else' keyword to do so.Consider this code, which prints a message indicatingwhether an integer N is even or odd.If N is even,this line is run.Otherwise, this line is run.Code indented under the 'else' is what happens when this condition evaluates to false.The 'else' keyword is always followed by a colon and doesn't need a Boolean expression.So, if we set N to be 4 like this,the if statement here would evaluate to true and we would print the following string.Alternatively, if we set N to be 15,this condition would evaluate to false so we would skipthis block and execute the code under the else.If you have more than two possible cases,you can also use elif,short for 'else if',to deal with the situation.This saves the multiple indentation that would be needed ifwe used 'else' and then another 'if' statement inside the 'else' block.Like 'if', an elif statement always requires a conditional expression.For example, let's say we wanted to print whatto do with the garden based on the current season.If we set season equal to spring, like this,then we can see that plant the garden isprinted as its first condition evaluates to true.Alternatively, if we set season to winter,then each of these conditions will evaluate to false until we hadthis condition which will evaluate as true and print stay indoors.Notice here we're using double equal sign again.Remember, a single equal sign is for assignment like we didhere when we were setting a season as a particular string.Two equal sign is for evaluation where we are evaluating a condition as true or false.

### 02. If Statements-jWiIUMrwPqA.en

We've been running code that simply executes every line one by one from the top down.Many times however, we want to run code only if a particular condition holds.To demonstrate this concept,let's take a look at this billing system for a pay-as-you-go mobile phone.Say a customer has a credit balance fortheir phone which they can use for calls and messages.The customer can then set up a link to their bank account so thatif their phone credit balance goes below a threshold amount,in this case five dollars,then 10 more dollars of credit are added and their bank balance is build.Here is a simple way of representing this billing system in code.If the phone balance is below five,add 10 to phone balance and subtract 10 from bank balance.This is an example of an if statement.An if statement is a conditional statement that runs orskips code based on whether a condition is true or false.In an if statement, the if keyword is followed by the condition to be checked,in this case, phone balanced less than five,and then a colon.The condition is specified ina Boolean expression that evaluates to either true or false.After this line is an indented block of code to be executed if the condition is true.So in this case,these lines will only execute if it is true that phone balance is less than five.If phone balance is three,this condition evaluates to true and these indented lines of code are executed.You can observe these changes in the output.If phone balance is eight however,this condition evaluates to false and these lines are not executed.As you see in the output,there were no changes to phone or bank balance,which is what we expect to happen since the condition in this if statement was false.

### 02. Indentation-G8qUNOTHtrM.en

As you have just seen, indentation is important.It's how we tell Python what code is in the body of an if statement,and what code is outside of it.Indentation doesn't just matter in if statements.Soon, you'll see it used in other contexts.Some other languages use braces to show where blocks of code begin and end.In Python, we use indentation to enclose blocks of code.This indentation conventionally comes in multiples of four spaces.It's important to be strict about following this convention becausechanging the indentation can completely change the meaning of the code.If you are working on a team of Python programmers,it's important that everyone follows the same indentation convention.

### 07. Complex Boolean Expressions-gWmIKWgzFqI.en

All the if and elif statements we've seen so far have beenfollowed by a single Boolean expression that checks only one condition.However, more complicated Boolean expressions can be useful as well.If the condition is working with the numerical variable,you might want to check whether a value lies ina certain range or even do some calculation in order to make a comparison.Notice this condition uses logic and algebra,and will still run correctly in Python.Storing values for height and weight,we can quickly print the result for any individual.Some situations may call for logical operators.If it's rainy and sunny,I might look for a rainbow.Notice that the if statement here requires that both ofthese individual variables hold true to evaluate as true.If either is false,then this line will evaluate as false,and our print statement will not run.Let's say I want to send a promotional email to a customer,if they have not requested to be taken off the email list,and they're in a location where they'll be able to redeem the offer.For really complicated conditions,you might need to combine some ands,ors, and nots together.Use parentheses if you need to make the combinations clear.However, simple or complex,the entire line in an if statement must bea Boolean expression that evaluates to either true or false,and it is this value that decides whether theindented block in an if statement executes or not.

### 07. Good And Bad Examples-95oLh3WtdhY.en

We'll go over some good and bad examples of Boolean expressions used in if statements.While true is a valid Boolean expression,it's not exactly useful as a condition as it always evaluates to true.So the intended code will always get run.Similarly, if false is not a condition you should run either,the condition and the if statement would never occur.In this example, is cold or not is cold,will always evaluate to true.If it is cold, then is cold will be true.If it is not cold, then not is code will be true.This has no use as a condition because the indented code will always get run.This code is valid in Python,but it is not a Boolean expression,although it reads like one.The reason is that the expression to the right ofor is not a Boolean expression, it's a string.Later, we'll discuss what happens when you usea non-boolean type object in place of Boolean.It takes a few more characters,but this is now unambiguously a Boolean expression.The expressions on both sides of the logical operator or are each checking something.This is a valid condition,but we could express it more briefly and clearly.Is cold is a Booolean expression on its own right,so we can make the code more readable by using the variable itself instead.If you want to check whether a Boolean is false,you can use the "not" operator.Let's summarize the lessons from these examples.True and False are both Booleans,but it doesn't make sense to use if True or if False.Logical operators and, or,and not have specific meanings thataren't quite the same as their meanings in plain English.Make sure your Boolean expressions are being evaluated the way you expect them to.Don't compare a variable that is a Boolean with equals true or equals false.This comparison isn't necessary since the variable itself is a Boolean expression.

### 07. Truth Value Testing-e52uw7ejV8k.en

So far, the conditions we've used wereBoolean expressions that evaluate to a Boolean object,either true or false.If we put some other object that is not a Boolean in the IF statement,Python will check for its truth value anduse that to decide whether or not to run the indented code.Here, the condition is an integer variable and evaluates to true.The Python documentation listsall the objects that are considered false in this situation.Anything that isn't listed as having a truth value false will evaluate as true.In this code, errors has a truth value true because it's a non-zero number.So the error message is printed.This is a nice way of writing a succinct IF statement.Similarly, in this example you saw earlier,the string in the second part of this condition will evaluate to true.So this statement will always evaluate to trueregardless of what value is stored in weather.Alternatively, this will evaluate to true If weather is the string snow,and will evaluate to false if weather is any other string.However, these types of conditions aren'treally common knowledge and are not common in practice.

### 10. For Loops-UtX0PXSUCdY.en

Now that you've learned about conditional statements,let's move on to loops,which allow us to repeat blocks of code.Python has two kinds,for loops and while loops.Let's first take a look at the for loop,which we can use to iterate over an iterable.An iterable is an object that can return one of its elements at a time.This can include sequence types such as strings, lists,and tuples as well as non-sequenced types such as dictionaries and files.You can define objects within iter method to allow them to be used as an iterable,which you can find more information on in the notes below.Consider this for loop that iterates through a list of cities,capitalizes each one, and prints it.The for keyword signals that this is a for loop.Cities is the iterable and city is the loop's iteration variable.That is the variable that represents the elementin the iterable that the loop is currently processing.So, in the first iteration,city would be New York City.In the second iteration,it would be Mountain View,and so on.We can use the city variable to refer to an elementwithin the indented body of a for loop during any iteration.This indented body is executed once for each element in cities.You can name iteration variables however youlike though this example demonstrates a common pattern.The name of the list cities is the plural form of city,the name of the iteration variable.Naming lists and iteration variables in this style makes iteasier for other programmers to understand the purpose of each variable.So far, the loops we've written extract information from lists.We can also use for loops to create lists and to modify lists.To create a new list,we can start with an empty list,and then use the append method to add new items.This for loop iterates through each city and cities and appends it to capitalized cities.Modifying a list is a bit more involved and requires a use of a new function, range.Range is a built in function used to create immutable sequences of numbers.It has three arguments which must all be integers;start, stop, and step.Start is the first number of the sequence.Stop is one above the last number of the sequence.And step is the difference between the numbers in the sequence.If unspecified, start defaults to zero and step defaults to one.Calling range with one integer will make that the stop argument andreturn a sequence of numbers from zero to that integer minus one.So, range four returns zero through four minus one, which is three.Calling range with two integers will make those the start and stop,and return a sequence of numbers from the first number to the second number minus one.Range two, six returns a sequence from two to five.Calling range with three integers will return a sequence of numbersfrom the first to the second minus one separated by the third.So, range 1, 10,2 returns a sequence from one to nine incremented by two.Notice in these examples,we adopt range in a list before printing it.This is because printing just the output of range only shows you a range object.You can view the values in the range object byconverting it to a list or iterating through it in a for loop.Back in our cities example,we can use the range function to generate the indices of each value in the cities list.This let's us access the elements of the list with cities bracket index.So, that we can modify the values in the cities list in place.Let's go through one iteration of this toshow exactly how all these pieces work together.Len cities provides four,so the list will be a range from zero to three.The index will take on each of these values one at a time.So, in this first iteration,we'll have an index of zero.This part here will then index the first city, New York City,capitalize it using title,then place it back in place of the original New York City spot.This same process would then occur for each of the additional cities.We can use print to see the change in the cities list at each iteration.By getting a list of indices with the range function,we were able to index into each element of a list in a for loop to apply a change.While this modification is one application of the range function,that isn't the only thing it's useful for.You will frequently use range to repeat an action a certain number of times.

### 20. L3 08 While Loops V3-7Sf5tcPlKQw.en

"For loops" are an example of definite iteration,meaning that the loop's body is run a predefined number of times.A "for loop" over a list,executes the body once for each element in the list.A "for loop" using the range function willexecute the number of times specified by the range function.This differs from indefinite iteration,which is when a loop repeats an unknown number oftimes and ends when some condition is met.Consider this "while loop" that simulates a blackjack dealer by drawing cards froma deck list into a hand list stopping whenthe value of the cards in the hand is 17 or more.This example features a new function "sum" and a new list method "pop".Sum is pretty intuitive,it returns the sum of the elements in a list.Pop is the inverse of the append method,it removes the last element from a list and returns it.You can read more about this in the official documentation.In this line, we are computing the sum ofthe list hand and checking if that is less than or equal to 17.In this line, we're popping the last elementfrom card deck and appending that to the hand list.Let's talk about how this "while loop" works.The while keyword indicates that this is a while loop.Next is the condition.In this example, sum hand is less than or equal to 17.If this condition is true,the loop's body will be executed.Each time the loop's body runs,the condition is evaluated again.This process of checking the condition and then runningthe loop repeats until the expression becomes false.The indented body of the loop should modify at least one variable in the test expression.If the value of the test expression never changes,the result is an infinite loop.In this example, the loop's body appends numbers to the hand lists,which increases the value of sum hand.Eventually, the value of sum hand becomes large enough that the condition becomes false.

### 25. Break and Continue-F6qJAv9ts9Y.en

For loops iterate over every element in a sequence,and while loops iterate until they're stopping condition is met.This is sufficient for most purposes but wesometimes need more precise control over when a loop should end.In these cases, we use the break keyword.A loop will terminate immediately if it encounters a break statement.We can use this to end the loop if we detect that some condition has been met.The break keyword can be used in both for and while loops.For example, suppose you want to load a cargo ship with a list of items called manifest.This list contains tuples of items and their weights.Ideally, you would like to load all the items onthe ship but the ship has a maximum weight capacity.Therefore, when the ship's capacity is reached,you want to stop loading.To accomplish this, let's use a for loop loadingeach item and keeping track of the weight of all the items we have loaded so far.Here, we check if the ship's total cargo weight reachesits maximum capacity of 100 with each addition of cargo.If it does, we use a break statement to stop loading.If not, we load the next item and add on its weight.Here's what we end up loading.That's not good.The boat is severely over its weight limit of 100.The break statement did prevent us from putting every item onthe boat but we still exceeded the limit by 111.It's difficult to see what's gone wrong.One strategy we can use is to add print statements in the code.This is a really handy technique as it can give us some insightinto what happens in the code as it's running step-by-step.Having print statements frequently that give context canreally assist us in understanding what went wrong here.Here's the loop with debugging statements added.Debugging is the process of identifying and removing errors in your code.Here, we can see that we didn't break until the weight exceeded 100,when really, we should break before that item is added.Additionally, we can see that the cheeses still couldhave fit if the machine wasn't added.This brings us to another statement.Sometimes, rather than breaking out of the loop completely,there will be times we want to skip simply one iteration of the loop.In this case, we would use the continue keyword.In this example, we iterate through a list offoods and increment account if the food is a fruit.Here, we terminate an iteration if the food is not found in fruit.Otherwise, we add one to fruit count.So when food is hummus or toast in this loop,the rest of the loop is completely skipped.

### 28. Zip and Enumerate-bSJPzVArE7M.en

Looking back at our list of cargo,notice each element is a tuple of size two.Iterating through a list with multiple values,can be pretty helpful.It's actually really easy to combine and split lists like this.If we originally started,with these two separate lists, items and weights,and wanted to combine them to create a list,like manifest, we can use a built-in function called zip.Zip, returns an iterator.So, we need to convert it to a list to see the elements.Or, iterate through it with a for loop,if we want to print the values, similar to range.You could also unpack each tuple in a for loop like this.In addition to zipping two lists together,you can also unzip a list using an asterisk.For example, using the manifest lists like this,you can separate it into an items and weights list, like this.The next function we'll look at, is enumerate.Many times, you'll find it useful to iterate through the values of a list,along with the index.This is one way you could do it.This uses a for loop to iterate through a list of tuples containing the index,and value of each item in the list.The indices are created by getting a range object from zero,to the length of items minus one,and zipping that with the values in items.Enumerate, is a special built-in function that makes this a lot simpler.Enumerate returns these tuples,containing the indices and values of a list,in an iterable for you.You'll be getting some practice using Zip and Enumerate and see how helpful they can be,in the following quiz section.

### 31. List Comprehensions-6qxo-NV9v_s.en

In Python, you can create lists really quickly andconcisely with a cool tool called List Comprehensions.In the cities example from earlier,we created a list of capitalized cities from the cities list in a for loop.With a list comprehension,we can get the same result like this.List comprehensions allow us to create a list using a for loop in one step.You create a list comprehension with brackets includingan expression to evaluate for each element in an iterable.This line called city.title foreach element in cities to create each element in the new list.Notice this part looks just like the first line of a for loop without a colon,and the action you want to take on each element is taking on the element here,and append it to this new list.In the list comprehension,we don't need to create a new listbeforehand and append to it like we would in a for loop.List comprehensions are not found in other languages,but are very common in Python.Here's another example that creates a list of squares from 0 to 64.This line called X to the power of two for every elementin range nine to create each element in the new list, squares.We can write this as a list comprehension like this.Again, we are just looping through each element inthis iterable and evaluating this expression to get each new element in our list.You can also add conditionals to list comprehensions.After the iterable, you can use the If keyword to check a condition in each iteration.In this example, X to the power of two is only evaluated if X is even.This gives us a list only including squares of even numbers.If you want to add an Else,you will get a syntax error doing this.If you'd like to add Else,you have to move the conditionals to the beginning ofthe list comprehension right after the expression like this.Details on why this is the case are in the notes below.

### 34. Congrats!-vDoqpwCHxs4.en

Congratulations on completing this lesson on Control Flow.You learned how to use conditional statements, loops,useful built-in functions, andlist comprehensions to add more functionality to your code.Next, you'll learn how to organize your code into functions. Great job.

## Part 03-Module 01-Lesson 04_Functions

### 01. Introduction-p5L4rTV1Pgk.en

Welcome to this lesson on functions.Previously, we used several of Python's built-in functions.Here, we will write functions of our own.Functions are useful chunks of code that allow you to encapsulate a task.Encapsulation is a way to carry out a whole series of steps with one simple command.For example, imagine you want to bake a cake.You would need to buy the ingredients,mix them together in a certain way for specific amounts of time,put it all in the oven, and then let it cool.In computer programming, functionsencapsulate all the steps of a process into one command.In this case, we might just havea bake cake function and throw all of these directions into this one function.Now any, time you want to bake a cake,we just use this function without worrying about all the specifics.Functions are also used to help organize and optimize code. Let's get started.

### 02. Default Arguments-cG6UfBZX2KI.en

Let's revisit the cylinder volume function,but with one modification.This function includes a default argument.If radius is not specified,then the variable radius will default to five when used in the function body.Calling the function like this would be the same thing as calling it like this,because radius is set to five if not specified as another value.Default arguments allow functions to use default values when those arguments are omitted.This is helpful because it can make your code more concise inscenarios where there is a common value you can use for a variable,although you still want it to be customizable.In this example, the default radius is five but we can still change this.This time when we call the function,we are overriding the default value of five and calculatingthe cylinder volume for a cylinder with a radius of seven instead.Also notice here, we're passing values to our arguments by position.It's possible to pass values in two ways,by position and by name.Each of these function is evaluated in the same way.This one is simply taking the arguments byposition while this function is accepting arguments by name.There are some nuances for when you can performthe passing of values in each of these ways.You'll see more applications of this in the following quiz section.

### 02. Defining Functions-IP_tJYhynbc.en

As our first function,we'll write a function that calculates the volume of a cylinder.The formula for this is the cylinder's height,multiplied by the square of its radius, multiplied by pi.Here's what that formula looks like when defined in a function called cylinder volume.After defining the cylinder volume function,we can use it like this.This value is the volume of a cylinder 10 inches tall with a radius of three inches.Let's see how this works behind the scenes.A function definition includes several important parts.First, let's look at the function header.Defining a function always starts withthe def keyword to indicate that the code that follows is a function definition,and ends with a colon.Following def is the name of the function.In this case, cylinder_volume.This needs to be a one word with no gaps.That's why this has an underscore.The rules for function names are the same as those for variable names.If you need a reminder on this,see the text below this video.After the function name are parentheses thatincludes the arguments that the function expects, separated by commas.These arguments are values that are passed in as inputswhen the function is called and are used in this body.If you write a function that doesn't take arguments,then just leave these parentheses empty.Here is an example of a function that takes no arguments.In this case, there is no input data we want to work with in the body of the function,so no arguments are necessary.Next, let's discuss the body of a function.The body of the function is indented afterthe header and is where the function does its work.Within this body, we can refer to the argument variables and define new variables.The pi variable that we define here is a local variable.Meaning; it can only be used within the body of this function.Attempting to access this variable outside the function isn't possible.This is due to what is called variable scope,which determines which variables you have access to in Python.We'll discuss this in detail later in the lesson.Often, the body of a function will includethis return key word which is used togive back an output value when the function is called.The value of the expression that follows return is the output of the function.In this example, we return the value evaluated fromthis formula for the volume of a cylinder.Rather than returning the value as it is calculated,an alternative technique would be to calculate the volume earlier inthe function body and then store it in a variable named volume.In this case, we would return volume like this.Functions like this can be imagined as little machines that takeinputs or arguments and process them into output or return values.This is a good image but it's incomplete.Some functions like print don't return anything at all.Print displays text on the output window but as we see here,the value it returns is none.None is when a function will return bydefault if it doesn't explicitly return anything else.The difference between print and return is often confused.Print provides output to the console while return providesthe value that you can store and work with and code later.You'll get some practice with this in the quizzes that follow.It's not necessary that every function has a return statement.Notice this function, print_greeting,doesn't have a return statement,but it's still a valid function.

### 05. Variable Scope-rYubQlAM-gw.en

Scope refers to which parts of a program a variable can be referenced or used from.If a variable is created inside a function,it can only be used within that function.Consider these two functions,word count and nearest square.The first function uses answer to count words in a document.The second function uses answer to check values while searching for the nearest square.Both functions include this answer variable,but they are distinct variables that canonly be referenced within their respective functions.Alternatively, we might have a variable defined outside of functions, like this.And we can access it anywhere outside or within these functions.Scope is essential to understanding how information is passed throughout programs,in Python, and really any programming language.

### 08. Documentation-_Vl9NJkA6JQ.en

One of the key advantages of functions is thatthey can help break a program down into smaller chunks.This makes code easier to write and also easier to read because the pieces of your code,the functions, are reusable.If a program needs to calculate volumes of multiple cylinders,it can call the cylinder volume function multiple times,which is much cleaner than writing out the formula over and over again.Functions make code easier to read because they give human readable names to processes.While the cylinder volume formula isn't that complicated,it is still harder to recognize than a precisely-named function.There is an additional technique that makes functions more readable,documentation strings or docstrings.Docstrings are a type of comment used to explainthe purpose of a function and how it should be used.Here is a function for population density with a docstring.Docstrings are surrounded by triple quotes.The first line of the docstring is a brief explanation of the function's purpose.If you feel that this is sufficient documentation,you can end the docstring here.Single line docstrings are perfectly acceptable.If you think that the function is complicated enough to want a longer description,you can add a more thorough paragraph after the one-line summary.The next element of a docstring is an explanation of the function's arguments.Here, you list the arguments,state their purpose and what types the arguments should be.Finally, it's common to provide some description of the output of the function.Every piece of the docstring is optional.However, docstrings are part of good coding practice.They assist you and future users in understanding the code you produced.You can read a more thorough explanation of docstring conventions in the link below.

### 11. L4 08 Lambda Expressions V3-wkEmPz1peJM.en

In Python, you can use lambda expressions to create anonymous functions.That is, functions that don't have a name.They are very helpful for creatingquick functions that aren't really needed later in your code.Later, we'll learn about higher-order functions.Are functions that take in other functions as arguments,where lambda expressions become especially useful.Let's compare the structure of a function and a lambda expression.Here, is a simple function that doubles a number.It takes in a number x and returns x multiplied by two,calling double three, would return six.Here is the equivalent in a lambda expression.The lambda keyword is used to indicate that this is a lambda expression.Following lambda or one or more arguments for the anonymous function and then a colon.These are equivalent, similar to the way argument names in a function are arbitrary.Last is an expression that is evaluated and returned in this function.This is a lot like the expression you might see as a return statement in a function.With this structure, lambda expressions aren't ideal for complex functions,but can be very useful for short symbol functions.If you want to specify multiple arguments in a lambda function,you can include them before the colon, separated by commas.Here's a lambda function that multiplies two numbers together.In the following quizzes,you'll get some practice using lambda functions and see how useful they can be.

### 14. Iterators And Generators-tYH8X4Zeh-0.en

If you recall from the previous lesson,iterables are objects that can return one of its elements at a time.Lists are one of the most common iterables you have used.It turns out many of the built-in functions we've used so far,such as enumerate, returns something called an iterator.An iterator is an object that represents a stream of data.This is different from a list which is also an iterablebut not an iterator since it is not a stream of data.Soon, you'll see some reasons that iterators are favored in different situations.Here we will learn how to create iterators using generators.Generators are a simple way to create iterators using functions.However, it's not the only way to create iterators.You can read more about this in the notes below.These terms may be a little confusing.The term generator is often used to refer to the generator function,but it's also used to refer to the iterator object produced by the function.Here I'll differentiate these by referring to the function as a generator function,and what it produces as the iterator.Here is a generator function called 'my range' thatproduces a stream of numbers from zero to x minus one.Notice that instead of using the return keyword, this uses yield.This allows the function to return values one at a timeand start where it left off each time it's called.This yield keyword is what differentiates a generator from a typical function.As you can see here,calling my range four returns an iterator that we can then iterate through.Using a for loop,we can print values from this stream of data.Here this prints zero, one, two and three.In the next section,you'll get some practice writing generator functions to create your own iterators

### 18. Conclusion-QRnLr7pwHyk.en

Congratulations on completing this lesson on functions.You learned about functions, variable scope,documentation, lambda expressions, iterators, and generators.Next, you'll learn how to write scripts inlocal environments with many functions pieced together.That's a lot of stuff.It's okay if you haven't mastered everything yet.You've already accomplished a ton by getting this far.We'll practice more of these concepts in the next lesson.

## Part 03-Module 01-Lesson 05_Scripting

### 01. Scripting-Qxe_gCiXUDg.en

Welcome to this lesson on scripting.Here, you'll learn how to combine all the concepts you've learned throughoutthis course to write and run your own scripts locally on your computer.You'll learn about good scripting practices,working with raw input from users,reading and writing files, handling errors,importing local scripts, and working with different libraries. Let's get started.

### 02. Python Installation-2_P05aYChqQ.en

In all of the previous lessons,we've been using these boxes to represent a code editor wherewe wrote our Python code and output window where we viewed results.And in the classroom,you use these programming quizzes to write Python and runyour code but instead of running your code online in the classroom,you can run Python on your own computer by installingPython and useful tools for your programming environment.Since Python is a popular open-source programming language,it's free to download,change and use and the fact that it's already popular meansthere are online instructions to help you get itinstalled for most common operating systems.Because it's popular and open-source,there's a large global community working on developing Python.For you, as a Python user,that means there'll be new versions coming out as Python is improved by the community.You might find yourself needing to update Python from time to time especially if you'recollaborating with other people as you writePython programs that are compatible with the version they're using.Now, it's time for you to install Python.

### 05. Running A Python Script-vMKemwCderg.en

Now that you've successfully installed Python,let's try to run a script in your terminal.First, download the file first_script.pyattached at the bottom of this page and move it to an appropriate directory.I put my file here by dragging it from the downloads folder to this folder.This might be a good time to set upa new directory for your learning if you don't have one already.Next, open your terminal and use CDto navigate to the directory containing the downloaded file.I can double check that the file is located here by typing ls or dir on Windows.Now that you're in the directory with the file,you can run it by typing python followed bythe name of the file like this. Then press enter.You'll know you've run the scripts successfully if you see this message.Whenever you type Python followed by the file name for a Python script,that script is run and the output is shown immediately afterwards.Once the codes finished running,and all the output has been shown,your command line prompt reappears.Note if you typed and entered just Python without a script to run,it'll start an interactive interpreter for Python.You'll learn more about this later.For now, you can just type in exit with parentheses to get out or alternatively,control D on Mac and Linux or control Z and enter on Windows.Now, I'm back to the command line prompt.It's your turn. Download the file below and run the script in your terminal.

### 06. Programming Environment Setup-EKxDnCK0NAk.en

There are many ways to setupa programming environment and there isn't a right or wrong way to go about it.However, if you're just starting out programming in Python,you might like to see this setup to get some ideas.Over here is a text editor.This particular editor is called atom.But there are many additional editors I recommend in the notes below.A text editor is really different from a word processor,so don't mix them up.Code needs to be written line-by-line with intentional line breaks andindentations that fundamentally change how the code is read and run.Good text editors havesimple but vital features such as line numbers and syntax highlighting.You can see the different ways atom highlights a text based onthe types of words and objects it recognizes from the Python language.Syntax highlighting really helps make code more readable and organized.It can also prevent us from making some syntax errors.For example, if I forgot a quote at the end of the string,I'll know I missed it when the rest of the line is highlighted like a string.Atom knows to use syntax highlighting forPython because of this.py at the end of the file name.This tells atom it's dealing with a Python file.Editors are highly customizable to suit your style.One setting that I use is soft tabs with the top length of four.Which means that when I press tab it is converted into four spaces.You can also add cool add-ons.Like this Python linter,that uses Pylint to point out helpful things in my code.For example, in this line,pylint gives me a warning because I'm not followingPEP 8 guidelines by including a space before my closing bracket.I also have this cool add-on which let's me use my terminal in my editor.Atom isn't the only thing on my screen right now.I do use this terminal add-on from time to time butgenerally I like to have a separate window for my terminal where I can run my scripts.Generally, I can't write a whole lot of code in one go becausesomething will go wrong and I'll need to find the issue and debug it.When there's an error it will show up here when I run the script.I make small changes to the code ina text editor and then run the script again to see the output.If I see a problem I'll try to fix it or if things seem to be working I'll move on.I've also got the Udacity classroom and results from a question I googled in my browser.Normally, I'll need a browser open to look up documentationor search for the solution to a bug I encounter along the way.From now on, we recommend that you try out all your code in this way.Put code into a new file in your text editor and don't forget the.py extension.Make sure you're in the correct directory in your terminal and then run your script.

### 08. Scripting With Raw Input-Fs9uLV2qfgI.en

Now that you have your local program environment all set up,it's time to try out different inputs.Programs can get a lot more interestingwhen they can interact with external information.First, let's try getting raw input from the user.Here's what I mean.This program prompts the user to enter a name,and picks in that input as a string to use in its code.This input function is what takes in the user's input as a string.It has an optional argument that you can use to specify a prompt shown to the user.Since the input function interprets input as a string,you'll need to wrap the result with int or float if you want to use it as a number.If you don't do this and try to use the input as a number, you'll get an error.Here, I wrap the input with int and added 20 to it.But what if the user inputs a non integer number?You can just wrap it as a float to catch non integer numbers as well.But what if you need an integer,like if you're multiplying a string by it to repeat it a certain number of times?This wouldn't work with the float even if it's an integer number.We can actually wrap this int with float,and then convert it into an int like this.Okay, that works.Clearly, you're trying to imagine and handleevery possible case with user input can quickly get complicated.We addressed some cases here,but there are also loads of other cases we didn't address that could lead to more errors.We'll learn a better way to handle these scenarios in the next section.Before we move on to practice,here's another way we can interpret user input.Eval is a built-in function that evaluates a string as a line of Python.You can even include variables in the string like this.Now, let's write some fun scripts that interact with user input.

### 11. Errors And Exceptions-DmthSiy2d0U.en

As you saw in the last video,trying to handle every kind of scenario whendealing with unexpected input can be a bit much.There's actually a much simpler way of dealing with this in Python.Here, you learn how to handle errors with try and except blocks.First, let's learn more about what errors in Python are.We've seen error messages due to several reasons in this course,and we can separate them into two types,syntax errors and exceptions.Syntax errors occur when Python can't interpretour code since we didn't follow the correct syntax for Python.These are errors you're likely to get when you make a typo,or you're first starting to learn Python.Here, I get a syntax error because Imissed the ending quotation mark at the end of the string.I can see this in the error message,SyntaxError: End of line when scanning string literal.The other kind of error,exceptions occur when unexpected things happen during the execution of the code.Here, although our code is syntactically correct,we run into an exception because a string spelling out10 isn't a valid argument for the int function.There are different types of built-in exceptions in Python,and you can see which exception is being thrown here in the error message.Value error is one type of exception.It occurs when a built-in operation or function isgiven an argument with the correct type but an inappropriate value.Here's another example that there was an exception.Here, we try to reference a variable name that we didn't define yet.So, we get a name error exception.Again, this code is syntactically correct,but when Python gets to this line of code,it can't find the value for this variable when it tries to run it.So, it raises an exception and stops the code.

### 13. Handling Error Specifying Exceptions-EHW5I7shdJg.en

Here, the code in the except block occurswhen any kind of exception occurs while executing the try block.Really though we just want to adjust the ValueError exception.If I try to exit the program with Ctrl C,it doesn't stop the program because the except handles any error,including the KeyboardInterrupt error.We can actually specify which error we want to handle like this.Now it catches the ValueError exception,but not other exceptions like KeyboardInterrupt.Notice in this case when I press Ctrl C,it ends the program.This is because a KeyboardInterrupt exception was raised,which isn't handled by this try statement.Also notice that although the program crashed,this code in the finally block was still executed.Attempted Input is printed no matter what as the program is exiting this try statement,even if that means exiting the program.If we want this handler to address more than one type of exception,we can include a tuple after the except with the exception like this.Or if we want to execute different blocks of code depending on the exception,you can have multiple except blocks like this.Here, I have two handlers,one for ValueError and one for KeyboardInterrupt.When I press Ctrl C,it says No input taken and breaks from the while loop.

### 13. Handling Errors Try Except Finally-S6hwBZG0KwM.en

In Python, there are ways to handleexceptions so they don't always crush our program when they occur.Let's look back at this example that takes an input from the user.We saw that when we ran this code,it got an error when the user input something that can't be converted to an int.We can actually handle this error using a try statement.In a try statement,the code inside the try block is first runand if Python runs into any exceptions while it's running this block,it will jump to the code in the except block.Here if I run this code and enter the word ten,it prints, that's not a valid number and moves on to the rest of the code.The program continues to run whether or notit runs into an exception during this try block.For example, if I have a line after this try statement that prints attempted input,you'll see this is printed in both cases.If we want this code to keep running until the user inputs a valid number,we can use a while loop and break the loop ifall the code in the try block successfully executes.Here, the program keeps taking in input until I enter a valid number.If I enter a valid number,this statement in the try block doesn't raise an exception.So it moves on to the next line where it breaks from the loop.However, since it breaks the loop,it never prints attempted input.If we want this last line to always run after the try statement no matter what,there is an optional component of the statement we can use.Finally, now, attempted input will be printed whenthe program is exiting this try statement under any conditions,whether there is a break statement,return statement or error that causes this program to crash within except block.You can read more about this in the notes below.The finally block is useful for cleaning up actions in your code.Later in the lesson, we will use this to closea file after attempting to open one in a try statement.

### 17. Reading And Writing Files Part II-1GRv1S6K8gQ.en

Let's see how we read information from a file into Python.To read from a file,we first need to open it,which we can accomplish with the built-in function open.We include a string with the path tothe file along with any optional parameters we want to specify.The open function returns a file object which isa Python object through which Python interacts with the file itself.Here we assigned this file object to the variable f.This second optional parameter specifies the mode in which we open the file.In this case r, or read only.We are using this mode since we only want to read from the file.We don't want to change any of its contents.This parameter isn't really necessary for us to include here,though, since the mode defaults to read only if unspecified.Once we opened a file and created a file object,we can use the read method to access the contents of this file.This read method takes the text contained in a file and puts it into a string.Here we assign the string returned from this method into the variable file_data.When we are finished with the file f, we should close it.This will free up any system resources taken up by the file.Here's an example that uses this file, some_file.txt.It's important to rememberto always close files we have opened once we no longer need them.If we open a lot of files without closing them,we can run out of file handles and we won't be able to open any new files.Exactly how many files you can open beforerunning out of handles will depend on your operating system.To convince yourself, you can try running the following script in Python.At some point for a large enough number,you will receive an error.We can see that at 7164 open files,the system no longer had available resources to open any new files.To avoid this, it is always a good idea to close any files you no longer need.Opening a file object is like opening a window to look into a file.To be more precise,it's a window that's only one characterwide and it always starts off at the very start of the file.This is very different from reading a book or a document,where you can look at multiple words or even pages at once.Think instead of the file as a long stream of characters.The file object can look at just one character at a time and order.In addition to reading from a file,you can also write to a file,in which case you will change the content of the file.To do so, you must open the file in writing mode.Be careful, once you open a file in writing mode,anything that it contained previously will be deleted.If you're interested in adding to an existing file without deleting its content,you should use append instead of write.You can visit the Python documentation formore information on the different modes in which you can open a file.Since we're in write mode and I don't want to delete what's in this file,let's use another one.If the file does not exist,Python will create it for you.We can now write to the file like this,and after we're done we will close it like good citizens.Here you can see that Python created another file which contains the text "Hello World!"

### 17. Reading And Writing Files Using With-OQ-Y0mMjm00.en

It could be easy to forget to close a file when you're done using it.So Python provides a special syntax that auto closes it.This with keyword allows you to open a file,do operations on it,and automatically close it after the indented code is executed,in this case, reading from the file.Now, we don't have to call f.close.This code, as f assignsthe file object created by the open function to the variable name,f. This line of code is basically this line of code,except you can only access the file object,f within this block.This is another kind of scope.Once you leave this indented block,the file is closed and you are no longer able to interact with it.For example, trying to call f.read outside this block like this would return an error.However, just because you closed the file doesn't mean you lose the data.Here, we read in the file in this line and getthis file data string that has the text contained in the file.Calling file data outside the block works fine.We can use all the usual string methods on this file data string to process its contents.

### 17. Reading And Writing Files-w-ZG6DMkVi4.en

In order for a program to be really useful,it needs to interact with real-world data.Images, web pages, and databases are all examples of files,and we routinely create, move,manipulate, and read these files in our daily digital lives.All the data we've used so far has been defined insidethe Python script or raw input from the user during execution of the script.Next, we're going to massively increase the variety of what we can achieve inour Python programming by introducing how to read and write files in Python.This will allow us to interact with and processlarger amounts of information from many more sources.All kinds of files have a similar structure on a computer.There are strings of characters that encode some information.The specific file format,often indicated by the extension of the file name such as.py,TXT, HTM, CSV, and many more,will indicate how those characters are organized.The characters in a file are interpreted bythe various programs we use to interact with them.For example, an image editing program will interpretthe information of a digital photograph file and display the image.If we then edit the image in the program,were using this program to make changes to the characters in the file.In Python, we can read those file characters directly.The experience will seem quite different from opening a file in a desktop application.Opening files in Python gives usa common programmatic interface to all kinds offiles without the need for a graphical user interface,which means we can automate tasks involving files with Python programs.

### 20. Importing Files-qjeSn6zZbR0.en

In addition to reading in data from files,we can actually import Python code from other scripts.This is especially helpful if you're working on a bigger project where you want toorganize your code into multiple files and reuse code in those files.If the Python script you want to import is in the same directory as your current script,you just type import,followed by the name of the file without the dot PY extension.Imports statements are written at the top of a Python script,each one on a separate line.Python will still run if imports statements are included later in the script,but it's common practice to always have these at the top.You're only able to access what you've imported after this statement so,it's just less confusing to have these written first.It's also nice for readers to see what a scriptdepends on before reading the rest of the code.Let's start with a really simple example.Here, I have a Python file called other_script.py that just prints two plus three.In this file demo.py,I import other_script and then print four.When we run demo.py,this import statement tells Python to run code from that file which prints five.It then continues to execute the rest of the code in this file printing four.If instead this file had the line num equals two plus three,and we try to access this in demo.py,referencing it with just the name of the variable would return an error.To access objects in other_script.py,we need to use the name of the file followed by a dot,followed by the object to tell Python to lookfor this object in the other_script file we imported.Now, it accesses the variables successfully.When Python runs the script.It only has direct access to objects defined in the script.One of these objects is a module called other_script.A module is just a file with Python definitions and statements.When we import a Python file like this,it creates an object called other_script with a type module.Let's see a more useful example of importing a module.Here, we have a Python file that contains useful functions we would like to use.One, that returns the mean of a list and one that adds five to each element of a list.We can import them into demo.py,and type useful_functions dot the name of the function to use them.You can imagine this would be very helpful ifwe had many functions used a lot in different files.Although, it seems a little annoying that we have to type outthe whole name of the file each time we want to use a function from it.We can make this much simpler by adding an alias.Here, I made an alias for the useful_function module UF.Now, I can just type UF instead of the whole module name when calling functions from it.This is useful when we have objects we wantto import from other Python scripts like functions.But, what if that script also includesexecutable statements in addition to function definitions that we don't want to import.For example, what if useful_functions.py has codeat the bottom of the script that tests its functions and prints the results?This code is nice if we run useful_functions.py to test out these functions,but unnecessary if we're just trying to use these functions in another script.Here is where we can use this statement if name equals main.By including these executable statements inside this if main block,we tell Python to execute this code only whenthe main program being executed is this useful_functions.py.If we run this file the code inside this block is run.However, if we run another script that simply importsuseful_function.py this code is not run.Generally, it's good practice to write executable statements insidean if name block or alternatively include themin a function called Main and call this in the if name block.You're probably wondering what this Name and Main stuff is.Whenever we run a script like this,Python actually sets a special built in variable calledName with two underscores before and after it for any module.Here, since we ran Python demo.py,Python recognizes this module as the main programand sets the name variable for this module to the string Main.For any modules that are important in the script,this built-in name variable is just set to the name of that module.

### 21. The Standard Library-Fw3vf0tDrJM.en

You've seen how helpful it could be to import your own modules.But what if I told you there wasan entire library of built-in modules that just come with Python?This is the Python standard library.Up here, you see built-in objects we use throughout this course;like built-in functions, data types, and exceptions.Below that though, are tons of useful modules that you can import.Think of this library as a very big set oftools that you can use to help you program in Python.It provides new types of objects andfunctions for a range of common and specialized tasks.Other people have already writtenthis code and put it into useful modules for you to use.Using modules from the Python standard library toeasily access and use existing code gives you a lot of programming power.The Python standard library is organized into modules.Many modules are simply Python files,like the Python scripts you've already written and imported.Let's import this math module.Again, an import statement runs the code in the module.Modules typically contain a lot of definitions and usually don't show any output.Running the code will makeall the modules' functions and types of objects available to use.Math has a factorial function,which finds the product of a number and all the positive integers less than it.Four times three, times two,times one is 24,so math.factorial(4) for prints 24.The Python standard library has good documentation for each of its modules,and it's a good idea to read the relevant page whenever you use one.Here's a documentation for the math module.So far, you've seen only one module in the Python standard library.It's a good one, but there are many more.If you look at the documentation for the whole of the Python standard library,modules are listed in groups based on their uses.Clicking on a name takes you to the documentation for that module,which often includes example code that you should feel free to test out.

### 24. Techniques For Importing Modules Part II-aASigWQ_XU0.en

Some of the modules in the Python Standard Library have a lot in them.In order to manage the code better,they're split down into sub-modules that are contained within a package.A package is simply a module that contains sub-modules.A sub-module is specified with the usual dot notation.For example, the OS module,which is useful for dealing with file systems has a sub-module os.path,which is used specifically for dealing with path names.Modules that are sub-modules are specified bythe package name and then the sub-module name separated by a dot.You can import the sub-module os.path like this.You can then use the objects from the sub-module in the usual way.However, this syntax for importing will only work for sub-modules.You cannot import a function from a module in this way.If you want to use other parts of the OS module two,you could import OS instead and everything in the os.path will still be accessible.Sometimes, naming can be a point of confusion when working with modules.For example, a module might be named afterone of the important classes or functions within it.In this case, you will need to think carefully about your import statements.This imports the datetime class from the datetime module.Note that after this, using datetime,will refer to the datetime class, not the module.

### 24. Techniques For Importing Modules-jPGyFgcIvsM.en

So far, you've imported modules with import followed by the module name,which makes all the classes of objects andfunctions of that module available via.notation.There are some other variance of importing that are useful in other situations.You can import an individual function or class from a module like this,from, the module name,import, the object name.This gives access only to defaultdict from the collections module.Defaultdict will be accessible with its own name without the module name before it.Trying to access collections or even callingcollections.defaultdict will give a name error.Importing individual objects from a module means you only takewhat you need and you don't need to use.notation to access them.You can import multiple individual objects from a module by separating them with commas.This technique is very common when importing pieces from large libraries.And as you saw before,you can import a module and give it a different,usually shorter name, like this.If the name of a module is particularly long orif there's a clash with something with the same or similar name,renaming the module like this can be helpful.Check code examples in the documentation as these will ofteninclude a standard abbreviation if one is used for this module.Using an abbreviation that is consistent with others will make your code more readable.For example, the standard abbreviation for the multiprocessing module is MP.You can combine the previous two pieces of syntax toinput an item from a module and change its name.Again, you'll be able to accessonly that individual item directly with its newly specified name.No.notation is needed.This can be useful if you have multiple objectswith similar names from different packages in your namespace.For example, perhaps you want a CSV reader and JSON reader.You could import them from their respective modules and give them descriptive names.Another way of importing that you may see inother people's code but that you should not use is this.Using this asterisk will import every object froma module individually and allow you to access each of them directly via its name.The real problem with this is that modules may contain many objects,each of which has a name.Including all these names may overwrite or maybeoverwritten by other names you're using in your program.Import star also makes it impossible forcollaborators to find where an important object was defined.A reader can search for a definition of a function and not find it andthey won't know which import star statement introduced a function.These problems can lead to a lot of confusion.Do not use from module name import asterisk.If you really want to use all the objects from the Random module,use the standard import random instead and access each of the objects with the.notation.

### 26. Third Party Libraries And Package Managers-epOze9gC6T4.en

Compared to most languages,Python has a large standard library.In fact, people say Python comes with batteriesincluded because it comes with the libraries you need to get right to work.However, the standard library doesn't come with everything you might want.Some tasks are too specialized to be accommodated by the standard library.Fortunately, there are tens of thousands ofthird-party libraries written by independent developers.How do we get these packages though,if they aren't included with Python itself?We can install libraries using pip,a package manager that is included with Python 3.Pip is a standard package manager for Python, but it isn't the only one.One popular alternative is Anaconda,which is designed specifically for data science.Here, we'll use pip,which is a general standard.Let's use pip to install this library,which is used for working with time zones or remarkably complicated task.This command will download and install the package,so that it's available to import in our programs.Once installed, we can import third-party packagesusing the same syntax we used to import from the standard library.In this example, I importa time zone package along with a datetime from the standard library.It's standard practice to put the import statements forthird-party libraries after imports from the standard library.This example stores the current time expressed in terms ofCoordinated Universal Time or UTC in the variable now.It then translates this time into IST,Indian Standard Time and stores that in the variable ist_now.Larger Python programs might depend on dozens of third-party packages.To make it easier to share these programs,programmers often list of projects dependencies in a file called requirements.txt.This is an example of a requirements.txt file.Each line of the file includes the name of a package and it's version number.The version number is technically optional,but it usually should be included.Libraries can change subtly or dramatically between versions,so it's important to usethe same library versions that the program's author used when they wrote the program.You can use pip to install all of the project's dependencies at once with this command.

### 27. Experimenting With An Interpreter-hspPtnQwMPg.en

If you open your terminal and type python,you should see something like this.This is the python interactive interpreter.You can type here to interact with Python directly.You just type your code,press enter and the output will appear on the following line.This is a good place to experiment and try bits of Python code at a time.Notice here, I didn't have to print the type to see the output.In the interpreter, the value of the last linein a prompt will be outputted automatically.If you had multiple lines where you'd want output values,you'd still have to print.If you start to define a function,you will see a change in the prompt to signify that this is a continuation line.You'll have to include your own indentation as you defined the function.A drawback of the interpreter is that it's tricky to edit code.If you made a mistake when typingthis function or forgot to indent the body of the function,you can't use a mouse to click your cursor where you want it.You have to navigate with the arrow keys to movethe cursor forwards and backwards to the line itself for editing.It would be helpful for you to learn useful shortcutsfor actions like moving to the beginning or end of the line.Notice, I can reference any objects I defined earlier in the interpreter.One useful trick is using the up and down arrowto cycle through your recent commands at the interactive prompt.This can be useful to rerun or adapt code you've already tried.To quit the python interactive interpreter,use the command exit with parentheses or hit controlD on mac or linux or control Z and enter for Windows.There's actually a really good alternative to the default python interpreter.IPython which comes as many additional features liketab completion which completes words for youor shows what options are available if there are multiple.This can be useful if you want to see what methods are available for an object.Another useful check is this question mark to get details about a particular object.I can quickly see what this function does without looking up the documentation.You can also execute system shell commands usingan exclamation point and some common ones don't even require it.You can learn more about IPython in the notes below.Using an interpreter can be really helpful for experimenting and testing python code.Those not necessarily just for experimentation.I also used an interpreter when I want to quicklyinvestigate or modify files using Python.

### 29. Conclusion-rEMrswkLvh8.en

Awesome job completing this course.After covering lessons on Python data types and operators, control flow, functions,and scripting, you've developed the understanding andskills necessary to tackle projects and courses in Python.Congratulations again on completing this course.

### img

## Part 04-Module 01-Lesson 01_Introduction

### 03. Essence Of Linear Algebra Intro -EHcxDZpeGFg.en

Hey everyone! So, I'm pretty excited about the next sequence of videos that I'm doing.They'll be about linear algebra,which as a lot of you know is one of those subjects that'srequired knowledge for just about any technical discipline,but it's also I've noticed generallypoorly understood by students taking it for the first time.A student might go through a class and learn how to compute lots of things likematrix multiplication or the determinant or cross products,which use the determinant or eigenvalues.But they might come out without really understandingwhy matrix multiplication is defined the way that it is.Why the cross product has anything to do withthe determinant or what an eigenvalue really represents.Oftentimes, students end up well practiced in the numerical operations ofmatrices but are only vaguely aware of the geometric intuitions underlying it all.But there's a fundamental difference between understanding linear algebraon a numeric level and understanding it on a geometric level.Each has its place but roughly speaking,the geometric understanding is what let's youjudge what tools to use to solve specific problems,feel why they work,and know how to interpret the results.And the numeric understanding is what let's youactually carry through the application of those tools.Now, if you learn linear algebra without gettinga solid foundation in that geometric understanding,the problems can go unnoticed for a while untilyou've gone deeper into whatever field you happen to pursue.Once you're in a class or a job for that matter that assumes fluency with linear algebra,the way that your professors or yourco-workers apply that field could seem like utter magic.They'll very quickly know what the right tool to useis and what the answer roughly looks like ina way that would seem like computational wizardry ifyou assume that they're actually crunching all the numbers in their head.Here, as an analogy,imagine that when you first learned about the sine function intrigonometry you were shown this infinite polynomial.This by the way is how your calculator evaluates the sine function.For homework, you might be asked to practice computing approximations ofthe sine function by plugging in various numbersto the formula and cutting it off at a reasonable point.And in fairness, let's say you had a vague idea thatthis was supposed to be related to trianglesbut exactly how had never really been clear and was not the focus of the course.Later on, if you took a physics course wheresines and cosines are thrown around left and right andpeople are able to tell pretty immediately how to applythem and roughly what the sine of a certain value will be,it would be pretty intimidating, wouldn't it?It would make it seem like the only people who are cutout for physics are those with computers forbrains and you would feel unduly slow or dumb for taking so long on each problem.It's not that different with linear algebra.And luckily, just as with trigonometry,there are a handful of intuitions,visual intuitions, underlying much of the subject.And unlike the trig example,the connection between the computation andthese visual intuitions is typically pretty straightforward.And when you digest these and reallyunderstand the relationship between the geometry and the numbers,the details of the subject as well as how it's used inpractice start to feel a lot more reasonable.In fairness, most professors do make an effort to convey that geometric understanding.The sine example is a little extreme.But I do think that a lot of courses have students spendinga disproportionate amount of time on the numerical side of things,especially given that in this day and age we almost always get computersto handle that half while in practice humans worry about the conceptual half.So, this brings me to the upcoming videos.The goal is to create a short,watchable series animating those intuitions from the basics ofvectors up through the core topics that make up the essence of linear algebra.I'll do what I can to keep things well paced throughoutbut it's hard to simultaneously account fordifferent people's different backgrounds and levels of comfort so Ido encourage you to readily pause and ponder if you feel that it's necessary.Actually, I'd give that same advice forwatching any math video even if it doesn't feel tooquick since the thinking that you do onyour own time is where all the learning really happens, don't you think?So with that is an introduction, I'll see you next video.

### img

## Part 04-Module 01-Lesson 02_Vectors

### 01. Vectors 1-oPBz-MLVUHk.en

The fundamental root of it all building block for linear algebra is the vector.So, it's worth making sure that we're all onthe same page about what exactly a vector is.You see, broadly speaking,there are three distinct but related ideas about vectors,which I'll call the physics student perspective,the computer science student perspective and the mathematician perspective.The physics student perspective is that vectors are arrows pointing in space.What defines a given vector,is its length and the direction it's pointing.But as long as those two facts are the same,you can move it all around and it's still the same vector.Vectors that live in the flat plane,are two-dimensional and those sitting in broader space,that you and I live in, are three-dimensional.The computer science perspective is that vectors are ordered lists of numbers.For example, let's say you were doing some analytics about house prices,and the only features you cared about were square footage and price.You might model each house with a pair of numbers,the first indicating square footage and the second indicating price.Notice, the order matters here.In the lingo, you'd be modeling houses as two-dimensional vectors,where in this context vector is pretty much just a fancy word for list,and what makes it two-dimensional,is the fact that the length of that list is two.The mathematician, on the other hand,seeks to generalize both these views,basically saying that a vector can be anythingwhere there's a sensible notion of adding two vectors,and multiplying a vector by a number,operations that I'll talk about later on in this video.The details of this view are rather abstract,but the reason they bring it up here,is that it hints at the fact that the ideas ofvector addition and multiplication by numbers,will play an important role throughout linear algebra.But before we talk about those operations,let's just settle in on a specific thought to have in mind,when I say the word vector.Given the geometric focus that I'm shooting for here,whenever we introduce a new topic involving vectors,I want you to first think about an arrow, and specifically,think about that arrow inside a coordinate system,like the x, y plane,with its tail sitting at the origin.This is a little bit different from the physics student perspective,where vectors can freely sit anywhere they want in space.In linear algebra, it's almost alwaysthe case that your vector will be rooted at the origin.Then, once you understand a new concept in the context of arrows in space,we'll translate it over to the list of numbers point of view,which we can do by considering the coordinates of the vector.Now, while I'm sure that many of you are already familiar with this coordinate system,it's worth walking through explicitly,since this is where all of the important back andforth happens between the two perspectives of linear algebra.

### 02. Vectors 2-R7WiQYixvRQ.en

Focusing our attention on two dimensions for the moment,you have a horizontal line called the x-axis and a vertical line called the y-axis.The place where they intersect is called the origin,which you should think of as the center of space and the root of all vectors.After choosing an arbitrary length represent one,you make tick marks on each axis to represent this distance.When I want to convey the idea of 2D space as a whole,which you'll see comes up a lot in these videos,I'll extend these tick marks to make grid lines,but right now they'll actually get a little bit in the way.The coordinates of a vector is a pair of numbers that basically givesinstructions for how to get from the tail of that vector at the origin to it's tip.The first number tells you how far to walk along the x-axis.Positive numbers indicating rightward motion,negative numbers indicating leftward motion,and the second number tells you how far to walk parallel to the y-axis after that.Positive numbers indicating upward motion,and negative numbers indicating downward motion.To distinguish vectors from points,the convention is to write this pair of numbersvertically with square brackets around them.Every pair of numbers gives you one and only one vector,and every vector is associated with one and only one pair of numbers.What about in 3-dimensions?Well, you add a third axis called the z-axis which isperpendicular to both the x and y axis and in this case,each vector is associated with an ordered triplet of numbers.The first tells you how far to move along the x-axis,the second tells you how far to move parallel to the y-axis andthe third one tells you how far to then move parallel to this new z-axis.Every triplet of numbers gives you one unique vector in space.And every vector in space gives you exactly one triplet of numbers.

### 03. Vectors 3-mWV_MpEjz9c.en

All right. So, back to vector addition and multiplication by numbers.After all, every topic in linear algebra is going to center around these two operations.Luckily, each one's pretty straightforward to define.Let's say we have two vectors,one pointing up and a little tothe right and the other one pointing right and down a bit.To add these two vectors,move the second one so that its tail sits at the tip of the first one.Then, if you draw a new vector from the tailof the first one to where the tip of the second one now sits,that new vector is their sum.This definition of addition by the way,is pretty much the only time inlinear algebra where we let vectors stray away from the origin.Now, why is this a reasonable thing to do?Why this definition of addition and not some other one?Well, the way I like to think about it is that each vector represents a certain movement,a step with a certain distance and direction in space.If you take a step along the first vector,then take a step in the direction and distance described by the second vector,the overall effect is just the same as if youmove along the sum of those two vectors to start with.You can think about this as an extension of how wethink about adding numbers on a number line.One way that we teach kids to think about this,say with 2 + 5 is to think of movingtwo steps to the right followed by another five steps to the right.The overall effect is the same as if you just took seven steps to the right.In fact, let's see how vector addition looks numerically.The first vector here has coordinates 1,2.And the second one has coordinates 3,-1.When you take the vector sum using this tip to tail method,you can think of a four-step path from the origin to the tip of the second vector.Walk one to the right then two up then three to the right then one down.Reorganizing these steps so that you first do all of the rightward motion,then do all the vertical motion,you can read it saying first move 1 + 3 to the right,then move 2 - 1 up.So, the new vector has coordinates 1 + 3 and 2 + -1.In general, vector addition in this list ofnumbers conception looks like matching up their terms and adding each one together.The other fundamental vector operation is multiplication by a number.Now this is best understood just by looking at a few examples.If you take the number two and multiply it by a given vector,it means you stretch out that vector so that it's two times as long as when you started.If you multiply that vector by say one third,it means you squish it down and so that it's one third the original length.When you multiply it by a negative number like negative 1.8,then the vector first gets flipped around then stretched out by that factor of 1.8.This process of stretching or squishing or sometimesreversing the direction of a vector is called scaling.And whenever you catch a number like two or one third ornegative 1.8 acting like this scaling some vector,you call it a scalar.In fact, throughout Linear Algebra,one of the main things that numbers do is scale vectors.So, it's common to use the word scalar pretty much interchangeably with the word number.Numerically, stretching out a vector by a factor of say,two corresponds with multiplying each of its components by that factor two.So, in the conception of vectors as lists of numbers,multiplying a given vector by a scalar meansmultiplying each one of those components by that scalar.You'll see in the following videos what I mean when I say thatlinear algebra topics tend to revolve around these two fundamental operations,vector addition and scalar multiplication.So, they're your vector basics and in the next video,I'll start getting into some pretty neat concepts surrounding vectors like span,basis and linear dependence. See you then.

### img

## Part 04-Module 01-Lesson 03_Linear Combination

### 01. Linear Combinations 1-fmal7UE7dEE.en

In the last video, along with the ideas of vector addition and scalar multiplication,I described vector coordinates,where there's this back and forth between,for example, pairs of numbers and two-dimensional vectors.Now, I imagined the vector coordinates were already familiar to a lot of you,but there's another kind of interesting way to think about these coordinates,which is pretty central to linear algebra.When you have a pair of numbers that's meant to describe a vector,like three negative two,I want you to think about each coordinate as a scalar.Meaning, think about how each one stretches or squishes vectors.In the x, y coordinate system,there are two very special vectors,the one pointing to the right with length one,commonly called i hat or the unit vector in the x-direction,and the one pointing straight up with length one,commonly called j hat or the univector in the y-direction.Now, think of the x-coordinate of our vector as a scalar that scales i hat,stretching it by a factor of three and the y-coordinate as a scalar that scales j hat,flipping it and stretching it by a factor of two.In this sense, the vector that these coordinatesdescribe is the sum of two scaled vectors.That's a surprisingly important concept,this idea of adding together two scaled vectors.Those two vectors, i hat and j hat,have a special name by the way.Together, they're called the basis of a coordinate system.What this means basically is that when you think about coordinates as scalars,the basis vectors are what those scalars actually scale.There's also a more technical definition,but I'll get to that later.By framing our coordinate system in terms of these two special basis vectors,it raises a pretty interesting and subtle point.We could have chosen different basis vectors and gottena completely reasonable new coordinate system.For example, take some vector pointing up and tothe right along with some other vector pointing down and to the right in some way.Take a moment to think about all the different vectors thatyou can get by choosing two scalars,using each one to scale one of the vectors,then adding together what you get.Which two-dimensional vectors can you reach by altering the choices of scalars?The answer is that you can reach every possible two-dimensional vector.And I think it's a good puzzle to contemplate why.A new pair of bases vectors like this still gives us a valid way togo back and forth between pairs of numbers and two-dimensional vectors,but the association is definitely different from the one that youget using the more standard basis of i hat and j hat.That anytime we described vectors numerically,it depends on an implicit choice of what basis vectors we're using.So, anytime that you're scaling two vectors and adding them like this,it's called a linear combination of those two vectors.Where does this word linear come from?Why does this have anything to do with lines?Well, this isn't the etymology,but one way I like to think about it is that if you fix oneof those scalars and let the other one change its value freely,the tip of the resulting vector draws a straight line.Now, if you let both scalars rangefreely and consider every possible vector that you can get,there are two things that can happen.For most pairs of vectors,you'll be able to reach every possible point in the plane.Every two-dimensional vector is within your grasp.However, in the unlucky case where your two original vectors happen to line up,the tip of the resulting vector is limitedto just the single line passing through the origin.Actually, technically, there's a third possibility too,both your vectors could be zero in which case you'd just be stuck at the origin.Here's some more terminology.The set of all possible vectors that you can reach with a linear combination ofa given pair of vectors is called the span of those two vectors.So, restating what we just saw in this lingo,the span of most pairs of 2D vectors is all vectors of 2D space.But when they line up, their span is all vectors whose tip sits on a certain line.Remember how I said that linear algebrarevolves around vector addition and scalar multiplication?Well, the span of two vectors is basically a way of asking what areall the possible vectors you can reach using only these two fundamental operations,vector addition and scalar multiplication?

### 02. Linear Combinations 2-RsKJNDTb8nw.en

This is a good time to talk about how people commonly think about vectors as points.It gets really crowded to think about a whole collection of vectors, sitting on a line.And more crowded still,to think about all two-dimensional vectors all at once filling up the plane.So when dealing with collections of vectors like this,it is common to represent each one with just a point in space.The point at the tip of that vector,where as usual, I want you thinking about that vector with its tail on the origin.That way, if you want to think aboutevery possible vector whose tip sits on a certain line,just think about the line itself.Likewise, to think about all possible two-dimensional vectors all at once,conceptualize each one as the point where its tip sits.So, in effect what you will be thinking about isthe infinite flat sheet of two-dimensional space itself,leaving the arrows out of it.In general, if you are thinking about a vector on its own, think of it as an arrow.And if you are dealing with a collection of vectors,it's convenient to think of them all as points.So, for our span example,the span of most pairs of vectors ends up beingthe entire infinite sheet of two-dimensional space.But if they line up, their span is just a line.The idea of span gets a lot more interesting if westart thinking about vectors in three-dimensional space.For example, if you take two vectors in 3D space,that are not pointing in the same direction.What does it mean to take their span?Well, their span is the collection of all possible linear combinations,of those two vectors.Meaning all possible vectors you get byscaling each of the two of them in some way and then adding them together.You can kind of imagine turning two different knobs tochange the two scalars defining the linear combination,adding the scaled vectors and following the tip of the resulting vector.That tip will trace out some kind offlat sheet cutting through the origin of three-dimensional space.This flat sheet is the span of the two vectors, or more precisely,the set of all possible vectors whose tips siton that flat sheet is the span of your two vectors.Isn't that a beautiful mental image?So, what happens if we add a third vectorand consider the span of all three of those guys?A linear combination of three vectors isdefined pretty much the same way as it is for two.You'll choose three different scalars,scale each of those vectors and then add them all together.And again, the span of these vectors is the set of all possible linear combinations.Two different things could happen here.If your third vector happens to be sitting on the span of the first two,then the span doesn't change.You're sort of trapped, on that same flat sheet.In other words, adding a scaled version of that third vector tothe linear combination doesn't really give you access to any new vectors.But if you just randomly choose a third vector,it's almost certainly not sitting on the span of those first two.Then, since it is pointing in a separate direction,it unlocks access to every possible three-dimensional vector.One way I like to think about this,is that as you scale that new third vector,it moves around that span sheet of the first two sweeping it through all of space.Another way to think about it,is that you are making full use of the three freely changing scalars that youhave at your disposal to access the full three dimensions of space.Now, in the case where the third vector was already sitting onthe span of the first two or the case where two vectors happen to line up,we want some terminology to describe the factthat at least one of these vectors is redundant.Not adding anything to our span.Whenever this happens, where you havemultiple vectors and you could remove one without reducing the span,the relevant terminology is to say that they are linearly dependent.Another way of phrasing that would be to say that one ofthe vectors can be expressed as a linear combination of the others,since it is already in the span of the others.On the other hand, if each vector really does add another dimension to the span,they are said to be linearly independent.So with all of that terminology and hopefully with some good mental images to go with it,let me leave you with a puzzle before we go.The technical definition of a basis of a space isa set of linearly independent vectors that span that space.Now given how I described a basis earlier and givenyour current understanding of the words span and linearly independent,think about why this definition would make sense.In the next video, I will get into matrices and transforming space. See you then.

### img

## Part 04-Module 01-Lesson 04_Linear Transformation and Matrices

### 09. Linear Transformations 1-99jYIxBRDww.en

Hey everyone, if I had to choose just one topic that makes all of the others inlinear algebra start to click and which too oftengoes unlearned the first time a student takes linear algebra,it will be this one.The idea of a linear transformation,and its relation to matrices.For this video, I'm just going to focus on whatthese transformations look like in the case of two dimensions,and how they relate to the idea of matrix-vector multiplication.In particular, I want to show you a way to think about matrix-vector multiplication,that doesn't rely on memorization.To start, let's just parse this term, linear transformation.Transformation is essentially a fancy word for function,it's something that takes in inputs,and spits out an output for each one Specifically in the context of linear algebra,we like to think about transformations that take in some vector,and spit out another vector.So, why use the word transformation instead of function,if they mean the same thing?Well, it's to be suggestive of a certain way to visualize this input-output relation.A great way to understand functions of vectors,is to use movement.If a transformation takes some input vector,to some output vector,we imagine that input vector moving over to the output vector.Then to understand the transformation as a whole,we might imagine watching every possible input vector,move over to its corresponding output vector.It gets really crowded to think about all of the vectors,all at once, each one has an arrow.So, as I mentioned in the last video,a nice trick is to conceptualize each vector not as an arrow,but as a single point.The point where it's tip sets That way,to think about a transformation taking every possible input vector to some output vector,we watch every point in space moving to some other point.In the case of transformations in two dimensions,to get a better feel for the whole shape of the transformation,I like to do this with all of the points on an infinite grid.I also sometimes like to keep a copy of the grid in the background,just to help keep track of where everything ends up,relative to where it starts.The effect for various transformations moving around all of the points in space is,and you've got to admit, beautiful.It gives the feeling of squishing,and morphing space itself.As you can imagine though arbitrary transformations can look pretty complicated.But luckily, linear algebra limits itself to a special type of transformation.Ones that are easier to understand, called linear transformations.Visually speaking, a transformation is linear if it has two properties.All lines must remain lines without getting curved,and the origin must remain fixed in place.For example, this right here would not be a linear transformation,since the lines get all curvy.And this one right here although it keeps the line straight,is not a linear transformation,because it moves the origin.This one here fixes the origin,and it might look like it keeps lines straight,but that's just because I'm only showing the horizontal,and vertical grid lines.When you see what it does to a diagonal line,it becomes clear that it's not at all linear,since it turns that line all curvy.In general, you should think of linear transformations askeeping grid lines parallel, and evenly spaced.Some linear transformations are simple to think about,like rotations about the origin.Others are a little trickier to describe with words.

### 10. Linear Transformations 2-imtEd8M6__s.en

So how do you think you could describe these transformations numerically,if you're say, programming some animations to make a video teaching the topic?What formula do you give the computer,so that if you give it the coordinates of a vector,it can give you the coordinates of where that vector lands?It turns out that you only need to record where the two bases vectors,i hat and j hat,each land, and everything else will follow from that.For example, consider the vector V with coordinates negative one, two.Meaning that it equals negative one times i hat,plus two times j hat.If we play some transformation and follow where all three of these vectors go,the property that grid lines remain parallel andevenly spaced has a really important consequence.The place where V lands, will be negative one times the vector where i hat landed,plus two times the vector where j hat landed.In other words, it started off as a certain linear combination of i hat and j hat,and it ends up as that same linear combination of where those two vectors landed.This means you can deduce where V must go based only on where i hat and j hat each land.This is why I like keeping a copy of the original grid in the background.For the transformations shown here,we can read off that i hat lands on the coordinates one, negative two.And j hat lands on the x axis over at the coordinates three, zero.This means that the vector represented by negative one i hat plus two times j hat,ends up at negative one times the vector one negative two,plus two times the vector three, zero.Adding that all together you can deduce that it has to land on the vector five, two.This is a good point to pause and ponder because it's pretty important.Now, given that I'm actually showing you the full transformation,you could have just looked to see that V has the coordinates five, two.But the cool part here is that this gives usa technique to deduce where any vectors land,so long as we have a record of where i hat and j hateach land without needing to watch the transformation itself.Write the vector with more general coordinates x and y,and it will land on x times the vector where i hat lands,one negative two, plus y times the vector where j hat lands, three, zero.Carrying out that sum,you see that it lands at one x plus three y,negative two x plus zero y. I give youany vector and you can tell me where that vector lands using this formula.What all of this is saying is thata two dimensional linear transformation is completely described by just four numbers,the two coordinates for where i hat landsand the two coordinates for where j hat lands.Isn't that cool? It's common to package these coordinates intoa two by two grid of numbers called a two by two matrix.Where you can interpret the columns asthe two special vectors where i hat and j hat each land.If you're given the two by two matrix describing a linear transformation andsome specific vector and you want to knowwhere that linear transformation takes that vector,you can take the coordinates of the vector,multiply them by the corresponding columns of the matrix,then add together what you get.This corresponds with the idea of adding the scaled versions of our new basis vectors.

### 11. Linear Transformations 3-g_yTyRwMzXU.en

Let's see what this looks like in the most general case,where your matrix has entries a, b, c,d. And remember, this matrix is just a way ofpackaging the information needed to describe a linear transformation.Always remember to interpret that first column, a, c,as the place where the first basis vector lands and that second column,b,d, as the place where the second basis vector lands.When we apply this transformation to some vector x,y, what do you get?Well, it will be x times a,c, plus, y times b,d. Putting this together,you get a vector ax + by, cx + dy.You could even define this asmatrix vector multiplication when you putthe matrix on the left of the vector like it's a function.Then you could make high schoolers memorize this withoutshowing them the crucial part that makes it feel intuitive.But isn't it more fun to think aboutthese columns as the transformed versions of your basis vectors,and to think about the results as the appropriate linear combination of those vectors?Let's practice describing a few linear transformations with matrices.For example, if we rotate all of space 90 degrees counter-clockwise,then i_hat lands on the coordinates (0,1,) and j_hat lands on the coordinates (-1, 0).So the matrix we end up with has columns (0, 1) (-1,0) to figure out what happens to any vector after a 90 degree rotation.You could just multiply its coordinates by this matrix.Here's a fun transformation with a special name called a shear.In it, i_hat remains fixed.So, the first column of The Matrix is 1,0 but j_hat moves over to the coordinates (1,1) which become the second column of The Matrix.And at the risk of being redundant here,figuring out how a shear transforms a given vectorcomes down to multiplying this matrix by that vector.Let's say we want to go the other way around.Starting with a matrix,say with columns 1,2 and 3, 1,and we want to deduce what its transformation looks like.Pause and take a moment to see if you can imagine it.One way to do this is to first move i_hat to 1,2, then move j_hat to 3, 1.Always moving the rest of space in such a way thatkeeps grid lines parallel and evenly spaced.If the vectors that i_hat and j_hat land on are linearly dependent,which if you recall from last video means that one is a scaled version of the other,it means that the linear transformation squishes all of2D space onto the line where those two vectors sit,also known as the one dimensional span of those two linearly dependent vectors.To sum up, linear transformations are a way to move aroundspace such that grid lines remain parallel and evenly spaced,and such that the origin remains fixed.Delightfully, these transformations can be described using only a handful of numbers,the coordinates of where each basis vector lands.Matrices give us a language to describe these transformations where the columns representthose coordinates and matrix vector multiplication isjust a way to compute what that transformation does to a given vector.The important takeaway here is that every time you seea matrix you can interpret it as a certain transformation of space.Once you really digest this idea,you're in a great position to understand linear algebra deeply.

### img

## Part 05-Module 01-Lesson 01_Jupyter Notebooks

### 02. Jupyter-qiYDWFLyXvg.en

Now I'm going to introduce you toJupyter notebooks. Notebooks are an amazingtool for data analysis, where text, code,and images all sit in one document in yourbrowser.Here's an example notebook where Iexplored predicting body fat percentagewith various regression models. Up tophere you see what's called a text cell.Cells are these guys and they can contain textor code. If I double-click on the textcell, I can edit the text in here. It'swritten in markdown, a text format withsyntax that renders to HTML. So for instance,if I want to write a link, this is thesyntax for it, and if I render thetext, it's a link. This is a code cell.You can see here I'm importing somepackages like Numpy and Pandas. I can runthis cell and the code is executed thesame way as it is in the terminal or aPython script. This command here"%matplotlib inline" will render imagesgenerated with matplotlib in thenotebook instead of a separate window.If the cell returns some output, you see here.For instance, data. head() returns an HTMLtable displaying some of the data. Rightbelow here you see the data visualizedwith a grid of scatter plots andhistograms. Notebooks even render math ina text cells.This is just an example of what you cando with notebooks. You have your code,documentation, visualizations, math, all inone place.Next, I'll show you how to use notebooksin your workflow. It should take about anhour to get through the lesson, so seeyou in the classroom!

### img

### media

## Part 05-Module 01-Lesson 02_NumPy

### 03. NumPy 0 V1-vyjMs8KFHlE.en

NumPy is short for Numerical Python and isa library designed for efficient Scientific Computation.It's built on top of the programming language C,which works at a lower level on our computer.To understand what this means for the speed of our code,see the link in the instructor notes.At the core of NumPy,is its N-dimensional array object.This is just a multi-dimensional array thatholds a group of elements that all have the same data type.In other words, it's like a grid that can take onmany shapes and enforces every element in that grid to have the same type,whether that's string, float,boolean, or something else.Making arrays only able to hold one data type at a timehelps NumPy make very quick computations with vector operations.These arrays, along with many useful functions in NumPy,can significantly optimize and simplify operations on data.Here's a simple example that demonstrates this.First, let's import NumPy with the standard alias for this library, NP.This generates an array of 100 million floats between zero and one.Let's compare the time it takes playing Pythonversus NumPy to calculate the mean of this array.Instead of Python, we do this by getting the sum ofX and dividing that by the length of X, pretty straightforward.Using the time package,we can check how long this line of code takes to run.That took about 9.3 seconds.Makes sense that it took a while with 100 million values.Now, let's see how long it takes NumPy.That was ridiculously faster.NumPy took only 0.092 seconds,while plain Python took 9.31.As you saw, NumPy can make a difference of orders of magnitude in computation time.Imagine how much this speeds up the process formore complex situations that require many more calculations.For example, let's say we're engineering a whole new feature or column in a dataset,which you compute by multiplying the values and two columnstogether and dividing that by the values in another for each row.Unlike NumPy, plain Python would requirea massively long loop through all ofthe rows to compute the new value for each individual row.In many machine learning problems,you'll often find yourself using NumPy arrays in many situations.For instance, you might use a NumPy array to holdthe pixel values of an image that will be fed into a model for image classification.Later, we'll also learn about a popular data science package,Pandas, which is very useful for manipulating datasets.It's actually built on top of NumPy,which is why its computations are so fast.Now that you've learned a bit about the power of NumPy, let's get started.

### 04. NumPy 1 V1-EOHW29kDg7w.en

Generally, there are two ways to create Numpy arrays.First, using Numpy's array function to createthem from other array-like objects such as regular Python lists.And second, using a variety ofbuilt-in Numpy functions that quickly generate specific types of arrays.In this section, we will start with the first way.Let's import Numpy and create our first array.Here's a one-dimensional array that contains integers.Note that for clarity,the examples throughout these lessons will use small, simple arrays.We'll start by creating one-dimensional or 1D Numpy arrays.Let's print the array we just created,as well as, it's type.You can see that the type is Numpy's ndarray or n-dimensional array.Numpy arrays have useful attributes that provideus information about them in a very intuitive way.For example, this dtype attribute.Dtype returns the data type of the elements in that array.Notice, dtype is different from the datatype of the array itself.This d type let's us know that the elements of Xare stored in memory a signed 64-bit integers.An additional advantage of Numpy is that it handles more datatypes than Python.You can check out all the different datatypes supported by Numpy in it's documentation.Another useful attribute is shape.This returns a tuple of n positive integers that specifiesthe sizes of each dimension n being the number of dimensions in the array.X has one dimension.So, shape returns an integer indicating the length of the array, five.If we had a two-dimensional array,this shape attribute would return a tuple with two values,one for the number of rows and one for the number of columns.To see this, let's create a two-dimensional array from a nested Python list.Here is one that contains integers.And let's print an additional attribute size.Looking at the tuple returned by shape,we know that Y has two dimensions since there are two elements.One is for the size of the first dimension which is the number of rows,four, and the other is for the second dimension,which is the number of columns, three.The size attribute gives us the total number of elements in Y which is 12.Let's pause for a second to introduce some useful terminology.In general, we say that an array with n dimensions has a rank n. So,we refer to the 1D array we created earlier asa rank one-array and we refer to the 2D array we just created as a rank two-array.For our next example,let's create a rank one-array that contains strings,and let's print those same attributes.The type of the array object itself isn't any different,it's still just a Numpy array.However, the dtype of this array is different.Here elements are stored as unicode strings of five characters.Notice that when Numpy creates an array,it automatically assigns it's dtypebased on the type of the elements you used to create the array.But what happens when we try to create a Numpy arraywith a list that contains both integers and strings?We can see that even though the Python list had mixed datatypes,the array function created a Numpy array with elements of all the same datatype namely,unicode strings of 21 characters.Remember, unlike Python lists,Numpy arrays must contain elements of the same type.Up until now, we've only used elements that were integers or strings.Let's try another example with mixed datatypes using integers and floats.When we input a list with both integers and floats,Numpy assigns all elements,the float 64 dtype,this is called upcasting.Since all the elements of a Numpy array must be of the same type,Numpy upcasts the integers in the array tofloats in order to avoid losing precision in numerical computations.Numpy also allows you to specifya particular dtype you want to assign to the elements of an array,you can do this using the keyword dtype in the array function.Here, you can see Numpy created an array ofints even though we passed it a list of floats.Specifying the datatype of the elements ina Numpy array can be useful in cases where you don't want to accidentallychoose the wrong datatype or when you only needa certain amount of precision in your calculations and want to save memory.Once you create a Numpy array,you may want to save it to a file to be read later or to be used by another program.Numpy provides a way to save the arrays into files for later use.We can save X into the current directory like this.This saves the array into a file named my array dot npy.You can later load this file into a variable by using the load function like this.

### 05. NumPy 2 V1-KR3hHf9Zxxg.en

In the last video,we learnt how to create NumPy arrays by convertingexisting array like objects such as Python lists,using NumPy's array function.But one great time saving feature of NumPy isits ability to generate specific kinds of NumPy arrays from nothing,using just one line of code.Here, we will see a few of the most useful builtin functions for generating Numpy arrays.Let's start by creating a NumPy array of zeroes with a shape that we specify.We can do this by using NumPy's zeros function.This function takes as an argument the shape of the array you want to create.Passing in the tuple (3,4) gives us a 3 by 4 array of zeros.By default, this creates an array with the data type float 64.If you want to use a different data type,you can change this with the keyword dtype.Similarly, we can create a NumPy array of ones using this function,which also takes shape as an argument.In addition to ones and zeros,you can create an array filled with any constant value using the full function.This takes two arguments,a shape for the array and the constant you want to fill it with.The full function by default creates an arraywith a data type of the value you inputted as a constant.Here, since we used the Integer 5 as our constant,this generated an array with the dtype int64.Use the key word dtype to specify otherwise.A fundamental array in Linear Algebra is the identity matrix.A matrix is just another term used to describea two dimensional array with rows and columns.And, an identity matrix isjust a square shaped matrix that has ones along its main diagonal,and zeros everywhere else.NumPy's eye function can be used to create this.Since all identity matrices are square,this only takes a single integer as an argument.Using 5 gives us a 5 x 5 identity matrix.And its main diagonal which goes from the topleft to the bottom right, is filled with ones.We can also use Numpy's diag function to create a diagonal matrix.This function takes in as input a sequence ofvalues to use as the main diagonal of a square matrix,and fills in the rest with zeros.NumPy also has useful functions to generate arrays with specific numerical ranges.One useful one is arange,which creates a one dimensional array of evenly spaced values within a given interval.This takes three arguments: start, stop, and step.But we can still use this whether we want to specify one,two or three arguments.Let's see each case.When only one integer is specified,arange uses this as a stop argument,and generates an array of integers from zero to that integer minus one.The stop argument is exclusive,which is why we need to subtract one.For example, arange 10 gives us an array from zero to 10 minus one which is nine.When used with two arguments,arange uses the first as a start argument,and the second as a stop argument.The start is inclusive,and the stop is exclusive.Arange (4,10) gives us an array from four to nine.When you use the three arguments,arange generates an array from the first integer to the second minus one,evenly spaced by the third.This third argument, the step,is the distance between any two values in this array.When we specified only one or two arguments in arange earlier,step defaulted to one.Even though NumPy's arange function allows for a non integer steps such as 0.3,the output is usually inconsistent due to finite floating point precision.For this reason, when we want non integer steps,it's usually better to use a different NumPy function, linspace.This takes three arguments, start, stop,and n. This returns n evenly spaced numbers from start to stop,both start and stop being inclusive.Unlike arange, Linspace requires at least two arguments for start and stop.If n is not specified,it defaults to 50.Let's see some examples.Here's a rank one array that has 10 numbers evenly spaced from zero to 25.Again, note that both the start and stop points are inclusive.However, you can let the stop endpoint of the interval beexcluded just like it is in the arange function,if you set the keyword endpoint to false like this.As you can see, because we have excluded the endpoint 25,the spacing between the values had to change inorder to fit 10 evenly spaced numbers in the given interval.So far, we've only used the functions arange and linspace to create rank 1 arrays.However, we can use these functions to create rank 2 arrays ofany shape by combining them with NumPy's reshape function.This function converts any NumPy array into a specified shape.It's important to note that the new shape specified here,should be compatible with a number of elements in the array.For example, you can convert a rank one arraywith 20 elements into a 4 by 5 rank two array,or a 10 by 2 array,since both of these rank two arrays still have 20 elements.However, you can't reshape this to a 5 by 5 array,since this rank two array would have 25 elements,which is greater than the number of elements in the original NumPy array.One great feature about NumPy is that some functions can also be applied as methods.This allows us to apply different functions in sequence,in just one line of code.NumPy array methods are similar toits attributes in that they are both applied using dot notation.Let's see how we can accomplish the same result as this example using one line of code.This gives us the same result.Notice that when using reshape as a method,we don't need to pas in the array as an argument.Similarly, we can also use reshape to create rank two arrays with the linspace function.Lastly, we are going to create NumPy arrays that contain random numbers.Often in machine learning,you need to create random matrices.For example, when initializing the weights of a neural network,NumPy offers a variety ofrandom functions to help us create random NumPy arrays of any shape.Let's start by using NumPy's random function to create an array of a given shape,with random floats between zero and one,where zero is inclusive,and one is exclusive.The following functions including this random function,are contained in NumPy's random module.So we type np.random to access this module,and then.random to access the function in the module.NumPy also let's us create NumPy arraysthat contain random integers within a particular interval.We can use the function randint to do this.This takes three arguments, the lower bound,inclusive, and the upper bound,exclusive, and the shape.In some cases, you may need to create NumPy arrayswith random numbers that satisfy certain statistical properties.For example, you may want the random numbers in the array to have an average of zero.NumPy allows you to create random arrays withnumbers drawn from various probability distributions.The function np.random.normal for example,creates an array with the given shape thatcontains random numbers picked from a normal distribution,with a given mean and standard deviation.This creates a 1000 by 1000 array ofrandom floats drawn from a normal distribution with the mean of zero,and a standard deviation of 0.1.As we can see, the average of the random numbers in the array is very close to zeo,and the standard deviation is also very close to 0.1.Both the maximum and minimum values next are symmetric about zero, the average.And we have about the same number of positive and negative integers.

### 07. NumPy 3 V1-Rt4aydeo9F8.en

Now that you know how to create a variety of NumPy arrays,let's see how NumPy allows us to effectively manipulate the data within them.NumPy arrays are mutable,meaning the elements in them can be changed after the array has been created.NumPy arrays can also be sliced in many different ways.This allows us to achieve any subset of the array we want.You'll often use slicing to separate data.For example, when dividing a dataset into training,cross validation, and testing sets.We will start by looking how the elements ofa NumPy array can be accessed or modified by indexing.Let's create a rank one array that contains integers from one to five.Elements can be accessed by specifyingtheir positions using indices inside square brackets.Positive indices are used to specify positions from the beginning of the array.Notice that to access the first element in the array,we have to use index zero not one.We can also use negative indices to specify positions from the end of the array.Notice that the same element can be accessed using both negative and positive integers.As mentioned earlier, positive indices areused to access elements from the beginning of the array,while negative indices are used to access elements from the end of the array.Now, let's see how we can modify the elements in an array.We can do this by accessing the element we want to change,and then reassigning it using the equal sign.Let's modify the array X we just created.We can change the fourth element in X from four to 20.We can also access and modify specific elements of rank two in NumPy arrays.The only difference is that we need to providetwo indices separated by a comma inside the square brackets.Here's a three by three array containing the integers from one to nine.Let's access some elements in that array.Remember that zero, comma,zero refers to the element in the first row and the first column, which is one.Elements in rank two arrays can also be modified in the same way.We can change the zero, comma,zero element, in X from one to 20 like this.Now that we know how to access and modify elements in an array,let's take a look at how we can add and delete elements.We can delete elements using NumPy's delete function.This function takes in an array,list of indices to delete,and an axis to delete from.For rank one arrays,the axis keyword is not required.For rank two arrays,axis zero is used to select rows and axis one is used to select columns.Let's see some examples.Here is a rank one array,we can delete the first and last element of X like this.And here is a rank 2 array,and delete the first and last column of Y like this.We can add values to NumPy arrays using the append function.This function takes in an array,a list of elements to append,and the axis to append it on.Let's see some examples.We can append an element six to this rank one array like this.And we can append multiple elements,say seven and eight in a list like this.In a rank two array,we can append a new row containing 10,11, and 12 like this.And we can append a new column containing 10,11, 12 like this.Notice that when appending rows or columns to rank two NumPy arrays,the rows and columns must have the correct shape to match the shape of the array.Now, let's see how we can insert values into NumPy arrays.We can use the insert function for this,which takes in an array,index, elements, and axis.This inserts the given list of elements to the array right beforethe given index along the specified axis. Here's a rank one array.We can insert the integers three and four between these elements two and five like this.In this rank two array,we can insert a row between this first and last row like this.And insert a column full of fives between the first and second column like this.NumPy also allows us to stack NumPy arrays on top of each other or side by side.The stacking is done using either NumPy's Vstack forvertical stacking or Hstack function for horizontal stacking.It's important to note that in order to stack arrays,the shape of the arrays must match.Consider this rank one and rank two array X and Y.We can stack X on top of Y with Vstack like this.And we can stack X on the right of Y with Hstack like this.We need to reshape X to match the shape of Y before stacking it horizontally.

### 08. NumPy 4 V1-jeU7lLgyMms.en

As we mentioned earlier,in addition to accessing individual elements one at a time,we can access subsets of NumPy arrays with slicing.Slicing is performed by combining indices with a colon inside the brackets.In general, you will come across three ways of slicing.First, slicing from a starting index to an ending index.Second, slicing from a starting index to the end of the array,by leaving this blank, or slicing from the beginning ofthe array to an ending index, by leaving this part blank.In the first and third methods,the ending index is always excluded.While here, the starting index is always included.NumPy arrays can be multidimensional.So, when slicing, you usually have to specify a slice for each dimension of the array.Let's see some examples of slicing,with a rank two array.Here is a four by five array that contains integers from one to 20.Let's say, you wanted to grab this subset from the array, these nine values.Here is one way we can do this.The part before the comma,specifies what indices you want to grab from the rows,and the part after the comma specifies what indices you want to grab from the columns.Remember, the starting index is included and the ending index is excluded.And indexing always starts with zero.So, the first row is zero and the second column is one.So, this grabs the rows one, two, and three.And this part grabs the columns two, three, and four.Here is another way we can accomplish that same thing.Remember, if we don't include an ending index after the colon,it will just go all the way to the last index.Let's try to grab another subset that uses these same columns,but instead of the last three rows,grabs the first three rows.This grabs this subset.Remember, not including a starting index beforethe colon just makes it go all the way to the beginning of the array.So, this grabs the rows zero, one, and two.For our next example,let's try to select all the elements in the third row.If we leave both sides of the colon blank,this will just grab all the rows in the array.So, X, colon, comma,two will get all the rows in column two.So, zero, one, two it would grab this column.If we want to select all the elements in the third column butreturn it in a rank two array, we can do this.Notice that when we first selected all the elements in the third column this way,the slice returned a rank one array instead of a rank two array.But slicing X like this,in a slightly different way,can get us a rank two NumPy array.It's important to note that when we perform slices onNumPy arrays and save them into new variables,like we did here, the data is not actually copied into the new variable.This is one feature that often causes confusion for beginners.So, we'll look into this in more detail.In these examples, the slice of the original array X,is not copied into the variable Z.Rather, X and Z are now just two different names for the same array.We say that slicing only creates a view of the original array.This means, if you make any changes in Z,you'll also be changing the elements in X.Let's see an example with that four by five array.We'll select the elements over here again,and assign that to the variable Z.Now, we'll change the last element in Z to 555.Now, if we print X,we can see that it has also been affected by this change.If we want to create a new NumPy array that contains a copy of the values in the slice,we need to use NumPy's Copy function.This function can also be used as a method,as we saw before with the reshape function.Let's repeat this example with the copy command.Here's X again, we'll create a copy of the same slice using the copy function.We can also use Copy as a method like this.If we change the last element in Z to 555,we can see that X has not been changed.By using the Copy command,we are creating a new NumPy array,that is completely independent of the original.It's often useful to use an array as indices to make slices,select, or change elements in another NumPy array.Let's see some examples.Let's create a rank one array that will serve as indices to select elements from X.We'll use this indices NumPy array to select the second and fourth row of X.Now, we'll use the same array to select the second and fourth column of X.NumPy also offers built-in functions to select specific elements within NumPy arrays.For example, NumPy's diag function can extract elements along the diagonal of an array.Remember, X was this.We can also print the elements above the main diagonal of X by setting a parameter,K equal to one.This grabs the elements one, seven,13, and 19.If K is a negative number,this will grab the values below the main diagonal, five,11, and 17.By default, K is zero,which is why it gets the main diagonal.It's also often useful to extract only the unique elements in a NumPy array.We can find the unique elements in an array by using NumPy's Unique function.Here's a three by three array with repeated values.We can see the unique values like this.

### 09. NumPy 5 V1-vGjI-WTnEbY.en

Up to now we've seen how to make slices andselect elements of an NumPy array using indices.This is useful when we know the exact indices of the elements we want to select.However, there are many situations inwhich we don't know the indices of the elements we want.For example; Suppose we have a 10,000 by 10,000 array of random integersranging from one to 15,000 and we only want to select integers that are less than 20.Boolean indexing can help us in these cases by helping us selectelements using logical arguments instead of explicit indices.Let's see some examples.Consider this five by five array ranging from zero to 24.We can use boolean indexing to select elements greater than 10.Like this. Instead of indices,we are using a boolean expression.Let's also get the elements that are less than or equal to seven.And now both greater than seven and less than 17.We can use boolean indexing to assign the elements that arebetween 10 and 17 to the value of negative one.In addition to Boolean indexing,NumPy also allows for set operations.This is useful when comparing two NumPy arrays.For example, to find common elements.Consider these two rank one arrays.We can create arrays for the intersection, difference, and union.Like this. We can also sort NumPy arrays.Let's use NumPy sort function to sort rank one and rank two arrays in different ways.Like with other functions we saw before,the sort function can also be used as a method.However, there's a big difference on how the data is stored in memory in this case.When sort is used as a function,it sorts the NumPy array out of place,meaning they don't change the original array.However, when you use sort as a method,the array is sorted in place,meaning the original array is changed.Let's create an unsorted rank one array.We can sort x using sort as a function.This will sort x out of place and leave the original array as is.As you can see Numpy.sort did sort the x array,but x itself did not change.Notice that this sorts the array and leaves repeating values.If you want to sort only the unique elements in x,you can combine it with a unique function like this.Now, let's see how we can sort arrays in place by using sort as a method. Here's x again.If we sort x like this,we will see that this affects the original x and sorts it.When sorting rank two arrays,we need to tell the sort function whether we are sorting by rows or by columns.This is done by using the keyword axis.Here is an unsorted rank two array.We can sort x by rows like this,which you can see here or we can sort X by columns like this, which you can see up here.

### 11. NumPy 6 V1-wtLRuGK0kW4.en

Let's see how NumPy does arithmetic operations on arrays.NumPy allows element-wise operations,as well as matrix operations.In this video, we will only be looking at element-wise operations.Consider these two rank one arrays.We can perform basic element-wise operations using arithmetic symbols or functions.Both of these forms will do the same operation.The only difference is that if you use this function approach,the functions usually have options that you can tweak using keywords and methods.Let's also try element-wise subtraction, multiplication, and division.Again, these also have the function approach.In order to complete these operations,NumPy sometimes uses something called broadcasting.Broadcasting is a term used to describe how NumPy handleselement-wise arithmetic operations with arrays of different shapes.An important thing to note is that,since we are doing element-wise operations,the arrays being operated on must have the same shape or be broadcastable.We'll talk more about this in a minute.Let's perform the same element-wise arithmetic operations on rank two arrays.Again, remember that in order to do these operations,the arrays being operated on must have the same shape or be broadcastable.Here are two two-by-two matrices.Let's do the same arithmetic operations using the symbol notation.We can also apply mathematical functions suchas square root to all the elements of an array at once.Here's that rank one array we were using before.We can get the square root of each element like this,and the exponential of each like this.And also, each element to the power of two.Another great feature of NumPy is its statistical functions.Statistical functions such as mean,provide us with statistical information about the elements in an array.Let's see some examples.Here is that rank two array again.We can get the average of the matrix like this,as well as the averages of individual rows and columns like this.We can do the same for statistics like the sum,and others like the standard deviation,median, maximum, and minimum.Finally, let's see how NumPy can add single numbers toall the elements of a NumPy array without the use of complicated loops.Here's our rank two array again.We can add three to each element like this,and subtract three from each like this.Multiply each by three,and divide each by three.In the examples above,NumPy is working behind the scenes to broadcastthree along the X array so that they have the same shape.This allows us to add three to each element of X in just one line of code.Subject to certain constraints,NumPy can do the same for two NumPy arrays of different shapes.Consider this three-by-three array Y,and this one-by-three array X.If we do Y plus X, this adds zero,to the first column in Y, one,to the second column in Y,and two, to the third column in Y.As before, NumPy is able to add a one-by-three array toa three-by-three array by broadcasting the smaller array along the big array,so that they have compatible shapes.In general, NumPy can do this provided that the smaller array,such as the one-by-three array,can be expanded to fit the shape of the larger array,so that the resulting broadcast is unambiguous.We can do the same with a three-by-one array.This adds zero, the first row of Y,one, to the second row of Y,and two, to the third row of Y. Checkout the NumPydocumentation for more information on broadcasting and its rules.

### img

## Part 05-Module 01-Lesson 03_Pandas

### 04. Pandas 1 V1-iXnYN8cnhzs.en

Pandas is a powerful tool for data analysis and manipulation.If you remember from the last lesson,this package is built on top of NumPy which makes it very fast and efficient.In this lesson, we will go over the two main data structures in Pandas.The Pandas series and the Panda's dataframe.Let's start off by learning about the Pandas series object, and how to create one.When importing Pandas, use a standard alias, pd.Let's create a series containing grocery items.To access the series object,we just type pd.Series with a capital S.A panda series is a one-dimensional array-like object that can hold many data types,such as numbers and strings.This is different from a NumPy array which can only hold one data type.Another big difference between a Pandas series anda NumPy array is that you can assign an index label to each element in the Pandas series.Here, we pass two arguments,the data and the indices.For our grocery series and we will use food names asindex labels and the quantities we need to buy as our data.We can see that a Pandas series is displayed with the indices in the first column,and the data in the second column.Notice that the data is not indexed zero to three,but rather with the names of the foods that we put in,eggs, apples, milk, and bread.Also notice that the data in our Pandas series has both integers and strings,just like NumPy arrays.Pandas series have attributes that allow us to getinformation from them in an easy way. Let's see some of them.Shape gives us the sizes of each dimension of the data,ndim gives us the number of dimensions of the data,and size gives us the total number of values in the array.We can also print the index labels and the data of the Pandas series separately.This is useful if you don't happen to know what the index labels of a series are.This gives us the index labels of the series object,and this gives us the data into series object.If you're dealing with a very large Pandas series,and you're unsure whether an index label exists,you can always check using the In command.This let's us know that bananas is not one of the index labels in the grocery series,and this tells us that bread is.

### 05. Pandas 2 V1-B7MuFIwboKU.en

In the last video,we created this Panda series of a grocery list.Now, how do we access or modify its elements?One great advantage of the series object,is that it allows us to access data in multiple ways.One way is accessing elements with their index labels.This accesses the quantity of eggs using the eggs label in square brackets.We can get multiple elements by providing a list of index labels.Another way to access elements is numeric indices,very similar to how we access elements in NumPy arrays.Here, we get the quantity of the first item,eggs, using zero as our index.Now, let's get the last element with the index negative one.And again, we can grab multiple items using a list of numerical indices.In order to remove any ambiguity from rather we're referring to an index label,or a numerical index,Panda series have two attributes loc,and iloc, to explicitly state what we mean.The attribute loc stands for a location,and it's used to explicitly state that we're using a labelled index.Similarly the attribute iloc,stands for integer location,and is used to explicitly state that we are using a numerical index.Panda series are also mutable like NumPy arrays,which means we can change the elements of a series after it's been created.Let's change the number of eggs we need to buy from our grocery list.Let's see our grocery list again.We can change the number of eggs from 30 to two by reassigning the element like this.Now, we can see that the data for eggs has been modified.We can also delete items from a Panda series using the drop method.This method removes an element with the given label from the Panda series.As you can see, apples is no longer included in the series returned by this method.However, this drops elements from the series out of place.Meaning, up here, this just returned the modified series,and didn't actually change the original one as you can see here.We can make this happen inplace,and change the original series by setting the parameter inplace to true.Now, notice the drop method modifiedthe actual series instead of returning another series with the modification.

### 06. Pandas 3 V1-yhMT0X6YPFA.en

Just like we did with NumPy arrays,we can perform element-wise arithmetic operations on Pandas series.In this video, we will look atarithmetic operations between Pandas series and single numbers.Let's create a new series that holds a grocery list of fruits.The first argument we pass in is the data,and the second argument is the index labels.We can modify the data in fruits by performing basic arithmetic operations.We can add two to each element in fruits,subtract two, multiply by two, and divide by two.We can also apply mathematical functions fromNumPy such as square root to all the elements of a series.Let's import NumPy and take a look at our fruit series again.Using NumPy, we can get the square root of each element like this.And the exponential of each element,and each element to the power of two.Pandas also allows us to apply arithmetic operations on selected items in a series.Here's the fruit series again.We can add two to just the banana's item, like this.And let's subtract two from apples using its numerical index.We can double the apples and oranges like this,and divide apples and oranges by two like this.You can also apply arithmetic operations on a Pandas series ofmixed data types provided thatthe arithmetic operation is defined for all data types in the series.To demonstrate this, let's go back to our grocery's list from the previous video,and let's multiply this series by two,since the multiplication operation is defined for both strings and numbers.This code doesn't return an error.Multiplying a string by two simply repeats it.If you were to apply an operation that was valid for numbers but not strings,for instance division, you would get an error.So, when you have mixed data types in your Pandas series,make sure the arithmetic operations youuse are defined for all the data types in your series.

### 08. Pandas 4 V1-eMHUn9v9dds.en

The second main data structure in Pandas is a DataFrame,which is a two-dimensional object with labeled rows andcolumns and can also hold multiple data types.If you're familiar with Excel,you can think of a DataFrame as a really powerful spreadsheet.We can create Pandas DataFrames manually or by loading data from a file.We will start by creating a DataFrame manually from a dictionary,containing several pandas series.Let's create that dictionary and then pass it into Pandas DataFrame function.Here's one that contains the shopping carts of two people,Alice and Bob on an online store.Each series contains the price of the items and is labeled with the item names.And let's confirm the items is of the datatype dictionary.Now that we have a dictionary,we are ready to create a DataFrame by passing it to the DataFrame function.Remember, when using the DataFrame function,capitalize the D and F in DataFrame.There are several things to notice here.First, we see that DataFrames are displayed in a tabular form,much like a spreadsheet with the labels of the rows and columns in bold.Also notice that the row labels of the DataFrame are built fromthe union of the index labels we provided in the series,and the column labels of the DataFrame are taken from the keys of the dictionary.The columns are arranged alphabetically and not in the order given by the dictionary.Later, we will see that this is not the casewhen we load data into a DataFrame from a file.Lastly, notice the NaN values that appeared in a DataFrame.NaN stands for not a number,and is Pandas way of indicating that it doesn't havea value for this particular row and index.For example, if we look at the column Alice,we see that it has NaN in the watch index.This is because the dictionary over here didn't have an item for Alice called watch.Whenever a DataFrame is created,if a particular column doesn't have values for a particular index,Pandas will put a NaN there.If we were to feed this data into a machine-learning algorithm,we would have to remove these NaN values first.In a later video,we will learn how to deal with Nan values and clean our data.For now, we will leave these values in our dataframe.In this example, we created a Pandas DataFrame froma dictionary of pandas series that had clearly defined index labels.If we don't provide index labels however,Pandas will use numerical row indices when it creates the DataFrame.Let's create the same dictionary without the index labels.We can see that pandas indexes the rows of the DataFrame starting from zero,just like NumPy indexes its arrays.Like we did with the pandas series,we can also extract information from a DataFrame using attributes.Let's print some information on our shopping carts DataFrame from earlier.We can get the index labels,column labels and data from our dataframe with these attributes,and we can use the same attributes to get information about its shape.This dataframe has two dimensions with five rows and two columns,making a total size of 10.When creating the shopping carts DataFrame,we pass the whole items dictionary to the DataFrame function.However, there might be cases where you're only interested in a subset of the data.Pandas let's us select which data we want to put inour DataFrame with the keywords, column and index.Let's see some examples.Here's a DataFrame that only loads Bob's shopping cart,and here's one that only has selected items for both Alice and Bob.And this is one that only has selected items from Alice's shopping cart.You can also manually create DataFrames from a dictionary of lists or arrays.The procedure is the same as before,we start by creating the dictionary and then pass it into the DataFrame function.In this case however,all the lists or arrays in a dictionary must be of the same length.Here's a dictionary of integers and floats.Notice that since the data dictionary we created doesn't have index labels,Pandas automatically uses numerical row indices when it creates the DataFrame.We can however add these labels by using the index keyword in the DataFrame function.The last method we'll look at for manually creatingPandas DataFrames is using a list of Python dictionaries.Here's an example, again we don't have index labels.So Panda put numerical row indices here,and let's assume we're going to use this DataFrame.to hold the number of items a particular store has in stock.We'll rename the index labels to store 1 store 2.

### 09. Pandas 5 V1-lClsJnZn_7w.en

We can access elements in a DataFrame in different ways.In general, we can access rows, columns,or individual elements by using the row and column labels.Let's see some examples.Here's a DataFrame we created in the last video.We can access the bikes column using the column label,like this, and use a list of the column labels to access multiple columns.We can access a row using the row index label,and the value at a specific row and column like this.It's important to know that when accessing individual elements in a DataFrame,like in this last example,the column label always comes first,and then the row label.If you provide the row label first, you'll get an error.We can also modify our DataFrames by adding rows or columns.Suppose we decided to add a shirts column to our DataFrame, to do this,we can define our shirts column containingthe quantity for each of the two store rows like this.This added a new column to the end of our DataFrame.We can also add new columns usingarithmetic operations on other columns of our DataFrame.For example, we can create a new column calledsuits by computing the sum of the shirts and pants column.Suppose now, that you openeda new store and you want to add that as a row on your DataFrame.To add rows to our DataFrame,we first have to create a new DataFrame with those rows,and then append it to the original DataFrame.Let's see how this works.Here is a dictionary of the items inour new store that we will use to create a DataFrame.We can now add this row to our store items DataFrame using the append method.After appending to the DataFrame,the columns have been placed in alphabetical order.We can also add new columns to our DataFrame byselecting data that already exists in our DataFrame.For example, let's say you want to stock stores two and three with new watches.So you decide to add a new column called new watches to your DataFrame,and say you want the quantity ofthese new watches to be the same as the watches already in stock.Since we only want the stock stores two and three,we can index them like this.It is also possible to insert new columns into the DataFrame anywhere we want.The insert method allows us to specify the location,label, and data of the column we want to add.Let's use this to add a new column called shoes right before the suits column.The first argument is loc or location, we put five.So, 0, 1, 2, 3, 4, 5.So, we actually inserted the shoes column before the watches column.The second argument is the label,and the last is the data.In addition to adding rows and columns,we can also delete them.We can use the pop and drop methods to do this.The pop method allows us to delete columns whilethe drop method can be used to delete both rows and columns by using the axis keyword.Let's see some examples.We can delete the new watches column using pop which is used to remove columns.Now, we will move the watches and shoes columns with drop,which can be used to remove columns if we set axis to one.We'll use that same method to removethe rows store two and store one by setting axis to zero.Sometimes, we might need to change the row and column labels.Let's change the bikes column label to hats using the rename method.This takes in a dictionary with original label as keys and the new labels as values.We can see the bikes has been renamed to hats.Now, let's change the row label using the rename method.We can change the index label of store three to last store like this.Notice here, we set the index parameter to the dictionary.Whereas in the previous cell,we set columns to the dictionary.We can also set the index to be one of the existing columns in the DataFrame, like this.Now, we're using the values in the pants column as our row index labels.

### 10. Pandas 6 V1-GS1kj04XQcM.en

Before we can begin analyzing data or using it to train our learning algorithms,we need to clean it.This means we need a way to detect and correct errors in our data.While any given dataset can have many types ofbad data such as outliers or incorrect values,the type of bad data we encounter almost always is missing values.As we saw earlier,Pandas assigns the value NaN,or not a number to missing data.In this section, we will learn how to detect and deal with these NaN values.In this DataFrame, we have three NaN values.One in store one,and two in store three.However, in cases where we loadvery large datasets into a DataFrame possibly with millions of items,the number of NaN values isn't easily visualized like this.For these cases, we can use a combination ofmethods to count the number of NaN values in our data.This combines to is null and sum methods to countthe number of NaN values in our DataFrame. Let's break this down.The is null method returns a DataFrame witha Boolean for each value in the store items DataFrame.True, when the value is NaN.In Pandas, logical true values have the numerical valueone and logical false values have the numerical value zero.Therefore, we can count the number of NaN values bycounting the number of logical true values from this DataFrame.Using sum ones gives us a number of NaNs in each column.We use sum again to get the total number of NaNs for the entire DataFrame.Instead of counting the number of NaN values,we can also do the opposite and count the number of non-NaN values with the count method.Now that we know how to find out if our dataset has any NaN values,the next step is to decide what to do with them.In general, we have two options.We can either remove or replace missing values.To remove them, we can use the drop na method.We can set the axis parameter to zero to eliminate any rows with NaN values.We can also set axis to one to eliminate any columns with NaN values.The dropna method drops these rows or columns out of place.Meaning, the original DataFrame is not modified.You can remove them in-place by setting the keyword in-place to true like this.Now, instead of eliminating NaN values,let's replace them with actual values.As an example, we could choose to replace all NaNs with the value zero.The fillna method can be used for this.We can also replace NaNs with values fromthe previous row or column with something called forward filling.This replaces each NaN with the value from the previous value along the given axis.Here's the original DataFrame again.Notice that the two NaN values in store threehave been replaced with the previous values in their column.However, notice that the NaN value in store one did not get replaced.That's because there were no previous values inthis column since NaN is the first value in this column.However, if we do forward fill using the previous row values this won't happen.We see that in this case,all the NaN values have been replaced with the previous value in that row.Similarly, you can choose to replacethe NaNs with the values that go after them in the DataFrame.This is known as backward filling.This replaces each NaN with the next value in that column.The NaN value in store one has been replaced by the next value in it's column.But the two NaN values in store three didnot since they are the last values in their columns.The fillna method fills the NaN values out of place.So, set the parameter in place to true if you want to modify the original DataFrame.We can also choose to replace NaNs by using different interpolation methods.For example, this uses linear interpolation toreplace NaN values using the values along the column axis.The two NaN values in store three have been replaced with linear interpolated values.However, NaN value in store one did not get replaced sincethere is no data before it to allow the interpolation function to calculate a value.We can replace NaN values with linear interpolation using row values like this.Like the other methods we saw,the interpolate method replaces NaN values out of place.

### 12. Pandas 7 V1-ruTYp-twXO0.en

When working with data you'll most likely use databases from many sources.Pandas allows us to load databases of different formats into DataFrames.One of the most popular formats used to store data is CSV or comma separated values.We can load CSV files into DataFrames using the read CSV function.Let's load Google stock data into a DataFrame.This file was taken from Yahoo finances and contains Google stock data from August 19th,2004 to October 13th, 2017.The DataFrame consists of 3,313 rows and seven columns.Let's look at the stock data.We can see it's a pretty large dataset and that Pandasautomatically assign numerical row indices to the DataFrame.Pandas also use the names that appeared in the CSV file to assign the column labels.When dealing with large data sets like this one,it's often useful to just take a look atthe first few rows of data instead of the whole dataset.We can take a look at the first five rows using the head method.We can also take a look at the last five rows of data using the tail method.Both of these accept an integer as an argumentif you want to specify a different number of rows to return.For example, I can get the last eight rows like this,or the first two rows like this.Let's do a quick check to see whether we have any none values in this data set.To do so, we will use the "Is no" method followed bythe "Any" method to check whether any of the columns contain none values.This shows us that we have no missing data.When dealing with large data sets,it's often useful to get statistical information from them.Pandas provides a described method which returnsdescriptive statistics on each column of the DataFrame.If desired, we can apply the describe method on a single column like this.Similarly, you can also look at one statistic byusing one of the many statistical functions that Pandas provides.Here are some examples.This gives us the maximum value in each column,and this gives us the mean of each column,and this gives us the minimum value in a specific column.Another important statistical measure is data correlation.We can use the core method to get the correlation between different columns.Lastly, let's look at the group by method,which allows us to group data to get different types of information.For the following examples,we are going to load data about a fake company.This data contains information for the years 1990 through 1992.And for each year, we see the name of the employees,the department they worked for,their age, and their annual salary.Now, let's use the group by method to get information.Let's calculate how much money the company spent on salaries each year.To do this, we will group the data by year and then addup the salaries of all of the employees with the sum method.The company spent a total of $150,000 in 1990,$162,000 in 1991, and $174,000 in 1992.If we want to know the average salary for each year,we can repeat the last step replacing the sum method with the mean method.Now, let's see how much in total each employee got paid during those five years.In this case, we will group the data by name and then add up the salaries for each year.Now let's see what the salary distribution per department was per year.In this case, we will group the data by year andby department and then add up the salaries for each group.We can see that in 1990,the Admin department paid $55,000 in salaries,HR $50,000 and R and D, $48,000.And in 1992 the admin department paid $122,000 in salaries, and so on.

### img

## Part 06-Module 01-Lesson 01_Descriptive Statistics - Part I

### 01. Instructors Introduction-lIvm8urf4GE.en

Statistics is at the core of analyzing data.For the stats portion of this class,you'll be learning from Sebastian Thrun and Josh Bernhard.Sebastian is a statistician and Stanford faculty member,as well as founder of Udacity and Google X.He'll be showcasing a number of examples for each of the statistical topics covered.Josh who is also a statistician teacher,has taught statistics at the University of Colorado andpreviously was working as a machine learning engineer for KPMG.He'll be working alongside Sebastian to assureyou can implement the statistical applications in Python.With that, let's get started.

### 03. What Is Coming Up-oDJsnQcCPr4.en

A quick overview leading up to your first project.We will start with an overview of data types andthe most common statistics used when analyzing data.We'll discuss measures of center and spread.The most common shapes that data takes on and how to handle outliers.You will take this farther by using spreadsheets to handle these calculations for you.You'll learn how to build visuals to better convey your message as well as how to usethese features of spreadsheets to take your data game to a whole new level.All of this is aimed to help you succeed onthe very first project and for analyzing your own data.

### 04. What is Data-ldTDAjrVsA8.en

The word "data" is defined as distinct pieces of information.You may think of data as simply numbers on a spreadsheet,but data can come in many forms.From text to video to spreadsheets and databases to images to audio,and I'm sure I'm forgetting many other forms.Utilizing data is the new way of the world.Data is used to understand and improve nearly every facet of our lives.From early disease detection to social networksthat allow us to connect and communicate with people around the world.No matter what field you're in,from insurance and banking, to medicine,to education, to agriculture,,to automotive, to manufacturing, and so on.You can utilize data to make better decisions and accomplish your goals.We will be getting you started on the right foot to using your data in this course.

### 05. Data Types-gT6EYlsLZkE.en

In this video, we'll be taking a look atthe different data types that exists in the world around us.When sitting at coffee shops,I enjoy watching the dogs pass.I often wonder, how many crossed my path?I wonder if more pass on weekdays or weekends.Maybe the number differs from Mondays to Tuesdays.I also pay attention to the breeds of the dogs.I wonder if more collies stopped by on Monday than on Wednesday.I wonder what's the most common breed.Is that breed the most common at all coffee shops?If I walked across the street to my favorite breed,would the most common breed change?This introduces two main data types: quantitative data,like the number of dogs,and categorical data, like the breed.Quantitative data takes onnumeric values that allow us to perform mathematical operations.In the previous example,we saw this with the number of dogs.If I see five dogs on Monday,and six dogs on Tuesday,I've seen a total of 11 dogs so far this week.Alternatively, categorical data frequently are used to label a group or set of items.We saw this with the breeds of the dogs.

### 07. Categorical Ordinal  Nominal Data-k5bLaPGY2Vw.en

We can divide categorical data types further intocategorical ordinal and categorical nominal.First, let's look at categorical ordinal data.Remember those dogs at the coffee shop?Let's say I give each a rating of how nice it is to me.Sometimes I shake hands with the dog and we become best friends.Other times, it pees on my shoe.I rate these interactions from very positive to very negative.These ranked categories are known as categorical ordinal data.Notice, this is different than the breeds,which are known as categorical nominal data,and do not have a rank order.

### 08. Continuous vs. Discrete Data-BzgZebZD9kk.en

We can also divide quantitative data types further.I assume most of my positive interactions occur with older dogs,as they've had more time to train.The age of a dog is a continuous quantitative data type,while the number of dogs I interact with is a discrete quantitative data type.In the world, we kind of make all data discrete,so it can be difficult to see the difference between discrete and continuous.Continuous data can take on any numeric value including decimal values,and sometimes even negative numbers.The age of a dog in this situation is an example of continuous data,as we could split this variable into smaller and smaller pieces,and still something exists.For example, we could talk about age in terms of years,or months, or days,or hours, or minutes,or seconds, and still there are units that are smaller.This is true of continuous data.However, discrete data like the number of dogs,only takes on countable values.

### 09. Data Types Summary-T-KrQoAJUpI.en

To summarize, we have two main data types,each with two subgroups.Quantitative data can be divided into continuous and discrete.Categorical can be split into nominal and ordinal.Identifying data types is important,as it allows us to understand the types ofanalyses that we can perform and the plots that we can build.Before we dive deeper, check your understanding.There are quizzes available in the next module toassure that you've mastered the concepts we've introduced.

### 12. Introduction to Summary Statistics-PCZmHCrcMcw.en

In the next lessons,we will discuss how to use statistics to describe quantitative data.You will gain insight into a process of how data iscollected and how to answer questions using your data.Throughout this lesson, I hope you learn to be critical ofyour analysis that happened under the hood and what the numbers actually mean.As an example of an analysis that we do here at Udacity,we look at how long a nanodegree program takes students to complete.We try to provide an estimate of the number of months or hours that students will spend.One way we might start is by reportingthe average amount of time it takes to complete the nanodegree program.But that doesn't tell the whole story.I'm sure there are differences in completiondepending on what students knew before entering the program.The shortest amount of time needed to completethe nanodegree program might just be a few weeks.How did those people complete the course so fast?Well, the longest might be a couple of years.What proportion of students finish fast within two months?What proportion take longer than eight months?Using a variety of measures,like measures of center,give you an idea of the average student.Measures of spread give you an idea of how students differ.Visuals can provide usa more complete picture of how long it takes any student to complete a program.The material in the next sections will show you how to usethese measures in a way that is informative and understandable to others.

### 13. Calculating the Mean-1nzZxmJ8xvU.en

When analyzing both discrete and continuous quantitative data,we generally discuss four main aspects.The center, the spread,the shape and outliers.In this lesson, we will focus on measures of center.There are three widely accepted measures of center.The mean, the median and the mode.To illustrate how each of these measures is calculated,consider this table of the number of dogs I see at the coffee shop in a week.From the table, we can see that on Monday, I saw five dogs.On Tuesday, I saw three dogs.On Wednesday, I saw eight dogs and so on.A friend might ask you,how many dogs would you expect to see on any given day?We might choose to respond to this in a lot of different ways.Like, it depends on the day or it depends on the week.But commonly, the word expect is associated with the mean or the average of our data set.The mean is calculated as the sum of all ofthe values in our data set divided by how many data points we have.As you can see calculated here,this value is 12.57 dogs.That is, the sum of the number of dogs observed oneach day divided by the number of days in a week.The mean isn't always the best measure of center.For this data set,you can see that the mean doesn't really seem like it's in the middle of the data at all.There are only two of the seven days that have recorded more dogs than the reported mean.It also is splitting our dogs intodecimal values which will seem strange when we're reporting back to our friend.

### 15. The Median-WlT3eeW0rb0.en

A more appropriate measure in this case, might be the median.The median is a value that divides our data set such that50% of the values are larger while the remaining 50% are smaller.For our data set,we have a median of eight.This is a much better response than the 12 and a half dogs reported by the mean.Not only does eight sit in the middle of our data set,but it doesn't split any of our dogs in half.As a note on calculating the median,the actual calculation of the median depends on whether we're working witha data set with an even number of values or an odd number of values.Let's consider a couple of examples to illustrate this point.The first thing we should do is order the values from smallest to largest.In this top case,we are working with seven values.In notation we might say, n equals seven.Because this is an odd number,we have the median as the exact middle value,in this case, the number three.In a second case,let's consider n to be an even value.Let's find the median of the following list.Again, we should first put these values in order.Because there isn't an exact center value, we will,in this case,take the mean of the two values in the center as our median.Notice, this does not even need to be a number in our data set.Because n is equal to eight for this example,the mean of the fourth andthe fifth values will provide to us the median for this data set.Moving four values from each of the sides,makes it so that we take the mean of three and five to obtain the median of four.

### 17. Measures of Center - The Mode-NE81NZgECqg.en

The third measure of center,aims at providing us the most common value in the data set.In this data set,this is the value of three.The value that occurs most often is known as the Mode.These are all three potential measures of center.The Mean or the average,the Median or the middle value,and the Mode or the most frequent value.

### 19. What is Notation-MaHV5cKfcmE.en

Previously, we listed the four main aspects of analyzing quantitative data;center, spread, shape and outliers.We also looked specifically at measures of center,by introducing means, medians and modes.Before we look at measures of spread,it's important to understand notation.You might not even know it,but you use notation all the time.Consider this example, of five plus three.Plus is an English word.This symbol is notation and its universal.Notation is a common math language used to communicate.Regardless, of whether you speak English, Spanish,Greek or any other language,you can work together using notation as a common language to solve problems.Like learning any new language,notation can be frightening at first,but it's an essential tool for communicating ideas associated with data.We will work together through some examples to assure you completely master this concept.

### 20. Notation for Random Variables-8NxTW1u4s-Y.en

As a first example,let's apply this new idea of notation to something you've used before.Spreadsheets. Spreadsheets are a common way we hold data in the real world.In our spreadsheet, we have rows and columns.To better understand how we use spreadsheets to hold data,let's work through an example.Before even collecting data,we usually start with a question or many questions.Consider I run a small blog about mybest and worst adventures with the dogs at the coffee shop.Which also sells trinkets related to those adventures.Everything from fetch toys to leashes,to doggie bags, and everything in between.Questions I might have are: How many people visit my site?Or how much time do visitors spend on my site?Are there differences in traffic,depending on the day of the week?How many visitors purchase an item through the blog?In order to answer these questions,say we keep track of the date of the visit,the day of the week of the visit,the amount of time spent on the site,and whether or not an individual buys an item.We can think of each of these as a column.A column in our dataset is associated with a random variable.Explaining what a random variable is in English is complicated.But in notation, it's simple.In English, a random variable is a placeholder for the possible values of some process.In notation, it's X.For our website, the date of the visit,the day of the week of the visit,the amount of time spent on the site,and whether or not an individual buys an item are all variables.Let's say we have a visitor on Thursday, June 15.The visitor stays on our site for five minutes and doesn't buy an item.Then a second visitor,visits the site on the exact same day for 10 minutes and they do buy an item.Notice how each of these individuals has been added to our spreadsheet.We might have many more visitors,and we could update our spreadsheet accordingly.When using spreadsheets, we frequentlyanalyze a full column to answer our questions of interest.For example, to answer the question of how much time do visitors spend on our site?We need to look at this column.To answer the question of,Are there differences in traffic,depending on the day of the week?We need to look at this column.And to answer the question of,How many purchases occur through our blog?We need to look at this column.Mathematically, we usually consider a random variable or column using a capital letter.Commonly, we use the capital letter X,but we can just as easily use Y,Z or any other capital letter.We might say, consider the random variable X,which signifies the amount of time an individual spends on our website.Therefore, X relates to this entire column.Consider we also have a random variable Y,which signifies whether or not an individual purchases an item from the website,so Y relates to this entire column.

### 22. Random  Observed Values-KFIt2OC3wCI.en

Connecting this full circle,these capital letters X and Y relate to a random variable.This is an abstract idea.How much time an individual spends on our website,can take on lots of different values.So, this capital letter X is not a number,it's an entire set of possible values.We can think of capital X as a placeholder for any of these possible values.When we look at an individual outcome of our random variable,we signify this with a lower case letter.Often the lower case letter has a subscript,that helps us attach notation to each specific value in our dataset.For this dataset, we would say the amount of timean individual spends on our site is provided by an amount,which we would notate with a capital X.The first visitor spends five minutes on the site,which we would notate with x1.Notice, this is a lowercase x.The second visitor spends 10 minutes on the site,which we wouldn't notate as x2.Again, since this is an observed value this is a lowercase x.The labeling could continue until we reach the final visitor in our dataset,who we'd called the nth visitor.We would label this value as xn.Again, this is a lowercase value.Notation is an essential tool for us to communicate mathematical ideas.We have now discussed the idea thatcapital letters are used as notation for random variables.When we observe a particular value of the random variable we use a lowercase letter,with a subscript that signifiesthe specific value of the random variable we are considering.Notation can be tricky.Before we dive deeper, check your understanding.There are quizzes available in the next moduleto assure you've mastered the concepts we've introduced.

### 24. There Must Be A Better Way-oBp8YX2AgJw.en

In the next concepts,we are going to combine what we know about how to calculate the mean with notation.The purpose of this video is not to relearn how to calculate the meanbut rather to introduce notation using a measure you already know.Let's consider the amount of time someone spends on our website in minutes.Imagine we collect the amount of time spent on our site for five individuals.From the last video,we saw that we could label the values in this way,x1 for the first value,x2 for the second value,x3 for the third value and so on.Imagine we want to add the first two numbers together.We could write this in notation as x1 plus x2 which in this example,would be the same as 15 plus 10.If we wanted to add more than just two values,it would be tedious to continue the same process.Imagine if we had 100 values.We would have a, x1 plus x2 plus x3plus and we'd have to continue this process until x 100.Someone must have come up with a better way to notate this.And it turns out, there is a better way.

### 25. Aggregations-ADx1x2ljFB4.en

There are common ways to notate most aggregations.An aggregation is just a way to turn multiple numbers into fewer numbers,usually just one number.Common aggregations include the measures of center we introduced earlier,like the mean, the median, and the mode.Each of these takes many numbers and providesa single value to give information about the data set.In this example, we want to aggregate all the values into a single sum.In other words, add them up.The Greek alphabet is a popular place to pull notation.Similar to English, there are both lower and upper case letters in the Greek alphabet.For summing many values together,we use a Greek symbol known as Sigma.Specifically, we use the uppercase Sigma.We generally use the symbol in the following way:you will notice that instead of writing multiple x values,each with a different subscript,we write a single x with a subscript of i.Here, the I is a placeholder that tells us which x values we'll be summing up.In our first case,summing only the first two values,we want this i to be the value of either one or two.Now you might be asking,how does this notation show which values we're summing together.This is a great question.If we would like to sum just the first two values,we would write something like this.Notice at the bottom,we are giving our starting point.That is, we would like the first x value to be where i equals one.You can imagine replacing the one into this subscript.Then the value at the top gives us an ending point for where we stop.Here, our ending is two.The Sigma tells us we would like to sum fromthis bottom value up through all the values until we hit this top value.The new notation, with the summation symbol,is the same as our original notation of x1 + x2.

### 26. Notation for the Mean-3EF15AoRxyM.en

But now, if we want to sum all the values in our original example,we no longer need to write out all of the xs.Instead, we can write our summation starting at i=1 and ending at the fifth value.This is way better when wanting to extend to 10 or 20 or even 100 values.We no longer need to write out all of those xs.However, we can be even more efficient than this.Each time our data set changes,I have to change this number at the top to indicate how many values I'm summing.If we really just want to sum all of our values,we can replace this top value with n. Now our notation will work for any data set.So far, we have a way to sum all ofour values regardless of how many are in our data set.In order to finish this calculation of the mean,we need to divide our sum by how many values are in our data set.But this is just n. You would commonly see the mean of the data set notated like this,which we pronounce as x-bar,and it is calculated with this notation which says you sumall of the values in the data set and thendivide by the number of values in the data set.Learning notation can be tough.Don't be afraid to watch this video more than once.Before we move on to the next sections,there are quizzes available in the next module toassure you've mastered the concepts we've introduced here.

### img

## Part 06-Module 01-Lesson 02_Descriptive Statistics - Part II

### 01. 26 Spread Part 1-zb76Z_viYLY.en

That last section was intense.I hope the quizzes reinforce what was shown in the videos.I know the first time I saw a notation,it totally went over my head.We will begin this lesson looking atthe second aspect with regard to analyzing quantitative variables, the spread.When we discussed measures of spread,we are considering numeric values that areassociated with how far points are from one another.Common values of spread include the range,the inter-quartile range, the standard deviation, and the variance.

### 02. Histograms-4t10RgUv2Fc.en

It is easy as to understand the spread of our data visually.The most common visual for quantitative data is known as a histogram.In this video, we will take a look at exactly how histograms are constructed.In order to understand how histograms are constructed,consider we have the following data-set.First, we need to bin our data.You as the histogram creator ultimately choose how the binning occurs.Here, I have chosen our bins as one to four,five to eight, 9-12 and 13-16.Because these first four values are between one and four,they go into the first bin.These next three values are between five and eight,so they fall in the next bin,then these two values fall in this bin and 15 falls into our last bin.The number of values in each bin determine the height of each histogram bar.Changing the bins will result in a slightly different visual.There really isn't a right answer to choosing our bins,and in most cases software will choose the appropriate bins for us.But it is something to be aware of.

### 03. What is the Difference-I3tQvrCgNrQ.en

Here, are two histograms comparing the number of dogs that Isaw on weekdays to the number of dogs I saw on weekends from last year.You will notice that the tallest bins forboth weekdays and weekends are associated with 13 dogs.So the number of dogs I expect to see areessentially the same for weekdays as on weekends.And the measures of center would basically be the same here.Both have a mean, median,and mode that are about 13 dogs.But something is different about these two distributions.So what's the difference?Well, the difference is how spread out the data are for each group.You can see that the number of dogs I see on weekdays, ranges from 10-16.While on weekends, it ranges from 6-18.In the upcoming sections,we will look at the most common ways to measure the spread of our data.

### 04. 29 Number Summary-gzUN5zKLHjQ.en

One of the most common ways to measure the spreadof our data is by looking at the Five Number Summary,which gives us values for calculating the range and interquartile range.The Five Number Summary consists of five values, the maximum,the minimum, the first quartile,the second quartile which is also the median, and the third quartile.Consider we have the following dataset.The first thing we need to do to calculatethe Five Number Summary is to order our values.Once ordered, the minimum and the maximum valuesare easy to identify as the smallest and largest values.As we calculate it in the section on measures of center,the median is the middle value in our dataset.We also call this Q2 or the second quartile because50% of our data or two quarters fall below this value.The remaining two values to complete the Five Number Summary are Q1 and Q3.These values can be thought of as the medians of the data on either side of Q2.That is the median of these data points is Q1,this value is such that 25% of our data fall below it,and the median of these data points is Q3 or the third quartile.This value is such that 75% of our data fall below this mark.Notice, Q2 was not an either a set of these points used to calculate Q1 or Q3.This provides our Five Number Summary as the following.Let's consider another example for in our dataset has an even set of values.Again, we first need to order the values.We can quickly identify the maximum and the minimum.Remember, with an even number of values,the median or Q2 is given as the mean of these two values here.In order to find Q1 and Q3,we divide our dataset between the two values we use to find the median.This provides these two datasets.Finding the median of each of these will provide Q1 and Q3.For this dataset, Q1 will be the mean of these two values,and Q3, will be the mean of these two values.This provides our Five Number Summary as the following.Once we've calculated all the values for the Five Number Summary,finding the range and interquartile range is no problem.For the first dataset,the range is calculated as the maximum minus the minimum.For the first dataset,this was 10 minus 1 or 9.And the interquartile range is calculated as Q3 minus Q1,which is 8 minus 2 or 6.

### 06. 5 Number Summary to Variance-Ljhau0hrZ1g.en

Looking back at the distributions we found for the number of dogs I see,we can mark the values of our five number summary like this.If we take just these marks,this makes a common plot for data known as a box plot.Though I prefer a histogram in most cases,a box plot can be useful for quicklycomparing the spread of two data sets across some key metrics,like our quartiles, and the maximum and minimum.From both the histogram and the five-number summary,we can quickly see that the number of dogs I see onweekends varies much more than the number of dogs I see on weekdays.We can also visualize the distance from here to here as the range,while the distance from here to here is the interquartile range.There are lots of useful metrics we can get from these box plots.But what if we want to compare the spreads of these distributions without having to carryaround all five of these values for each distribution?What if I wanted just a single value to be able to compare the two distributions spreads?

### 07. What is the Standard Deviation Measuring-IbwUJ3ORZ5s.en

The most common way that professionals measure the spread of a dataset with a single value is with the standard deviation or variance.Here, we will focus on the standard deviation,but we will actually learn how to calculate the variance in the process.If you have never heard of these measures before,this calculation will probably look pretty complex.When all's said and done with this calculation,the standard deviation will tell us on averagehow far every point is from the mean of the points.As a quick mental picture,imagine we wanted to know how far employees were located from their place of work.One person might be 15 miles,another 35, another only one mile,and another might be remote and is 103 miles.We could aggregate all of these distances together to show thatthe average distance employees are located from their work is 18 miles.But now, we want to know how the distance to work varies from one employee to the next.We could use the five number summary as a description.But if we wanted just one number to talk about the spread,we'd probably choose the standard deviation.The standard deviation is on average how much each observation varies from the mean.For this example this is,how much on average the distance each person is fromwork differs from the average distance all of them are from work.So, this one is three miles farther from work than the averagewhile this individual is four miles closer to work than the average.The standard deviation is how far onaverage are individuals located from this mean distance.So, it is like the average of all of these distances.We will take a closer look at this but hopefully this gives youa strong conceptual understanding of what we'll be calculating in the next sections.

### 08. Standard Deviation Calculation-H5zA1A-XPoY.en

In the last video,we got an idea of what the standard deviation is measuring.In this video, we will look atthe math that actually occurs when calculating this measure.We will work with data to calculate this measure as well as associate notation with it.It is worth noting that after this lesson,you probably won't calculate this measure by hand ever again,because you'll learn software to do it for you.The calculating it yourself will give you intuition behind what it's actually doing.And this intuition is necessary to become good atunderstanding data and choosing the right analysis for your situation.Imagine we have a data set with four values,10, 14, 10, and 6.The first thing we need to do to calculate the standard deviation is to find the mean.In notation, we have this as X bar.For our values, the sum is 40,and we have four numbers.So the mean is 40 over 4 or 10.Then we want to look at the distance of each observation from this mean.Two of these observations are exactly equal to the mean.So the distance here is zero.One is 4 larger the 14,while the other is 4 smaller the 6.In notation, each of these is XI minus X bar.Then, if we were to average these distances,the positive would cancel with the negative value.And the value of zero isn't a great measure of the spread here.Zero would suggest that all the values are the same or that there's no spread.So instead, we need to make all of these values positive.The way we do this when calculating the standard deviation is by squaring them all.If we do that here,our negative and positive 4 values will become 16s.Now, we could average these to find the averagesquared distance of each observation from the mean.This is called the variance.Finding the average, just as we did before,means adding all of these values and dividing by how many there are.In our case, we had 0, 16,0, 16 and we divide by 4 because we have four observations.However, this is an average ofsquared values which we only did to get positive values in the first place.So to get our standard deviation,we take the square root of this ending value.Here, our standard deviation is 2.83.So this is on average how far each point in our data set is from the mean,which is the definition of the standard deviation.

### 11. Why the Standard Deviation-XlTBvjQ2t8w.en

So it might seem absurd to do this calculation of the standard deviation.I mean, it's such a complicated way to measure the spreadof our data compared to the five number summary we saw earlier.But it turns out that the standard deviation is used all thetime to get a single number to compare the spread of two data sets.It is kind of nice to be able to talk about how spread out our data are from oneanother without having to report an entire table of values.We can just compare the standard deviation forone group to the standard deviation of another group.And we have a way to tell which dataset is more spread out.Having one number simplifies the amount ofinformation that the person you're reporting to needs to consume.Having the single value also hasother advantages with regard to what is known as inferential statistics,but that's beyond what we need to know now.For now, we just need to know that we have a way to take all ofour values and get a single numberthat tells us how spread out they are from one another.

### 12. Variance  Standard Deviation Final Points-vXUgp2375j4.en

A few quick final points to keep in mind about the standard deviation.First, the standard deviation is frequently used to comparethe spread of different groups to determine which is more spread out.Second, when data pertains to money or the economy,having higher standard deviation is associated with having higher risk.In comparing stock prices,a stock price that changes with higher standard deviation over time is considered morerisky than a stock price that fluctuates with lower standard deviation.Third, for a comparison to be fair,all data must be in the same units.If you're measuring in dollars for one data set,but euros for another,it's not fair to compare these data sets to determine which has the greatest spread.Finally, the variance has squared units of your original measurements.For example, if you were measuring revenue in dollars,the variance has units of dollars squared,which isn't particularly useful.For this reason, the standard deviation,which is the square root of the variance,is often deemed a more useful measurement ofspread as it shares the units of the original data set.If you measure revenue in dollars,the standard deviation also has units of dollars.

### 17. Shape of Distributions-UnN99AAYf8k.en

Now that we've discussed how to build a histogram,we can use this to determine the shape associated with our data.Here we have three histograms,showing the shape for three different data sets.The histogram that has shorter bins on the left and taller bins on the right,is considered a left skewed shape.This histogram that has shorter bins on the right and taller bins on the left,is considered a right skewed shape.Any distribution where you can draw a line down the middle andthe right side mirrors the left side is considered symmetric.One of the most common symmetric distributions,is known as a normal distribution and it's also called the Bell Curve.The shape of the distribution can actually tell usa lot about the measures of center and spread.Symmetric distributions, like this one have a mean that's equal to the median,which also equals the mode.Each of these measures sits here in the center.The mode is essentially the tallest bar in our histogram.When we have skewed distributions,it's the case of the mean is pulled by the tail of the distribution,while the median stays closer to the mode.For example, in this right skewed distribution,the mean would be pulled higher,resulting in a mean that's greater than our median.Alternatively, in a left skewed distribution,our mean is pulled down here,resulting in a mean that's less than our median.In order to relate this to the visual of a histogram,back to the five number summary we saw on the earlier lessons,here are the corresponding box plots below each histogram.Notice how the whiskers stretch in the direction of the skew,for each of the skewed distributions.That is the longer whisker is on the left,for a left skewed distribution and it's on the right,for the right skewed distribution.Alternatively, the symmetric histogram also has a symmetric box spot.

### 18. Data in the Real World-HmipezTjTDY.en

If you're working with data,you can always build a Quick Plot to see the shape.Just to apply some context,some examples of approximately Bell-Shaped data include heights and weights,standardized test scores, precipitation amounts,the mean of a distribution,or errors in manufacturing processes.Common data that follow Left Skewed Distributions include GPAs,the age of death,and asset price changes.Common data that follow approximatelyRight Skewed Distributions include the amount of drug left in your bloodstream over time,the distribution of wealth,and human athletic abilities.There are links below inthe instructor notes in case you want to learn more about each of these cases.Though these three, Right Skewed,Left Skewed and Symmetric,are the most common distributions,data in the real world can be messy and it might not follow any of these distributions.We will talk about this more in the next section.

### 20. Outliers-HKIsvkZUZfo.en

In this video, we want to look at the final aspectused to describe quantitative variables, Outliers.Outliers are data points that fall very far from the rest of the values in our data set.In order to determine what is very far,there are a number of different methods.My usual method for detecting outliers isn't very scientific.Usually, I just look at a histogram and see ifthe point is really far from any of the other data points.Again, a quick plot of your data canoften help you understand a lot in a short amount of time.In order to illustrate the impact that outliers canhave on the way we report summary statistics,let's consider the salaries of entrepreneurs.Imagine I select ten entrepreneur earnings and Ipull these nine values here as earnings in thousands of dollars,and the tenth is the CEO of Facebook.According to a post by CNN in 2016,he earned 4.4 million dollars per day.Here, we can calculate the mean of these salaries forentrepreneurs based on this data to be approximately 160 million dollars.This is incredibly misleading.Literally zero of the entrepreneurs earned this salary.None of the ten salaries are even close to this amount.A better measure of center would certainly be the median.The median here is calculated at 62,000 dollars a year and isa better indication of what an entrepreneur is likely to earn based on our data.Our standard deviation is also not a great measure in this case.At approximately 482 million dollars,all this suggests is that our earnings for entrepreneurs are really spread out,but that really isn't fair either.Just one point is really far from the rest.Like really really far.

### 21. Working with Outliers-4RnQjtJB8t8.en

How should we work with these outliers in practice?At the very least,we should note that they exist.We need to realize the impact they have on our summary statistics.In this case, greatly increasing our mean and our standard deviation.If the outliers are typos or data entry errors,this is a reason to remove these points.Or if we know what they should be,we could change them with the correct values.In cases like the example above,we might try to understand,what was so different about the outlier when compared to the other individuals?How did this entrepreneur become so successful?And why are the earnings so large in comparison?There is an entire field aimed at this idea called the anomaly detection.You will notice measures associated with the five number summary, like the minimum,the maximum and the quartiles one, two,three, are more informative when outliers are present.This example shows that you need to be careful about how we share our results andstate our conclusions using summary statistics when we have outliers.A single number can be very misleading about what is actually happening in our data.Some statistics are more misleading than others.If you are the consumer of information based on data,which we all are,it's important to know how to askthe right questions regarding the statistics around you.

### 22. Outliers Advice-BhhDoTgYQmI.en

If you're the one doing the reporting,here are some of my personal guidelines when analyzing data.First, plot your data.Second, if you have outliers,determine how you should handle them.This might require a domain expert of the field.Should you remove them?Should you fix them? Should you keep them?Third, if you're working with data that are normally distributed,the bell shape that we saw before,you can find out every little detail aboutthe data with only the mean and the standard deviation.This may seem surprising but it's true.However, if our data are skewed,the five-number summary provides much more information forthese data sets than the mean and the standard deviation can provide.Again, the most useful and informative summary you can get is frequently a visual.In upcoming lessons, we will focusspecifically on the visuals that will best convey our message.

### 27. Descriptive vs. Inferential Statistics-XV9pd8-RZ78.en

The topics covered this far have all been aimed at descriptive statistics.That is, describing the data we've collected.There's an entire other field of statisticsknown as inferential statistics that's aimed at drawingconclusions about a population of individualsbased only on a sample of individuals from that population.Imagine I want to understand what proportion of all Udacity students drink coffee.We know you're busy,and in order to get projects in on time,we assume you almost drink a ton of coffee.I send out an email to all Udacity alumni andcurrent students asking the question, do you drink coffee?For purposes of this exercise,let's say the list contained 100,000 emails.Unfortunately, not everyone responds to my email blast.Some of the emails don't even go through.Therefore, I only receive 5,000 responses.I find that 73% of the individuals that responded to my email blast,say they do drink coffee.Descriptive statistics is about describing the data we have.That is, any information we have and share regarding the 5,000 responses is descriptive.Inferential statistics is about drawing conclusionsregarding the coffee drinking habits of all Udacity students,only using the data from the 5,000 responses.Therefore, inferential statistics in our example is all about drawing conclusionsregarding all 100,000 Udacity students using only the 5,000 responses from our sample.The general language associated with this scenario is as shown here.We have a population which is our entire group of interest.In our case, the 100,000 students.We collect a subset from this population which we call a sample.In our case, the 5,000 students.Any numeric summary calculated from the sample is called a statistic.In our case, the 73% of the 5,000 that drink coffee.This 73% is the statistic.A numeric summary of the population is known as a parameter.In our case, we don't know this value as it'sa number that requires information from all Udacity students.Drawing conclusions regarding a parameter based on our statistics is known as inference.

### 31. Descriptive Statistics Summary-Fe7Gta2SfLA.en

Congratulations on making it through the statistics portion of this program.This foundation in working with data will make later sections using Spreadsheets,SQL and Tableau more intuitive.I hope this section reinforce some ideas you're already familiar with,while also introducing you to some new ones that you've now mastered.

### img

## Part 06-Module 01-Lesson 03_Admissions Case Study

### 01. Admissions Case Study Introduction-FGbxq1hQgtk.en

Welcome to the first lesson in the practical statistics course.In this case study,you're going to witness an instance of Simpson's paradox.A phenomenon that shows how powerful and dangerous statistics can be.Sometimes just grouping your data differently for your analysiscan make your conclusions disappear or even be reversed.Let's walk through an interesting case study that demonstrates this.

### 02. Admissions 1-CLgVLQAEYw8.en

The problem I'd like to tell you about is motivated by an actual studythe University of California Berkeley,which many years back wanted to know whether it's admissions procedureis gender biased.I looked at various admission statistics to understand whether than admissions policieshad a preference for a certain gender.And while the numbers I'll be giving you are not the exact same that UC Berkeley found,the paradox is indeed the same and is often called, "Simpson's Paradox."I'm just giving you a simplified version of the problem.Here is the data.Among male students, we find that from 900 applicants in major A 450 are admitted.Please tell me what the acceptance rate is in percent.

### 02. Admissions 1-f3y_weFskL4.en

Obviously, it's 50%.

### 03. Admissions 2-o91iPvtqt78.en

And the answer is 10%.

### 03. Admissions 2-pJrwiukN3Ls.en

In a second major B 100 students applied, of which 10 were admitted.What is the acceptance rate?

### 04. Admissions 3-iKTYAsZLbhc.en

The same statistic was run for female students.Again, I made up the data to illustrate the effect.Females tended to apply predominantly for major B with 900 applications for major Band just 100 for major A.The university accepted 80 out of 100 applications in major A and 180 out of 900 in major B.Please tell me the rate of acceptance in percent for major A for the females student population.

### 04. Admissions 3-rDw0TIpwJ-c.en

Of course it's 80%--80/100.

### 05. Admissions 4--GMhV1twy6Y.en

Please do the same for the major B in the female population over here.

### 05. Admissions 4-GD6cQhkoqS4.en

180 is 20% of 900.

### 06. Gender Bias-DeWp0hnRq4g.en

So, just looking at these numbers for the two different majors,would we believe--in terms of the acceptance rate--is there a gender bias?  Yes or no?

### 06. Gender Bias-JWl8lPGhlbY.en

And I would say yes, in part becausethe acceptance rate is so different for the different student populations,even though  the numbers are relatively large.So, it doesn't seem just like random deviations.But the thing that will blow your mind away is a different question.

### 07. Aggregation-55eZrE82TqA.en

Who is being favored--the male students or the female students?And looking at the data alone, it makes sense to saythe female students are favored because for both majors,they have a better admission rate than the corresponding male students.But now, let's do the trick.Let's look at the admission statistics independent of the major.So, let's talk about both majors, and I would wonder how many male students applied.And of course, the answer is 1000.How many were admitted?

### 07. Aggregation-8j5hria6Rc8.en

And the answer is 460.

### 08. Aggregation 2-udXhxyls5Dw.en

And the answer is, of course, 46%.It's 460/1000 x 100%.

### 08. Aggregation 2-xhpEqsHTf3g.en

So, what is the admissions rate for male students across both majors in percent?

### 09. Aggregation 3-YkaVgZ-yFrM.en

Now, do the same for the female student population, and we had a 1000 applicants,same number as in the male case, and 260 students admitted.So, what's the percentage rate for admission?

### 09. Aggregation 3-tPSj6_m-0_M.en

The answer is 26%.

### 10. Gender Bias Revisited-4YY-hmqSz30.en

So, across both majors, I'm asking you the same question again now.Who is actually being favored?Males or females?

### 10. Gender Bias Revisited-dOa4Cl0wM0s.en

And surprisingly, when  you look at both majors together,you find that males have a much higher admissions rate than females.I'm not making this up.These numbers might be fake, but that specific affectwas observed at the University of California at Berkley many years ago.But when you look at majors individually,then you find in each major individuallythe acceptance rate for females trumpsthat of males, both in the first major and the second major.Going from the individual major statistics to the total statistics,we haven't added anything.We just regrouped the data.So how come, when you do this,what looks like an admissions bias in favor of femalesswitches into admissions bias in favor of males?

### 11. Dangers Of Statistics-UYZXqP562qg.en

As you've seen in this example, on Simpson's paradox,the way you choose to look at your data can lead to completely different results.And often, you can majorly impact what peoplebelieve to be true with how you choose to communicate your findings.You can guess how people intentionally orunintentionally come to false conclusions with these choices.Next, you're going to walk through a similar example ofSimpson's paradox on a full dataset in a Jupyter notebook.

### 14. Conclusion-XiR_37bYA84.en

I hope this example made you think and learn to be skeptical,of your own results and the results from others.Moving forward even when you feel veryconfident about the statistics you use for your analysis,take a moment to reconsider other ways oflooking at your data and whether you chose wisely.Stay tuned as we dive into the basics of statistics.We'll begin with probability theory.

### img

## Part 06-Module 01-Lesson 04_Probability

### 01. Introduction to Probability-HeoQccoqfTk.en

Statistics and probability are different but strongly related fields of mathematics.In probability, we make predictions aboutfuture events based on models or causes that we assumewhereas in statistics we analyze the data fromthe past events to infer what those models or causes could be.There's almost an opposite relation between these two.In one, you're predicting data and in the other,you're using data to predict.Although not all topics and both fields require an understanding of the other,you'll need a good understanding in probabilityfor the foundation you'll be building in statistics.With that in mind, let's get started with the basics of probability.

### 02. Flipping Coins-OpNufHYgJCg.en

I have here a U.S. dollar coin.It has two sides, one showing a head and one showing what's called tails.In probability, I'm giving a description of this coin, and I'm making data.We just make data.[sound of coin spinning]So if we look at the coin, it came up heads.So I just made a data point of flipping the coin once, and it came up heads.Let me do it again.[sound of coin spinning]And--wow! It came up heads again.So my new data is {heads, heads}.And you can see how it relates to the data we studied beforewhen we talked about histograms and pie charts and so on.Let me give it a third try.[sound of coin spinning]And, unbelievably, it comes up once again heads.So let me ask a statistical question to test your intuition.Do you think if I twist this coin more frequently will it always come up heads?And say I try to twist it as fairly as I possibly can.

### 02. Flipping Coins-lgUDXtUyLLg.en

And you can debate it, but I think the best answer is no.This is what's called a fair coin,and that means it really has a 50% chance of coming up tails.So let me spin it again.[sound of coin spinning]And, not surprisingly, it actually came up tails this time.So probability is a method of describing the anticipated outcomeof these coin flips.

### 03. Fair Coin-9LrlrexpW_o.en

Let's talk about a fair coin.The probability of the coin coming up heads is written in this P notation.This reads probability of the coin coming up heads.And in a fair coin, the chances are 50%.That is, in half the coin flips, the coin should come up heads.In probability we often write 0.5,which is half of 1.So a probability of 1 means it always occurs.A probability of 0.5 means it occurs half the time.And let me just ask youwhat do you think, for this coin, is the probability of tails?

### 03. Fair Coin-fSKL742j-zk.en

And I would say the answer is 0.5.Let me now go to a coin that is what is called "loaded."

### 04. Loaded Coin 1-T0EjWSjLGjQ.en

A loaded coin is one that comes up with one of the two much more frequently than the other.So, for example, suppose I have a coin that always comes up heads.What probability would I assess for this coin to come up heads?What would be the right number over here?

### 04. Loaded Coin 1-sNvQeSikRFY.en

And the number is 1.That's the same as 100%.1 just means it always comes up in heads.

### 05. Loaded Coin 2-Y7tnbth-gag.en

And, given that, what number would you now assessthe probability of tails to be?

### 05. Loaded Coin 2-dGffszQYzqc.en

And, yes, the answer is zero.And we find a little law here we just want to point out,which is the probability of heads plus the probability of tailsequals 1.And the reason why that's the case is the coin either comes up heads or tails.There is no other a choice.So no matter what happens, if I look at heads and tails combinedthe chances of either of those occurring is 1,because we know it's going to happen.So we can use this lawto compute the probability of tails for other examples.

### 06. Loaded Coin 3-HohMRlmHoMQ.en

And the answer is 0.25,which is 1 - 0.75 using the law down here.As you can verify, 0.75 + 0.25 =1.

### 06. Loaded Coin 3-P4uljJ_OP6I.en

So suppose the probability of heads is 0.75,that is, 3 out of 4 times we're going to get heads.What is the probability of tails?

### 07. Complementary Outcomes-YseJqD-1oUg.en

So we just learned something important.There's a probability for an outcome; I'm going to call it A, for now.And we learned that the probability of the opposite outcome,which we're going to call A(this over here just means "not")is 1 minus the probability as expressed right over here.That's a very basic law of probability, which will become handy as we go forward,so please remember it.

### 08. Two Flips 1-1txkcmxk3vU.en

That was a tricky question, and you couldn't really know the answerif you've never seen probability before,but the answer is 0.25.And I will derive it for you using something called a truth table.In a truth table, you draw out every possible outcomeof the experiment that you conducted.There were two coin flips--flip 1 and flip 2--and each hada possible outcome of heads, heads; heads, tails; tail, heads; and tail, tail.So when we look at this table, you can see every possible outcome of these two coin flips.There happens to be four of them.And I would argue because heads and tails are equally likely,each of these outcomes is equally likely.Because we know that the probability of all outcomes has to add up to 1,we find that each outcome has a chance of a quarter, or, 0.25.Another way to look at this is the probability of heads followed by headsis the product .What are the chances of the first outcome to be headsmultiplied by the probability of the second outcome to be heads?The first is 0.5, as is the second.And if you multiply these two numbers, it's 0.25, or, a quarter.

### 08. Two Flips 1-yUIz7SgUwJg.en

In our example, we observed heads twice.So now I want to ask you a really tricky question:What's the probability of observing heads and headsif you flip the same unbiased coin twice?This means in each flip we assume the probability of heads is 0.5.Please answer here.

### 09. Two Flips 2-pT0FXiH_5nI.en

And the answer is 0.4 becauseheads comes up 0.6,and 1 - 0.6 = 0.4.

### 09. Two Flips 2-uhrL5fatt3E.en

Let me now challenge youand give you a loaded coin I flipped twice.And for this loaded coin, I assumed the probability of heads is 0.6.That really changes all of the numbers in the table so far,but you can apply the same method of truth tablesto arrive at an answer for what is the probability of seeing heads twiceunder the assumption that the probability of heads equals 0.6?And I want to do this in steps,so rather than asking the question directly,let me help you derive it by first asking: What's the probability of tails?

### 10. Two Flips 3-3NSPqjp6pFY.en

And the answer using our product rule isheads, heads comes out to 0.6  0.6, which is 0.36.Heads followed by tails is 0.6  0.4, which is 0.24.Tails followed by heads is, again, 0.24.And tails followed by tails is 0.16, which is 0.4  0.4.

### 10. Two Flips 3-uimwo-puQWY.en

And now please fill out the entire truth table.There are four values over here, so please compute them for me.

### 11. Two Flips 4-bNoS6LQEFrI.en

And, not surprisingly, it's 1.That is, the truth table always has a probability that adds up to 1because it considers all possible cases,and all possible cases together have a probability of 1.So we just check this and make sure it's correct.Reading from this table, we find that the probability of (H,H) is 0.36.And you can do the same over here.0.6  0.6 = 0.36So that's our correct answer.

### 11. Two Flips 4-rRPwknIDuI0.en

If you add up these numbers over here--please go ahead and add them upand tell me what the sum of those numbers is.

### 12. Two Flips 5-G28YyiGFGWA.en

Let's now go to the extreme, and this is a challenging probability question.Suppose the probability of heads is 1,so my coin always comes up with heads.What is the probability of (H,H)?

### 12. Two Flips 5-HB8b7sZQFGs.en

And the answer is 1.To see this, we know that the probability of tails is 0.All the probability goes to heads.1  1 = 11  0 = 00  1 = 0And 0  0 = 0.And it's easy to verify that all these things add up to 1.Our (H,H) is just 1.

### 13. One Head 1-T4A5uyqesjo.en

And the answer shall be 0.5.And this is a nontrivial question.Let's do the truth table.So, for flip-1, we have the outcomes of heads, heads, tails, tails.For flip-2, heads and tails and heads and tails.These are all possible outcomes.And we know for the fair coin each outcome was equally likely.That is, exactly one quarter.

### 13. One Head 1-lHuZpDkfwq8.en

The truth table gets more interesting when we ask different questions.Suppose we flip our coin twice.What we care about is that exactly one of the two things is heads,and thereby exactly the other one is tails.For a fair coin, what do you think the probability would bethat if I flip it twice we would see heads exactly once?

### 14. One Head 2-64EjAbqrtmo.en

And, yes, it's in the second case and in the third case.The extreme cases of heads, heads and tails, tails don't satisfy this condition.So the trick now has been to take the 0.25 probability of these two casesand add them up, which gives us 0.25 + 0.25 = 0.5.This is the number which is correct for this inquiry.

### 14. One Head 2-JHx3ucNS9f4.en

Given that, we now have to associate a truth tablewith the question we're asking.So where exactly is, in the outcome, heads represented once?Please check the corresponding cases.

### 15. One Of Three 1-bDCXSxkochE.en

And this answer is tricky. We will derive it through the truth table. Now there's eight possible cases.Flip one can come of heads or tail; same for flip two, heads, tail, heads, tail;and the same for flip three and if you look at this every possible combination is represented.For example, these are heads, tail, tail.Now each of those outcomes has the same probability of an eighth, because it's eight cases.So 8 x 1/8 sums up to 1. In how many cases do we have exactly one H?It turns out that it's true for only three cases.The H could be in the first position, in the second position, or in the third position.So three out of eight cases have a single H.Each of those carries a probability so we sum those cases up to carry a total of 3/8 of a probability.These are the same as 0.375.

### 15. One Of Three 1-rxfHfjy9Mm4.en

Let me now make it really, really challenging for you.I take a fair coin and flip it 3 times, and I want to knowthe probability that exactly 1 of those 3 flips comes up heads.

### 16. One Of Three 2-27Ed1GI4j84.en

And my answer is 0.288. How do I get that? Let's look at the three critical cases.H T T is 0.6 for H times 0.4 for tails times another 0.4 for tails again and it gives me 0.096.Now it turns out this case over here has the same probability because all we do is we order it0.4 x 0.6 x 0.4 and we know that in multiplication the order doesn't matter,so you get the same 0.096, and by the same logic, if third one also gets me 0.096.So adding this 0.096s together, if we get them, gives me 0.288.So I did not have to fill the entire truth table, which you might have done in the derivation.I only have to fill out the cases I care about, yet they give me their correct result.

### 16. One Of Three 2-gGgqTGZ9TKg.en

Now that was a challenging question.I'm going to make it even more challenging for you now.I'll give you a loaded coin--the probability for H is 0.6.I expect this will take you awhile on a piece of paperto really calculate this probability over here.But you can do exactly the same thing.You go through the truth table.You apply the multiplication I showed you beforeto calculate the probability of each outcome; they're not the same anymore.H, H, H is clearly more likely than T, T, T.And when you've done this, add the corresponding figures up,and tell me what the answer is.

### 17. Even Roll-DrnAR4SqlEE.en

So let's do one final exercise.Now I am throwing dice.The difference between dice and coins is that there are now 6 possible outcomes.Let me just draw them, and say it's a fair die,which means each of the different sides comes up with a probability over 6for any of the numbers you can plug in over here.What do you think the probability is the die comes up with an even number?I'm going to write this as the outcome of the die is even.And you can once again use a truth table to calculate that number.

### 17. Even Roll-M3L0a5V4Nf0.en

In truth table-speak, there are 6 outcomes, 1 to 6.Each has the same probability over six.Half of those numbers are even--2, 4,  and 6,so if we add those up, we get 3  1/6--the same as a half.The outcomes is 0.5.Now I'm finally going to make, as my final quiz,a really challenging question for you.

### 18. Doubles-On_Guw8wac8.en

Suppose we throw a fair die twice. What do you think the probability of a double is?Double means both outcomes are identical with the same number regardless of what that number is.The actually an important number because in many games involving two dice,have different rules when these come up with the same number.So, it might be important to know what the probability is.

### 18. Doubles-fkUyTJNbdzU.en

And once again, we can answer this using a truth table.Now the truth table will have 36 different entries,six for the first throw times six for the second throw,and there isn't enough space on this tablet to draw all the 36 entries.So, let me just draw the ones that really matter, one-one, two-two, and so on all the way to a six-six.So, each one of those is a probability of 1/6 for the first outcome times 1/6 for the second,which gives me 1/36, and the same logic applies everywhere.So, for all of these six outcomes, I have 1/36 of a chance this outcome would materialize.Adding them all up gives me 1/6, why?Because, I get 6 times 36 and I can simply this back to 1/6 that's just the same as 0.16667.So, 1/6 times, you will get a doubleNow, when you're play a game like backgammon, which is played with two dice,it might not feel like this, I can swear I don't get a double of 1/6 moves,but it's actually true that that's the right--that's the correct probability.

### 19. Probability Conclusion-dsVKoXymYDU.en

You now have a basic understanding of probability. Great job!Let's quickly summarize what was covered in this lesson.You learned about the probability of an event.Such as the outcome of a coin flip.You learned that the probability ofthe opposite event is one minus the probability of this event.And you learned about the probability of a composite event,which was in the form of P times P times P and so on.This rule is true because the events we'veobserved so far are independent of one another,which means the outcome of one event doesn't affect the outcome of another,which is also the case in the next lesson on binomial distributions.However, if we're looking at the probability of dependent events,say the probability of rain tomorrow given that it rained today, this rule doesn't hold.Dependent events where the outcomes of later events dependon what has already happened are at the heart of the following topics,unconditional probability and Bayes Rule.

### img

## Part 06-Module 01-Lesson 05_Binomial Distribution

### 01. Binomial-3koDdc9r73E.en

So we talked about coin flips, and we flipped some coins.Now, I want to flip many coins including this 2-dollar coinwhose country of origin I just don't remember.Perhaps you can post on the forum where this 2-dollar coin might be from.So let's ask an easy quiz. Suppose we do 2 coin flips.I would like to know how many outcomes of these 2 coin flips are thereand which number of heads equals the number of tailswhich in this example means you would have head exactly once and tails exactly once.Give me that number.

### 01. Binomial-x1yamZeOMPY.en

And the answer is 2. If you look at the truth tablehead-head, head-tail, tail-head, and tail-tailthese are the four possible outcomes.Those two outcomes over here yield an equal number of heads and tails.

### 02. Heads Tails-iyX0-eXStbw.en

In going through the truth table, there's a bit more info.So let's find the one where the number of heads and tails are the same.Those three on the left side and those three on the right side for a total of 6.

### 02. Heads Tails-yo55zJtJQwo.en

Let's now go to 4 coins and ask the same question.

### 03. Heads Tails 2-S87Z5DgPJeo.en

Now. Let's go to 5 coin flips.How many outcomes have the same number of heads and tails? This is a trick question.

### 03. Heads Tails 2-vLhdJtXx060.en

And the answer is 0. With an odd number of coin flips, one has to be more than the other.There's no other way. Okay. I tricked you a little bit.

### 04. 5 Flips 1 Head-4LVRNqpdxsw.en

The answer is 5. There's 5 different ways in these 5 outcomes.To place heads--could be first, second, third, fourth or fifth. So these are 5 different ways.

### 04. 5 Flips 1 Head-VEfOdACY9rA.en

With 5 coin flips, how many outcomes will have exactly 1 heads, hence 4 tails.

### 05. 5 Flips 2 Heads-69je8wHh2mQ.en

And the answer is 10 and this is a non-trivial answer.So you could go and place the first heads anywhere in these five elements--say here--and there's five different ways to place heads.You can now place the second heads among the remaining four--for example, you could place itover here and it gives you a factor of four different ways of placing the second heads.But when you do this, you over count--you over count exactly by a factor of 2 in the business.You place the first heads over here, the second over here, but if it was chosen to placethe first head on the right side and the second head over here on the left side,and the outcome would've been exactly the same, so you counted the one twicewhich means we have to divide it by 2--5 times 4/2 is 10

### 05. 5 Flips 2 Heads-lhhUjxnbad8.en

Let's now make you think really hard. In 5 coin flips, how many outcomes will you have 2 heads.This is a serious and non-trivial question.

### 06. 5 Flips 3 Heads-1PHs2w_NNTg.en

Let's now go to 3 heads if you think the result is no.

### 06. 5 Flips 3 Heads-pOKmt4w8T3g.en

And that log is 10 again. There's two proofs.One is I can just flip heads and tails. So three heads means two tails.I can do the exact same game as before where I placed tails as opposed to headsand it gives me the same equation as before, but let's do it the new way, three heads.I can place 5*4*3--the first heads, the second and the third.**For the first, I have five positions, for the second--four, and for the third--three are left.This gives me the combinatorics for those heads, but now I'm over counting.How much am I over counting?Well, suppose I'm committed to put the three heads into the three slots over here and that's not given.And I just wonder in which order I've put them in,so I might put the first one here, the first one here, the first one here.Then for the first one placed in here, there's now three different ways of placing it.For the second one, there's two different ways of placing it.For the third one, it's not deterministic--there's just one slot left.So I over count this by a factor of 6--there are 6 different ways of placing these three headsinto these three slots, so the result is 543/321 producing the 5*2=10.And that is insightful.

### 07. 10 Flips 5 Heads-Qm4KTLfFMzo.en

I was given 10*9*8*7*6 which is 30,240 with a pick of those five headsand we're over counting by 5*4*3*2*1 which is 120 and this gives us as a quotient 252.

### 07. 10 Flips 5 Heads-mOPFQlKBg2M.en

Let's say I have 10 coins. You just get 10 of these shiny silver coins here. So we got 10 coins.And I want about 4 heads in these 10 coins, and just apply the same logic to find an answer.The first time I'll do it for you.I can place those heads in 1098*7 different slots if they were counted.Now that I'm committed to having chosen these slots, the implementations of those are 432*1.That gives me 5,040/24, also known as 210, so that's 210 outcomes out of 2 to 10th outcomeswhich is 1024 in which exactly 4 heads are observed and 5 tails.Now it's your turn. 5 heads out of 10 coins. One of them coming up to heads.

### 08. Formula-DTdS-LlMTQ0.en

So let me give you the mathematical formula that you might be familiar with.n! for any number n is the same as n(n-1)(n-2) all the way to times 1.Let's call it factorial. So 10 factorial for example will be 109876 and so on.If you look at this equation over here, I'll give you a couple of choices how to write it.It could be n!; it could be n!/k! where this over here is k which is 5and this over here is n.It could be n!/k!*k! or it could be n!/k!*(n-k)!These are four choices. One of this is actually correct for the formulathat we've computed before.Pick the correct one.

### 08. Formula-yTr8zCHdo5M.en

And this one is the correct one and I have to admit I made a mistake a little bitby taking a symmetric example where k is exactly half of n.

### 09. Arrangements-GeINbOOYkF8.en

The key observation is that this thing over here is n!/(n-k)!and to see this let's go back to the case where we had 4 heads.In the 4 heads case, we multiplied all the way to times 5 over here.We only multiplied all the way to 7. These are the 4 heads that we placed.So n! goes from 10 all the way to 1. (n-k)! goes from 10-4 and that's 6 all the way to 1.So this blue expression over here will go from 10 x 9 x 8 and so on all the way to 1over now n-k, 10-4 is 6. 6*5*4 and so on.When you look at this, the 6*5*4 occurs on top as well.So we can cut those out and what remains is 10*9*8*7. This one over here.Now once we place these 4 heads, we have to divide by a 4 factorialwhich is different ways of placing those 4 points into the predefined bins.So put differently, this is k! so if we put all this together you get n!/(n-k)!*k!This is the expression over here. Let's practice this one last time.Say you have 125 coins and you ask how many ways exists in which 3 coins come up heads.What is the resulting outcome? Be careful when you use your calculator.It might result in an overflow, but the answer is easy to compute.

### 09. Arrangements-NRPcnpmFCg8.en

So, I get 317,750, and the logic is I just plug in these numbers, 125!/122! 3!.When I evaluate this, I find that these guys can be easily reduced to 125*124*123.From 122 on, these factors exactly cancel. 3 expands to 3*2*1 also known as 6.When you divide these two things, you get 317,750.

### 10. Binomial 1-07vOaYwecII.en

Now, we know from our previous considerationthat there are five ways in which the number of heads could be one.5!/4! 1! happens to be 5.We also know that there are 32 possible outcomes. It is 2=32 outcomes.This is the size of a truth table. So, 5 out of 32 outcomes has exactly 1 head.So, I would suggest that the answer is 5/32, and my calculator tells me this is 0.15625.So, that is actually interesting.If you flip a coin five times, there's a chance that it only comes at head exactly onceand that is the probability 0.15625.

### 10. Binomial 1-RBfFHxEjsIU.en

And let's really ask about probabilities.Let's say for now, we have a fair coin with a probability of heads is 0.5.If I flip a coin five times, what's the probability the number of heads is exactly 1.You should be able to compute this.

### 11. Binomial 2-Uy7b3aMPnEY.en

And the answer is 5!/5-3 is 2!/3! and that gives us 10. 10/32 gives us the probability of 0.3125.

### 11. Binomial 2-d4LWnxyvrTQ.en

Let's now modify it and ask, what are the chances it becomes a head three times?What's the probability for that to happen?

### 12. Binomial 3-Jp2xJOtNQZ0.en

Now, I'm going to make it really difficult. I'm going to give you a coin--let's call it loaded.So, the probability for heads will now be 0.8 and therefore the probability for tails is 0.2.To make it easier, assume only a 3 coin flipsand ask the probability of heads coming up exactly once.What is that probability for the loaded coin that I gave you here.I recommend answering it using a truth table.

### 12. Binomial 3-YIELbuet-ZE.en

So here is my truth table of these eight different outcomes.The ones that has head exactly once are this one, this one, and this one,but they're not all equally likely.Heads, heads, heads is much more likely than say tails, tails, tailsbecause heads has a probability of 0.8^3.This one here has an outcome probability of 0.8 while this one has an outcome probability of 0.2^3.All of the green ones have the same outcome probability.They all have exactly one head 0.8 and two tails.So as before, we took the probability to be one of these heads in the truth table.This time each of those has a probability of 0.032. That's each one of the three.Now, we have to consider all of these three outcomes, which means you're going to add 0.032for each one of those three guys over here and this gives me as an answer 0.096.That's a nontrivial question.

### 13. Binomial 4-lPrKmvckG4E.en

So interestingly enough, we can do the trick as before 5!/4!*1!,which is the number of outcomes at exactly 4 heads and we know that's 5.Four heads means one tails. There's five ways to place one tails and this one over here.The question now what's the probability of those?Well, they have heads four times, (0.8) and tail once to (0.2).So we do is we multiply the total number of outcomes that have this propertywith each one probability, which happens to be the same because we get exactly four times headsand 1 times tail and multiplying these things together gives us 0.4096.This over here is indeed 0.4096 and the 5*0.2 cancels the others out,so that's the result in this specific case.The cancellation doesn't always happen. It's a rare circumstance.

### 13. Binomial 4-mvJUNYfHngY.en

Let's now say for the same loaded coin, we flip that coin five times with that same load of probabilityover here and we care about the out of five times heads comes exactly four times.Now, I want you to compute this probability over here, It's a completely nontrivial question.If it's too hard, hit the next button.

### 14. Binomial 5-8jcCGD986jk.en

And here is my answer 5!/3!*2! is 10,and we have 3 heads, so we put 3 in here and 2 tails, so we put 2 in here.Putting these all together gives us 0.2048, which is half the probability of the previous question.

### 14. Binomial 5-yof0QiP2mzk.en

So, this does not go and get the same for 3 heads.I leave this here, but obviously, the numbers aren't correct anymore.

### 15. Binomial 6-CQHRYIU6v9Q.en

Now, you're ready for the real challenge.You flip the coin 12 times in the care about how likely it is to get heads 9 times out of the 12.This is not a trivia question, but you should be able to get it right.

### 15. Binomial 6-n_OrWrZ8tKY.en

And the approximate answer is 0.236.That's the probability of exactly 9 heads out of 12 coin flipsfor this heavily loaded coin that mostly gives you heads.And again, the answer is 12!/12-9. This is 3! 9!.Then we have to compute the probabilities being (0.8)and 1-0.8 is 0.2 and that is the number over here.

### 16. Binomial Distribution Conclusion-9gjCYs8f_PU.en

In this lesson, we found the probability thata coin would land on heads k times out of n flips.If the probability of heads for the coin is p,we get the following formula for the probability that the number of heads will be k.The first half is n factorial over n minus k factorial times k factorial.This part keeps track of the total number of wayswe can get k heads and n total flips of the coin.For example, if we were looking at three heads and four flips,we could get this set of flips,or this set, or this set, or this set.There are four possible ways we can get three heads out of four flips.This part of the formula keeps track ofhow many possible ways we can get these k heads and n flips.In this case four.Each one of these ways have the following probability of event occurring.P to the k times 1 minus p to the n minus k. The pto the k keeps track of the probability associated with the number of heads.And this part keeps track of the probability associated with the number of tails.Altogether, this formula givesthe probability of what's called the binomial distribution.You've now seen how you can take very large experiments withmany coin flips and compute the probability thatthe coin will land heads a certain number of times with this formula.Although the examples in this lesson use coin flips,you can really perform these calculations with any events that have two outcomes.Does a customer buy or not?Is a transaction fraud or not?Or if a coin flip is heads or tails.Binomial distributions are used to give us insight about all of these.Later in the course,you will explore large experiments again using the normal distribution.But first, let's move on toconditional probability where events are no longer independent.Let's explore cases where the outcome ofan event can affect the outcome of the events that follow.

### img

## Part 06-Module 01-Lesson 06_Conditional Probability

### 01. Introduction to Conditional Probability-Ok8948Wcbmo.en

In real life, events often depend on each other.Say you can be an early bird or night owl.And for the sake of simplicity,let's assume there's a 50 percent chance for each one.Now whether you decide to go for a run at 5:00 a.m. tomorrow is not entirely independent.I would argue that going for a run at 5:00 a.m. is not likely for most people.But it is certainly more likely for individuals who are actually awake at that hour.If you're an early bird,the probability might be a two percent chance,while if you're a night owl,the probability might be zero.You can think of these events as two consecutive coin flips.The first coin determines whether you're an early bird,and the second coin is only flipped given that you are an early bird,and it determines if you go for the run.We are no longer observing independent events,like we did in the last lesson,where the outcome of the first coin flip didn't affect the outcome of the second.Starting with this lesson,we're going to look at case studies where the outcome of thefirst does impact the outcome of the second.

### 02. Medical Example 1-E1ph6NP3_v4.en

And the answer is 0.9 with just 1 minus the cancer.

### 02. Medical Example 1-mFfbts1lAEo.en

To do so, let's study a medical example--supposed there's a patientin the hospital who might suffer from a medical condition like cancer.Let's say the probability of having this cancer is 0.1.That means you can tell me what's the probability of being cancer free.

### 03. Medical Example 2-FV_hc3MzS_8.en

Of course, in reality, we don't know whether a person suffers cancer,but we can run a test like a blood test.The outcome of it blood test may be positive or negative, but like any good test,it tells me something about the thing I really care about--whether the person has cancer or not.Let's say, if the person has the cancer, the test comes up positive with the probability of 0.9,and that implies if the person has cancer, the negative outcome will have 0.1 probabilityand that's because these two things have to add to 1.I've just given you a fairly complicated notation that says the outcome of the test dependson whether the person has cancer or not.That is more complicated than everything else we've talked about so far.We call this thing over here a conditional probability,and the way to understand this is a very funny notation.There's a bar in the middle, and the bar says what's the probability of the stuff on the leftgiven that we assume the stuff on the right is actually the case.Now, in reality, we don't know whether the person has cancer or not, and in a later unit,we're going to reason about whether the person has cancer given a certain data set,but for now, we assume we have god-like capabilities.We can tell with absolute certainty that the person has cancer,and we can determine what the outcome of the test is.This is a test that isn't exactly deterministic--it makes mistakes,but it only makes a mistake in 10% of the cases, as illustrated by the 0.1 down here.Now, it turns out, I haven't fully specified the test.The same  test might also be applied to a situation where the person does not have cancer.So this little thing over here is my shortcut of not having cancer.And now, let me say the probability of the test giving me a positive results--a false positive resultwhen there's no cancer is 0.2.You can now tell me what's the probability of a negative outcome in casewe know for a fact the person doesn't have cancer, so please tell me.

### 03. Medical Example 2-VLLG0rYC7To.en

And the answer is 0.8.As I'm sure you noticed in the case where there is cancer, the possible test outcomes add up to 1.In the where there isn't cancer, the possible test outcomes add up to 1.So 1 - 0.2 = 0.8.

### 04. Medical Example 3-Iz4ViIg9ZlQ.en

Look at this, this is very nontrivial but armed with this,we can now build up the truth table for all the cases of the two different variables,cancer and non-cancer and positive and negative tests outcome.So, let me write down cancer and test and let me go through different possibilities.We could have cancer or not, and the test may come up positive or negative.So, please give me the probability of the combination of those for the very first one,and as a hint, it's kind of the same as before where we multiply two things,but you have to find the right things to multiple in this table over here.This is not an easy question.

### 04. Medical Example 3-Rf6WfB_1EJQ.en

And the answer is probability of cancer is 0.1, probability of test being positive given that he hascancer is the one over here--0.9, multiplying those two together gives us 0.09.

### 05. Medical Example 4-pL8Bf6tck_A.en

Moving to the next case--what do you think the probability is thatthe person does have cancer but the test comes back negative?What's the combined probability of these two cases?

### 05. Medical Example 4-udduksMWMB4.en

And once again, we'd like to refer the corresponding numbers over here on the right side0.1 for the cancer times the probability of getting a negative result conditioned on havingcancer and that is 0.1  0.1, which is 0.01.

### 06. Medical Example 5-fqt7NIvMB0s.en

Moving on to the next case. What do you think the answer is?

### 06. Medical Example 5-ys9w-NNKCcU.en

And here the answer is 0.18 by multiplying the probability of not having cancer, which is 0.9,with the probability of getting a positive test result for a non-cancer patient 0.2.Multiplying 0.9 with 0.2 gives me 0.18.

### 07. Medical Example 6--lC9xztr4zA.en

Here you get 0.72,which is the product ofnot having cancer in the first place0.9and the probability of getting anegative test resultunder the condition of not having cancer.

### 07. Medical Example 6-iyE5h48qPFQ.en

Let's just quickly do the final one,because it's the most likely one.

### 08. Medical Example 7-cw_zgQbAWNU.en

Now quickly, do me a favor and add all of those up.What do you get?

### 08. Medical Example 7-jPspIs-fNxg.en

And as usual, the answer is 1.That is, we study in the truth table all possible cases.and when we add up the probabilities,you should always get the answer of 1.

### 09. Medical Example 8-7k5oAaZamCA.en

Now let me ask you a really tricky question.What is the probability of a positive test result?Can you sum or determine,irrespective of whether there's cancer or not,what is the probability you get a positive test result?

### 09. Medical Example 8-btGdX0ZpkNU.en

And the result, once again, is found in the truth table,which is why this table is so powerful.Let's look at where in the truth table we get a positive test result.I would say it is right here,right here.If you take corresponding probabilities of0.09 and 0.18,and add them up,we get 0.27,and that's the correct answer for getting a positive result.

### 10. Total Probability-YSYpzFR4k1I.en

Putting all of this into mathematical notationwe've given the probability of having cancerand from there, it follows the probability of not having cancer.And they give me 2 conditional probabilitythat are the test being positive.If we have have cancer, from which we can now predict the probabilityof the test being negative of having cancer.And the probability of the test being positive can be cancer freewhich can complete the probability ofa negative test result in the cancer-free case.So these things are just easily inferredby the 1 minus rule.Then when we read this,you complete the probability of a positive test resultas the sum ofa positive test result given cancertimes the probability of cancer,which is our truth table entry for the combination of P and Cplus the same given we don't have of cancer.Now this notation is confusing and complicatedif we ever dive deep into probability, that's calledtotal probability,but it's useful to know that this isvery, very intuitiveand to further develop intuition let me just give youanother exercise of exactly the same type.

### 11. Two Coins 1-QIQBb4nLsHc.en

This time around, we have a bag,and in the bag are 2 coins,coin 1 and coin 2.And in advance, we know that coin 1 is fair.So P of coin 1 of coming up heads is 0.5whereas coin 2 is loaded, that is, P of coin 2coming up heads is 0.9.Quickly, give me the following numbersof the probability of coming up tailsfor coin 1 and for coin 2.

### 11. Two Coins 1-SYnYIjLpbjE.en

And the answer is 0.5 for coin 1and 0.1 for coin 2,because these things have to add up to 1for each of the coins.

### 12. Two Coins 2-hoVOT8qcQ7c.en

And lets do the truth table.You have a pick event followed by a flip eventWe can pick coin 1 or coin 2.There is a 0.5 chance for each of the coins.Then we can flip and get headsor tails for the coin we've chosen.Now what are the probabilities?I'd argue picking 1 at 0.5and once I pick the fair coin, I knowthat the probability of heads is, once again, 0.5which makes it 0.25The same is true for picking the fair coinand expecting tailsbut as we pick the unfair coin with a 0.5 chancewe get a 0.9 chance of headsSo 0.5 times 0.95 gives you 0.45whereas the unfair coin, the probability of tails is 0.1multiply by the probability of picking it at 0.5 gives us 0.05Now when they ask you, what's the probability of headswe'll find that 2 of those cases indeed come up with headsso if you add 0.25 and 0.45 and we get 0.7.So this example is a 0.7 chancethat we might generate heads.

### 12. Two Coins 2-tI0J14yQr1s.en

So now what happens is, I'm going toremove one of the coins from this bag,and each coin, coin 1 or coin 2,is being picked with equal probability.Let me now flip that coin once,and I want you to tell me,what's the probability that this coinwhich could be 50% chance fair coinand 50% chance a loaded coin.What's the probability that this coin comes up heads?Again, this is an exercise in conditional probability.

### 13. Two Coins 3-GO6kbL3QRBE.en

Now let me up the ante by flipping this coin twice.Once again, I'm drawing a coin from this bag,and I pick one at 50% chance.I don't know which one I have picked.It might be fair or loaded.And in flipping it twice,I get first heads,and then tails.What's the probability that if I do the following,I draw a coin at random with the probabilities shown,and then I flip it twice, that same coin.I just draw it once and then flip it twice.What's the probability of seeing heads firstand then tails?Again, you might derive this using truth tables.

### 13. Two Coins 3-JIWv5fU3GLA.en

This is a non-trivial question,and the right way to do this is to go through the truth table,which I've drawn over here.There's 3 different things happening.We've taken initial pick of the coin,which can take coin 1 or coin 2 with equal probability,and then you go flip it for the first time,and there's heads or tails outcomes,and we flip it for the second time with the second outcome.So these different cases summarize my truth table.I now need to observe just the cases wherehead is followed by tail.This one right here and over here.Then we compute the probability for those 2 cases.The probability of picking coin 1 is 0.5.For the fair coin, we get 0.5 for heads,followed by 0.5 for tails.They're together is 0.125.Let's do it with the second case.There's a 0.5 chance of taking coin 2.Now that one comes up with heads at 0.9.It comes up with tails at 0.1.So multiply these together, gives us 0.045,a smaller number than up here.Adding these 2 things together results in 0.17,which is the right answerto the question over here.That was really non-trivial,and I'd be amazed if you got this correct.

### 14. Two Coins 4-9R44IyZ-aQI.en

And the answer is depressing.If you, once again, draw the truth table,you find, for the different combinations,that if you've drawn coin 1,you'd never see tails.So this case over here, which indeed has tails, tails.We have 0 probability.We can work this out probability of drawing the first coin at 0.5,but the probability of tails given the first coinmust be 0, because the probability of heads is 1,so 0.5 times 0 times 0,that is 0.So the only case where you might see tails/tails iswhen you actually drew coin 2,and this has a probability of0.5 times the probability of tailsgiven that we drew the second coin,which is 0.4 times 0.4 again,and that's the same as 0.08would have been the correct answer.

### 14. Two Coins 4-cDub-OOrIRE.en

Let me do this once again.There are 2 coins in the bag,coin 1 and coin 2.And as before, taking coin 1 at 0.5 probability.But now I'm telling youthat coin 1 is loaded, so give you heads with probability of 1.Think of it as a coin that only has heads.And coin 2 is also loaded.It gives you heads with 0.6 probability.Now work out for me into this experiment,what's the probability of seeing tails twice?

### 15. Summary-yepMH9VswI8.en

So there're important lessons in what we just learned,the key thing is we talked about conditional probabilities.We said that the outcome in a variable, like a testis actually not like the random coin flipbut it depends on something else,like a disease.When we looked at this,we were able to predictwhat's the probability of a test outcomeeven if we don't know whether the person has a disease or not.And we did this using the truth table,and in the truth table,we summarized multiple lines.For example, we multiplied the probability of a test outcomecondition on this unknown variable,whether the person is diseasedmultiplied by the probability of the disease being present.Then we added a second row of the truth table,where our unobserved disease variabletook the opposite value of not diseased.Written this way, it looks really clumsy,but that's effectively what we did when we went to the truth table.So we now understand that certain coin flipsare dependent on other coin flips,so if god, for example, flips the coin of us having a disease or not,then the medical test againhas a random outcome,but its probability really depends on whether we have the disease or not.We have to consider this when we do probabilistic inference.In the next unit,we're going to ask the real question.Say we really care about whether we have a disease like cancer or not.What do you think the probability is,given that our doctorjust gave us a positive test result?And I can tell you,you will be in for a surprise.

### img

## Part 06-Module 01-Lesson 07_Bayes Rule

### 01. Bayes Rules-CohZnkZMOxE.en

So this unit is a tough one.We're going to talk about perhaps the holy grailof probabilistic inference.It's called Bayes Rule.Bayes Rule is based on Reverend Thomas Bayes,who used this principle to infer the existence of God,but in doing so, he created a new familyof methods that has vastly influenced artificial intelligenceand statistics.So let's dive in!

### 02. Cancer Test-CNpSrdnYvbo.en

Let's use the cancer example from my last unit.There's a specific cancer that occurs in 1% of the population,and a test for this cancer and with 90% chanceit is positive if they have this cancer, C.That's usually called the sensitivity.But the test sometimes is positive,even if you don't have C.Let's say with another 90% chanceit's negative if we don't have C.That's usually called the specificity.So here's my question.Without further symptoms, you take the test,and the test comes back positive.What do you think is the probability of havingthat specific type of cancer?To answer this, let's draw a diagram.Suppose these are all of the people,and some of them, exactly 1%, have cancer.99% is cancer free.We know there's a testthat if you have cancer,correctly diagnose it with 90% chance.So if we draw the area where the test is positive,cancer and test positive,then this area over hereis 90% of the cancer circle.However, this isn't the full truth.The test sent out as positiveeven if the person doesn't have cancer.In fact, in our case,it happened to be in 10% of all cases.So we have to add more area,because as big as 10% of this large areais as big as 10% of this large areawhere the test might go positive,but the person doesn't have cancer.So this blue area is 10% of all the area over hereminus the little small cancer circle.And clearly, all the area outside these circlescorresponds a situation of no cancer,and the test is negative.So let me ask you again.Suppose we have a positive test,what do you think?Would a prior probability of cancerof 1%,a sensitivity and specificity of 90%,Do you think your new chances are now90%or 8%or still just 1%?

### 02. Cancer Test-FnNveASivMA.en

And I would argue it's about 8%.In fact, as we see, it will come out at 8 1/3%mathematically.And the way to see this in this diagram isthis is the region that should test as positive.By having a positive test, you know you're in this region,and nothing else matters.You know you're in this circle.But within this circle, the ratio of thecancerous region relative tothe entire region is still pretty small.It increase, obviously, having a positive testchanges your cancer probability,but it only increases by a factor of about 8,as we will see in a second.

### 03. Prior And Posterior-GlmS_jox08s.en

Obviously, P(C) is 0.01  (times) 0.9 is 0.009, whereas 0.99  (times) 0.1, this guy over here, is 0.099.What we've computed is here is the absolute area in here which is0.009 and the absolute area in here which is 0.099.

### 03. Prior And Posterior-o2Tpws5C2Eg.en

So this is the essence of Bayes Rule,which I'll give to you to you in a second.There's some sort of a prior, of which we meanthe probability before you run a test,and then you get some evidence from the test itself.and that all leads you to what's called a posterior probability.Now this is not really a plus operation.In fact, in reality,it's more like a multiplication,but semantically, what Bayes Rule does isit incorporates some evidence fromthe test into your prior probabilityto arrive at a posterior probability.So let's make this specific.In our cancer example, we know that theprior probability of cancer is 0.01,which is the same as 1%.The posterior of the probability of cancer giventhat our test is positive,abbreviate here as positive,is the product of the priortimes our test sensitivity, which iswhat is the chance of a positive resultgiven that I have cancer?And you might remember,this was 0.9, or 90%.Now just to warn you, this isn't quite correct.To make this correct, we also have to computethe posterior for the non cancer option,which there is no cancer given a positive test.And using the prior, we know that P of not C is 0.99.It's minus P of CTimes the probability of getting a positive test resultgiven not C.Realize these 2 equations are the same,but I exchanged C for not C.And this one over here takes a moment to computer.We know that our test gives us a negative resultif it's cancer free, 0.9 chanceAs a result, it gives us a positive resultin the cancer free case, with 10% chance.Now what's interesting is this is about the correct equationexcept the probabilities don't add up to 1.To see I'm going to ask you to compute those,so please give me the exact numbersfor the first expressionand the second expression written over hereusing our example up there.

### 04. Normalizing 1-5Tbd3_a5Vug.en

And, yes, the answer is 0.108.Technically, what this really means is the probability of a positive test result--that's the area in the circle that I just marked.By virtue of what we learned last, it's just the sum of two things over here, which gives us 0.108.

### 04. Normalizing 1-9SbUxcyDTaQ.en

The normalization proceeds in two steps.We just normalized these guys to keep ratio the same but make sure they add up to 1.So let's first compute the sum of these two guys. Please let me know what it is.

### 05. Normalizing 2--pOzdj6pnbA.en

The answer is 0.0833.

### 05. Normalizing 2-WYA5Zbf8HC4.en

And now finally, we come up with the actual posterior,whereas this one over here is often called the joint probability of two events.And the posterior is obtained by dividing this guy over here with this normalizer.So let's do this over here--let's divide this guy over here by this normalizer to getmy percent distribution of having cancer given that I received the positive test result.So divide this number by this.

### 06. Normalizing 3-V96RcbbVP7Q.en

Let's do the same for the non-cancer version, pick the numberover here to divide and divide it by this same normalizer.

### 06. Normalizing 3-etrUbOAoh1U.en

The answer is 0.9167 approximately.

### 07. Total Probability-_hXCgF-aMB0.en

Why don't you for a second add these two numbers and give me the result?

### 07. Total Probability-fAaE5K9OZJc.en

And the answer is 1 as you will expect.Now this was really challenging. You can see a lot of math in this slide.Let me just go over this again and make it much, much easier for you.

### 08. Bayes Rule Diagram-b8M9CWxRyQ4.en

Well, we really said that we had a situation wherethe prior P(C), a test with a certain sensitivity (Pos/C), and a certain specificity (Neg/C).When you receive, say, a positive test result, what you do is,you take your prior P(C) you multiply in the probability of this test result, given C,and you multiply in the probability of the test result given (Neg/C).So, this is your branch for the consideration that you have cancer.This is your branch for the consideration of no cancer.When you're done with this, you arrive at a numberthat now combines the cancer hypothesis with the test result.Look for the cancer hypothesis and the no cancer hypothesis.Now, what you do, you add those up and then normally don't add up to one.You get a certain quantity which happens to be the total probabilitythat the test is what it was in this case positive.And all you do next is divide or normalize this thing over here bythe sum over here and the same on the right side.The divider is the same for both cases because this is your cancer branch, your non-cancer branch,but this score does not depend on the cancer variable anymore.What you now get out is the desired posterior probability,and those add up to 1 if you did everything correct, as shown over here.This is the algorithm for Bayes Rule.

### 09. Equivalent Diagram-aUFWZ2uJuBE.en

Now, the same algorithm works if your test says negative.We'll practice this in just 1 second. Suppose your test result says negative.You could still ask the same question:Now, what's my probability having cancer or not? But now all the positives in here become negatives.The sum is the total probability of negative test results,  and we may now divide by this score,you now get the posterior probability for cancer and non-cancer assuming you had anegative test result, which of course to be much, much more favorablefor you because none of us wants to have cancer.So, look at this for a while and let's now do the calculation for the negative caseusing the same numbers I gave you before,and with the step by step this time around so it can really guide you through the process.

### 10. Cancer Probabilities-7ZLe_JP5wRY.en

And obviously this is still 0.99 as before 0.1 and 0.1. I hope you got this correct.

### 10. Cancer Probabilities-CMQBKuYjPBM.en

We begin with our prior probability, our sensitivity and our specifitivity,and I want you to begin by filling in all the missing values.So, there's the probability of no cancer, probability of negative, which is negation of positive,given C, and probability of negative-positive given not C.

### 11. Probability Given Test-41HCYR-NW-w.en

Now assume the test comes back negative, the same logic applies as before.So please give me the combined probability of cancer given the negative test resultand the combined probability of being cancer-free given the negative test result.

### 11. Probability Given Test-omC0zbJyzUY.en

The number here is 0.001 and it's the product of my prior for cancer which is 0.01,and the probability of getting a negative result in the case of cancer which is right over here, 0.1.If I multiply these two things together, I get 0.001. The probability here is 0.891.And when I'm multiplying is the prior probability of not having cancer which is 0.99with the probability of seeing a negative result in the case of not having cancer,and that is the one right over here, 0.9.So, we'll multiply 0.99 with 0.9, I actually get 0.891.

### 12. Normalizer-G9yQ_URDrDQ.en

Let's compute the normalizer. You now remember what this was.

### 12. Normalizer-W5i-gRAvZxs.en

And the answer is 0.892. You just add up these two values over here.

### 13. Normalizing Probability-V_Gqm42WodI.en

And now finally tell me what is posterior probability of cancer given that we know we had anegative test result and the probability of negative cancer given there is a negative test result.Please give me the numbers here.

### 13. Normalizing Probability-yYqN9Mf4jqw.en

This is approximately 0.0011, which we get by dividing 0.001 by the normalizer 0.892,and the posterior probability of being cancer-free after the testis approximately 0.9989, and that's obtained by dividing this probability over here by the normalizerand not surprisingly, these two values indeed add up to 1.Now, what's remarkable about this outcome is really what it means.Before the test, we had a 1% chance of having cancer,now, we have about a 0.9% chance of having cancer.So, a cancer probability went down by about a factor of 9.So, the test really helped us gaining confidence that we are cancer-free.Conversely, before we had a 99% chance of being cancer free, now it's 99.89%.So, all the numbers are working exactly how we expect them to work.

### 14. Disease Test 1-05upwXtARuo.en

Let me now make your life harder.Suppose our probability of a certain other kind of disease is 0.1,so 10% of the population has it.Our test in the positive case is really informative,but there's a 0.5 chance that if I'm cancer-free the test, indeed, says the same thing.So the sensitivity is high, the specificity is lower.And let's start by filling in the first 3 of them.

### 14. Disease Test 1-qDGSvvabN18.en

Obviously, these are just 1 minus those:0.9, 0.1, and 0.5.Notice that these two numbers may very well be different.There is no contradiction here.These guys have to add up to 1, so given C,the probability of positive and negative have to add up to 1,but these guys don't.It takes a lot of practice to understand which numbers have to add up to 1.But I set it up in a way that you should have gotten it right.

### 15. Disease Test 2-FQM7i07EqGo.en

And the answer is 0.01.P(C) = 0.1,and P(NegC) is also 0.1,so if you multiply those two they are 0.01.

### 15. Disease Test 2-GsneDVJB75E.en

Now comes the hard part:What is P(C, Neg)?

### 16. Disease Test 3-PfEYA6z-19w.en

And what's the same for P(C, Neg).

### 16. Disease Test 3-a61GPGk-Qy4.en

And the answer is 0.45.P(C) is 0.9, and P(NegC) is 0.5.So 0.9 * 0.5 = 0.45.

### 17. Disease Test 4-UERKMwmkAsM.en

Well, you just add up these two numbers to get 0.46.

### 17. Disease Test 4-ztkKTrMZHXg.en

What's the score over here?

### 18. Disease Test 5-4qW7a5E74No.en

So tell me what the final two numbers are.

### 18. Disease Test 5-nUxwwMNKIYo.en

The first one is 0.01 divided by normalized 0.46 and that gives us 0.0217,and the second one is called over here 0.45 divided by 0.46 and that gives us 0.9783and uses the correct posteriors, restarted our chance of 10% of having cancer.We had a negative result. We're down now to about 2% of having cancer.

### 19. Disease Test 6-OdVAt79eQak.en

So once again, we have 0.9, 0.1, and 0.5 over here.Very quickly multiplying this guy with this girl over here 0.09. This guy with this girl over here 0.45.Adding them up gives us 0.54, and dividing those correspondingly 0.9 divided by 0.54gives us 0.166 and so on and 0.833 and so on for dividing 0.45 by 0.54.And with this means, with the positive test result, our chance of cancer increased from 0.1 to 0.16.Obviously, our chance of having no cancer decreased accordingly.You got this, so let's just summarize.

### 19. Disease Test 6-cdFrLeXIkZU.en

Let's now consider the case that the test result is positive, and I want you tojust give me the two numbers over here and not the other ones.

### 20. Bayes Rule Summary-RgXQ8GRsjfc.en

In Bayes rule, we have a hidden variable we care about--whether they have cancer or not.But we can't measure it directly and instead we have a test.We have a prior of how frequent this variable is true and the test is generally characterizedby how often it says positive when the variable is trueand how often it is negative and the variable is false.Bayes rule takes a prior, multiplies in the measurement,which in this case we assume to be the positive measurement to give us a new variableand does the same for all actual measurement, given the opposite assumption aboutour hidden variable of cancer and that multiplication gives us this guy over here.We add those two things up and then it gives us a new variable and thenwe divide these guys to arrive the best estimate of the hidden variable c given our test result.And this example, I used the positive example is a test resultbut it might do the same with a negative example.This was exactly the same as in our diagram in the beginning.There was a prior of our case, we have this specific variable to be true.We noticed inside this prior, it can cover the region for which our test result applies.We noticed that test result also apply when the condition is not fulfilled.So, this expression over here and this expression over here corresponds exactlyto the red area over here and the green area over here.But then we noticed that these two areas don't add up to 1.The reason is that's lots of stuff outside, so we calculated the total areawhich was this expression over here, pPos.And then we normalized these two things over here by the total area to get the relative areathat is assigned the red thing versus the green thing and at this time by just dividing by the total areain this region over here; thereby, getting rid of any of the other cases.

### 21. Robot Sensing 1--TBAfU1cjRU.en

In this example, it gives us funny numbers. It was 3 for red as 0.8 and one for the green as 0.2.And it's all to do with the fact that in the beginning where there had no clue where it is.The joint for red after seeing red is 0.4. The same for green is 0.1. 0.4+0.1, S to 0.5.If you normalized 0.4 divided by 0.5, you get 0.8, and if you normalized 0.1 by 0.5, you get 0.2.

### 21. Robot Sensing 1-_DjfTytro6I.en

Now, I should say if we got this, you don't find any immediatesignificant about statistics and probability.This is totally nontrivial, but it comes in very handy.So, I'm going to practice this with you using a second example. In this case, you are a robot.This robot lives in a world of exactly two places. There is a red place and a green place, R and G.Now, I say initially, this robot has no clue where it is,so the prior probability for either place, red or green, is 0.5.It also has a sensor as it can see through its eyes, but his sensor seems to be somewhat unreliable.So, the probability of seeing red at the red grid cell is 0.8,and the probability of seeing green at the green cell is also 0.8.Now, I suppose the robot sees red.What are now the posterior probabilities that the robot is at the red cell given that it just saw redand conversely what's the probability that it's at the green cell even though it saw red.Now, you can apply Bayes Rule and figure that out.

### 22. Robot Sensing 2-aBBmlnd7okQ.en

And the answer is,the prior isn't affected by the measurement,so the probability of 0 is at red,and the probability of 1 at green,despite the fact that it's all red.To see this, you find the joint of seeing it red and seeing redis 0 times 0.8, that's 0.That's the same join for green is1 times 0.2.So you have to normalize 0 and 0.2.The sum of those is 0.2.So let's divide 0 by 0.2, gives us 0,and 0.2 divided by 0.2 gives us 1.These are exactly the numbers over here.

### 22. Robot Sensing 2-t22oDruXhuo.en

If I now change some parameters--say the robot knows the probability that it's red,and therefore, the probability 1 is under the green cell as a prior.Please calculate once again using Bayes rule these posteriors.I have to warn you--this is a bit of a tricky case.

### 23. Robot Sensing 3--6l4_oprDOk.en

To change this example even further.Let's make this over here a 0.5and revert back to a uniform prior.Please go ahead and calculate the posterior probability.

### 23. Robot Sensing 3-m1LSU9SPZ2k.en

Now the answer is about 0.615or 0.385.These are approximate.Once again, 0.5 times 0.8 is 0.4.0.5 minus this guy is again 0.5.0.25, add those up,0.65,normalizing 0.4 divided by 0.65gives approximately 0.615.0.25 divided by 0.65 is approximately 0.385,so now you you've got it.

### 24. Robot Sensing 4-d_fbDqAGVdE.en

And just like before, we multiplythe prior, this guy over here,that gives you 0.3.

### 24. Robot Sensing 4-vasdN2Gol0M.en

I will now make your life really hard.Suppose there are 3 places in the world, not just 2.There are a red one and 2 green ones.And for simplicity, we'll call them A, B, and C.Let's assume that all of themhave the same prior probability of 1/3 or 0.333, so on.Let's say the robot sees red,and as before, the probability of seeing redin Cell A is 0.9.The probability of seeing green in Cell B0.9.Probability of seeing green in Cell Cis also 0.9.So what I've changed is,I've given the hidden variable,kind of like the cancer/non cancer variable,3 states.There's not just 2 as before, A or B.It's now A, B, or C.Let's solve this problem together,because it follows exactly the same recipe as before,even though it might not be obvious.So let me ask you,what is the joint of being in Cell Aafter having seen the red color?This is the joint as before.

### 25. Robot Sensing 5-PGG9agooCvw.en

Well, the answer is you multiply our prior of 1/3with the probability of seeing red in Cell B,as seeing green at 0.9 probability,so red is 0.1.So 0.1 times this guy over heregives 0.033.

### 25. Robot Sensing 5-tIrqdYTT_9Q.en

What's the joined for Cell B?

### 26. Robot Sensing 6-Se-ddM2Wdac.en

Finally, probability of C and Red.What is that?

### 26. Robot Sensing 6-hXyXlk0gYzk.en

And the answer is exactly the same as this over here,because the prior is the same for B and C,and those probabilities are the same for B and C,so they should be exactly the same.

### 27. Robot Sensing 7-clFL503NPyY.en

And the answer is, you just add those up.

### 27. Robot Sensing 7-goEMc0w58xM.en

So here's the $100,000 question.What is our normalizer?

### 28. Robot Sensing 8-hyAQ28MYmc4.en

And now we calculate the desired posterior probabilityfor all 3 possible outcomes.So please plug them in over here.

### 28. Robot Sensing 8-lmuonrQp_lM.en

As usual, we divide this guy over hereby the normalizer,which gives us 0.818.Realize all these numbers are a little bit approximate here.Same for this guy, it's approximately 0.091.And this is completely symmetrical, 0.091.And surprise, these guys all add up to 1.

### 29. Generalizing-SdMk3aROgSc.en

So what have you learned?In Bayes Rule, there will be more than just 2underlying causes of cancer/non cancer.There might be 3, 4, or 5, any number.We can apply exactly the same math,but we have to keep track of more values.In fact, the robot might also have more thanjust 2 test outcomes.Here was red or green, but it could be red, green, or blue.And this means that our measurement probability will be more elaborate.I have to give you more information,but the math remains exactly the same.We can now deal with very large problemsthat have many possible hidden causesof where the world might be,and we can still apply Bayes Rule to find all of these numbers.Let me give you one final test.

### 30. Sebastian At Home-R4zq6mPPMxs.en

And I get 0.0217, which is a really small thing.And the way I get there is what taking home times the probability of rain at homenormalizing it using the same number of a year plus the calculation for the sameprobability of being gone is 0.6 times the rain I've been gone has a probability of 0.3and that results is 0.0217 or the better of 2%--did you get this?If so, you now understand something that's really interesting.You're able to look at a hidden variable,understand how a test can give you information backabout this hidden variable and that's really coolbecause it allows you to apply the same schemeto great many practical problems in the world--congratulations!In our next unit, which is optional, I like you to program all of this so you can try the same thingin an actual program interface and writes software that implements things such as Bayes rule.But not to worry, this is optional.If you don't know how to program just skip the next unit.

### 30. Sebastian At Home-TtmQ7YCw_1Y.en

This test is actually directly taken from my life and you'll smile when you see my problem.I used to travel a lot. It was so bad for a while.I would find myself in a bed not knowing what country I'm in. I kid you not.So let's say, I'm gone 60% of my time and I'm at home only 40% of my time.Now at summer, I live in California and it truly doesn't rain in the summer.Whereas in many of the countries I have traveled to, there's a much higher chance of rain.So let's now say, I lie in my bed, here I am lying in bed, and I wake upand I open the window and I see it's raining.Let's now apply Bayes rule--What do you think is the probability I'm homenow that I see it's raining--just give me this one number.

### 32. Reducing Uncertainty-zuFMhmKQ--o.en

Let's talk a bit more about why uncertainty is soimportant in the field of robotics and self-driving cars.We know that measurements like the speed, the direction,and the location of a car are challenging to measure and we can't measure them perfectly.There's some uncertainty in each of these measurements.We also know that many of these measurements affect one another.For example, if we are uncertain about the location of a car,we can reduce that uncertainty by collecting dataabout the car's surroundings and its movement.Self-driving cars measure all of these things from a car's speed,to the scenery and objects that surround it, with sensors.And though these sensor measurements aren't perfect,when the information they provide is combinedusing conditional probability and something called Bayes rule,they can form a reliable representation of a car's position,its movement and its environment.Let's take a look at Bayes Rule.

### 33. Bayes' Rule and Robotics-meNSO42JF6I.en

Bayes rule is extremely important in robotics and it can be described in one sentence.Given an initial prediction,if we gather additional related data,data that our initial prediction depends on,we can improve that prediction.For example, let's say our initial prediction also known as a prior belief,is an estimate of a car's location on a road.This might be the location given by a slightly inaccurate GPS signal.Then, we use sensors to gather dataabout the car's surroundings and how the car is moving.How do you think this sensor data can help us improve our initial location prediction?

### 35. Using Sensor Data-vhl-SADfti8.en

Once we gather sensor data about the car's surroundings and its movement,we can then use this information to improve our initial location prediction.For example, say we sense lane markers and specific terrain, and we say, hmm.Actually, we know frompreviously collected data that if we sense landlines close to the sides of the car,the car is probably located in the center of the lane.We also know that if we sense that our tires are pointing to the right,we're probably on a curved section of the road.So this sensor data,combined with what we already know about the road and the car,gives us more information about where our location is most likely to be.So using the sensor information,we can improve our initial prediction and better estimate our car's location.Bayes rule gives us a mathematical way to correct our measurements,and let's us move from an uncertain prior belief to something more and more probable.You'll see Bayes rule come up again and again in robotics.And in this lesson, you'll gain a greater understanding of Bayes rule.

### 37. Bayes Rule Conclusion-vlfDGCD8w0s.en

Great job finishing this lesson on Bayes rule.At this point, you've now gained a ton of valuable knowledge about probability,conditional probability and Bayes rule.To reinforce your understanding of these topics,let's go through some probability practice using Python.You'll do this in the next lesson.

### img

## Part 06-Module 01-Lesson 08_Python Probability Practice

### 01. Python Probability Introduction-tFMdvAN7WDY.en

Now that you've seen some examples and the math involved with them,you're going to apply this knowledge to problems using Python.This lesson includes screencast and Jupyter notebooks to help youpractice using Python to explore the topics ofprobability you just learned. Let's get started.

### 02. Simulating Coin Flips-7YtQNZ3iy6o.en

NumPy has a module for random sampling,that makes it really easy for us to simulate random events like coin flips and Python.We're going to start with a simple example,simulating a single coin flip.For this, we'll use a function called randint.This generates however many random integers,we specify between a lower bound inclusive and upper bound exclusive.We can use these integers to represent the outcomes of our events,like coin flips. Let's try this out.We call randint from NumPy's random sampling module like this.Let's use zero to represent heads and one to represent tails.Let's make this function randomly produce zero or one.The lower bound would be zero and the highest would be two, because it's exclusive.Since the lower bound is zero,this is actually the default.We don't need to include it.If we run the cell again and again,we'll keep getting a random outcome of zero or one or,we can specify a size to just give us more events.Cool. Now, we have the results of 10,000 random coin flips.The average of these outcomes produced here should be very close to 0.5,since right now there's an equal probability of getting a zero or a one.But what if we want to flip a biased coin,that had a higher probability of landing on heads.There's actually another function for this called random.choice.This function works a little differently.It randomly chooses a number of values from an array that you provide and youcan also give it a set of probabilities for each value in that array.Let's call random.choice and give it the array of the possible outcomes zero and one.If we don't specify probabilities,it gives us each value and an equal probability by default.We can run this similar to the above example in this way.So the average is very close to 0.5 Again.To make this a biased coin,we can specify the parameter p with an array of probabilities.Say 0.8 for heads and 0-2 for tails.Now, you can see that the mean is close to 0.2 which makessense because the zero or heads should be chosen 80% of the time.You probably noticed that the mean values we get fromthese outcomes don't always reflect the true probabilities perfectly.However, they do tend to reflect the true probabilitymore closely as we increase the number of flips.You'll learn more about this later.

### 04. Simulating Many Coin Flips-AqpWQIj2V5Y.en

So far, we've been stimulating event outcomes by generatingrandom numbers with NumPy's random.randint and random.choice.However, there is a better function forsimulating large binomial experiments like coin flips.Here, you see NumPy's random.binomial function,which simulates a number of events n,which each have probability of success p.Success just represents one of the two outcomes of the event.Really, either outcome could be the success.For example, if our event is flipping a fair coin 10 times,we could define success as the number of heads.n would be the number of flips.In this case, 10,and p would be the probability of heads for each flip which is 0.5.To try this out,let's set n equal to 10 and p equal to 0.5.Notice, this returns 1 integer instead of an array of 10 outcomes.Since this function is only for binomial outcomes,it can simplify the output by just returning the number of successes.In this case, 4.This is the number of heads.Again, we can run the simulation many times.Let's run this 20 times.Each number in this array represents the number ofheads that resulted from each test of 10 coin flips.This is the number of heads on the first 10 coin flips,while this one is the number on the second 10 coin flips and so on,for 20 different flippings of the coin.Let's find the mean number of heads for these tests.Since this is a fair coin,we would expect this mean to be close to 5.You can see that this is a little bit of a ways away,probably because we only ran 20 tests.Let's see what happens if we increase the number of tests.Since these events are random,it's not guaranteed that each simulation will perfectly average five heads.However, as this number of tests increase,the simulation more closely reflects the fairness of the coin.With 10,000 tests, the mean is much closer to five heads.Let's use matplotlib to plot a histogram of the outcomes in this simulation.As expected, the distribution is centered around five heads.

### 08. Python Probability Conclusion-4JYar5GykXk.en

In this lesson, you put your new probability skills to practice using Python.In order to gain an understanding of some of the complex topics in the next sections,we'll be working with Python more and more.It's often easier to understand these complex ideas throughsimulations and Python than it is to prove them mathematically.You will see a bit of both in the upcoming lessons.So your practice here will definitely come in handy.

## Part 06-Module 01-Lesson 09_Normal Distribution Theory

### 01. Maximum Probability-5zkupL6EWh8.en

I just programmed and ran the experiment,and the answer is 10.The reason why the answer is 10 is because the number of combinations to place 10 positivesand 10 negatives into our list of 20 is larger than any other number.This term over here is maximized when k is exactly half of N--so 10.

### 01. Maximum Probability-b2zvrFL8AUw.en

What you are about to see is one of the most transformative parts of modern statisticsand it uses things if like we've never seen before.We start out with the binomial distribution that you're familiar with from our last unitand then we move into the central limit theorem which basically meanswe take a number of coin flips to infinity.From that, we arrive at the normal distribution which is basis to so much in statistics--all of testing and confidence intervals are defined though the normal distribution.And the reason why this matters is much of what we've done in coin flips had one or two coin flipsbut in statistics experiments, you often have 1000 of patients or 1000 of data pointsand then starting the normal distribution as an approximationto the binomial distribution is much more practical.So let's start--so when we start with our now well established formula for binomial distributionswhere N is the number of coin flips, k is how often it comes up with headsand p is the probability that the coin comes up with heads, usually 0.5.And I want to graph for fix n this function here. K can go from 0 all the way to 20.Instead of letting you compute this thing over here for all those different values of k,I'm going to ask a different question--suppose you have a fair coin,what do you think this value takes on its maximum value, which value of k maximizes our expression.I know you can't really know this but with some thought,I believe you'll arrive at the correct answer.

### 02. Shape-DjsL64Kjr1Q.en

The other interesting thing is things fall down in the interesting fashionas you deviate from 10, 11, 12, 13, 14 all the way to 0 or 20.Obviously we got a curve that looks a bit like this.This curves is often called a bell curve because it's quite feasible to think of it as a church bell--that's move left and right and rings the bells.I did a related experiment--actually I bought a piece of software to flip a coin 1000 timesand if you did the last optional units on programming, you wrote a piece of softwareto flip a coin a 1000 times and from that, I looked at the empirical frequencywhich is the same as that count of heads divided by a 1000, but this one scales between 0 and 1.I called this thing an experiment--I flip the coin a 1000 times, out comes a one singular numberwhich is the ratio of heads to the total number of experiments.It should be 0.5 in the ideal case but often it's a little bit off 0.5.I repeated this experiment 1000 times and that meansI've got 1000 samples of this ratio over here. When I do this, I got a whole bunch of means.When I run a histogram over those, I might see a curve.What shape do you think the curve has--is it going to be like this, is it going to be like this,like this or like this, all are focused on 0.5.Which one do you think is it?

### 02. Shape-w5qcGO8krMw.en

And it's this one. Let me show you.Here's a typical one, and I apologize the axis over here can't really be read,but you can take with faith that the center is 0.5,and you can see the characteristic bell curve for this simple coin-flipping experiment.For this run the mean was 0.50006.If I run it again, I get a different sample.And there is some randomness involved as this bar over here illustrates,but over the randomness you can clearly see the bell-shaped curve that flattens off on the sides.

### 03. Better Formula-vMAl1m8ZtoI.en

The answer is a resounding yes.Even more so, what I'm going to show you doesn't just applyto binomial distributions with fair coins.It applies to almost any distribution that is sampled many, many times,which is a  very deep statistical result.I will construct for you the formula that is being used.

### 03. Better Formula-z2xsu2Kehyo.en

The question really is can we find a better formula for this bell-shaped curve.The answer is, well, take your guess.

### 04. Quadratics-1R44jvxIPJY.en

I will define for you a normal distribution with a specific meanthat's often called , Greek letter ,and a variance that's often called .We already know that variance is a quadratic expression.In normal land we often use  and . Let's do this.The very first element is that for any outcome x we write the quadratic differencebetween this outcome x and .This is indeed a function in x. So, look at this.Here are four possible hypotheses of what this function might look like.Each case is  is on the right side some where.The horizontal axis is x, and we're graphing f(x).The first I'll give you is a triangular function.The second is a quadratic function.The third one is a negative quadratic function.And the fourth one is a quadratic function that doesn't quite touch .So, which one in your opinion best describes this formula over here?

### 04. Quadratics-GzRNoodJZxk.en

I would submit that it's this one over here.The reason is this expression is 0 when x = .As a result, it can't be the fourth of these choices.It's strictly non-negative, so it can't go down into the negative area,so this one is being out ruled. It's quadratic.Hence, a function like this doesn't make sense, so it must be this one over here.

### 05. Quadratics 2-HjpgML5zsUE.en

The next thing I'll do is I'll divide it by .Without telling you why I'm doing this, I want to see what the effect is.Suppose  = 4. That means we have a variance of 4 and a standard deviation of 2.I've given you already the quadratic function when it isn't divided by .It's the same as saying  = 1.What I'd like to know is whether our new version where  = 4makes this quadratic wider or whether it makes it narrower,assuming that this is our new function f(x).So, pick one of the two, or perhaps it stays the same. Then pick the third.

### 05. Quadratics 2-N-wpkttwcoA.en

The answer is it makes it wider.To see why is this affects the vertical dimension--the outputand scales it down by a factor of 4.So that we said before gets dragged down by a factor of 4.That means this point over here finds itself here.This one over here finds itself here.That means we'll widen out the quadratic.Observe that large variances yield wide quadratics.Small or tight variances yield sharp quadratics.

### 06. Quadratics 3-Ny2vcRZ6Aws.en

In particular, if we now look at the quadratic over here, which is much tighter,which of the following potential  would you think is best representativeof this narrow function over here, provided that this is the quadratic that correspondsto  = 1. Check one of those four--4, 1, , and 0.

### 06. Quadratics 3-YSMWpFM92S0.en

It follows it's got to be something like a quarter.We already learned that 4 widens the quadratic, so that can't be it.One is already shown over here.Zero makes no sense because we have division by 0which you can think of as a quadratic that shoots up almost like a straight line,but honestly it doesn't make any sense.A quarter is the one that really describes this particular quadratic the best.Now, that's great. Now we understand this expression over here.

### 07. Quadratics 4-yimIE9fCvi8.en

Let's go further, and let's now take this function and multiply it by -.Again, I ask you what the affect it.If this is your original quadratic, then what do we get?We already know that it's going to flatten it, because you are dividing the f value by half,but are we going to get something like this or perhaps something like this?Pick one of those two choices.

### 07. Quadratics 4-zB2Y-5YEIec.en

And quite interestingly, we inverted the sign,so all of a sudden the function is negative.Quite obviously, green is the correct answer.So, now we have a quadratic that points down into the negative spacewhose maximum value is 0 and otherwise, it's strictly negative. That's this function f over here.

### 08. Maximum-02v8ui9riew.en

to understand the solution, it's useful to draw the exponential function.e is 1 and then it goes up exponentially to really large numbers.If you've ever heard Ray Kurzweil  talk about the future of society,you've seen these curves--everything goes up exponentially.Everything is just exponential.And further, if you go back in time to negative values,this thing slowly drifts down to 0.Of course, that's not very exciting, so we never talk about exponential in the negative space.However, it turns out that all the arguments of the exponential are at best 0and otherwise are negative, because the exponential is monotonic--that is the larger its argument, the larger its exponential value.It ought to be optimized where this thing over here is the largest,and where is that the case?Well, it's exactly where  hits 0.

### 08. Maximum-MZoYGBZTh-g.en

And now I'm going to take the most extreme of all steps.I'm going to make this the exponent of the e function.Remember, the inner argument is a quadratic that points down.This a bit does depend on . This mean is  so I call this f(x) where f(x) maximize.And I'll give you several choices for x=, x=0, x=-infinity, or x=+infinity.Where will this thing be the largest?

### 09. Maximum Value-rjpcSymYulE.en

Quite interestingly, even though this formula looks complex, it a really easy answer,which is when x =  this thing here is 0.That makes the entire thing 0.e--any value to the 0--is going to be 1.

### 09. Maximum Value-z_eElEkVOPY.en

Let me ask you another question.What is the value of this function if we go to the point where it's maximum,which is x = ? That's the way to write this.Compute for me in your head this what this thing will be when x = .

### 10. Minimum-MEbJxfw3NVs.en

The answer is now  .If you look at this, if you put a really large positive or negative value in,the difference to any  will be enormous.The square will be even more enormous.Therefore, this entire expression on the right side will be huge.Put a minus sign in front of it, and you have a hugely negative number.You have e^-.e^- drive the e curve all the way to the left where this just ends up to be minimized.

### 10. Minimum-tiv8VKPL7jg.en

Next I'd like to know where is f(x) minimized?For what value of x would we get the possible smallest valueof this entire expression over here?Again, pick one or more of those choices over here.

### 11. Minimum Value-LNzmJUj8K8w.en

In fact, what do you think is the value of this where x = ?

### 11. Minimum Value-LconwqN7hJs.en

The answer is 0.As x goes to infinity, this expression goes to negative infinity.The exponential of negative infinity converges to 0.So, now we've got basically the normal distribution function.

### 12. Normalizer-mQ_IjrtmmAk.en

We have a function f that assumes the value 1 when x = that goes to 0 when x goes to .It so happens that it looks like a bell curve.The fact that it looks like a bell curve is not entirely obvious,but you have to take my word for it.This--what I would consider a relatively simple formula--describes the limit of making infinitely many coin flips.In fact, it describes the limit of computing a mean over any set of experiments.This is a very powerful result.No matter what you do when you drive n to very large numbers you get a bell curve like this.There is one flaw here, and I'll tell you about the flaw without going into detail.That is the area underneath this curve doesn't always add up to 1.In fact, without proof, it adds up to 2.The reason why this matters is deeply buried in probability theory.But it turns out we want all these areas to add up to 1 just as much as we wanteda coin flip and its complement to add up to 1.The true normal distribution is normalized by just the inverse of this thing over here--1/2.So, that is the normal distribution of any value x indexed by the parameter  and .So, this is a very deep piece of mathematics.Now, we will apply it a little bit for you to practice how the normal distribution looks in the field.

### 13. Formula Summary-zqo1RJEHT_0.en

So, here is our normal distribution again.I'm going to write it as "exp" for exponential         {- (x - )/}.The truth is when you're new to this this looks really cryptic.When you're with statistics for many years as I have been,you wake up in the middle of the night and you can recite this formula.It's as normal as getting breakfast in the morning or having a beer after dinner.What I want to get into your brains is not the complexity of the formula.I want you to really understand how this formula is constructed.I you to understand the quadratic penalty term of deviations from the expectationof the mean of this expression.Then the exponential that squeezes it back into the curves.That's basically what it is.We can draw values from this normal distribution just the same way as we flipped coins before.The way to look at this is any value x has this probability up here.This is nothing else but a notation of the probability of x for a normal distribution with  and .So, a value x that has twice the high bar than some other value x'will have twice as much of a probability of being drawn.Now, obviously, the normal has an entire continuous space of outcomes.And obviously that renders each individual outcome of probability 0.But, in essence, you can think of the height of this thing over hereas being proportional to the probability that this value is being drawn.

### 14. Central Limit Theorem-36KLIHioAvA.en

As I'm sure you've guessed this is the probabilityfor a single coin flip. This is the formula wecan use for multiple coin flips and as wego to very large numbers, the [UNKNOWN] is often agood approximation for the outcome of many coin flips.I should warn you, if the coin flip is zeroto one, then the mean is always in thepositive. This can assume negative values, but it's an approximation

### 14. Central Limit Theorem-9I8ysrRlmbA.en

So let's look for a second at different waysto compute probabilities for coin flips. You have acoin that has probability P of coming up heads.I will give you now 3 very different formulas.Here's a single probability, here's our common [UNKNOWN] formulaand here's our Gaussian exponential. So what I wantto know from you is which one of theseformulas is best suited to compute a probability fora single coin flip, a few coin flips, or many coin flips,possibly even infinity coin flips as a limit value? So click exactlythree of those buttons, one each row and one each column forthe best correspondence of the formula to the case on the left

### 15. Summary-VP-PMcgqhc8.en

What I've shown you in the beginning of class have from a coin flip to a binomial distributionall the way to a normal distribution, and you might think that this was challenging and indeed it was.As it turns out, you can treat all this things about the same.In fact, if you're a medical doctor and you have one patient, you might think of it as a coin flip.If you have 10 patients, you might think of it as binomial distribution.If you do what's normally done, when your test say a new drug and you have, say 10,000 patientsthen this thing over here is a beautiful and very compact representation of it.Otherwise, it would be almost impossible to compute.That's the purpose of normal distribution for the sake of this class.As you go forward and look into hypotheses testing and confidence then develops.You don't do this for this relatively complicated expressions over here.We just do it for the normal distribution that I think relatively easy to compute.Welcome to the world of normal distributions.

### img

## Part 06-Module 01-Lesson 10_Sampling distributions  and the Central Limit Theorem

### 01. Introduction-SvdlBB-ZjcQ.en

In this lesson, you'll be learning about sampling distributions.In order to gain a firm grasp of how sampling distributions work,it's important to first have a strong grasp of inferential statistics.We will do a recap in the next concepts to make sureyou're comfortable with the ideas surrounding inferential statistics.This is also a shift from earlier content,where you were thinking of probability functions.From this point, you'll be thinking more about statistics.That is, we'll be learning from data to draw our conclusions,rather than using probability to draw our conclusions.

### 02. Descriptive vs. Inferential Statistics-XV9pd8-RZ78.en

The topics covered this far have all been aimed at descriptive statistics.That is, describing the data we've collected.There's an entire other field of statisticsknown as inferential statistics that's aimed at drawingconclusions about a population of individualsbased only on a sample of individuals from that population.Imagine I want to understand what proportion of all Udacity students drink coffee.We know you're busy,and in order to get projects in on time,we assume you almost drink a ton of coffee.I send out an email to all Udacity alumni andcurrent students asking the question, do you drink coffee?For purposes of this exercise,let's say the list contained 100,000 emails.Unfortunately, not everyone responds to my email blast.Some of the emails don't even go through.Therefore, I only receive 5,000 responses.I find that 73% of the individuals that responded to my email blast,say they do drink coffee.Descriptive statistics is about describing the data we have.That is, any information we have and share regarding the 5,000 responses is descriptive.Inferential statistics is about drawing conclusionsregarding the coffee drinking habits of all Udacity students,only using the data from the 5,000 responses.Therefore, inferential statistics in our example is all about drawing conclusionsregarding all 100,000 Udacity students using only the 5,000 responses from our sample.The general language associated with this scenario is as shown here.We have a population which is our entire group of interest.In our case, the 100,000 students.We collect a subset from this population which we call a sample.In our case, the 5,000 students.Any numeric summary calculated from the sample is called a statistic.In our case, the 73% of the 5,000 that drink coffee.This 73% is the statistic.A numeric summary of the population is known as a parameter.In our case, we don't know this value as it'sa number that requires information from all Udacity students.Drawing conclusions regarding a parameter based on our statistics is known as inference.

### 06. Example of Sampling Distributions - Part I-1XezzP6kxUE.en

In this video, we'll be defining the term Sampling Distribution,and looking at an example of one specific sampling distribution.A sampling distribution is the distribution of a statistic.This could be any statistic,but what does it really mean,to look at the distribution of a statistic?Consider again the coffee drinking habits of all Udacity students.Let's say, each of these cups represents a student,and a green cup represents a student that drinks coffee,while a red cup represents a student that doesn't drink coffee.And even though there are more students than we represent here,pretend that this represents all Udacity students.If we were to select this group of students to ask about their coffee drinking habits,what would our population,parameter, sample and statistic be?Use the quiz below to answer.You may want to pause the screen here to return and check your answers.

### 07. Example of Sampling Distributions - Part II-PKf3Nu6zAxM.en

If we wanted to identify the sample and statistic, from this visual,we would only use these cups,which give a sample of five students where four of them don't drink coffee.This gives us a statistic that 20% of students drink coffee.Remember, a population is our entire group of interest.Therefore, we have a population of 21 students,and a portion that drink coffee of 71%.Moving back to our sample and statistic,what if we didn't select these five individuals,but instead we selected these five here?How would you now answer the questions regarding the population,parameter, sample and statistic?

### 08. Example of Sampling Distributions - Part 3-E_4lvTWkSNI.en

So you probably notice that though our sample is still five students,our statistic changed, because we chosefive different students than were chosen in the first sample.We could select all possible combinations of five cups,and we could recompute the proportion of coffee drinkers for each of these samples.If we were to look at how these statistics change from one sample to the next, that is,if we looked at the distribution of the proportions across all samples of size five,this is what is known as the sampling distribution.

### 11. Introduction To Notation-ISkBSUVH49M.en

It's important to understand notation.You might not even know it,but you use notation all the time.Consider this example of five plus three.Plus is an English word.This symbol is notation, and it's universal.Notation is a common math language used to communicate.Regardless of whether you speak English, Spanish, Greek,or any other language,you can work together using notation as a common language to solve problems.Like learning any new language,notation can be frightening at first.But it's an essential tool for communicating ideas associated with data.We will work together through some examples to assure you completely master this concept.

### 12. Notation Parameters vs. Statistics-webref_dLrA.en

There are common ways to notateparameters that are different than the way we notate statistics.In general, parameters are notated with Greek symbols,where statistics are either notated bylower case letters or the same Greek symbol with a hat on it.Here, this symbol called mu represents the mean of a population,while these represent the mean of a sample.Similarly, this Greek symbol calledsigma represents the standard deviation of a population,while either of these can be used to represent the standard deviation of a sample.This same pattern continues.Here, you can see common parameters and statistics.You can see the parameter and statistic values fornot only the mean and standard deviation but also for the variance,proportions, and the values of coefficients and regression.You'll see all of these in later lessons in the statistics class.This notation will be consistently used throughout these lessons,as well as beyond this course,in books, blogs, and other use cases.

### 14. Other Sampling Distributions-Bxl0DonzX8c.en

You now have seen how a sampling distribution provides how a statistic varies.As we saw, the proportions change with different samples of students.However, we might also look at the distribution of other statistics,like how the sample standard deviation, the variance,difference in mean or any other statistic varies from one sample to the next.Notice, we are not looking at the distribution parameters.As you saw in the first example,the parameter that is a numeric summary of a population is a fixed value,so these values do not change.However, statistics will change based on a sample you select from the population.So what are the common traits of sampling distributions??

### 15. Two Useful Theorems-jQ5i7CALdRQ.en

There are two mathematical theorems that arecommonly discussed when looking at sampling distributions,the Law of Large Numbers and the Central Limit Theorem.First, let's go through the Law of Large Numbers.This theorem makes a lot of sense and it tells us that ifwe choose the right statistics to estimate a parameter,the larger our sample size,the closer the statistic gets to the parameter.Makes a lot of sense, right?If you want to know the mean of a population and you estimate it with the sample mean,the larger the sample size,the better your sample mean will be,estimating the population mean.So you might be wondering,what makes a statistic the right estimate of a parameter?There are a number of ways to estimate parameters.In the instructor notes below,I've provided links to some of the most popular methods.

### 17. Two Useful Theorems - Central Limit Theorem-L79u8ywRmG8.en

The second theorem is one of the most popular theorems in all of statistics.And it pertains specifically to the sample mean and sample proportions statistics.The central limit theorem states,that with a large enough sample size,the sampling distribution of the mean will be normally distributed,since a proportion is like a mean of zero and one data values,it also abides by the central limit theorem.To give you some practice with this,work through the notebook and quiz questions that follow.

### 20. When Does the CLT Not Work-uZGTVUEMfrU.en

You now have gained some intuition for how the Central Limit Theorem works.But it doesn't work for all sampling distributions.Sure, a mean and proportion are normally distributed with large enough sample sizes.But what does it mean for a sample size to be large enough?Is a sample size of 10 large enough?Or 30? Or 100?You might memorize a list of statistics for which the Central Limit Theorem applies.It applies for the mean, and for proportions,and applies for the difference in means,and difference in proportions.However, the Central Limit Theorem doesn'tapply to the sampling distribution for the variance,or for the correlation coefficient.It doesn't apply for the sampling distribution of the maximum value in a data set.With all the data being collected in the world,and tons of compute power available at our fingertips,the Central Limit Theorem is still useful for some sampling distributions.But instead of relying on this theorem,what if we just simulated whatever sampling distribution we were interested in instead?

### 22. Bootstrapping-42j3YclcZ4Q.en

So in the last video,we talked about how relying on mathematical theorems,like the central limit theorem leads togaps in whether we've achieved a large enough sample size or,which statistics the theorem applies to,and that instead of relying on theorems we could simulate the sampling distribution.This introduces a technique known as bootstrapping.Bootstrapping in statistics, means sampling with replacement.If we want to bootstrap sample five individuals from this group,we could randomly sample these five here.However, in bootstrap sampling,we are sampling with replacement.So we could actually end up sampling these four,and this one might end up being sampled again.We could look at this a bit slower.In sampling, we might end up choosing this individual.And immediately after choosing it as a part of the sample,it goes back to potentially be chosen again.It is possible, though unlikely,in bootstrap sampling to choose the same individual for every random draw.We could end up choosing this individual or any of these five times in a row.After a little more practice with this,you'll be ready to apply bootstrapping to simulate sampling distributions.

### 23. Bootstrapping  the Central Limit Theorem-GJGUwNr_82s.en

Here's the idea of using bootstrapping tosimulate the sampling distribution for any statistic.This might take more than one time through to fully grasp. So bear with me.We know that in inferential statistics,we want to use a statistic to try and saysomething about the corresponding population parameter.Imagine we treat our sample as if it were the entire population.So although these 21 cups represent only a sample of Udacity students,imagine we treat them as though they were in our entire population.If these 21 individuals are truly representative of our population,we can bootstrap sample from them to understand howthe proportion of coffee drinkers might change from one sample to the next.Here, I set up an array of 21 ones andzeroes to represent the cups and the previous image.I also set the seed,in case you want to follow along and get the same results.We can calculate the proportion of coffee drinkers in our original sample.Now, let's take our first bootstrap sample ofthe same size as the original sample and calculate the proportion of coffee drinkers.Notice, this proportion doesn't match the original because we'renot just sampling the original observations again but rather,we're sampling them with replacement.We could write a loop to reformthe same sampling 10,000 times to see how the proportion will change.Finally, let's plot the proportions to look at the sampling distribution.Use your sampling distribution results to answer the questions in the next concept.

### 25. Background Of Bootstrapping-6Vg5kGoDl7k.en

If bootstrap sampling seems pretty amazing,that's because it kind of is.But the application of bootstrap sampling actually goes beyond even the use cases here.Bootstrapping techniques have been used for leading machine learning algorithms.More on this as provided in the instructor notes below.This technique is credited to Bradley Efron in 1979,a Minnesota born statistician.The name bootstrapping was given to the techniquebecause of the amazement and how well it works is similar tothe idea of being able to walk into quicksand and pullyourself out of the quicksand to safety by your own bootstraps.The way we can draw inference about a population parameter by onlyperforming repeated sampling within our existing sample is just as amazing.We actually gain confidence about where parameter is likelyto exist without having to collect any additional data.This is an amazing concept and it's wowed people for years.

### 26. Why Are Sampling Distributions Important-aDFDOCJKoH0.en

In this lesson, you've looked a lot at sampling distributions.We might still not understand the use of this idea in practice.In the next lessons,you're going to learn more about inference.Specifically, you'll be learning about Confidence Intervals and Hypothesis Testing.When looking at these techniques online,you might find a lot of formulas andbuilt-in calculators for computing the final results for these techniques.However, these built-in calculators hide the assumptions and potential biases.With your new understanding of sampling distributions and bootstrapping,you're ready to tackle not only using the built-in techniques,but to extend these techniques to a multitude of other situations.

## Part 06-Module 01-Lesson 11_Confidence Intervals

### 01. Confidence Intervals Introduction-crleT4000ak.en

In this lesson, you'll be learning about confidence intervals.When we try to estimate a population parameter based on a sample statistic,this is like fishing with a fishing pole.And there's nothing wrong with fishing this way.It's a perfectly good way to fish.However, you'd be much more likely to catch a fish if you cast an entire net.How much more likely are you that you'll catch a fish?Well, that depends on the size of the net.The larger the net, the more confident you can be that you'll actually capture a fish.This net is much more likely to catch a fish than the smaller net.Though this example isn't exactly the same as building a confidence interval,because we consider a parameter to bea non-moving number and not a moving fish amongst water.This idea that providing an interval and not just a single estimate will help usgain confidence in our ability to capturea population parameter is at the core of confidence intervals.And the wider our interval,the more confident we can be that we capture our parameter of interest.Let's take a closer look.

### 02. Sampling To Distributions To Confidence Intervals-QYMLkDToigc.en

In the previous lesson,you saw how we can do sampling distributions andbootstrapping to understand the values of a statistic that are possible.It turns out that we can use these sampling distributionsto understand the most likely values for a parameter as well.In the real world,we don't usually know the value of a parameter as our populations ofinterest tend to be things like everyone in the world or all the past,present, and future transactions of a company.We just don't have all the information we might like to about these populations.So now, the question is,how do we use what we do know about sampling distributions to infersomething about these parameters for these large populations?And here is where the interesting part starts.Imagine this distribution here is the sampling distribution for some statistic,any statistic of interest.We can actually use this sampling distributionto build a confidence interval for our parameter of interest.If we want a 95 percent confidence interval,we could cut off two and a half percent fromhere and another two and a half percent from up here.These values would then give the range where we believe the parameter would be,with 95 percent confidence.If alternatively, we wanted a 99 percent confidence interval,we would cut off a half a percent from each side,and these values would then give the range where webelieve the parameter to be with 99 percent confidence.

### 03. Sampling Distributions  Confidence Intervals-gICzUhMVymo.en

Let's work together through an example of building a confidence interval.Specifically, we'll be looking at how heightsrange based on coffee drinking habits and based on age.Here, I have two datasets,which I have labeled coffee full,and coffee red for coffee reduced.Notice that the second is just a subset of the first.The csv is linked in the resources so you can follow along,as well as a workspace on the next concept.I set the seed, which you can do to follow exactly my steps.Consider that the larger dataset is all of the individuals in our population.While the smaller dataset is just a random sample that we selected.In the real world, we wouldn't necessarily have all the information in the full dataset,but it will be useful for some of the examples we'll be working through.Let's calculate some statistics about the individuals who drink coffee in our sample.We can see that we have approximately 57 percentof individuals that drink coffee in our sample,and we can see the average height of these individuals is approximately 68.52 inches.Now, let's boot strap from the sample to build a confidence interval.Remember, we don't have the population data to work with.So, we cannot use that to build the interval.By cutting off the bottom two and a half and the top two and a half percent,we built 95 percent in the middle portion.We can interpret these values as the bounds where we believethe mean height of all coffee drinkers in the population to be,with 95 percent confidence,between 68.06 and 68.97 inches tall.Let's go back and see what the population mean actually was.Looks like in this case, we were successful using our confidence interval.But that's not always the case. Your turn.Use the notebook and prompts in the next concept to answer the quiz question that follow.

### 05. Confidence Interval for a Difference In Means-8hrWGzjyhck.en

Now that you've seen how you can usesampling distributions to build confidence intervals for a single parameter,let's take a look at how we might do somethingsimilar to estimate the difference in two parameters.Here, we might have a question.What is the difference in average heights forthose who drink coffee versus those who do not?Do we have evidence of a difference in the average height?In order to build a confidence interval forthe difference in the average heights for these two groups,we can do something similar to what you've already done.But for each iteration of taking the mean for each group,we're also going to take the difference.Now, we can iterate this process some large number of times anduse the resulting differences tobuild the confidence interval for the difference in the means.Here, I've set up a difference list and we'll append the differences into it.We could again plot the difference in the means of the two groups and wecould cut off the bottom two and a half and top two and a half percent,to build the 95 percent confidence interval for wherewe believed the difference in the means of the two groups to exist.In this case, you saw that our confidence interval doesn't contain zero.And therefore, this would suggest that there is a difference in the population means.Further, we would suggest that on average,coffee drinkers are actually taller than non-coffee drinkers.

### 07. Confidence Intervals Applications-C0wgmeRx9yE.en

In the previous concepts,you saw how we could build confidence intervals for different parameters,like the mean or difference in means.So what are some of the scenarios for which we wouldwant to build a confidence interval for the difference in means?Well, we could look at the effectiveness ofdifferent drugs by comparing two groups who take two different drugs,or comparing a group that takes a drug to a group that did not take a drug at all.This type of testing is common for any of the health conditions around the world.Another use case specific to learning could be to implementtwo different ways of teaching the same topic and see which way improves retention.One of the most common use cases for comparingtwo groups in this way is known as A/B testing,where we compared different webpages to one another todetermine which web designs drive the largest amount of traffic.The last topic will be covered in more detail in a later lesson,as it's the key to survival for many online companies.

### 08. Statistical vs. Practical Differences-RKHD1wzxxPA.en

You're now been introduced to a few different applications for confidence intervals,as well as a bootstrapping approach to creating these confidence intervals.We found that creating a confidence interval for the differences in means of two groups,rather than just comparing two point value estimates,is important to assure that the differences are not justoccurring due to the randomness associated with the sample that was chosen.Confidence intervals are certainly useful.But there are issues that can arise whenexclusively using confidence intervals to make decisions.This brings up the ideas of Practical and Statistical Significance.Let's consider an example to illustrate the difference between these two terms.Imagine that I own a dog walking business and I'm advertising online.I would like to know which of these two ads wouldhelp me generate the most interest in my business.I send out each ad to the same number ofrandomly selected users and I build the confidence interval,that suggests that more people click this ad.Based on my confidence interval,I have statistical evidence to suggest that the second ad is better.This evidence we get from the confidence interval is just this, Statistical Significance.Let's say that both ads generateenough interest to achieve more dogs than I can even handle watching.And that the second ad is much more expensive and time-consuming to create.If a friend of yours decides that they would like to builda similar ad campaign to start their own dog walking business,which type of ad would you recommend to them?In this case, you might suggest something like the first ad,even though you statistically prove that the second ad was better.The first ad will generate enough interest intheir business and be less expensive and time intensive.Your suggestion for them to use the first ad is an example of Practical Significance.Practical significance takes intoconsideration additional aspects and the world around us,rather than just the numbers.And it's critically important to making your decisions.

### 10. Traditional Confidence Interval Methods-DmZwYHuz2eM.en

The way we've been building confidence intervals thus far isbased on bootstrapping and our knowledge of sampling distributions.This is an extremely effective method for buildingconfidence intervals for essentially any parameters we might be interested in.However, if you've taken a statistics course elsewhere,you might be confused as to whythis notation is different than what you've done in those other courses.You might have seen equations like these for capturing a population mean or proportion.Similarly, you might have seen equations like these forcapturing the difference in means or the difference in proportions.All of these formulas have underlying assumptions that may or may not be true.If you truly believe that your data are representative of your population of interest,the bootstrapping method should providea better representation for where the parameter is likely to be.However, with large enough sample sizes,these formulas should providevery similar results to those that we've seen in bootstrapping methods.In the next concept, we'll take a look at a few examples.

### 11. Traditional vs. Bootstrapping Confidence Intervals-eZ8lyiumXDY.en

In this video, we're going to do a quick comparisonof the traditional approaches for building confidence intervals,which are actually built in the Python,and the approaches you've already seen using bootstrapping in this lesson.There are lots of different names forhypothesis tests and the way that we build confidence intervals.Like a one sample T- test,which is used for the population mean,or two sample T- test,which is used for comparing two means.There is also a paired T- test often used for comparing an individual to themselves,or a Z- test, or a chi-square test, or an F-test.There are so many hypothesis tests which arelinked to the way that we create the confidence intervals.And the bootstrapping approach can actually be used in place of any of these.In this video, we will illustrate this by example.First, let's read in our data and the libraries.I've also set the seed so you can follow along.Next, let's look at a confidence interval for the difference in means.Here's the bootstrap approach that you did in an earlier quiz.While that runs, I'm going to go look at StackOverflowfor a post that could be helpful for finding the difference in means.By finding confidence interval for a T-test,difference in comparison in means,I came across the stack overflow post.If we scroll down,you can see that this is the documentation thatthey used to get their confidence interval.So we already read in Numpy,I'm going to pull this part out.And here, you can see they're just setting up some random data to work with.So let's actually pull that. We don't need it.We want our data to be a comparison ofthose who drink coffee and the heights who don't drink coffee.So we just want the heights of each of those groups,so actually, it's going to look a lot like this.But instead of a bootstrap sample,we just want it from our original sample set.And this print statement will only work in 2.7.Notice that intervals for the bootstrapping method and the builtin using the traditional method are nearly identical.

### 12. Other Language Associated With Confidence Intervals-9KYVRx7-llg.en

In this video, you'll learn a few more terms associated with confidence intervals,as well as some of the relationships you can expect between what happens tothe inputs of a confidence interval and howthis affects the interval you get as a result.There are a few terms that we should discuss that are true for any confidence interval.In the literature, you will frequently see termslike the margin of error and the confidence interval width.A common way that we see political results is in the following way.Candidate A has 34 percent of the vote plus or minus 3 percent.And Candidate B has 22 percent of the vote plus or minus 3 percent.Then in small print,you might see something like,"These figures are based on the 95 percent confidence interval."In the sample, each candidate has this respective 34 and 22 percent,and this 3 percent is called the margin of error.In order to build a confidence interval,we actually add and subtract this amount.So our confidence interval for the true proportion that Candidate Acontrols for the population is at 31 to 37 percent,and for Candidate B, it would be 19 to 25 percent.If this confidence interval for each is larger than we want it,we could actually collect a larger sample size.Because of the Law of Large Numbers,we know that the larger the sample size,the better our estimates will be at approximating our parameter and therefore,this will narrow our interval.

### 14. Correct Interpretations of Confidence Intervals-IhYv_SlN7e8.en

You've already learned a ton about how to build,interpret, and use confidence intervals in practice.This video is just a reminder on the types of conclusions wecan make with confidence intervals and the types of conclusions we cannot make,which are commonly confused.When we build confidence intervals,they're aimed at parameters.That is, they're aimed at a single numeric value in our population.These values include the population mean,or the population standard deviation,potentially the difference in two population means,or any other numeric summary in the population.Notice that confidence intervals do not allow us to say somethingspecific about any individual in our population.More advanced techniques in Machine Learning do aim atgiving us information about every individual in a population,but commonly, confidence intervals are not aimed at solving these types of problems.Confidence intervals are commonly aimed attelling you about aggregate values in a population.

### 16. Confidence Intervals And Hypothesis Tests-T2d9AUnWl-I.en

This concludes this lesson on confidence intervals.You've learned a ton about how you can build and interpret confidence intervals.In the next lesson,you will learn another technique that's verysimilar to confidence intervals called hypothesis testing.The topics of confidence intervals and hypothesis testing essentially do the same thing.But depending on who you talk to or what source you're reading from,it's important to understand both.With that, check out the recap on this lesson onthe next concept and let's get ready for hypothesis testing.

### img

## Part 06-Module 01-Lesson 12_Hypothesis Testing

### 01. Hypothesis Testing Introduction-Qi6F2rJAmrA.en

Welcome to this lesson on hypothesis testing.Hypothesis testing is one of my favorite topics,because it really does bring an art to the way that we think about statistics.This can bring a lot of complexity.And you may need to watch videos in the section multiple times,or practice the quizzes multiple times.But this is a really rewarding subject to master.

### 02. Hypothesis Testing-9GbHHpiK6wk.en

Academic and industry professionals have questions about,well, just about everything.As data analysts, we try to help them answer these questions.But first, we need to translate the questions into what are known as hypothesis.Then, we need to be able to collect data tojustify which hypothesis are likely to be true.As an example, just the other day,I was in a debate with a friend,about what the most popular ice cream flavor in the world is?Where I assumed, the most popular was chocolate,and they were sure it was vanilla.In this case, we could generatehypothesis where the most favorite ice cream is chocolate,and collect data to see if this hypothesis is actually supported by the data.But how can we truly know this unless we talk to everyone?How do we know if our conclusions are reliable?Well, it turns out,you can use hypothesis testing,or confidence intervals which you just saw in the last lesson,to draw conclusions about a population,only using sample data.Not all hypothesis testing is this straightforward.Within medical studies, say I wanted to test ifa cancer drug is effective at helping patients.Well now, the hypothesis could go lots of different ways.Is the drug helpful if it makes patients feel better?Is it helpful if they live longer?Is it helpful if it reduces the tumor sizes?In this lesson, you will learn about how toset up and evaluate the results of hypothesis testing.Hypothesis testing is all about helping businesses makebetter and more informed database decisions. So let's get started.

### 03. Setting Up Hypotheses - Part I-NpZxJg4S6X4.en

When performing hypothesis testing,the first thing we need to do is translate a question into two competing hypotheses.One of these hypotheses is called the Null,and it's associated with the symbol,and the other is called the Alternative and it's commonly notated in this way.Setting up these hypotheses can be a bit subjective,but here are a few general rules that will go through.The null hypothesis is the condition we believe to be true before we collect any data.Mathematically, the null hypothesis is commonlya statement of two groups being equal or of an effect being zero.The null and the alternative hypotheses should becompeting and non overlapping hypotheses.The alternative hypothesis is oftenassociated with what you want or what you want to prove to be true.Mathematically, the null hypothesis tends to holdan equal sign while the alternative holds a greater than,less than, or not equal to sign.We will go through a couple of examples to connect these ideas to.In the US judicial system we say "Innocent until proven guilty."This is actually a setup for a hypothesis test.In a judicial case,every individual is either innocent or guilty of an act.This statement of "Innocent until provenguilty" is a statement that says "We believe everyone to beinnocent initially" that is the null hypothesis for every individual is innocent.It is this statement we believe to be true before we collect any data.Therefore, the competing alternative hypothesis is that an individual is guilty.We then collect evidence or data and use this data to see which hypothesis is supported.

### 05. Setting Up Hypotheses - Part II-nByvHz77GiA.en

In the previous video,you were introduced to the terms ofthe null and alternative hypotheses andcame up with a few guidelines for setting these up as shown here.You also saw how the claim of innocent untilproven guilty relates to the null and alternative hypotheses.In this video, I want to provide a second example.Imagine you create a new web page layout and we'd liketo know if this new page drives more traffic than the existing page.We could set this up to specifically ask,"Does the average web traffic increasewith the new web page as compared to the existing page?"Before we even implement this test,we might hope that the new page is better.That's why we built it after all.But we need to prove this.This is an indication that the new page beingbetter than the existing page belongs in the alternative hypothesis.Therefore, the null would be that the average web traffic is thesame for each of these groups or that the old page is actually better.Mathematically, we could set this up in this way,where the average traffic forthe existing page is equal to the average traffic for the new page.Then, the alternative would look like this,where the average traffic for the new page isgreater than the average traffic for the existing page.And again, we can collect data to see which hypothesis is supported.Here, our guidelines served very useful in defining the null and alternative hypotheses.Setting up hypothesis tests can be tricky because there isn't really one right answer.In the following concepts,you will see why this really matters for our decision-making process.For now, use this example along withthese four guidelines to get some practice setting up hypotheses.

### 07. Types Of Errors - Part I-aw6GMxIvENc.en

Now that you've had some practice with setting up hypotheses,you might be asking yourself,why does all this null and alternative stuff really matter?Well, it actually matters a lot.Let's consider again the judicial example from before.There are four potential reality decision combos that could be made.In order to look at each one of these outcomes,consider this grid where we have the truth of whether or notsomeone is innocent or guilty represented by this axis,and then the decision made by the juryrepresented as either innocent or guilty on this axis here.This creates a grid of four potential outcomes.In this corner, the truth is that an individual is innocent,and the jury also believes that individual to be innocent.While in this corner,the truth is that the individual is guilty,and the jury believes them to be guilty.In either of these other two,the jury has made a mistake as the truth doesn't match the jury decision.This provides us with two potential errors that are possible in hypothesis testing.The first error that is possible is that the jury mightconsider an individual innocent when they are truly guilty.This error sets guilty people free.The second type of error is that the jury considersan individual guilty when the truth is that they're innocent.This error punishes innocent people.

### 09. Types Of Errors - Part II-mbdSQ5CjdFs.en

From the quizzes, you've now been introduced to two types of errors.Type one and type two errors.Correctly setting up the null and alternative hypothesesis important for exactly this reason.They define the importance of the errors that we're making.In the previous example,a type one error is one where we decided that an individual is guilty,but they're actually innocent.The definition of a type one error isan error where the alternative hypothesis is chosen,but the null hypothesis is actually true.You might also hear this called,a false-positive, and it is frequently denoted with a symbol alpha.Type one errors are considered the worse of the two possible errors.The other type of error that might occur,is that we might set a guilty individual free.This is a type two error.By definition, a type two error is when the null hypothesis is chosen,and the alternative is actually true.What we can see in this example is that there are two potential extremes.The jury might decide that they never want to commit a type one error, in which case,regardless of the evidence,they just always set everyone as innocent,and then they're going to commit many more type two errors.Alternatively, if the jury just decided that everyone was guilty,then they would never commit a type two error,but they would commit many more type one errors.Because of this relationship between type one and type two errors,professionals frequently just seta threshold for how many type one errors they're willing to commit.And then, they try to keep the type two errorsas low as possible while still meeting this threshold.Common type one error rates are one percent from medical field,while they're five percent for research journals and other business applications.But really, this rate should be contingent on your application.In the next concept,you will see one application whereneither five percent nor a one percent error rate makes very much sense.

### 12. Types Of Errors - Part III-Z-srkCPsdaM.en

Hopefully, you're starting to feel more comfortableidentifying hypotheses and the types of errors that you can make.In this video, we'll go through one more example to iterate on these ideas.This example is one that really helped me connectall the dots of hypotheses testing and type I and type II errors.I hope it does the same for you.Imagine you own a skydiving shop.And as a part of your job,you must check parachutes to assure they worked correctly.There are two potential outcomes,either a parachute works or it doesn't.You can create these as our two potential hypotheses.You know that there are four potential outcomes for each skydiver-parachute combination.First, you check each parachute to make a decision.Either the parachute works or it doesn't.If you determine that it works,you put it on the shelf for skydivers to use.If it doesn't, you throw it out.Now, there are two possible truths,either the parachute actually works or it doesn't.For the parachute that we threw out, if it doesn't work,that's great, but if it did,then we probably lost like 30 bucks.Now, for the parachute we put on the shelf,if it works, then the skydiver uses itto jump out of a plane and land safely on the ground.However, if it doesn't,we clearly just committed the worst type of error possible.This should clearly be the type I error in this example.So, the other type of error is the type II error.This helps us align our null and alternative hypotheses to look like this.As we know that type I error rate should bechoosing the alternative when the null is true.And it helps us align an appropriate type I error rate asone or five percent as entirely too high.Committing five or even one of these errors outof every 100 individuals would just be unacceptable.

### 14. Common Types of Hypothesis Tests-8hv8KnvQ6JY.en

Now that you've had a chance to gain some familiarity with setting up hypothesis test,let's look at some of the most common tests that are done in practice.One common test is to test the mean orproportion of a population being equal to some value.For example, in finance,we could have the question of,if you can expect a return greater than six percent on an investment.So we could set up the hypothesis test in the following way,where the null is that you earn less than or equal to six percent,and the alternative is that you earn greater than six percent.Another common hypothesis test is to ask which oftwo marketing campaigns might drive more traffic to our website?In which case, we could set up a null and alternative that looks like this,where the null suggests that the proportion of individuals that clickthrough to our page is the same for each campaign,and the alternative is that one page drives more traffic.In which case, the proportion of traffic is different from one page to the next.If we really wanted to test if a new campaign was better than an old campaign,we might use a one sided hypothesis test like this one,where the greater than here,suggests that the proportion of individuals that travelto our site will be higher for the new campaign.You can do some algebra to change the exact same logic to look like this.Notice that all of these hypothesis tests are regarding parameters.They are not about statistics.This is always the case.There is no need to do hypothesis testing onstatistics as they are exact values in our data set.Our questions are about the entire population and therefore,so are our hypotheses.

### 16. How Do We Choose Between Hypotheses-JkXTwS-5Daw.en

Once we set up our null and alternative hypotheses,we need used data to figure outwhich hypothesis we actually think is more likely to be true.And there are two ways we might approach choosing one of these hypotheses.One is the approach that we saw using confidence intervals,where we simulate the sampling distribution of our statistic,and then we could see if our hypothesis isconsistent with what we observe in the sampling distribution.The second way we could approach choosinga hypothesis is simulating what we believe to bepossible under the null and then seeing if our data is actually consistent with that.This is what professionals tend to do in a hypothesis testing.

### 16. Using A Confidence Interval to Make A Decision-MghT95b6LbQ.en

I have attached the data for this example in the Resources tab.And you also can find it in the works pace on the next concept of our work through.This is the same coffee data you saw in the previous lesson on confidence intervals.So you might already have it saved on your local as well.What if we wanted to ask the question of ifthe average height for all coffee drinkers is greater than 70 inches.We could set up our known alternative hypothesis in the following way.Here, we have that the average height ofall coffee drinkers is less than or equal to 70 in the nullwhile in this statement we have thatthe average height is greater than 70 in the alternative.Notice, we are always testing a parameter.So I'm using new here to represent the mean of all coffee drinkers.Based on what we just did with confidence intervals,you can imagine a very intuitive approach for determining if the null is possible,is just to bootstrap a sample set of data and compute the sample mean again and again.And build the sampling distribution and corresponding confidence interval todetermine what are the reasonable values forthe population mean with some level of confidence.Let's put this to practice,imagine from our dataset we achieve this sample.Then we can bootstrap this in the following way.Now, let's bootstrap a number of times and compute the mean for each bootstrap sample.Here, we have our bootstrap sample, and here,I've created an empty vector of means that we'regoing to append each of our bootstrap means into.Now, we have all of our means and we can create our confidence interval.Here, I have the lower bound,and here is the upper bound.Additionally, we might choose to plot those. Here's what it looks like.

### 17. Simulating From the Null-sL2yJtHZd8Y.en

Consider the same example as earlier,where we asked if the mean height for all coffee drinkers was greater than 70 inches.We could again set up a null and alternative hypotheses like these.A second approach that is commonly done for makingdecisions in hypothesis testing is the following,we assume that the null is true and we know what the sampling distribution would looklike if we were to simulate from the closest value under the null to the alternative.That is, this value of 70.That's the closest value under this hypothesis to our alternative hypothesis.We could use the standard deviation of the sampling distribution to determine whatthe sampling distribution would look like if it came from the null hypothesis.We'll simulate from a normal distribution in this case.I'm going to pull over the code that we used before toget the standard deviation of our sampling distribution.So the standard deviation of our sampling distribution is equal to 0.2658.And we know that if it came fromthis null hypothesized value of 70 what it would look like.By the central limit theorem,we know that it would follow a normal distribution.Now, from the NumPy documentation on normal distributions,we see we can simulate draws from the normal using the hypothesized mean at70 and the standard deviation of our sampling distribution in the following way.So here, the loc tells us that the mean of 70 will go in this value,and the scale is the standard deviation that we want to use.So that's the standard deviation of our sampling distribution.And we can simulate, say,10,000 values from that.Each of the simulated draws here represents a possible mean from the null hypothesis.We can now ask the question of where the sample mean falls in this distribution.If we go back and look at what our sample mean was,we can see that it falls far below this distribution from the norm.If our sample mean were to fall closer to the center value of 70,it would be a value that we would expect from the null hypothesis and therefore,we think the null is more likely to be true.In this case, with our sample means so far out in the tail,it's far enough that we don't think it probably came from this null hypothesized value.

### 19. What Is A P-value Anyway-eU6pUZjqviA.en

In the previous video,you saw two methods for how we might choose between competing hypotheses.In the second method,we ask the question if the null hypothesis is true,what is the probability of obtaining the statistic we observed inour data or one more extreme in favor of the alternative hypothesis?This probability is what is called a P-value.Finding the P-value involves a mix of ideas that you've learned about,sampling distributions and conditional probability.Imagine we have a null that a population mean is equal to zero.Then we collect sample data and we find the sample meanto be five and the sample standard deviation to be two.Assuming the sampling distribution of your statistic follows the null hypothesis.What is the probability of observingthe actual value of the statistic from your data in this distribution?If we want to know the probability thatthe population mean is actually greater than zero,you could update the hypothesis.Values like 6, 7,10 all fall out here and are even more of an indication that the alternative is true,that our population mean is greater than zero.Additionally, what is the probability you observedsomething even more in favor of the alternative hypothesis?This shaded region here provides that probability, the P-value.Notice the P-value is dependent onyour alternative hypothesis as it determines what is considered more extreme.If your alternative hypothesis is that your parameter is greater than zero,then we shade greater than the statistic as shown here.However, if we change our null and alternative to look like this,then we shade to the left of our statistic.Our shading for the P-value would now look like this.Notice that this would create a very large probability inthis case as almost the entire distribution is shaded.And if you have a not equal sign in the alternative,then your shading is associated with extremes thatare just far from the null in either direction.In these cases, we just care aboutstatistics that are far from the null in either direction.So we end up marking on both sides andshading away from the null hypothesis to find our P-value.This often takes people a while to wrap their heads around.And there are two parts to understand.First, you have to fully conceptualize the definition of a P-value,which is the conditional probability of your data given that the null hypothesis is true.Then, you need to figure out what to compute,which is visually summarized by these three images.You will get some practice with both of these before movingon and applying the ideas of P-value to make decisions.

### 20. Calculating the p-value-_W3Jg7jQ8jI.en

In the last programming video,you saw how we could simulate draws from the null hypothesis,and that if our statistic was in the bulk of the distribution,this suggested that the statistic was likely from that null.However, if the statistic was farther out from the bulk of the distribution,this suggested that the null wasn't likely to have generated our statistic.Then you saw that we can calculate p-values based on the shaded region starting atthe value of our observed statistic through the tail of the distribution,where the shaded region is dependent on our alternative.Based on the previous video,you had simulated values of the sampling distribution from the null, like this.Imagine that we have the alternative hypothesisthat the population mean is greater than 70,then we could calculate the p-value as the proportionof the simulated draws that are larger than our sample mean.Here, you can see that that would give us a p-value of one.Remember, large p-value suggests that we shouldn't move away from the null hypothesis.In this case, that suggests that we should stay with the mean being less than 70.Here, we've calculated the null values that are greater than our sample mean.Since this is one,our p-value is large and therefore we wouldn't move away from the null hypothesisthat suggests that our population mean is truly less than or equal to 70.If our new null and alternative hypotheses look like this instead,we would calculate our p-value a little differently.Here, because our alternative is less than 70,we would now look at the shaded region to the left of our statistic.This would change our p-value to the following.Now that our p-value is zero,this suggests that we reject the null hypothesis in favor of an alternative,suggesting that the population mean is less than 70.So if this was our null and alternative hypothesis,we would now want to look at the values that are more extremethan our sample mean in either direction away from the null hypothesis.That looks like this equation here.Let's take a look at where these values fall on the above histogram.You can see that if we were to shade more extreme than either of these regions,there are essentially no data points fromour null hypothesis that fall outside of this region.Again, we would have evidence to suggest thatthe null hypothesized value did not generate our sample statistic.

### 23. Connecting Errors and P-Values-hFNjd5l9CLs.en

In the last video,we saw that a p-value is the probability of obtainingour data or more extreme values from the null hypothesis.So how does this connect to making decisions and the types of errors that we can make?If the p-value is really small,this suggests it's less likely to observe our statistic from the null.And it's more likely that it came from the alternative.But how small does the p-value need to bebefore we no longer stay with the null hypothesis?Well, I guess that depends on how willing you are to make certain types of errors.If you are willing to commit five percent of errors,where you choose the alternative incorrectly,then your p-value needs to be smaller thanthis threshold in order to choose the alternative.However, if your probability of getting the data from the null is, say,an eight percent chance,this is enough of a chance that you would stay withthe null under a five percent type one error threshold.The fast rule is that if our p-value is less than a type one error rate,then professionals say we reject the null.That is, we choose the alternative hypothesis.If the p-value is greater than our type one error rate,then we fail to reject the null.That is, we stay with the null hypothesis as our decision.What's actually true?We can't really be sure in practice,but we now have a database way to make our decisions.

### 24. Conclusions In Hypothesis Testing-I0Mo7hcxahY.en

When making a decision about whether you're choosing the null or alternative hypothesis,you might see certain professionals, specifically statisticians,cringe, with concluding remarks like,"So based on the data,we accepted the null hypothesis to be true," or,"Based on the data, we accepted the alternative."Remember, when setting up our null and alternative hypotheses,we automatically set the null to be true before any data were collected.Therefore, this statement is a default.It isn't like we're uncertain about what hypothesis we'll choose and then we choose one,rather, you are by default choosing the null.Going back to the judicial example,everyone is innocent until proven guilty.You don't choose someone as innocent.By default, everyone is innocent.Therefore, in hypothesis testing,we say, "Based on the data,we have evidence to reject the null hypothesis," or alternatively,if we don't have enough evidence to reject the null, we say,"Based on the data, we fail to reject the null hypothesis."Many people will only care about making the right decision.So this distinction might seem just a bit picky.But it stresses that the null is much more likely to bechosen as you start with the statement being true.

### 27. What If Our Sample Is Large-WoTCeSTL1eM.en

When conducting a hypothesis test,there are really two components that you have control over.First, you should be asking yourself,is my sample representative of my population of interest?Are there ways to assure that everyone inmy population is accurately represented in my sample?If your sample isn't representative,then your conclusions are likely to be incorrect.Second, you should know the impact of your sample size,and the role that it plays on your results.As your sample size increases,even the smallest differences betweenthe two groups will appear as notable and statistically significant.With the really large sample sizes that are becoming more frequent,as a part of the data world,we are seeing a large change in techniquesaway from hypothesis tests for exactly this reason.Imagine we are interested in which of two coffee types will sell better on the shelf,and on average, maybe type one sells better than type two.However, there are hundreds, thousands,maybe even millions of individuals who still prefer a different type.A hypothesis test just tells us that,on average, type one will sell more than type two.But with large sample sizes,we should do better than this.Discussing averages leaves out an entire part of the population who preferred type two,or maybe did not care for either of these types,but like a different type.But with large sample sizes,we can do better than this.We can do better than hypothesis testing.And this is what more and more people are finding out.Using machine learning, we can individualize an approach.So maybe, we sell twenty types of coffee and we knowwhat type every member in our population wants.So these large sample sizes prove to be pretty detrimental to a hypothesis test,but they are spectacular forindividual-aimed approaches using machine learning techniques.We will get a glimpse of two ofthese machine learning techniques in the last lessons of this class.

### 28. Multiple Testing Corrections-DuMgeHrkIF0.en

You've learned about the types of errors that are possible on hypothesis testing.And you've learned about how we can createa threshold for how often we allow these to happen.But consider if we run 20 of the exact same types of hypothesis tests.Even if the null is actually true,if we have a five percent type one error rate,we can expect one of these to have results where we choose the alternative.Often, researchers all over the world are performing very similar studies.So when one researcher comes up with significant results,how can we be sure that they are one of these type one errors?Well, the hard part is we can't really know in many cases.And that's a problem. But there are a few methods thatstatisticians have come up with to assist with this problem.One of the most conservative and common approaches is known as the Bonferroni Correction.This simple correction says that if you're performing m-tests,you should divide your type one error rate bym to assure that you actually maintain the error rate here.So if you really want to havea five percent type one error rate and you're performing 10 hypothesis test,your new threshold is actually 0.5 percent for choosing the alternative hypothesis.This is just one popular correction method.But other methods include the Tukey correction.And in the biomedical field,they commonly use a method called Q-Values.More on these methods is provided in the instructor notes below.

### 29. How Do Confidence Intervals  Hypothesis Tests Compare-KEmsEViOoMA.en

So you saw earlier that hypothesis tests and confidence intervals are pretty similar.But you might be surprised just how similar these two techniques are.It turns out that if you are to doa hypothesis test that contains a not equal in alternative hypothesis,your conclusions are identical to a confidence interval,as long as 100 minus your confidence interval levelis the same as the type one error rate in your hypothesis test.That is, if you build a 95 percent confidence interval,this is the same as doinga two-sided hypothesis test with a five percent type one error rate,and building a 99 percent confidence interval will providethe same results as a hypothesis test with a one percent type one error rate.With this in mind, many academic journals are moving awayfrom hypothesis tests as they find the results are often misinterpreted.Instead, more and more confidence intervals andother statistics like effect size or machine learning techniques are being used.There is more information on effect sizes as well asa link to a free course in the Udacity Nanodegree,link in instructor notes below.

### 32. Hypothesis Testing Conclusion-nQFchD4XPPs.en

You now have learned how to set up a null and alternative hypothesis.You have determined type one and type two errors.And you can calculate which hypothesis you should choose based on error threshold.You also have learned about some of the dangers of the conclusionsthat you might make when having a really large sample size,or if you were to perform more than one hypothesis tests.Finally, you learned how confidence intervals andhypothesis tests are highly related to one another.The ideas associated with hypothesis tests are central toA/B testing and making database business decisions.In the next lesson,you will take a hands-on approach to applying these ideas.

### img

## Part 06-Module 01-Lesson 13_Case Study AB tests

### 01. Case Study Introduction-J5uvdPxHIfs.en

Welcome to this case study on AB test.In this lesson, you all apply what you've learned in the previous lessons,to help a company decide whether to launch two new features on their web site.You'll do this by analyzing results from a widely practiced experiment called AB test.The work you've done so far onconfidence intervals and hypothesis testing, will really come in

### 02. AB Testing-EcWvhbIjT9o.en

When companies want to test new features or versions of a web page,they often use a method called A/B testing.The way this works is that one set of userscalled the control group is shown the old versionof a page while another set of users called theexperiment group is shown the new version of a page.Based on how both groups respond,we can determine if the new version is better and should be launched.This is actually just an application ofhypothesis testing where the null hypothesis wouldbe that the new version is no better oreven worse than the old version and the alternative,would be that the new version is better than the old version.A ton of companies use A/B testing to try out changes in features,layouts and even colors to increase a metric that measures interest from their users.For example, you could perform an A/B test to see if a new look foryour site's landing page increasesthe likelihood of a visit or subscribing to your mailing list.Or you could perform an A/B test to see if offeringpersonalized recommendations on a shopping site increases purchases.One thing to be aware of is that although A/B testing can be good totest if a new feature or version of a feature is better on a page,it's not useful for everything.For example, A/B testing can tell you the best way to rank a set of products on a sitebut it can't tell you that the site would reallybenefit from having two additional products added to the site.It's also not always great for testing whole new experiences forexisting users due to factors like change aversion and novelty effect.Existing users may give an unfair advantage to the older versionsimply because they're unhappy with a change even if it's ultimately for the better.Alternatively, they may give an unfair advantage to the new versionbecause they're excited or drawn to the change even if it isn't for the better.In addition to these,there are many other factors that can bias the results of an A/B test.You'll learn more about those in this lesson.

### 04. Business Example-Wzz7omSDfEk.en

In this case study,you'll be analyzing AB test results for an online education company called Audacity,that offers courses on finance.Let's first see what a typical user flow might look like on Audacity site.New users would land on the home page,and if they're interested,they would click to explore courses.As they browse the course list,a course may catch their eye,and they click on it to learn more.Once they're on the course overview page,they may decide to enroll.After enrolling, they'd hopefully complete the course.In this flow, commonly called the customer funnel,users lose interest, and leave at different stages of the funnel,and few make it to the end.Of course, this is a simplistic model,and isn't how all user experiences would play out.But it does capture pretty well the main steps froman initial exposure to course completion in order of decreasing probability.To increase student engagement,Audacity wants to perform AB tests to try out changes,at different points in this funnel.In this case study,we'll analyze test results for two particular changes they have in mind,and then make a recommendation on whether they should launch each change.

### 05. Experiment I-JLKAdT2JESk.en

The first change Audacity wants to try is on their home page.They hope that this new,more engaging design will increase the number of users that explore their courses.That is, move on to the second stage of the funnel.Now that we know the change we want to make,we need to choose a metric that measures that change.To measure how many people move on to the next stage,we can track how many people click onthe Explore Courses button on the home page with the new design and the old design.However, using just the number of users doesn't makesense if more total users view the page in one version of the experiment.More total clicks could occur in one versioneven if there is a greater percentage of clicks in the other version.So instead, we could use the fraction of page visitors who clicked.That is the number of clicks onThe View Courses button divided by the number of page views to the home page.This metric is commonly called click-through rate or CTR.You could take this metric one step further and make itthe number of unique visitors who clicked at least once,divided by the number of unique visitors to view the page.Now that we have our metric,let's set up our known alternative hypothesis.Our alternative hypothesis is what we want to prove to be true.In this case, the new home page design hasa higher click-through rate than the old home page design.And the new hypothesis is what we assumed to be true before analyzing any data,which is that the new home page design has a click-through rate that isless than or equal to that of our old home page design.As you've seen before,we can rearrange our hypothesis to look like this.

### 07. Metric - Click Through Rate-EpfoKAwV_Eg.en

As you saw in the last section,this dataset includes view and click actions on the home page of Audacity's site,from users that were shown the control and experimental versions of the A/B test.Our task is to analyze these actions to see if therewas a significant difference in performance for the two versions.To do this, let's first compute the click-through rate for each group.Let's start with the control group.We can extract all the actions from the control group like this.Now, to compute the click-through rate,we'll divide the number of unique users who actually clickthe Explore courses button by the total number of unique users who viewed the page.This gives us a click-through rate of about 28 percent.Let's do the same thing for the experiment group.Again, we'll take all the click actions,get the unique number of users,and divide that number by the number of unique users who viewed the page.That gives us a click-through rate of about 31 percent.So in this sample, the experiment group's click-through rate washigher than the control group's click-through rate by about 3 percent.Now that we know the observed difference in this sample,we have to see if this difference is significant and not just due to chance.Let's bootstrap the sample to simulatethe sampling distribution for the difference in proportions.Let's take a look at our sampling distribution.If you remember from the previous lesson,we can compute the p-value for our statistic which isthe observed difference in proportions by simulatingthe distribution under the null hypothesis and thenfinding the probability that our statistic came from this distribution.To simulate from the null,we'll create a normal distribution centered at zero,with the same standard deviation as our sampling distribution we simulated here.We could see the null distribution here.And this is where our observed statistic falls.We can't find the p-value like this as these areall the null values that are more extreme than our statistic in favor of our alternative.With a p-value of approximately a half of percent,the difference in click-through rates for the control andexperiment groups does appear to be significant.We can reject the null hypothesis,and based on these results,it looks like Audacity should launch the new version of the home page.

### 09. Experiment II-fq4eO7CybA4.en

The second change Audacity wants to try is on their course overview page.They created a new description for one of their courses to dedicatelarger portions to connecting concepts in the course to career skills,and less on the details of each concept.They hope that this change may encourage more users to complete the course.For this experiment, instead of choosing just a single metric,we're going to analyze multiple.We'll track the enrollment rate,the average reading time on the course page,the average time spent in the classroom,and the course completion rate.First, we'll analyze these individually and then,we'll see how they all come together.

### 11. Metric - Average Reading Duration-w6Y9ZxHDEbw.en

In addition to computing the enrollment rate,we can also compute the average reading durations with this dataset.The two analyses so far were comparing proportions.With this metric, we'll be analyzing the difference in means.This analysis will be quite similar.Since we're comparing reading durations,we only care about view action.So let's filter by that first.And let's only count each unique user once by findingtheir average reading duration if they visited the site more than once.Well, also group by group,just so we keep track of that information.This isn't necessary, but resetting the index isnice just so we keep the ID and group as column names.And it also let's us continue working in a data frame instead of a multi index series.Now, we can find the average reading durations for each group like this.On average, it looks like users in the experiment group spent15 more seconds on the course overview page than those in the control group.To see if this difference is significant,let's simulate the sampling distribution forthe difference in mean reading durations with bootstrapping.Here's what the sampling distribution looks like.Now to find the P value,let's simulate the distribution under the null and findthe probability that our observed statistic came from this distribution.We'll create the distribution centered at zeroand they're having the same spread as our sampling distribution.Here's our null distribution.And here is where I observed statistic falls.Our statistic definitely doesn't look like it came from this null distribution.Looks like the difference we observed is significant.

### 14. Analyzing Multiple Metrics-DtZghKNa7Ak.en

As you've seen in previous lessons,the more things that you test,the more likely you are to observe significant differences just by chance.This happens when we run evaluations from multiple metrics at the same time.The probability of any false positive increases as you increase the number of metrics.Luckily, this is something that we can fix.

### 16. Drawing Conclusions-s-4ghG9vrGQ.en

How do we make a recommendation when three out ofour four metrics had a significant difference for testing each metric individually,but insignificant differences when we use the Bonferroni correction?The Bonferroni method is a conservative one,and since we expect the metrics to be correlated,this would be best handled bya more sophisticated method that ideally takes this correlation into account.In the instructor notes,there is a list of possible other methods that are less conservative.Should you choose one of these methods,you should ensure that the assumptions of the method are truly met in your scenario.Choosing a poorly suited test that simply provides the results that you want isnot only bad practice but will likely lead tomisguided decisions that harm your company's performance in the long run.

### 18. Conclusion-qmGjRpMVBz8.en

Great job completing this case study.To summarize, you learned about the uses and values of AB testing,defining metrics that measure changes in your experiments,analyzing results with confidence intervals and hypothesis testing,handling multiple metrics, and common difficulties associated with AB testing.Congratulations again on completing this case study.

## Part 07-Module 01-Lesson 01_Linear Regression

### 01. Welcome To Linear Regression-zxZkTkM34BY.en

Hi I'm Louis.Welcome to the linear regression section of this Nanodegree.The two main families of algorithms andpredictive machine learning are classification and regression.Classification answers questions of the form yes-no.For example, is this email spam or not,or is the patient sick or not.Regression answers questions of the form how much.For example, how much does this house cost?Or how many seconds do expect someone to watch this video?In this lesson we'll learn how to answer this last type of questions.First, we will learn linear regression,then some ways to improve it,and finally some ways to generalize it to non-linear cases.Are you ready? Let's go!

### 02. DLND REG 01 Quiz Housing Prices V2-8CSBiVKu35Q.en

So let's say we're studying the housing market and our task isto predict the price of a house given its size.So we have a small house that costs $70,000 and a big house that costs $160,000.We'd like to estimate the price ofthese medium-sized house over here. So how do we do it?Well, first we put them in a grid where the x-axis represents the sizeof the house in square feet and the y-axis represents the price of the house.And to help us out, we have collected some previous data in the form of these blue dots.These are other houses that we've looked at and we'verecorded their prices with respect to their size.And here we can see the small house is priced at $70,000 and the big one at $160,000.Now it's time for a small quiz.What do you think is the best estimate for the price of the medium house given this data?Would it be $80,000, $120,000 or $190,000?Submit your answer.

### 03. Solution  Housing Prices-uhdTulw9-Nc.en

Well to help us out,we can see that these points can form a line.And we can draw the line that best fits this data.Now on this line, we can see that our best guess for the price of the house is this pointhere over the line which corresponds to $120000.So if you said $120000, that is correct.This method is known as linear regression.You can think of linear regression as a painter who would look atyour data and draw the best fitting line through it.And you may ask, "How do we find this line?"Well, that's what the rest of the section will be about.

### 04. Fitting A Line-gkdoknEEcaI.en

So here's a trick that will help us fit a line through a set of points.Let's say that these are points and we start by drawing some random line.We're going to ask every point what it wantsfor the model to be better and then listen to them.So let's go one by one,let's take this point over here and ask it what would it want the line to do?Well, the point would want the line to move closer.So, we listen to the point and move closer.Now, if we ask all the points they will tell us the same thing,so we move closer to all of them.Then we'll listen to them again and we take another step that gets closer to all of them,now it's trickier because we have a pointunderneath that pulls it down but that's still okay.Basically, the idea is that we take a few steps that make us gocloser to all the points and that's it, that's linear regression.In the next few videos, we'll see this in a bit more detail.

### 05. Moving A Line-8EIHFyL2Log.en

So, let's have a little refresher on how we move lines by changing the parameters.So, if we have a line of equation y equals w_1x plus w_2,where w_1 and w_2 are constants,it looks like this, w_1 is the slope andw_2 is the y-intercept which is where the line intersects the y-axis.Now, what happens if we increase w_1?Well, we increase the slope so that means the line rotates like this.If we decrease w_1,then we're decreasing the slope so the line rotates like this.Now, what happens if we increase the y-intercept w_2?Then the line moves in a parallel way up like this.And if we decrease w_2,then the line moves like that to the bottom.That's really all we're going to need for the tricks that are coming.

### 06. Absolute Trick-DJWjBAqSkZw.en

So, here's our first trick that we will aligncloser to a point that we're going to use in a linear regression.It's called the absolute trick and it works like this,we start with a point,and a line, and the idea is that the point wants the line to come closer to it.So, let's put some numbers here,the point has coordinates p comma q,where p is a horizontal coordinate and q is the vertical coordinate.The line has equation y equals w_1x plus w_2.In here w_1 is the slope and w_2 is the y-intercept.So, an easy way to move the line closer to the pointpq is to just add one to the y-intercept,and then there is, the line moves up.Now let's add something to the slope two tomake the line rotating the direction of the point.This is going to look a little strange but it's going to make sense very soon.So, this distance over here is p because it'sthe horizontal distance from the y-axis to the point.So let's just add that, let's add p to the slope,and now our new slope is w_1 plus p,and that rotates the line in this direction.Now our new equation is y equals w_1 plus p times x plus w_2 plus one,and that's pretty much what the absolute trick is about.Notice some subtleties though,notice that we moved the line a little too much,and we actually went over the point and kept going,we don't want this, in general inmachine learning we never want to take big steps like this.Instead what we want to do is take tiny steps.So, in order to take a tiny step we'll just dothe exact same thing we did except we multiply everything by a small number.So let's take a small number called the learning rate, let's say alpha,and instead of adding one to the y-intercept and p to the slope,we will add alpha times one to the y-intercept,and alpha times p to the slope.Now the line moves up by a little bit and rotates a little bit,so we don't have that risk of going too far.Our new equation is going to be w_1 plus p times alpha times x plus w_2 plus alpha.So our new slope is w_1 plus p times alpha and our new y-intercept is w_2 plus alpha.So we're doing better but there's still a little subtlety.What happens if the point is not on top of the line but underneath the line?Well, same thing except now instead of adding we just subtract to getour new equation w_1 minus b times alpha times x plus w_2 minus alpha.The reason is if we subtract alpha to the y-intercept the line moves down instead of upand if we subtract p times alpha to the slope the line rotates in this direction instead.Something more interesting is this,and that will explain the reason p is there.If the point is not on the right of the y-axis but on the left of it,than we still add one to the y-intercept because we need the line to move up,but the fact that now we're adding p to the slope and p is nowa negative number means that our line now rotates in this direction,so that's a reason for p to be there.Another reason for p to be there is this, check this out.If this distance is small then p is small,so we're adding a small number to the slope.Now, if the distance is large then p islarge so we're adding a large number to the slope.It makes sense that if the point is reallyclose to the y-axis we want to increase the slope by a little bit.Whereas if it's far want to move it by a lot more.So, let's do an example to make this more clear.Let's say we have the point 5 comma 15 and the line y equals 2_x plus three,so this means the distance from the point to the y-axis is five,now let's say our learning rate is 0.1.So, we're adding 0.1 times one tothe y-intercept and that moves the line up by a little bit.Also we are taking five multiplying it by 0.1and adding that to the slope which makes the line move in this direction,this means our new equation is y equals 2.5 x plus 3.1.Now, check out what happens if the point is in the left,we're still adding 0.1 to the y-intercept to move the line up,but now to the slope we're going to add the product of0.1 and minus five, that's minus 0.5.This means our new equation is going to be 1.5 x plus 3.1.As you can see the slope moved ina different direction which made it go closer to the point.So that's it, that's the absolute trick,and we're going to use it extensively in linear regression.

### 07. Square Trick-AGZEq-yQgRM.en

So here's another trick that will help us move a point closer to a line,and it's very similar to the absolute trick but it has a little bit of extra gravy.It's based on this premise.If we have a point that is close to a line,then this distance is small and we want to move the line very little.But if the point is far from the line,then want to move the line a lot more.The absolute trick that we learned previously doesnot have this property because if we rememberthe absolute trick adds alpha to the y-intercept w_2 and p times alpha to the slope w_1.This has nothing to do with how far the point is from the line,since P is just a horizontal distance.So let's just add the vertical distance into this formula.Let's look at this vertical distance between the point and the line.The point over the line has coordinates say,( p,q').This distance is then q minus q' because q' isthe value at the line and q is the value of the y coordinate at the point.So what we do here is very simple.We just take this q minus q' and multiply it intowhat we're adding to both the y-intercept and to the slope.This will again make the line go up by a bit and rotate inthis direction except that now if the point is far from or close to the line,the amount the line moves will beaffected and here's our new equation with a factor of q minus q'.Notice that here we get something for free.If the point is underneath the line instead of over the line,then q minus q' is actually a negative value and if this is the case then we'resubtracting something from the slope thus rotating the line in this direction instead,still towards the point.So this trick also takes care of points that are underthe line and we don't have to have two rules like we had on the absolute trick.We just have one same rule for both.Again, let's clarify this with an example.In this example over here,we have the point (5,15) and notice thatthis distance is going to be two because the line goesthrough the (5,13) as two times five plus three is 13.Now to change things a bit,we'll use zero point 0.01 as the learning rate.So the absolute trick would be adding 0.01 to the y-intercept and 0.05 to the slope.But now as our neutral access,we multiply the two numbers two adding to the slope and the y-intercept by two;which means we're adding 0.12 to the slope and 0.02 to the y-intercept.So the equation of our new line is y equals 2.1_x plus 3.02.Notice that that line is going to be closer to the point.So that's it. That's the square trick.

### 08. Gradient Descent-4s4x9h6AN5Y.en

so now that we've learned the absolute trick and the square trick,and how they're used in linear regression,we still want to have some intuition on how these things get figured out.These tricks still seem a little too magical,and we'd like to find their origin so let's do this in a much more formal way.Let's say we have our points on our plan is to develop an algorithm whichwill find the line that best fits this set of points.And the algorithm works like this,first it will draw a random line,and it'll calculate the error.The error is some measure of how far the points are from the line,in this drawing it looks like it's the sum of these distances but it couldbe any measure that tells us how far we are from the points.Now we're going to move the line around and see if we can decrease this error.We move in this direction and we see thatthe error kind of increases so that's not the way to go.So we move in the other direction and see that the error decreased,so we pick this one and stay there.Now we'll repeat the process many times over and over every timedescending the error a bit until we get to the perfect line.So to minimize this error,we're going to use something called gradient descent.So let me talk a bit about gradient descent,the way it works is we're standing here on top of a mountain.This is called Mt rainierror as it measures how big our error is,and wanted descend from this mountain in order todescend from this mountain we need to minimize our height.And on the left, we have a problem of fitting the line to the data,which we can do by minimizing the error or the distance from the line to the points.So descending from the mountain is equivalent to getting the line closer to the points.Now, if we wanted to descend fromthe mountain we would look at the directions where we can walkdown and find the one that makes us descend the most.And let's say this is the direction,so we descend a bit in this direction,this is equivalent to getting the line a little bit closer to the points.So now our height is smaller because we're closer tothe points since our distance to them is smaller.And again and again we look at what makes us descend the mostfrom the mountain and let's say we get here.Now we're at a point where we're descended from the mountain and onthe right we found the line that is very close to our points.Thus, we've solved our problem and that is gradient descent.In a more mathematical way what happens is the following,we have a plot and here's a plot in two dimensionsallowing a reality that plot will be in higher dimensions.We have our weights on the x-axis and our error on the y-axis.And we have an error function that looks like this,we're standing over here and the way to descend is to actually takethe derivative or gradient of the error function with respect to the weights.This gradient is going to point to a direction where the function increases the most.Therefore, the negative of this gradient is going to pointdown in the direction where the function decreases the most.So what we do is we take a step in the direction of the negative of that gradient,this means we are taking our weights wi and changing them towi minus the derivative of the error with respect to wi.In real life we'll be multiplying this derivative bythe learning rate since we want to make small steps.This means the error function is decreasing and we're closer to the minimum.If we do this several times we get toeither a minimum or a pretty good value where the error is small.Once we get to the point that we've reacheda pretty good solution for our linear regression problem,and that's what gradient descent is all about

### 09. Mean Absolute Error-vLKiY0Ehors.en

In the last video, we'll learned how to decreasean error function by walking along the negative of its gradient.Now in this video, we're going to learn formulas for these error functions.The two most common error functions for linear regression are the mean absolute error,and the mean squared error.First, we'll learn the mean absolute error.So, here's a point and a line,the point has coordinates x, y,and the line is called Y-hat since it's the prediction.So, our prediction for this point is going to be the pointin the line with the same x coordinate as our point,that is the point x, y hat.This means the vertical distance from the point to the line is y minus y hat,and that's what we'll be calling the error.Notice that this is not the actual distance tothe line since this would be a perpendicular segment,but it's the vertical distance from the point tothe line which is the distance between the point and its prediction.Now, our total error is going to be the sum ofall these distances for all the points in our dataset.And sometimes we'll use this as our error,but in this case we'll use the average or the mean absolute error,which is the sum of all the errors divided by m,the number of points in our dataset.Using the sum or the average won't change our algorithms,since that would only scale our error by a constant,namely m. Notice something here which isthat we have an absolute value around the y minus y hat,the reason is that if the point is on top of the line,the distance is y minus y hat,but if it's under the line then it's y hat minus y.We want the error to always be positive,otherwise negative errors will cancel with positive errors.Therefore, we take the absolute value of y minus y hat.So, our mean absolute error is the average of all absolute errors or in other words,the sum of all these absolute values divided by the number of points,which is m. We're going to plot these error over here in a graph.Obviously as we mentioned before,the graph has more dimensions but this is a two-dimensional simplification of that graph.As we descend in this graph using gradient descent,we get a better and better line until we findthe best possible fit with the smallest possible mean absolute error.

### 10. Mean Squared Error-MRyxmZDngI4.en

Now in this video we'll learn the Mean Squared Error.The Mean Squared Error is very similar to the Mean Absolute Error.Again, here we have our point and our prediction,but now instead of taking the distance we're actually going todraw a square with this segment as its side.So the areas precisely y minus y hat squared.Notice that this is always non-negative,so we don't need to worry about absolute values.And our mean squared error is going to be the average of all these series of squares,and we're going to have this extra factor of one half for convenience later.So in summary the area's one half times the averageof the sum of all y minus y hat squared.Again, we can take the sum and call it the total square error,but we take the average and this won't make a difference in the algorithm.Notice, if the point is over the line or underneaththe line the square is always going to be a non-negative number,because the square of a real number is always going to be non-negative.The one half is going to be there for conveniencebecause later we'll be taking the derivative of this error.Again, we can multiply this error by any constant andthe process of minimizing it will be the exact same thing,so this one half does not affect anything.So here's a pictorial representation of the error.Here we have our points, our line,and the error is the average of the areas of all these squares.Here's our graph of the error.As we descend from this mountain we get tothe place where the error is the smallest possible,and that's the same as minimizing the average of the areas of the squares.

### 11. Minimizing Error Functions-RbT2TXN_6tY.en

So far, we've learned two algorithms that will fit a line through a set of points.One is using any of the tricks namely the absolute and the square trick,and the other one is minimizing any of the error functionsnamely the mean absolute error and the mean squared error.The interesting thing is that these two are actually the exact same thing.What I'm saying is that when we minimize the mean absolute error,we're using a gradient descent step and thatgradient descent step is the exact same thing as the absolute trick.Likewise, when we minimize the squared error,the gradient descent step is the exact same thing as the square trick.So let's see why, let's start with the mean squared error.Here's a point with coordinates (x,y) and here's our line with equation y-hat equals w_1 times x plus w_2.So y-hat is our prediction and the line predicts the y coordinate of this pointthat prediction gives us a point on the line thatmatches the x coordinate of the point (x,y).So it's the point (x,y-hat).Now, the error for this point is 1.5 times y minus y-hat squared,and the mean squared error for this set of points is the average of all these errors.But since average is a linear function,then whatever we do here applies to the entire error.Now, we know that the gradient descent step uses these two derivatives,namely the derivative with respect to the slopew_1 and the derivative with respect to the y-intercept w_2.If we calculate the derivatives,and you can see the calculation in detail in the instructor notes,we get negative times y minus y-hat times x for the one respect to the slopeand negative times y minus y-hat for the one with respect to the y-intercept w_2.And notice that the length of this red segment is precisely (y minusy-hat) and the length of this green segment is precisely x.And if you remember correctly,the square trick told us that we have to upgrade the slope byadding y minus y-hat times x times the learning rate alpha,and upgrade the y-intercept by adding y minus y-hat times the learning rate alpha.But that is precisely what this gradient descent step is doing.If you like, feel free to pause the video or actually write it downin a little piece of paper to verify that is the exact same calculation.So this is why the gradient descent step utilize when weminimize the mean squared error is the same as the square trick.We can do the same thing with the absolute trick.The procedure is very similar except we have to be careful about the sign.This is our error, the absolute value ofy minus y-hat and the derivatives of the error with respectto w_1 and w_2 are plus or minus x andplus or minus one based on the point is on top or underneath the line.Since the distance is x,then you can also check that this is precisely whatthe gradient descent step does when we minimize the mean absolute error.So that's it, that's why minimizing these errors with gradient descent isthe exact same thing that using the absolute and the square tricks.

### 14. Absolute Vs Squared Error-csvdjaqt1GM.en

So here's a question, what is better the mean absolute error or the mean squared error?Well, there's no real answer to this.Both are used for a lot of different purposes,but here's one property that actually tells them apart.So here's a set of data and we're going to try tofit it by minimizing the mean absolute error.So let's say we have line A, line B,and line C. And the question is,which one of these lines gives us a smaller mean absolute error?Enter your answer below.

### 14. DLND REG 12 Absolute Vs Squared Error 2 V1 (1)-7El1OH17Oi4.en

Well that was a tricky quiz.If you said the same,then you were correct because as you can see,moving this line up and down actually keeps the mean absolute error the same.You can convince yourself by looking at the picture andchecking that as you move a line up and down,you're adding a segment to two of the errors andremoving a segment of equal length to the other two errors.Now, let's repeat the quiz but with the mean squared error.The question is, which one of these three lines A,B, and C would give us a smaller mean squared error?Enter your answer below.

### 14. DLND REG 13 Absolute Vs Squared Error 3 V1 (1)-bIVGf_dDkrY.en

So this time there was a solution and the solution isB and here's the reason which is more subtle.Our mean squared error is actuallya quadratic function and quadratic functions have a minimum at the point in the middle.Over here, we can see that the error for line A would be around here,the error for line C would be around here,and the one for line B would be somewhere around here.So we can see that the minimum mean square error is given by line B.Here's a diagram of the errors and you can check with your eyesor even drawing a small example and calculating the areas;that line B would be giving you the smallest mean square error.

### 16. Higher Dimensions--UvpQV1qmiE.en

So in the previous example,we had a one column input and one column output.The input was the size of the house and the output was the price.So we had a two-dimensional problem.Our prediction for the price would be a line andthe equation would just be a constant times size plus another constant.What if we had more columns in the input,for example size and school quality?Well, now we have a three dimensional graph becausewe have two dimensions for the input and one for the output.So now our points don't live in the plane,but they look like points flying in 3-dimensional space.What we do here is we'll feed a plane through them instead of fitting a line,and our equation won't be a constant times one variable plus another constant.It's going to be a constant times school quality plusanother constant times size plus a third constant.That's what happens when we're in three dimensions.So what happens if we're in n dimensions?So in this case we have n minus one columns in the input and one in the output.So, for example the inputs are size,school quality, number of rooms, et cetera.Well, now we have the same thing except our data lives in n-dimensional space.So for our input, we have n minus one variables namely; x_1,x_2 up to x_n minus one and for the output of the prediction,we only have one variable y hat.Our prediction would be an n minus one dimensional hyperplane living in n dimensions.Since it's hard to picture n-dimensions just think of a linear equation in n variables,such as y hat equals w1x1 plus w2x2 plus all the way to w_nminus one x_n minus one plus w_n and that's how we do predictions for higher dimensions.In order to find the weights w_1 up to w_nthe algorithm is exactly the same thing for two variables.We can either do the absolute or square root tricks,or we can calculate the mean absolute or square errors,and minimize using gradient descent.

### 18. Closed Form Solution-G3fRVgLa5gI.en

So here's an interesting observation;in order to minimize the mean squared error,we do not actually need to use gradient descent or the tricks.We can actually do this in a closed mathematical form. Let me show you.Here's our data x_1,y_1 all the way to x_m,y_m; and in this case, m is five.And the areas of the squares represent our squared error.So our input is x_1 up to x_m and our labels are y_1 up to y_m,and our predictions are of the form y_i hat equals w_1 x_i plus w_2,where w_1 is a slope of the line and w_2 is the y-intercept.And the mean squared error is given by this formula over here.Notice that I've written the error as a function of w_1 and w_2,since given any w_1 and w_2 we can calculatethe predictions and the error based on these values of w_1 and w_2.Now, as we know from calculus,in order to minimize this error,we need to take the derivatives with respect tothe two input variables w_1 and w_2 and set them both equal to zero.We calculate the derivatives and you can seethe full calculation in the instructor notes and we get these two formulas.Now, we just need to solve for w_1 and w_2 for these two equations to be zero.So what do we have now?We have a system of two equations and two unknowns,we can easily solve this using linear algebra.So now the question is,why don't we do this all the time?Why do we have to go through many gradient descent stepsinstead of just solving a system of equations and unknowns?Well, think about this.If you didn't have only two dimensions in the input but you had n,then you would have n equations with n unknowns,and solving a system of n equations withn unknowns is very expensive because if n is big,then at some point of our solution,we have to invert an n by n matrix.Inverting a huge matrix is something that takesa lot of time and a lot of computing power.So this is simply not feasible.So instead this is why we use gradient descent.It will not give us the exact answer necessarily but it will get us prettyclose to the best answer which will give us a solution that fits our data pretty well.But if we had infinite computing power,we would just solve this system and solve linear regression in one step.

### 21. Polynomial Regression-DBhWG-PagEQ.en

So what happens if we have data that looks likethis where a line won't really do a good job fitting in?Maybe would like to have a curve or some polynomial.Maybe something along the lines of 2x cubed minus 8x squared, et cetera.This can be solved using a very similar algorithm than linear regression.All we have to do is instead of considering lines,we consider higher degree polynomials.This would give us more weights to solve our problem.For example, this problem here we'll make a solve for four weights;w_1, w_2, w_3, w_4.But the algorithm is the same thing.We just take the mean absolute or squared error and take the derivative with respect tothe four variables and use gradient descent tomodify these four weights in order to minimize the error.These algorithm is known as polynomial regression.

### 22. Regularization-PyFNIcsNma0.en

The following concept is one that works both for regression and classification.So in this video, we'll explain it using a classification problem.But as you will see, all the arguments here work with regression algorithms as well.The concept is called regularization,it's a very useful technique to improve our models and make sure they don't over fit.So let's look at some data, here's the data,and let's make two copies of it,and let's look at two models that classify this data.The first one is a line and the second one is a higher degree polynomial curve.So, the question is which one is better?Well, they both have their pros and cons, right?The one on the left makes a couple of mistakes.As you can see, there is a red point anda blue point in the wrong sides but it is much simpler.The one on the right makes zero mistakes but it's actually a bit more complicated.So let's say we want the one on the left because the one onthe right over fits and it just doesn't generalize well.So the problem is that when we train the model,the one on the right will appear more likely and the reason is the following.When we're training the model,the model takes an error and minimizes it.So, the model on the left has a small error since it ownmisclassifies two points but it's an error nonetheless.The model in the right has a really small error since itdoes not misclassifies any of the points.So, if we train a model to minimize error,it will build a boundary like the one on the right, not the one on the left.So the question is, how do we pick the one in the left? Well, here's an idea.Let's look at the equation and let's say the equation of the line on the left issomething like 3x_1 plus 4x_2 plus five equals zero.The equation of the polynomial is something more complex withhigh degree terms like x_1 squared x_1 x_2 x_2 cube, et cetera.If we look at the equation in the left,it's much simpler than the equation on the right.In particular, there are less coefficients, only three,four, five whereas the right one has many more.So, if we find a way to in commend the error by some function of these numbers,that would be very helpful because in some waythe complexity of the model will be added into the error.So a complex model will have a larger error and then a simple model.So, let's do that and I'll show you the details later but the idea is thatwe take this three and four and notice that we're forgetting about the constant term,and there's a reason for that.But if we take this three and four and say add them to the error,we get a slightly bigger error.But what if we take all these coefficients and add them to the error here,now we get a huge error.Now, we can see that the modeling the left is betterbecause it has a smaller combined error, so again,what we did is we took the complexity in the model into account,when we calculated the error and in that way,a simpler model has an edge over the complicated model.Simpler models have a tendency to generalize better so, that's what we want.So now, let me be more detailed on how to takethe complexity of a model and turn into part of the error.In summary, will take this highlighted coefficients and somehow add them to the error,this method is called L1 regularization and it's very simple, here's how it works.What L1 regularization does,it takes the coefficient and just adds the absolute values of them to the error.So in this case, we're adding absolute valueof two which is two plus absolute value of minus two,which is two again, et cetera and we see that they add to 21.In the linear case,we see that we're adding is the absolute value of three and four which is seven,so a seven is much less than 21,we can see that the complicated model gives us a much higher error.That's L1 regularization and the one is attached to the absolute value.L2 regularization is very similar and what we dohere is instead of adding the absolute values,we add the squares of the coefficients.So for the complicated case,we get two squared plus minus two squared,et cetera which gives us 85.For the linear case, we get three squared plus four squared which is 25,which is much smaller than 85.So again, we see that the complex model gets punished a lot more than the simple model.But now the question is,what if we punish the complicated model too little or what if we punish it too much?Maybe some models, like a model to send a rocket to the moon or a medical model,have very little room for error and we're okay with some complexity,or maybe other models like a video recommendation model,or model recommending potential friends on a social network have more roomfor experimenting and need to be simpler and faster to run a big data.So we're okay with having some error.So it seems that for every case,we have to tune how much we want to punish complexity in each model.This can be fixed with a parameter and this parameter is called lambda.What we do with lambda,is we multiply the complexity part of the error as follows.Let's look at the two models again and let'sremember that the yellow part of the error comes fromthe misclassified points and the green part comes from the complexity of the model,namely the coefficients in the polynomial.Let's say, we have a small lambda,so we take the green error and multiplied by small lambda which gives us something small.Therefore, the right model still wins becausethe complexity part of the error is small and it won't swing the balance.But if we have a large value for lambda,then we're multiplying the complexity part of the error by a lot.Which punishes the complex model more and then the simple model wins.So in summary, this is what happens,if we have a large lambda then we're punishingcomplexity by a large amount and we're picking a simpler model.Whereas if we have a small lambda,then we're punishing complexity by a small amount,so we're okay with having more complex models.Now the question is, which one to use L1 or L2 regularization?So here's a cheat sheet with some benefits for each one.L1 regularization is actually computationally inefficienteven though it seems simpler because it has no squares,but actually those absolute values are hard to differentiate.Whereas, an L2 regularization squares have very nice derivatives.So, these are easy to deal with computation.The only times where L1 regularization is faster thanL2 regularization is when the data is sparse.So let's say if you have a thousand columns of data butonly 10 are relevant and the rest are mostly zeros,then L1 is faster,L2 is better for non-sparse outputs whichis when the data is more equally distributed among the columns.L1 has one huge benefit which is that,it gives us feature selection.So let's say, we have again,data in a thousand columns but really only10 of them matters and the rest are mostly noise.So, L1 will detect this and will make the relevant columns into zeroes.L2 on the other hand won't do this and it just take the columns and treat them similarly.So that's it, that's regularization.

### 23. Conclusion-pyeojf0NniQ.en

Well that was it. In this lesson,you have learned linear regression and some of its generalizations.You have also gone hands-on and implementedthe gradient descent algorithm for linear regression.You are now ready for the upcoming project in which you'll be implementinga regression neural network to analyze real data. Great job.

### img

## Part 07-Module 01-Lesson 02_Naive Bayes

### 01. Naive Bayes Intro V2-vNOiQXghgRY.en

Hello again and welcome to the Naive Bayes section.Naive Bayes is a more probabilistic algorithm which is basedon playing with the concept of conditional probability.This algorithm has great benefits such as being easy to implement and very fast to train.We'll be studying one of very interesting applications, natural language processing.In the lectures and in the lab,we'll use it to analyze text,and emails, and speeches.

### 02. SL NB 01 Guess The Person V1 V1-tAOAjI-7ins.en

We'll start with an example.Let's say we're in an office and there are two people,Alex and Brenda, and they're both there the same amount of time.When they were in the office and we see someone passing by really fast,we can't tell who it is,but we'd like to take a guess.So far, with all we know,all we can infer is that since they're both in the office the same amount of time,the probability of the person being Alex is50 percent and the probability of the person being Brenda is also 50 percent.But now, let's try to use more information so wecan make a better guess of who the person is.When we saw the person running by,we notice that they were wearing a red sweater.So, we'll use that piece of information.We've known Alex and Brenda for a while,and actually we've noticed that Alex wears a red sweater two days a week,and Brenda wears a red sweater three days a week.We don't know which days, but we are sure of this fact.Also, when we say week we mean workweek,so five days, although at the end this won't matter much.So now what we'll do,is we'll use this piece of information to help us make a better guess.First off, since Alex wears a red sweater less than Brenda,It's easy to imagine that it's a bit less likely thatthe person we saw is Alex than that it is Brenda.But exactly how likely?Well, let's say that if we saw a person pass by five times,it would make sense to think that two of this times it was Alex,since he wears a red sweater twice a week.And the other three times it was Brenda,since she wears a red sweater three times a week.Therefore, from here we can infer that the probabilities are 40 and 60.We've used the formation about the color of the sweater to obtainbetter probabilities about who was the person who passed by.This is Bayes' theorem and we'll learn in more in detail in the next few videos.The initial guess we had, the 50/50 guess,is called the prior,since it's all we could infer prior to the new information about the red sweater.The final guess we have,the 60/40 guess is called the posterior,since we've inferred it after the new information has arrived.

### 03. SL NB 02 Known And Inferred V1 V2-DrYfZXiDLQI.en

In the last video, we saw an example of Bayes theorem.But here's the main idea fit and it's a very powerful theorem.What it does is it switches from what we know to what we infer.What we know in this case is the probability that Alexwears red and the probability that Brenda wears red.And what we infer is the opposite,is the probability that someone wearing red isAlex or that someone wearing red is Brenda.In other words, initially we know the probability of an event A.And to give us more information,we introduce a new event R which is related to A.We know the probability of R given A.What Bayes' theorem does is from these two,it infers the probability of A given R. Thisis the new probability of A once we know that the event R has occurred

### 04. SL NB 03 Guess The Person Now V1 V2-pQgO1KF90yU.en

Bayes Theorem can get a little more complex.Let's take a look at a small example and what we'lldo here is we'll mess a bit with the prior probability.So again, we have Alex and Brenda in the office,and we saw someone pass by quickly and we don't know who the person is.So let's say we look more carefully at their schedules and werealized that Alex actually works from the office most of the time.He comes by three days a week.And Brenda travels a lot for work, so,she actually comes to the office only one day a week.So initially, without knowing anything about the red sweater,all we know is that it's three times more likely to see Alex than to see Brenda.Therefore our prior probabilities are 0.75 for Alex and 0.25 for Brenda.And let's say that we have this happening throughout all the weeks,but now we use our extra knowledge which is that the person we saw had a red sweater.The rule is still as before,as Alex wears red twice a week and Brenda wears red three times a week.So, naively we would think that the real probabilities are not exactly0.75 or 0.25 because Brenda wears a red sweater more than Alex,so they should be a little closer to each other.Let's calculate them. So, we'll do the following,let's think of the columns as weeks instead.So, now for each five-day work week,Alex wears red twice and Brenda three times.So, we colored the days they wore red.Now, since we know the person wore red,we forget about the times that they didn't.So we have nine times someone wore red.Six of them are Alex and three of them are Brenda.Therefore, among nine times we saw someone wearing red,two-thirds of the times it with Alex and one third of the time it was Brenda.Thus, our posterior probabilities are two-thirdsor 0.67 for Alex and one third or 0.33 for Brenda.So it looks like we did a little bit of magic.Let's do this again in a more mathematical way.We saw a person and initially all we know is that it's Alex witha 75% probability and Brenda witha 25% probability sinceAlex comes to the office three times a week and Brenda once a week.But now new information comes to light which is that the person is wearinga red sweater and the data says that Alex wears red two times a week.So now we look at Alex.What is the probability that he's wearing red?Since a work week has five days and the probability of himwearing red is two-fifths or 0.4.And the probability of him not wearing red is the complement, so 0.6.Same thing with Brenda, since she wears red three a week,then the probability of her wearing red today is0.6 and the probability of her not wearing red is 0.4.Now, by the formula of conditional probability,the probability that these two will happen isthe product of the two probabilities P of Alex,times P of red given Alex.Therefore, the probability of the person we saw is Alex and that they'rewearing red is precisely 0.75 times 0.4.We multiply them and put the result here.We calculate the other probabilities in the same way,that probability of the person we saw is Alex and that he's notwearing red is 0.75 times 0.6.The probability of the person we saw is Brenda and that she's wearing red,is again the product of these probabilities,which is 0.25 times 0.6.And finally, the probability of the person we saw is Brenda andshe's not wearing red is 0.25 times 0.4.And now here's where the Bayesian magic happens, are you ready?We have four possible scenariosand you can check that these four probabilities add to one.But we know one thing,that the person we saw was wearing red.Therefore, out of these four scenarios,only two are plausible,the two when the person is wearing red.So, we forget about the other two.Now, since our new universe consists of only these two scenarios,then the probability should be higher,but their ratio should still be the same with respect to each other.This means, we need to normalize them or equivalently,divide them by something so that they now add to one.The thing we should divide them by,is the sum of the two.So, our new probability of the person being Alex is the top one, namely,0.75 times 0.4 divided by the sum of the two,namely, 0.75 times four,plus 0.25 zero times 0.6.This is precisely two-thirds or 0.67,and now we can see that the complement is the probability that the person is Brenda,which is one third or 0.33.If we take Brenda's probability and divide it by the sum ofboth probabilities we can see that we get one third as desired.And that's it, that is Bayes Theorem at its full potential.

### 05. SL NB 04 Bayes Theorem V1 V2-nVbPJmf53AI.en

So, let's look at a formal version of Bayes Theorem.Initially, we start with an event,and this event could be A or B.The probabilities for each are here,P of A, and P of B.Now, we observe a third event,and that event can either happen or not happen both for A and forB. R is going to help us find more exact probabilities for A and B in the following way.Let's say we can calculate the probability of R given A,and also, of R complement which is node R given A.And similarly for R given B,and R complement given B.Now, our set of scenarios are these four,R n A, R complement n A,R n B, and R complement n B.But since we know R occurred,then we know that the second and the fourth events are not possible.So, our new universe consists of the two events,R n A and R n B.We calculate the probability for A n R or equivalently A intersection R,and by the Law of Conditional Probability,this is P of A times B of R given A.Similarly, for B intersection R. Now,since these probabilities do not add to one,we just divide them both by their sum sothat the new normalized probabilities now do add to one.Thus, we get the following formulas for P of A given R,and P of B given R. These areour new and improved probabilities for A and B after we know that R occurred.Again, P of A and P of B are called the prior probabilities which is,what we knew before we knew that R occurred.P of A given R and P of B given R,are posterior probabilities which is,what we inferred after we knew that R occurred.And here it is,the formula for Bayes Theorem..

### 06. SL NB 05 Q False Positives V1 V2-ngA6v09eP08.en

Now, let's look at an interesting application of Bayes Theorem.Let's say we're not feeling very well and we go to the doctor,the doctor says there's a terrible disease going on,I'll administer a test for you.Moreover, she says that the test has 99 percent accuracy.More specifically, she says that for every 100 patients that are sick,the test correctly diagnosis 99 of them and for every 100 patients that are healthy,the test correctly diagnosis 99 of them.If we want to be tactical, these are actually called sensitivity and specificity.Then while we're waiting for our test,we researched the Internet and find that on average,one of every 10,000 people suffers from the disease.The next day the doctor calls with terrible news.She says that we have tested positive for the disease,so now we're panicking.But before panicking, let's turn to math and actuallycalculate what is the probability of being sick. So here's a quiz.Given the two pieces of information that the test has 99 percent accuracy,and that one out of every 10,000 people have the disease,what do you think the probability is that we're sick?Is it from 0-20 percent, from 20-40,40-60, 60-80, or 80-100?Take your guess and enter it below.

### 07. SL NB 06 S False Positives V1 V3-Bg6_Tvcv81A.en

Well, let's see. Let's use Bayes theorem to calculate it.We'll use the following notation,S will stand for sick,H will stand for healthy,and the plus sign will stand for testing positive.So since one out of every 10,000 people are sick,we get that P of S is 0.0001.Similarly, P of H is 0.9999.Since the test has 99 percent accuracy,both for sick and for healthy patients,we see that P of plus,given S is 0.99,the probability that the sick patient will get correctly diagnosed.And that P of plus given H is 0.01,the probability that the healthy patient will get incorrectly diagnosed as sick.So plugging that into the new formula,we get the probability of being diagnosed as positive when you're sick is exactly 0.0098,which is less than 1 percent.Really? Less than 1 percent?When the test has 99 percent accuracy?That's strange, but I guess that's the answer to the quiz.So, less than 1 percent falls in this category of 0 to 20 percent.I'm still puzzled though, why less than 1 percent Ifthe test is correct 99 percent of the time.Well, let's explore. Let's go back to the tree of possibilities.Let's say we start with 1 million patients,and they have two options, healthy and sick.Now, since 1 out of every 10,000 patients is sick,then from this group of 1 million patients,100 will be sick and the remaining 999,900 will be healthy.Now let's remember that for every 100 patients,99 get correctly diagnosed and one gets incorrectly diagnosed,this happens both for sick and for healthy patients.So, let's see how many of these patients will get diagnosed positively or negatively.Out of the 100 sick ones,99 will be correctly diagnosed aspositive and one will be incorrectly diagnosed as negative.Now, out of the healthy ones,1 percent or 9,999 will be incorrectly diagnosed as positive andthe remaining 99 percent or 989,901 will be correctly diagnosed as negative.Now let's really examine these four groups.The first group is the sick people who we will send for more test or treatment.The second is the unlucky sick people that will be sent home with no treatment.The third is a slightly confused healthy people who will be sent for more tests.And the fourth group or the majority isthe people who are healthy and were correctly diagnosed healthy and sent home.But now, here's the thing,we know we tested positively,so we must be among one of these two groups,the sick people who tested positively or the healthy people who tested positively.One group is much larger,it has 9,999 people,whereas the other one has only 99 people.The probability that we're in this group is much larger than that we're in this group.As a matter of fact, the probability that we are in the small group is99 divided by the sum, 99 plus 9,999,which is, you guessed it, 0.0098,which is smaller than 1 percent,this is the probability of being sick if you are diagnosed as positive.But why is the group of healthy people who tested positively somuch larger than the group of sick people who tested positively?The reason is because,even though the test only fails 1 percent of the time,that 1 percent is much,much larger than the one out of 10,000 rate of sickness among the population.In other words, in a group of 10,000 healthy people,1 percent or a 100 of them will get misdiagnosed as sick.On the other hand,in a group of 10,000 people,around one will be sick,this is much less.So if you know you've tested positively,you are still more likely to be among the 100 errors than among the ones sick.And how much more likely?Around 100 times, and that's why our probability of beingsick while being diagnosed positively is around 1 percent.This phenomenon is called the False Positive,and it has been a nightmare for the medical world,the legal world and many others.Search False Positives on Google,and you'll see many cases in which people have been misdiagnosed, misjudged etc.So always be aware of false positives,they are very sneaky.

### 08. SL NB 07 Q Bayesian Learning 1 V1 V4-J4BmsKXPnkA.en

Now the question is, how do we use this wonderful Bayes theorem to do machine learning.And the answer is repeatedly.Let's look at this example,a spam email classifier.So let's say, we have some data in the form of a bunch of emails.Some of them are spam and some of them are not spam, which we call ham.Spam are, "Win money now!" "Make cash easy!"et cetera. And the ham are,"How are you?" "There you are!"et cetera. And now, what we'll do is,a new email comes in say,"easy money" and we want to check if it's spam or ham.So, we take it word by word.Of course, we can be more effective if we took into account the order of the words,but for this classifier, we won't.It's surprising how good it can be even if itdoesn't take into account the order of the words.So let's study the first word say, "easy."We can see that the word "easy" appears once amongthe three spam emails and once among the five ham emails.And the word "money" appears twice amongthe three spam emails and once among the five ham emails.So, let's start with calculating some preliminary probabilities as an exercise.Given the data we have,what is the probability of an email containing the word"easy" given that it is spam? Here are some options.And let's also calculate it for the other word.Again given our data, what's the probability of an email beingspam given that it contains the word "money"?Here are the options.Enter your answer below.

### 09. SL NB 08 S Bayesian Learning 2 V1 V6-3rIYZgCXVXY.en

So let's see. We havethree spam emails and one of them contains the word 'easy,' which meansthe probability of an email containing the word 'easy' given that it's spam is one-third.Since two out of the three three spam emails containing the word 'money,' thenthe probability of an email containing the word money given that it's spam is two-thirds.And similarly, since there are five ham emailsand one of them contains the word 'easy,' thenthe probability of an email containing the word 'easy' given that it is ham is one-fifth.And same thing for the word 'money.'And the main gist of Bayesian learning is the following,we go from what's known,which is P of 'easy' given spam and P of 'money' given spam,to what's inferred, which is P ofspam given that it contains the word 'easy,' which is one-half,since there are two emails containing the word 'easy' and only one of them is spam.And P of spam given that it contains the word 'money,' which is two-thirdssince there are three emails containing the word 'money' and two of them are spam.

### 10. SL NB 09 Bayesian Learning 3 V1 V4-u-Hj4RsJn1o.en

So, let's do this calculation a bit more in detail.Since we have eight emails in total and three of them arespam and five of them are non-spam or ham,then our prior probabilities are three over eight for spam and five over eight for ham.So, onto calculate the posteriors.Say we have a spam email,since there are three of them and one contains the word easy and two don't.Then, the probability for containing the word easy is one-third,and for not containing it is two-thirds, if you're spam.And as we had calculated before,the probability of containing the easy if your ham isone-fifth and of not containing it if your ham is four-fifths.Now, by the rule of conditional probability,probability of the email spam containing the word easy is the product of these two,three over eight times one-third, which one-eighth.In a similar way, we calculate the probability of beingspam and not containing the word easy, which is one-fourth.And probabilities of being ham containing the word easy is one-eighth,and not containing it is one-half.Now, this is where we apply Bayes' rule.We know that the email contains the word easy,so our entire universe consists of only these two cases: when the is spam or ham.Those two have the same probability, one-eighth of happening.So, once we normalize the probabilities,they both turn into 50 percent.Thus, our two posterior probabilities are 50 percent.For ham emails, we can do the same procedure.Our prior are three over eight and five over eight as before.Our probabilities of containing the word money and notcontaining it are two-thirds and one-third for the spam emails,and one-fifth and four-fifths for the ham emails.Our products of probabilities are then one-quarter,one-quarter, one-eighth, and one-half.But since the email contains the word money,then we only care about these two.Since one-fourth is twice as much as one-eighth,when we normalize them we get two-thirds or 66.7 percent for spam,and one-third or 33.3 percent for ham.These are the posteriors.

### 11. MLND SL NB Naive Bayes Algorithm-CQBMB9jwcp8.en

Now, here's where the word naive comes in Naive Bayes.We're going to make a pretty naive assumption here.Let's look at the probability of two events happening together,so P of A and B.We can also read this a P of A intersection B.And we're going to say that this is the product of P of A and P of B.Now, this only happens when the two events are independent.If they're not, then this is not true.For example, if A is the event of it being hotoutside and B is the event of it being cold outside,then they both have a positive probability.But now, what's the probability of both events happening at the same time?This will be zero, since it can't be hot and cold at the same time.So, this formula doesn't follow because the events ofbeing hot and being cold are dependent on each other.But in a Naive Bayes, we will assume that our probabilities are independent.This, as we said, is a false and naive assumption,but in practice, it works very well and it makes our algorithm very fast.Another formula I will use is a formula for conditional probability.These are two ways of writing P of A intersection B.And this is the basis for our base theorem.But the trick we'll use here is to forget about P of B.And now, we don't have these being equal.But we have P of A given B to be proportional to P of B given A times P of A.This will work very well because in the practice,P of B will cancel out,so the fact that these two are proportional is very useful.And now, here's what we want.We have an email that contains the words easy and money,and we want to know if it is spam.So we want this, the probability of the email beingspam given that it contains the words easy and money.We'll start by using a conditional probability rule that we just reviewed to write it asa product of the probability that the emailcontains the words easy and money given that it is spam,times the probability of the email being spam.In this formula, A represents beingspam and B represents containing the words easy and money.Now, we are ready to use our naive assumption.This first factor over here is a probability of the emailcontaining the words easy and money given that it is spam.We can write it as a probability ofthe email containing the word easy given that it is spam,times the probability of the email containing the word money given that it is spam.Again, huge naive assumptions as these may be dependent.It could be that containing the word easy actually makes it more likely thatthe email contains the word money. But that's okay.In many cases, this assumption won't affectthe results and it will make our calculations much easier.And this is the heart of the Naive Bayes algorithm.And now, we do the same thing for hand emails.We have both probabilities written as a product of factors.But what are these factors?Well, we've calculated them before based on our data.The first one P of containing the word easy given that it is spam,is one-third since there are three spam emails and one of them contains the word easy.For P of containing money given spam,that's two-thirds since there arethree spam emails and two of them contain the word money.And P of spam is very simple.It's three over eight since there are eight emails and only three of them are spam.We do a similar calculation for the bottom one and get one-fifth,one-fifth, and five over eight.Now we multiply them and we get that the probability of spam giventhat it contains the word easy and money is proportional to one over 12.And for ham, it's proportional to one over 40.Now remember that these values are not the actual probabilities,they are proportional to the actual probabilities.So what do we do to get the actual probabilities? Here's the magic.We know that an email has to be either spam or ham,so these two should add to one.So we need to normalize them, namely,multiply them both by the same factor so that they are stillproportional to one over 12 and one over 40, but they add to one.Let's try that in a quiz. Can you find two numbers that add to one and that they arein the same proportion to each other as one over 12 and one over 40?Enter your answer below.

### 12. MLND SL NB Solution Naive Bayes Algorithm-QDj3xzjuYmo.en

So the way to do this is to actually divide each one by the sum of both.This will make sure that they add to one.For the first one, we have one over 12 divided by one over 12 plus one over 40,which is 10 divided by 13.And for the second one, we have one over 40 divided by one over 12 plus one over 40,which is three over 13. So there we go.The answers are 10 over 13 for spam and three over 13 for ham.So, for this particular email,we conclude that it is very likely to be spam.Now, what happens in general?Well, let's say we have a bunch of words that we use asfeatures to tell if the email is spam or not.Say, easy, money,cheap, et cetera.Our first step is to flip the event and the conditional to get this,then we make the naive assumption to split this into a product ofsimple factors that we can quickly calculate by looking at our data.We do this both for spam and ham,and we get some values that don't add to one.As a final step, we normalize to getour final probabilities of our email being spam or ham.And that's it. That's how the Naive Bayes algorithm works.

### img

## Part 07-Module 01-Lesson 03_Clustering

### 02. Unsupervised Learning-Mx9f99bRB3Q.en

So Katie, this is going to be a unit on unsupervised learning.&gt;&gt; Unsupervised learning is something that's very important,because most of the time, the data that you get in the real world doesn't havelittle flags attached that tell you the correct answer.So what are you to do as a machine learner in that case?You turn to unsupervised techniques to still figure something outabout that data.&gt;&gt; Okay, let's talk about them.Given a dataset without labels over all the data points are of the same class.There are sometimes still things you can do to extract useful information.Like this dataset over here, where I would say this dataset is structured ina way that is useful to recognize for a machine learning algorithm.&gt;&gt; When we look at this by eye, it looks like there's clumps orclusters in the data.And if we could identify those clumps or clusters, we could maybe say somethingabout a new, unknown data point and what its neighbors might be like.&gt;&gt; Or here's a second example of data.Maybe the data looks just like this.There's something we can say here as well.&gt;&gt; Right.So all the data in this example looks like it lives on some kind of line orsome complicated shape that you seem to be drawing in there right now.&gt;&gt; Yeah. And it's, it's, it's used to be a two-dimensional space, with x andy over here.But some of it we can reduce it to a one-dimensional line.So that's called what?&gt;&gt; That's called dimensionality reduction, usually.&gt;&gt; Dimensionality reduction.So we learned about, a little bit about clustering.&gt;&gt; Clustering is what we'll learn in this lesson.&gt;&gt; And you can see here an example of something also called unsupervisedlearning of dimensionality reduction.&gt;&gt; Which we will get in a future lesson.&gt;&gt; So these kind of things where you find structure in the data without labels,they're called unsupervised learning.And we're now gong to dive into the wonderful,wonderful magical land of unsupervised learning.&gt;&gt; Sounds great, let's dive in.

### 03. Clustering Movies-g8PKffm8IRY.en

So here's an example that should make it intuitively clear the clusteringsometimes make sense.So take Katie and me, we both have a movie collection at home.And just imagine that both of us look at each other's movies, and all movies,and Katie gets to rank them from really, really bad to great.And I to get to rank the same movies from bad to great.Now it so turns out that Katie and I have very different tastes.Maybe some movies that I love, like all my James Bond movies, butKatie doesn't like as much.And there's others or these chick flicks, that Katie loves, and I don't.So somewhat exaggerated.It mind end up, that our movies fall into different classes,depending on who likes which movies.So say, say you're Netflix, andand you look at both my queue and Katie's queue, and you graph it like that.Then you can conclude, wow, there's two different classes of movies.Without knowing anything else about movies,you would say, here's class A and class B.And they're very different in characteristics.And the reason why Netflix might want to know this is next time Katie comes in,you want to kind of propose a movie that fits into class B and on to class A.Otherwise, she's very unlikely to watch this movie.And conversely, for me, you want a reach into class A versus class B.In fact, if you were to look at those movies,you might find that old style westerns are right over here.And modern chick flicks might be sitting over here.Who knows?But that's an example of clustering.Because here, there's no target labels given.Then they move down for you if class A and B existed.But after looking at the data, you could,through clustering, deduce as two different classes, and you could even lookat the movie titles to understand what these classes are all about.

### 04. How Many Clusters-8Ygq5dRV0Kk.en

Okay, and I would argue it's 2.There's a cluster over here.And a cluster over here.And the cluster centers respectively lie right over here andsomewhere over here.So that's the place we would like to find to characterize the data.

### 04. How Many Clusters-R6oIvdBtsZw.en

The perhaps the most basic algorithm for clustering, andby far the most used is called K-MEANS.And I'm going to work with you through the algorithm with many,many quizzes for you.Here is our data space.And suppose we are given this type of data.The first question is intuitively, how many clusters do you see?Truth telling, there is not a unique answer, it could be seven it could be one,but give me the answer that seems to make the most sense

### 05. Match Points with Clusters-lS5DfbsWH34.en

In k-means, you randomly draw cluster centers andsay our first initial guess is, say, over here and over here.These are obviously not the correct cluster centers.You're not done yet.But k-means now operates in two steps.Step number is assign and step number two is optimize.So let's talk about the assignment.For classes in number one, I want you to click on exactly those ofthe red points that you believe are closer to center one than center two.

### 05. Match Points with Clusters-wJV1cRjmIYY.en

And the answer is this guy is closer these guys over her are closer.And the way to see this is you can make a line between the cluster centers andthen draw an equidistant and orthogonal line and thatline separate the space into a half space that's closer to center number one,which is the one over here, and a half space that's closer to center number two,which is the space over here.

### 06. Optimizing Centers (Rubber Bands)-TN1rQMrx65c.en

So after the assign step, you can now see these little blue lines overhere that marry data points to cluster centers.And now we're going to think of what this rubber bands.They're rubber bands that like to be as short as possible.In the optimize step, we're not allowed, now allowed to move the green clustercenter to a point where the total rubber band is minimized.

### 06. Optimizing Centers (Rubber Bands)-nNR4hjhhGBc.en

So now we know that these four points correspond tothe present class center one that was randomly chosen.And these three points over here correspond to class center in the middle.That's the assignment step.obviously that's not good enough.Now we have to optimize.And what we are optimizing is, you are minimizing the total quadratic distance.Of our cluster center to the points.We're now free to move our cluster center.I think of these little blue lines over here as rubber bands, andthat we're trying to find the state of minimum energy forthe rubber bands, where the total quadratic error is minimized.And for the top one, I'm going to give you three positions.They're all approximate, butpick the one that looks best in terms of minimization.And that's a not a trivial question at all.So one could be right over here, one could be at andone could be right over here.Which one do you think best minimize, or is minimize of the three positions,the total quadratic length of these rubber bands?

### 07. Moving Centers 2-FY0DXe0lfrI.en

And then argue this is the one that minimizes it.In fact, a center somewhere over here minimizes the totalrubber bands in the bottom case.And we can argue where exactly it falls, but you get the principle

### 07. Moving Centers 2-uC1Xwc7warg.en

Same exercise for the optimization step for the center below.I give you a couple of hypotheses, four in total.Pick the one that minimizes the total bubble length should be easy now

### 08. Match Points (again)-5j6VZr8sHo8.en

And this example is now easy.You can see that all the four over here fit with the green one.In fact, the separating line will be somewhere here and, in fact,after the next iteration of optimize, with the assignment of those point, fourpoints of this cluster center and those three points of this cluster center.You can see that this cluster center will movestraight into the center of those four points.And this cluster center will move to the center of those three points.Have we truly achieved our result?We now have assumed it's two clusters.But our algorithm of iteratively assigning andoptimizing has moved the cluster center straight into what wewould argue is actually the correct centroid for those two clusters over here.That is called the k-Means algorithm.

### 08. Match Points (again)-9J3IwQFXveI.en

So, let's do it again now.With these new cluster centers, pick the one up here andclick on all of the seven data points that you now believe will be assigned forthe cluster center on the left

### 09. Handoff to Katie-knrPsGtpyQY.en

So you learn about k-means.Katie is going to give you one more example of how to apply k-means in practice.

### 10. K-Means Cluster Visualization-ZMfwPUrOFsE.en

And I hope you said three,it's pretty obvious that there should be three centroids here.So let's add three, one, two, three.So they're all starting out right next to each other, butwe'll see how as the algorithm progresses, they end up in the right place.

### 10. K-Means Cluster Visualization-iCTPBcowJRY.en

Now I want to show you a visualization tool that I found online that Ithink does a really great job of helping you see what k-means clustering does.And that should give you a good intuition for how it works.So I'd like to give a special shout out to Naftali Harris,who wrote this visualization and very kindly agreed to let us use it.I'll put a link to this website in the instructor notes that you can go andplay around with it on your own.So it starts out by asking me how to pick the initial centroids of my clusters.I'll start out with Randomly right now.What kind of data would I like to use?There are a number of different things here, andI encourage you to play around with them.A Gaussian Mixture has been really similar to one of the simple examples we'vedone so far.So Gaussian mixture data looks like this.These are all the points that we have to classify.The first question for you is,how many centroids do you think is the correct number of centroids on this data?

### 11. K-Means Clustering Visualization 2-fQXXa-CAoS0.en

One of the things that's immediately apparent once I start assigning mycentroids, with these colored regions, is how all the points are going tobe associated with one of the centroids, with one of the clusters.So you can see that the blue is probably already in reasonably good shape.I would say that we got a little bit lucky in where the,the initial centroid was placed.It looks like it's pretty close to the, the center of this blob of data.With the red and the green it looks like they're sitting kind of right on top ofeach other in the same cluster.So, let's watch as K-means starts to sort out this situation andget all the clusters properly allocated.So, I hit Go.The first thing that it does is it tells me explicitly which cluster each one ofthese points will fall into.So you see, we have a few blue that fall into the wrong cluster over here.And then, of course, the red and the green.So this is the association step is all the points are being associated withthe nearest centroid.And then the next thing that I'll do is I'm going to update the centroid.So now, this is going to move the centroids to the,the mean of all of the associated points.So in particular, I, I expect this green point to bepulled over to the right by the fact that we have so many points over here.So let's update.Now this is starting to look much better.If we were to just leave everything as is,you can see how the clustering was before.So now all these points that use to be green are now about to become red.And likewise with a few blue points over here.You can see how even just in one step from this bad initial condition,we've already started to capture the structure in the data pretty well.So I'm going to reassign the points.Iterate through this again to reassign each point to the nearest centroid.And now things are starting to look very, very consistent.There's probably just one, one ortwo more iterations before we have the centroid's right at the middle ofthe clusters so I update and reassign points.No points have changed sothis is the final clustering that would be assigned by k-means clustering.So in three or four steps, using this algorithm, I assigned every point toa cluster and it worked in a really beautiful way for this example.

### 12. K-Means Clustering Visualization 3-WfwX3B4d8_I.en

Now I'm going to show you another set of data that won't work out quite soperfectly, but you can see how k-means clustering is still.And the type of data that I'll use in this example is uniform points.This is what uniform points look like.It's just scattered everywhere.So I wouldn't look at this and say there's clear clusters in here that I want topick out, but I might still want to be able to describe that, say, these pointsover here are all more similar to each other than these points over there.And k-means clustering could be one way of mathematically describing that,that fact about the data.So I don't a priori have a number of centroids that I know I want to use here,so I'll use two.Seems like a reasonable number.One, two.And then let's see what happens in this case.Few points are going to be reassigned.Move the centroids.If you can see that there's a few more little adjustments here.But in the end, it basically just ends up splitting the data along this axis.If I try this again, depending on the exact initial conditions that I have andthe exact details of how these points are allocated,I can come up with something that looks a little bit different.So you can see here that Iended up splitting the data vertically rather than horizontally.And the way you should think about this is the initial placement ofthe centroids is usually pretty random and very important.And so depending on what exactly the initial conditions are,you can get clustering in the end that looks totally different.Now, this might seem like a big problem, butthere is one pretty powerful way to solve it.So let's talk about that.

### 13. Sklearn-3zHUAXcoZ7c.en

Now that I've explained the theory of k-means clustering to you,I'm going to show you how to use the scikit-learn implementation to deploy itin your own studies.So I start over here at Google, andI find that there's a whole page on clustering in scikit-learn.The first thing that I notice when I get to this page isthat there are many types of clustering, besides just k-means clustering.So all of these different columns right here are different types of clustering.We won't go into all of these, instead I want to use this page to navigate tothe k-means documentation that you can get a little bit of a better idea ofhow this is handled in scikit-learn.So here's a list of all of the different clustering methods that I have.And here the first item on the list we see is k-means, andsome summary information about the algorithm.And so one of the parameters that you have to define fork-means is the number of clusters.Remember, we had to say at the outset how many clusters we want to look for andthis is one of the things that can be most challenging actually about usingk-means is deciding how many clusters you want to try.Then he gives us some information about the scalability, which basically tellsus how the algorithm performs as you start to have lots andlots of data, or lots of clusters.A use case, which gives us a little bit of information that this is good forgeneral purpose when you have clusters that have even number ofpoints in them and so on.And, last, that the way that k-means clustering works is based onthe distances between the points.So, very consistent with what we've seen so far.Let's dig in a little bit deeper.Now we're at the k-means documentation page.And there are three parameters in particular that I want to call yourattention do.First and most important one is n_clusters.The default value for n_clusters is eight.But of course we know that the number of clusters in the algorithm is somethingthat you need to set on your own based on what you think makes sense.This might even be a parameter that you play around with.So you should always be thinking about whether you actually want touse this default value, or if you want to change it to something else.I can guarantee you that you're mostly going to want to change it tosomething else.The second parameter that I want to call your attention to is max_iter=300.Remember that when we're running k-means clustering wehave an iteration that we go through as we're finding the clusters,where we assign each point to a centroid and then we move the centroid.Then we assign the points again.We move the centroids again.And each one of those assign and move, assign andmove steps is an iteration of the algorithm.And so max_iter actually says how many iterations ofthe algorithm do you want it to go through.300 will usually be a very reasonable value for you.In fact most of the time I would guess that it's going toterminate before it gets to this maximum number.But if you want to have a finer level of control over the algorithm andhow many times it goes through that iteration process this isthe parameter that you want.And then the last parameter that I'll mention,another one that's very important.Is the number of different initializations that you give it.Remember we said that k-means clustering has this challenge,that depending on exactly what the initial conditions are,you can sometimes end up with different clusterings.And so then you want to repeat the algorithm several times sothat any one of those clusterings might be wrong, but in general,the ensemble of all the clusterings will give you something that makes sense.That's what this parameter controls.It's basically how many times does it initialize the algorithm,how many times does it come up with clusters.You can see that by default it goes through at ten times.If you think for some reason that your clustering might be particularly prone tobad initializations orchallenging initializations, then this is the parameter that you want to change.Maybe bump the number of initializations up to a higher number.But again, just to reiterate, of all those parameters,number of clusters is definitely the one that's most important.And that you should be playing around with andthinking really hard about the most.

### 14. Some challenges of k-means-e2CdlG5P4WA.en

Now this wraps up what we're going to talk about interms of the k-means algorithm.What I'll have you do is practice much more in the coding aspects ofthis in the mini project.But before we do that,here are few thoughts on things that k-means is very valuable forand a few places where you need to be careful if you're going to try to use it.

### 15. Limitations of K-Means-4Fkfu37el_k.en

So now we look at the limits of what k-means can orcannot do, and you're going to try to break it.And specifically, talk about local minima and to do this, I want to ask youa question that you can think about and see if you get the answer right.Suppose you use a fixed number of cluster centers, two or three or four.Will the output for any fixed training set, always be the same?So given a fixed data set, given a fixed number of cluster centers,when you run k-means will you always arrive at the same result?Take your best guess.

### 15. Limitations of K-Means-nvLhUSSUhiY.en

And the answer is no, as I will illustrate to you.K-means is what's called a hill climbing algorithm, andas a result it's very dependent on where you put your initial cluster centers.

### 16. Counterintuitive Clusters-StmEUgT1XSY.en

And the answer is positive, and I prove it to you.Suppose you put one cluster center right between those two points over here andthe other two somewhere in here.It doesn't even have an error.In your assignment step, you will find that pretty much everything left ofthis line would be allocated to the left cluster center.And as a result,this is the point where the total rubber band distance is minimized.So this cluster is very stable.These two guys over here,however, separate between themselves the data on the right.And they will fight for the same data points andend up somewhere partitioning the cloud on the right side.And that is a stable solution because in the assignment step, nothing changes.This guy will still correspond to all the guys over here, andthese guys will correspond to the guys over here.That's called a local minimum.And it really depends on the initialization of the cluster centers.If you had chosen these three cluster centers as your initial guesses,you would never move away from it.Thus, make sure it's really important in clustering to be aware ofthe fact it's a local hill climbing algorithm.And it can give you suboptimal solutions that, if you divide them again,it gives you a better solution.Obviously, in this case with three cluster centers,you want them over here, over here and just one on the right side over here.

### 16. Counterintuitive Clusters-aveIz1JYeAg.en

So let's make another data set.In this case, you're going to pick three cluster centers and,then, conveniently, we'll draw three clusters onto my diagram.Obviously, for three cluster centers, you want a cluster to be here, right here,and right over here.So my question is, is it possible that all these datapoints over here are represented by one cluster, andthese guys over here by two separate clusters.Given what you know about k-means, do you think it can happenthat all these points here fall into one cluster and those twofall into two clusters as one what's called a local minimum for clustering.

### 17. Counterintuitive Clusters 2-HyjBus7S2gY.en

And I would say the answer's yes.You could make it so that the cluster centers sit right on top of each other,and the separation line looks like this.And all the top points are associated to the top cluster center,and all the bottom points are associated to the bottom cluster center.Granted, it's unlikely, to have init-,,initialization like this, butif it happens, then the algorithm would believe this is one cluster.And this is another cluster.If we re-run it and you initialize differently.Say one of the cluster centers sits over here.Then a separation line will fall like that.And the classes would automatically resolve themselves.It's unlikely, but there exists a bad local minimum,even in the example I showed you over here.Now as a rule of thumb,the more cluster centers you have, the more local minima you find.But they exist, as a result, you are forced to run the algorithm multiple times.

### 17. Counterintuitive Clusters 2-xSQTzAeeoEc.en

Let me give another example and ask you a quiz.Suppose we have data just like this over here.Do you think there could be a local minimum if youinitialize this data set with two cluster centers?Is there a stable solution where which the two cluster would not end upone over here and one over here?Or put differently, is there, does there exist a bad local minimum?Yes or no?

### img

## Part 07-Module 01-Lesson 04_Decision Trees

### 01. MLND SL DT 00 Intro V2-l34ijtQhVNk.en

Hello, and welcome to the Decision Trees section.Let me introduce the concept of decision trees byplaying this fun game on the Internet. It's called the Akinator.And the way it works is the genie will ask you questions about some character,and based on these questions, it'll guess who it is.The questions, as you can see,will get more and more educated as the genie narrowed down who the person is.This is the exact same thing decisions trees do.They ask you questions and questions about the data until they narrowthe information down well enough to make a prediction.So let's play it, and I'll choose one of my favorite character from history,the great mathematician Hypatia.And here we have her Wikipedia page to help us out,so let's start answering questions.First question, is your character an adult man?Nope. Is your character older than 18?Yes, way older than 18.Has your character ever been pregnant?Well, it says there were no kids known,so I'm going to go for no. Is your character a YouTuber?Well, she would have been a wonderful Youtuber,but she was born way before Youtube was a thing, so no.Has your character recorded any albums? Again, no.Does your character have a cell phone?Nope, way before cell phones.Has your character really existed?Of course. Is your character a citizen of the United States?Nope, she was Egyptian.Has your character ever been married?Well, Wikipedia is not sure,so I'm going to go for a 'don't know'.Has your character been dead for more than a hundred years?Yes, way more than a hundred years.Is your character in the Bible?No. Is your character an orphan?I don't think so. Her dad was known, so no.Is your character European?Well, she was in the Roman Empire but she was born in Egypt,so I'm going to go for no.Is your character from Eastern Europe? Again, no.Oh, we're getting there. Is your character Egyptian?Yes. Is your character obsessed with waffles?No. Is your character a woman?Yes. Did your character know Cleopatra?Well, they didn't live in the same period of time, so no.Was your character a Pharaoh? No, she wasn't.Does your character live in Utah?No. Is your character a queen?No, she was a great mathematician but not a queen.Was your character a murderer?Yes, she was brutally murdered.Is your character a princess?No. Can your character cast spells?No. Is your character bad? No, she was very good.Oh, and the genie did it.He found the Hypatia of Alexandria. Good job, genie.The way decision trees work is very similar to these examples.So, let's dive deeper into them and learn how they work and how they get built.

### 02. MLND SL DT 01 Recommending Apps 1 MAIN V3-uI_yNrqqKVg.en

So, let's start with an example.Let's say we're in charge of writingthe recommendation engine for the App Store or for Google play.Our task is to recommend to people the app they're most likely to download,and we should do this based on previous data.Our previous data is this table with six people each in a row,and the columns are their gender, male or female,their occupation, work or study,and the app they downloaded.The options for the app are Pokemon Go, WhatsApp, and Snapchat.So, the model we'll create will take the first two columns and guess the third one.So let's start with some small quizzes to test our intuition with this data.The first quiz is the following: if we have a woman who works at an office,what app should we recommend to her,Pokemon Go, WhatsApp, or Snapchat?The second quiz is the following: if we have a man who works at a factory,what app should we recommend to him?And the third one says, if we have a girlwho's in high school, what app do we recommend to her?Enter your answers below.

### 03. MLND SL DT 02 Recommending Apps 2 MAIN V3-KSrIYqKZwCA.en

Well, let's see. A woman who works at an office.In the table, there's two women who work and both downloaded WhatsApp.So, let's say, it's safe to assume thatrecommending WhatsApp to this new person is the best idea.Now, for the man who works at a factory,we'll see that there's another man in the table who works and he downloaded Snapchat.So, we'll proceed to recommend Snapchat to the new person.And finally, the girl that is in high school.Well, we see that in the table,there are three students and they all downloaded Pokemon Go.So, we'll go ahead and recommend Pokemon Go to this new person.But now, that's how a human thinks.For the computer, it's harder to repeat this procedure.However, the computer can aska slightly more technical question like this one: between gender and occupation,which one seems more decisive for predicting what app will the users download?This question sounds slightly ambiguous, but if you think about it,the answer does make sense. Let's give it a try.

### 04. Recommending Apps-nEvW8B1HNq4.en

Okay. Well, let's see what happens if we split them by gender.If we split them by gender,we see that the women downloaded WhatsApp and Pokemon Go,while the men downloaded Snapchat and Pokemon Go.This tells us a bit, but not much.On the other hand, if split by occupation,we can see that the students all downloaded Pokemon Go,whereas, the people who work downloaded other apps.This is a good piece of information since from now on,whenever a student comes in,we'll recommend them Pokemon Go.Thus, occupation is a better featurehere for predicting what app will the users download.So, we can go ahead and make that decision.We'll do it by creating a node here that says,"To everybody that goes to school,we'll recommend Pokemon Go.And for the ones that work, let's see."If we forget about the students,now will look at the people who work.And now, it turns out that the gender split will help us.Because the women downloaded WhatsApp,and the men downloaded Snapchat.So, let's do that. Let's add another node here.The new node says "If you work,then I'll ask for your gender.And if the gender is female, we'll recommend WhatsApp.And if the gender is male,we'll recommend Snapchat." And we're done.So quick summary. If we got a student,well recommend them Pokemon Go.If we get someone who works, we'll ask for the gender.If it's a woman, we'll recommend her WhatsApp.And if it's a man,we'll recommend him Snapchat.Now, what we need to figure out is,how do we get the computer to measure the two features,and figure out that occupation is a better feature to split by? We will learn that later.

### 05. MLND SL DT 04 Q Student Admissions V3 MAIN V1-MOa335cQGI4.en

Now, in the last example,we constructed a tree with categorical features,namely gender and occupation,but we can also create a tree with continuous features.Let's go to this example,which you may see in other parts of this class.The example is an Admissions Office which takes two pieces of data from the students,their score in a test and their grades,and the better they do on them,the more likely that they'll be accepted at a university.So in here, the blue points are accepted and the red points are rejected,and our model will have the task of determininga rule for future students to be accepted or rejected.So, let's ask the same question as before.Between grades and tests,which one determines student acceptance better?And in order to help you out,let me translate this sentence into graphical terms.Since test is the horizontal axis and grades is the vertical axis,then the question becomes,between a horizontal and a vertical line,which one would cut the data in a better way,namely separating the red and the blue points as much as possible? Give this one a try.

### 06. Student Admissions-TdgBi6LtOB8.en

Well, let's try both.The best horizontal line would be somewhere around here.It does an okay job, but it doesn't really separate the points that well,at least a lot of blue points in the red area and vice versa.So, what happens if we try a vertical line?Well, it seems that the best cut is here around five.That does a pretty good job and only leavesfive red points on the blue side and five blue points on the red side.So, let's go for that one,and our answer is vertical line.This means that the best feature to separate this data is test,and the best threshold is five.Therefore, we can add our first node to the decision tree and this node asks,"Is your test greater than or equal to five or is it less than five?"And now, we can do even more.We can try dividing each of the two halves with a horizontal line,which is the equivalent of saying, "Okay.I've seen your tests.Now, let's see how you did in the grades."The left half can be cut with a vertical line over here at height seven.This means, if your test score is less than five,then you need seven or more in the grades to get accepted,otherwise, you get rejected.The right half can be separated with a vertical line at height two.This means, if your test score is greater than or equal to five,then you need to have two or more in your grades in order to get accepted,otherwise, you get rejected.So, we've built our Decision Tree in a similar way as before,except now at each node,we don't have a yes/no question,but we have a threshold which would cut the values in two.

### 07. Entropy-piLpj1V1HEk.en

Now in order to go further with Decision Trees,we need to learn an important concept called entropy.Entropy comes from physics and to explain it,we'll use the example of the three states of water.These are solid, which is ice,liquid, and gas, which is water vapor.Let's think of the particles inside ice, water, and vapor.Ice is pretty rigid in that its particles don't have many places to go.They mostly stay where they are.Water is a little less rigid in which a particle has a few places to move around.And water vapor is in the other end of the spectrum.A particle has many possibilities of where to go and can move around a lot.So, entropy measures precisely this,how much freedom does a particle have to move around?Thus, the entropy of ice is low,the entropy of liquid water is medium,and the entropy of water vapor is high.The notion of entropy can also work in probability.Let's look at these three configurations of balls inside buckets.The first bucket has four red balls.The second one has three red and one blue,and the third one has two red and two blue.And let's say balls from each color are completely indistinguishable.So we could say that entropy is given by how muchballs are allowed to move around if we put them in a line.We can see that the first bucket is very rigid.No matter how we organize the balls,we always get the same state,so it has low entropy.In the second one, we can reorganize the balls in four ways,so it has medium entropy.For the third one, we have six ways of reorganizing the balls,so it has high entropy.This is not the exact definition of entropy,but it gives us an idea,that the more rigid the set is or the more homogeneous,the less entropy you'll have, and vice versa.Another way to see entropy is in terms of knowledge.If we were to pick a random ball from each of the buckets,how much do we know about the color of this ball?In the first bucket, we know for sure that the ball is red,so we have high knowledge.In the second bucket, it's very likely to be red and not very likely to be blue.So if we bet that it's red,we'll be right most of the time.So we have medium knowledge of the color of the ball.In the third bucket, we know much less since it's equally likely to be blue or red.So here, we have low knowledge.And it turns out that knowledge and entropy are opposites.The more knowledge one has,the less entropy, and vice versa.Thus, we conclude that the first bucket has low entropy,the second one has medium entropy,and the third one has high entropy.Now when I say opposites,I don't mean additive inverse or multiplicative inverses.I only mean it in the colloquial sense of the word.When one of them is big,then the other one is small, and vice versa.Over the next few videos,we'll cook up a formula for entropy,namely, one that gives us low,medium, and high values for these buckets.

### 08. Entropy Formula-iZiSYrOKvpo.en

In order to cook up a formula for entropy,we'll consider the following game.In this game, we'll start with a configuration of balls;say red, red, red,and blue, and we'll put them inside the bucket.Now, what we do is we pick four balls out of this bucket withrepetition and we try to get the initial configuration;red, red, red, blue and if we do, we win.Let's say we pick our first ball and it's red.We record the color and put it back.Then, we pick a second ball and it's red.So, we again record the color and put it back.Then, we pick a third ball,say red and record the color and put it back.Notice that we pick the same ball again.This is completely okay. Finally, we pick a fourth ball.Now, we got blue, we record it, and put it back.We recorded red, red, red,blue. So, we win a lot of money.If say we got the colors red,blue, blue, red, then we will win no money at all.That's the game. Now, the quiz question is,which one of the three buckets is the best to play the gamewith and which one is the worst? Enter your answer below.

### 09. MLND SL DT 08 Entropy Formula 2 MAIN V2-6GHg70hrSJw.en

Well, it seems that the first bucket is the best one,because no matter what we do,we'll always pick red, red,red, red so we'll win every time.We can see that although it's not very easy to win in any of the other two,it's easier to pick red,red, red, blue in the second one,and much harder to get red,red, blue, blue in the third one.Thus, the answers are: the best bucket is the first one,the next one is okay,and the third one is the worst.But by how much more specifically?Let's ask the following question.What is the probability of winning in each of these games?So, let's start by the easy one.How likely is it to winning this game?Well, to get the first ball to be red,the probability is actually one.Same thing for the second one,the third one, and the fourth one.Since we put the ball back after recording each color,then these events are completely independent.So, the probability that they all occur is the product of the four probabilities.This means the probability is one,which matches our intuition that no matter what we do,we'll always pick red, red, red, red.Now let's go to the red,red, red, blue case.What is the probability that the first ball we'd pick is red?Well, it's three over four or 0.75 since there are three red balls, and four in total.Same thing for the second,and the third balls.Now what's the probability of the fourth ball we pick his blue?Well now it's one over four since there's only one blue ball among four.Therefore again, since the events are independent the probabilityof the four of them happening is the product of the four probabilities,which is 0.75 times 0.75 times 0.75 times 0.25.This is 0.105 or around ten 10 percent probability of winning here.And for the last one, well the chances here of getting a red ball are 50 percent,since there are two red balls and two blue balls.And the chance of getting a blue ball are the same.Thus, the chance of these balls being red, red, blue,and blue is the product,which is 0.0625 or roughly six percent for winning in this game.We summarize these results in the table over here,where the first column has a probability of a ball being red,the second one off the ball being blue,and the last one we highlight the probability of winning.Now products are confusing mainly for two reasons.The first one is that if we have,say a a thousand balls.Now, we take the product of a thousand numbers all between zero and one.This could be very tiny.The other reason is that a small change in one ofthe factors could drastically alter their product.We want something more manageable.And what's better than products?Let's ask our friend here.Yes, he is right.Sums are better than products.And now we just need to turn the products into sums.Would any of the following functions be able to help us?The options are sine,cosine, logarithm, or exponential.Enter your answer below.

### 10. Entropy Formula-w73JTBVeyjE.en

Correct logarithm is the answer since it satisfies that beautiful identity that says,the logarithm of a product is the sum of the logarithms.Thus, our product numbers becomes a sum of the logarithms of the numbers.In this case, we get minus 3.245.Now in this class, we'll be using log as a logarithm base two,and the reason is information theory.So here's our summary. We have our three configuration of red and blue balls.Now we have the probability of the ball being red and blue,and the product of them according to the sequence,which is the probability of winning the game.In the next column, we'll take the logarithm base two.But since the numbers are less than one,the logarithm is negative.Thus, if we take the negative of the whole equation,we're now dealing with positive numbers.And in the last column, we'll just divide by fourbecause what we'll use as definition of entropy is the average ofthe negatives of the logarithms of the probabilitiesof picking the balls in a way that we win the game.Thus, for the first bucket,we get 0 entropy,for the second one we get 0.81,and for the third one we get one.This is going to be our formula for entropy.In the slightly more general case offive red balls and three blue balls, we get the following.The negative of the sum offive times the logarithm of the probability of picking a red ball,which is five over eight,and three times the logarithm of the probability of picking a blue ball,which is three over eight.We can see that this is a large number since this set has a lot of entropy.In the more general case with m red balls and n blue balls, this is the formula.As the probability of picking a red ball is m divided by m plus n,and for a blue ball it's n divided by n plusn. And this is the general formula for entropy when the balls can be of two colors.

### 12. MLND SL DT 10 Q Information Gain MAIN V1-tVLOLPEtLFw.en

Okay, so now, we'll use what we know aboutentropy and information gain to build Decision Trees.Let's say, we have our data in the form ofthese red and blue points and we want to split it into two.So, I'll show you three ways of splitting it,and here are the three ways.Now, let's have a small quiz: In which ofthese do you think we have gained more information aboutour data and in which one did we gain less information? Enter your answers below.

### 13. Information Gain-k9iZL53PAmw.en

Correct. The first way to split them does not help us at all.We ended up with very similar sets ofred and blue points and we learn nothing about the data.The second way did a decent job.We managed to get most of the blue ones onone side and most of the red ones on the other side.So, now we know a bit more about it.And the third one did fantastic.It managed to send all the blue points onone side and all the red ones on the other side.Now we know a lot about our data.Actually, we'll learn how to calculate something called the information gain,and it will be zero for the first splitting,0.28 for the second, and one for the third one.And the formula for information gain is very simple.It's just the change in entropy.Let me be more specific.For every node in the Decision Tree,we can calculate the entropy of the data in the parent node.And then, we calculate the entropies of the two children.The information gain is a difference betweenthe entropy of the parent and the average entropy of the children.So, in our first example,you can calculate the entropy of the parent and its one.The entropy of each of the children is 0.72.So the average of the entropy of the children is actually 0.72.Thus, the change in entropy is one minus 0.72, which is 0.28.For this example, now, we can calculate the entropies of the children which are both one.Thus, the change in entropy is one minus one which is zero.This is a terrible splitting because they gave us zero information.And finally, in this example,the entropy of the children are both zero,since as we saw,a set where the points are the same color has zero entropy.Thus, the information gain is one minus zero which is one.This split gave us the largest information gain and as you can see,it is the best split because it managed toperfectly cut the data into the blue points and red points.So, here's our summary. The three cuts with the values of information gain.If the tree had to choose,it would go for the third cut,which is the one that gives it the largest amount of information gain.

### 14. Maximizing Information Gain-3FgJOpKfdY8.en

Okay, so now, let's go ahead and build a Decision Tree.Our algorithm will be very simple.Look at the possible splits that each column gives,calculate the information gain,and pick the largest one.So, let's calculate the entropy of the parent, which is this data.We'll calculate the entropy of the column of the labels.So there are three Pokemon Go's,two WhatsApp, and one Snapchat.The entropy is negative three over six,logarithm base two of three over six,minus two over six,logarithm base two of two over six,minus one over six,logarithm base two of one over six.This gives us 1.46.Now, if we split them by gender,we get two sets,one with one Pokemon Go and two WhatsApp,and one with one Snapchat and two Pokemon Go.The entropies for these sets are both 0.92.Thus, the average entropy of the children of this node is 0.92,and the information gain is 1.46 minus 0.92,which is zero point 54.Now, if we split by occupation,we get one set of three Pokemon Go's,and one of two WhatsApps, and one Snapchat.The first set has entropy zero,and the other has entropy 0.92.Therefore, the average of these is 0.46,and the information gain is 1.46 minus 0.46, which is one.So to summarize, splitting by the gender column gave us an information gain of 0.54,and splitting by the occupation column gave us an information gain of one.The algorithm says, pick the column withthe highest information gain, which is occupation.So we split by occupation. We'll get two sets.One is very nice,since everybody downloaded Pokemon Go,and in the other one, we can still do better.We can split based on the gender column.Since now, we get two very nice sets,one where everybody downloaded WhatsApp,and the other one where everybody downloaded Snapchat.And we're done, here's our Decision Tree.If we want to do this forcontinuous features instead of discrete features, we can still do it.We'll let you think about the details.So basically, the idea is to think of all the possible vertical and horizontal cuts,and seeing which one maximizes the entropy,and then iterating over and over as we build a Decision Tree.Here, we can see that our first cut is vertical at the value five.Our next cut will be a horizontal cut at the height seven.And our final cut will be horizontal at height two.And finally, we have our Decision Tree that cuts our data in two.

### 15. MLND SL DT 13 Random Forests MAIN V1-n5DhXhcYKcw.en

Now, here's a potential problem with Decision Trees.Let's say we have a humungous table with lots and lots of columns.So, we create our Decision Tree and let's say it looks like this.This is not a realistic tree, though, just an example.And we end up with answers like the following.If a client is male between 15 and 25 in the US,on Android, in school,likes tennis, pizza, but does not like long walks on the beach,then they're likely to download Pokemon Go.This is not good. This almost looks like the tree just memorized the data.It's overfitting. Decision Trees tend to overfit a lot.In the continuous case, this can also happen and it looks like this.The Decision Tree has many nodes which end up giving usa complicated boundary that pretty much borders every point with a small square.This is also overfitting as it doesn't generalize well to the data.So, how do we solve this? In the simplest possible way. Take a look at this.Let's take our data and say,pick some of the columns randomly.Build a Decision Tree in those columns.Now, pick some other columns randomly and builda Decision Tree in those, and do it again.And now, just let the trees vote.When we have a new data point,say this person over here,we just let all the trees make a prediction and pick the one that appears the most.For example, these trees decided thatthis person will download Snapchat, WhatsApp, and WhatsApp.So, the ensemble of trees will recommend WhatsApp.Since we used a bunch of trees on randomly picked columns,this is called a random forest.There are better ways to pick the columns than randomly and we'llsee this in the ensemble methods section of this Nanodegree.

### img

## Part 07-Module 01-Lesson 05_Introduction to Kalman Filters

### 01. -Ugx3mldc0lE.en

In this next unit,I really want to take you to a world ofamazing tools that we use to build self-driving cars.And these tools are called matrices,linear algebra or vectors.And they might look a bit scary at first,but they're very, very intuitive.In my experience, many students struggle with those tools.So my team has put together hopefully a very comprehensive introduction to those tools.If you're familiar, you're going to breeze through it.But hopefully you'll learn and demystifyall these cryptic things that you find in Wikipedia when you google Kalman filters.So join us and learn about matrices.

### 02. Introduction-XZL934YQ-FQ.en

Welcome to my second class on Kalman filters.I want to take you on a little tour to where it all began--Stanford University.Behind me is Vale, Stanford's Research Center.Let's go inside.This is Junior, Standord's most recent self-driving car.It's the child of Stanley, whom you can findin the National Museum of American History in Washington, D.C.Let me tell you something about the equipment that's on this car that makes it self-driving.This rotating thing over here is a laser-range finderthat takes distance scans 10 times a second, about a million data points.It'll be really important for the Kalman filter class I'm teaching you today.It's major function is to spot other cars so you don't run into them.There is also a camera on top. There is a stereo camera system over here.In the rear there are antennas for a GPS--global positioning system--that allows us to estimate where the car is in the world.This is a supplemental system to the localization class I just taught you.This is the data that comes from the laser.This is the car parked in the garage right now. We see the back wall.These are all range measurements that tell you how far things are away,and they are essential as the input to the Kalman filter that we're going to learn about today.

### 03. KALMAN Tracking Intro  RENDER V2-C73G7vfVNQc.en

So, I'd like to take my students onto a little journey to Stanfordand show them our self-driving car that uses sensors to sense the environment.So let me dive into the class very much right now.Through our last class,we talked about localization.We had a robot that lived in an environment and that it coulduse its sensors to determine where in the environment it is.So, here you can see the Google self-driving car using a roadmap localizing itself.But in addition what's shown here in red are measurements of other vehicles.The car uses lasers and radars to track other vehicles.Today, we're going to talk about how to find other cars.The reason why we'd like to find other cars is because we wouldn't want to run into them.So, we have to understand how to interpret sensor data to makeassessments not just where these other cars are as in the localization case,but also how fast they're moving,so that we can drive in a way that avoids collisions within the future.That's important not just for cars,it matters for pedestrians and bicyclists and understanding where the carsare and making prediction where they're going to move is absolutelyessential for safe driving in the Google car potrait.So in this class, we'll talk about tracking.The technique I'd like to teach you is called the Kalman Filter.This is an insanely popular technique for estimating the state of a system.It's actually very similar to the probabilistic localization method wetalked in the previous class, Monte Carlo localization.The primary differences are that Kalman Filtersestimate a continuous state whereas in Monte Carlo localization,we are forced to chop the word in the discrete places.As a result, the Kalman Filter happens to give usa uni-modal distribution and I'll tell you in a second what that means,whereas Monte Carlo was fine with multi-modal distributions.Both of these techniques are applicable torobot localization and tracking other vehicles.Consider the car down here.Let's assume it seizes measurement,an object here, here,here, and here for the time is t equals zero,t equals one, two and three.Where would you assume the object would be a t equals four?

### 05. KALMAN Gaussian Intro  RENDER 1 1 V3-S2v1CExswT4.en

You remember our Markov model,where the word was divided into discrete grids,and we assigned to each grid to the probability.Such a representation of probability of a space is called a histogram,and that it divides the continuous space into discrete,into finally many grid cells,and approximates the posterior distributionby a histogram over the original distribution,and the histogram is a mere approximation to this continuous distribution.In Kalman Filters, the distribution is given by what's called a Gaussian.Gaussian is a continuous function over the space oflocations and the area underneath sums up to 1.So, use Gaussian again and if you call the space x,then the Gaussian is characterized by two parameters, the mean,often abbreviated with the Greek letter Mu,and the width of the Gaussian,often called the variance.For reasons I don't wanna go into is often written as a quadratic variable, Sigma square.So, any Gaussian in 1D,which means the parameter space over here is one-dimensional,is characterized by Mu and Sigma square.So, rather than estimating the entire distribution as a histogram,our task in common phases is to maintain a Mu and a Sigma squareas our best estimate of the location of the object we are trying to find.The exact formula is an exponential ofa quadratic function where we take the exponent of this complicated expression over here.The quadratic difference of our query point x,relative to the mean Mu,divided by Sigma square,multiply by minus a half.Now if x equals Mu,then the numerator becomes 0,and if x of 0, which is one.It turns out we have to normalize this by a constant,1 over the square root of 2 Pi Sigma square.But for everything we talk about today,this constant won't matter, so we can ignore it.What matters is we have an exponential of a quadratic function over here.So, let me draw you a couple of functionsand you tell me which one you believe are Gaussian.

### 09. Maximize Gaussian - Artificial Intelligence for Robotics-fRYtUP0P4Lg.en

Starting with the following source code,I'm looking for a completion of this one line over here that returns the Gaussian functionwith arguments mu = 10, sigma2 = 4, and x = 8,and I want the output to be approximately 0.12.Here's my solution. This is the constant: 1/sprt(2pisigma2).Then I multiply with the exponential of (-.5(x-mu)*2/sigma2).Applying that to the following numbers over here gives me 0.12.Now, here's a question for you.How do I have to modify x the 8 to get the maximum return value for this function f?

### 09. Maximize Gaussian Solution - Artificial Intelligence for Robotics-2cD8T65E-jM.en

The answer is assess with the same value as mu,in which case this expression over here becomes zero, and we get the maximum.We get the peak of the Gaussian.We set x to the same value as mu, to 10, and the output is 0.2 approximately.

### 10. KALMAN QUIZ Shifting The Mean 01 RENDER 1 V2-gfBdoCFborg.en

In Kalman filters, we iterate measurement and motionis often called the measurement update and it's often called prediction.And the update will use Bayes rule,which is nothing else but a product or a multiplication.In this update, we use total probability which is a convolution or simply an addition.Let's talk first about the measurement cycle and then the prediction cycle usingour great Gaussians for implementing those steps.Suppose you're localizing another vehicleand you have a prior distribution that looks as follows;it is a very wide Gaussian with the mean over here.And now say we get a measurement that tells ussomething about the localization vehicle and it comes in likethis: it has a mean over herecalled mu and this example has a much smaller covariance for the measurement.This is an example in our prior we were fairly uncertain aboutlocation but the measurement told us quite a bit as to where the vehicle is.Here's a quiz for you with the new mean ofthe subsequent Gaussian B over here,over here, or over here.

### 11. KALMAN QUIZ Shifting The Mean 02 RENDER V1-L8vNIKvpJ1s.en

The answer is over here in the middle,it's between the two old means,the mean of the prior,and the mean of the measurement.It's slightly further on the measurement side because the measurement wasmore certain as to where the vehicle is than the prior.The more certain we are,the more we pull the mean on the direction of the certain answer.

### 12. KALMAN QUIZ Predicting The Peak 01 RENDER V1-_fGH3xJMxdM.en

Now, here's a question that's really, really hard.When we graph the new Gaussian.Graph one is very wide.It's very peaky.So if I were to measure where the peak of the new Gaussian is,this will be a very narrow and skinny Gaussian.This would be one that was width is in between the two Gaussian,and this is one is even wider than the two original Gaussians.Which one do you believe is the correct posterior after multiplying these two Gaussians?This is an insanely hard question,I'd like you to take your chances here,and explain me the answer in just a second.

### 13. KALMAN QUIZ Predicting The Peak 02 RENDER V1-mcwr6FcP2Vc.en

And very surprisingly, the resulting Gaussianis more certain than the two-component Gaussians.That is the covariance is smaller than either of the two covariances in the installation.Intuitively speaking, this is the case because we actually gain information.The two Gaussians together with high information content in either Gaussian installation.So it look like something like this.And it is completely non-obvious.You might have to take this with faith,but I can actually prove it to you.

### 14. KALMAN QUIZ Parameter Update 01  RENDER V3-UUXETqShme4.en

Suppose we multiply two Gaussians,as in Bayes rule, a prior and a measurement probability.The prior has a mean of Mu and a variance of Sigma square,and the measurement has a mean of Nu,and a covariance of r-square.Then, the new mean, Mu prime,is the weighted sum of the old means.The Mu is weighted by r-square,Mu is weighted by Sigma square,normalized by the sum of the weighting factors.The new variance term,I want to write Sigma square prime here for the new one after the update,is given by this equation over here.So, let's put this into action.We have a weighted mean over here.Clearly, the prior Gaussian has a much higher uncertainty thereforeSigma square is larger and that means the nu is weighted much much larger than the Mu.So, the mean will be closer to the nu than the mu,which means it will be somewhere over here.Interestingly enough, the variance term is unaffected by the actual means,it just uses the previous variances.It comes up with a new one that's even peak here,so the result might look like this.So, this is the common situation for the measurement update step where this is the prior,this is the measurement probability and this is the posterior.So, let's practice these equations with a simple quiz.So, here are our equations again and suppose I use the following Gaussians.These are Gaussian with equal variance, but different means.They might look as follows.Compute for me the new mean after the update and the new sigma square.

### 15. KALMAN QUIZ Parameter Update 02 RENDER V2-vl6GkkEgY4M.en

The answer for the new mean is just the one in the middle,and the reason is both weights over here are equivalent.So, we can take the mean between MU and NU, which is 11.Then the sigma square is two.If you take one over four plus one over four,then we get one over two.So, one over one over two equals two,which means the new variance term is half the size of the previous variance terms.

### 17. New Mean and Variance Solution - Artificial Intelligence for Robotics-SwxRWZaC1FM.en

Here's my answer. This is the expression for the mean.This is the one for the variance.I run it, and I get the exact same answer.I run it again for my other example of equal variances and 10 and 12 as means,and miraculously, the correct answer comes out--11 for the new mean and 2 for the new variance.If you programmed this correctly, then congratulations.You've just programmed an essential update step in the Kalman filter--the measurement update step.That's really the difficult step in Kalman filtering.The other one--the prediction step or the motion step--is much, much easier to program.

### 18. KALMAN QUIZ Gaussian Motion 01 RENDER V2-LFPT0R3VaPs.en

So, let's step a step back and look at what we've achieved.We knew there was a measurement update,an emotion update, which is also called prediction.We know that the measurement update is implemented bymultiplication which is the same as Bayes rule.The motion update is done by total probability or an addition.So, we tackle the more complicated case,this is actually the hop from mathematically,and we solve this, we give an exact expression.We even derived mathematically,and you were able to writea computer program that implements this step of the Kalman Filter.I don't want to go into too much depth here.This is a really, really easy step.Let me write it down for you.Suppose you live in a world like this,this is your current best estimate of where you are and this is your uncertainty.Now, say you move to the right side,a certain distance, and that motion itself has its own set of uncertainty.Then you arrive at a prediction that adds the motion command to the mean,and it has an increased uncertainty over the initial uncertainty.Intuitively, this makes sense.If you move to the right by this distance inexpectation you're exactly where you wish to be,but you've lost information because your motiontends to lose information as manifest by this uncertainty over here.Now, the math for this is really, really easy.A new mean, is your old mean plus the motion often called u.So, if you move over 10 meters,this will be 10 meters and you knew sigma square isyour old sigma square plus variance of the motion Gaussian.This is all you need to know,it's just an addition,and I won't prove it to you because it's really trivial.But in summary, we have a Gaussian over here.We have a Gaussian for the motion with u asthe mean and r square has its own motion uncertainty.The resulting Gaussian in the prediction step just adds these two things up,mu plus u and sigma square plus r square.Since it was so simple, let me quiz you,we have a Gaussian beforethe prediction step which mu equals 8 and sigma square equals 4.We then move to the right,a total of 10 with a motion uncertainty of 6.Now, describe to me the predicted Gaussianand give me the new mu and the new sigma square.

### 20. Predict Function - Artificial Intelligence for Robotics-DV2cX9W0tT8.en

[Thrun] Let's program this.I'm giving you a skeleton code.This is the same update function as before.Now I would like you to do the predict function,which takes our current estimate and its varianceand the motion and its uncertaintyand computes the new updated prediction: mean and variance.So for example, if our prior is 10 and 4, our motion is 12 and 4,I would like to get out to 22 and 8 according to the formulas I've just given you.

### 22. Predict Function Solution - Artificial Intelligence for Robotics-AMFig-sYGfM.en

And yes, it's as easy as this. We just add the two means andthe two variances. It's amazing, this entireprogram over here implements a one-dimensional Kalman filter.

### 23. Kalman Filter Code - Artificial Intelligence for Robotics-3xBycKfnCOQ.en

So now let's put everything together.Let's write a main program that takes these 2 functions, update and predict,and feeds into a sequence of measurements and motions.In the example I've chosen here are the measurements of 5., 6., 7., 9., and 10.The motions are 1., 1., 2., 1., 1.This all would work out really well if the initial estimate was 5,but we're setting it to 0 with a very large uncertainty of 10,000.Let's assume the measurement uncertainty is constant 4,and the motion uncertainty is constant 2.When you run this, your first estimate for position should basically become 5--4.99, and the reason is your initial uncertainty is so large,the estimate is dominated by the first measurement.Your uncertainty shrinks to 3.99, which is slightly better thanthe measurement uncertainty.You then predict that you add 1, but the uncertainty increases to 5.99,which is the motion uncertainty of 2.You update again based on the measurement 6, you get your estimate of 5.99,which is almost 6.You move 1 again. You measure 7. You move 2. You measure 9. You move 1.You measure 10, and you move a final 1.And out comes as the final result, a prediction of 10.99 for the position,which is your 10 position moved by 1,and the uncertainty--residual uncertainty of 4.Can you implement this so you get the exactly same outputs as I've gotten over here?

### 25. Kalman Filter Code Solution - Artificial Intelligence for Robotics-X7cixvcogl8.en

This piece of code implements the entire Kalman filter.It goes through all the measurement elements and quietly assumes there areas many measurements as motions indexed by n.It updates the mu and sigma using this recursive formula over here.If we plug in the nth measurement and the measurement uncertainty,it does the same with the motion, the prediction part over here.It updates the mu and sigma recursively using the nth motionand the motion uncertainty, and it prints all of those out.If I hit the Run button, I find that my first measurement updategets me effectively 5.0.It's 4.98.And that makes sense because we had a huge initial uncertainty,and [inaudible] of 5 with a relatively small measurement uncertainty.And in fact the resulting sigma square term is 3.98,which is better than 4 and 1,000, slightly better than 4.We're slightly more certain than the measurement itself.We now apply the motion of 1.We get to 5.9.Our uncertainty increases by exactly 2, from 3.9 to 5.98.And then the next update comes in at 6,and it gives us a measurement of 5.99and now a reduced uncertainty of 2.39.And then we go to move to the right again by 1,which makes the prediction 6.99.Uncertainty goes up.We measure 7. We get to 6.99, almost 7.Uncertainty goes down.We move 2 to the right, measure 9, 1 to the right,measure 10, and move 1 again.The final thing is the motion.And if you look at the end result, our estimate is almost exactly 11,which is the result of 10 + 1.And the uncertainty is 4.0 after the motionand 2.0 after the measurement.This code that you just wroteimplements a full Kalman filter for 1D.If you look at this,we have an update function that implementswhat actually is a relatively simple equation,and a prediction function which is an even simpler equationof just addition.And then you apply it to a measurement sequence and a motion sequencewith certain uncertainties associated,and this little piece of code over heregives you a full Kalman filter in 1D.I find this really amazing.Let's plug in some other values.Suppose you're really certain about the initial position.It's wrong. It's 0.It should be 5, but it's 0.And now we assume a really small uncertainty.Guess what's going to happen to the final prediction?As I hit the Run button,we find this has an effect on the final estimate.It's not 11. It's only 10.5.And the way this takes place is initially,after our first measurement update, we believe in the position of 0.This is 1.24 to the - 10th,but a really small uncertainty, even smaller than this one over here.We apply our motion update. We add a 1.We have a higher uncertainty.And now when the next measurement comes in, 6,we are now more inclined to believe the measurementbecause uncertainty is now basically 2 as opposed to 0.001.We update our position to be 2.666,which is now a jump away from 1, and we reduce our uncertainty.Motion comes in, 3.66.Uncertainty goes up.We now are willing to update even more.As you see the 7, we're willing to go to 5.1,but not quite all the way because we feel fairly confident on our wrong prior estimate.And this confidence makes it all the way to the endwhen we predict 10.5 as opposed to 11with an uncertainty of 3.98.We've corrected some of it.We were able to drag it into the right direction but not all the waybecause our false initial belief has such a strong weightin the overall equation.

### 26. KALMAN QUIZ Kalman Prediction V1-d8Gx4-RghD0.en

So, now we understand a lot about the 1D-Kalman Filter.You've programmed one, you understand how to incorporate measurements,you understand how to incorporate motion,and you really implement something that's actually really cool.Which is a full common filter for the 1D case.Now in reality, we often have many Ds,and then things become more involved.So, I'm going to just tell you how things work with an example,and why it's great to estimate in higher-dimensional state spaces.Suppose you have a two-dimensional state space of x and y,like a camera image,or in our case we might have a car that usesa radar to detect the location of a vehicle over time.Then what the 2D-Kalman filter affords to a something really amazing.And here's how it goes.Suppose at time t equals zero,you observe the object of interest to be at this coordinate.This might be another current traffic for the Google self-driving car.One time-step later you see it over here.Another time-step later you see it right over here.Where would you now expect at time t equals three the object to be?Let me give you three different places.And the answer is here.What the Kalman Filter does for you if you do estimation in higher dimensional spaces,is to not just go onto x and y spaces,but allows you to implicitly figure about the velocity of the object is,and then uses velocity estimate to make a really good prediction about the future.Now, notice the sensor itself only sees position.It never sees the actual velocity,the velocity is inferred from seeing multiple positions.So, one of the most amazing things about Kalman Filters in trackingapplications is that it's able to figure out,even though it never directly measures it,the velocity of the object,and from there is able to make predictionsabout future locations that incorporate velocity.That is just really really really great.And it's one of the reasons why Kalman filters aresuch a popular algorithm in artificial intelligence,and in control theory at large.

### img

## Part 08-Module 01-Lesson 01_Introduction to Neural Networks

### 02. Introduction-tn-CrUTkCUc.en

So let's start with two questions,what is deep learning, and what is it used for?The answer to the second question is pretty much everywhere.Recent applications include things such as beatinghumans in games such as Go, or even jeopardy,detecting spam in emails, forecasting stock prices,recognizing images in a picture,and even diagnosing illnesses sometimes with more precision than doctors.And of course, one ofthe most celebrated applications of deep learning is in self-driving cars.And what is at the heart of deep learning?This wonderful object called neural networks.Neural networks vaguely mimic the process of how the brain operates,with neurons that fire bits of information.It sounds pretty scary, right?As a matter of fact, the first time I heard of a neural network,this is the image that came into my head,some scary robot with artificial brain.But then, I got to learn a bit more about neural networks andI realized that there are actually a lot scarier than that.This is how a neural network looks.As a matter of fact, this one here is a deep neural network.Has lots of nodes, lots of edges,lots of layers, information coming through the nodes and leaving, it's quite complicated.But after looking at neural networks for a while,I realized that they're actually a lot simpler than that.When I think of a neural network,this is actually the image that comes to my mind.There is a child playing in the sand,with some red and blue shells and we are the child.Can you draw a line that separates the red and the blue shells?And the child draws this line.That's it. That's what a neural network does.Given some data in the form of blue or red points,the neural network will look for the best line that separates them.And if the data is a bit more complicated like this one over here,then we'll need a more complicated algorithm.Here, a deep neural network will do the job andfind a more complex boundary that separates the points.So with that image in mind,let's dive in and learn about neural networks.

### 03. Exemplo de classificao-Dh625piH7Z0.en

So, let's start with one classification example.Let's say we are the admissions office ata university and our job is to accept or reject students.So, in order to evaluate students,we have two pieces of information,the results of a test and their grades in school.So, let's take a look at some sample students.We'll start with Student 1 who got 9 out of 10 in the test and 8 out of 10 in the grades.That student did quite well and got accepted.Then we have Student 2 who got 3 out of 10 in the test and 4 out of 10 in the grades,and that student got rejected.And now, we have a new Student 3 who got 7 out of10 in the test and 6 out of 10 in the grades,and we're wondering if the student gets accepted or not.So, our first way to find this out is to plot students in a graph withthe horizontal axis corresponding to the score onthe test and the vertical axis corresponding to the grades,and the students would fit here.The students who got three and four gets located in the point with coordinates (3,4),and the student who got nine and eight gets located in the point with coordinates (9,8).And now we'll do what we do in most of our algorithms,which is to look at the previous data.This is how the previous data looks.These are all the previous students who got accepted or rejected.The blue points correspond to students that got accepted,and the red points to students that got rejected.So we can see in this diagram that the students would didwell in the test and grades are more likely to get accepted,and the students who did poorly in both are more likely to get rejected.So let's start with a quiz.The quiz says, does the Student 3 get accepted or rejected?What do you think? Enter your answer below.

### 04.  2 -46PywnGa_cQ.en

Correct. Well, it seems that this data can benicely separated by a line which is this line over here,and it seems that most students over the line getaccepted and most students under the line get rejected.So this line is going to be our model.The model makes a couple of mistakes since there area few blue points that are under the line and a few red points over the line.But we're not going to care about those.I will say that it's safe to predict that if a point is over the linethe student gets accepted and if it's under the line then the student gets rejected.So based on this model we'll look at the new student that we seethat they are over here at the point 7:6 which is above the line.So we can assume with some confidence that the student gets accepted.So if you answered yes, that's the correct answer.And now a question arises.The question is, how do we find this line?So we can kind of eyeball it.But the computer can't.We'll dedicate the rest of the session to show you algorithms that will find this line,not only for this example,but for much more general and complicated cases.

### 05. Linear Boundaries-X-uMlsBi07k.en

So, first let's add some math.We're going to label the horizontal axis corresponding to the test by the variable x1,and the vertical axis corresponding to the grades by the variable x2.So this boundary line that separates the blueand the red points is going to have a linear equation.The one drawn has equation 2x1+x2-18=0.What does this mean?This means that our method for accepting or rejecting studentssimply says the following: take this equation as our score,the score is 2xtest+grades-18.Now when the student comes in, we check their score.If their score is a positive number,then we accept the student and if the score isa negative number then we reject the student.This is called a prediction.We can say by convention that if the score is 0,we'll accept a student although this won't matter much at the end.And that's it. That linear equation is our model.In the more general case, our boundary will be an equation of the following wx1+w2x2+b=0.We'll abbreviate this equation in vector notation as wx+b=0,where w is the vector w1w2 and x is the vector x1x2.And we simply take the product of the two vectors.We'll refer to x as the input,to w as the weights and b as the bias.Now, for a student coordinates x1x2,we'll denote a label as Y and the label is what we're trying to predict.So if the student gets accepted,namely the point is blue,then the label is Y+1.And if the student gets rejected,namely the point is red and then the label is Y=0.Thus, each point is in the formx1x2Y or Y is 1 for the blue points and 0 for the red points.And finally, our prediction is going to be called Y-hat and it willbe what the algorithm predicts that the label will be.In this case, Y-hat is one of the algorithm predicts that the student gets accepted,which means the point lies over the line.And, Y-hat is 0 if the algorithm predicts that this didn't get rejected,which means the point is under the line.In math terms, this means that the prediction Y-hat is 1 if wx+bis greater than or equal to zero and 0 if wx+b is less than 0.So, to summarize, the points above the line haveY hat=1 and the points below the line have Y-hat=0.And, the blue points have Y=1 and the red points have Y=0.And, the goal of the algorithm is to have Y-hat resembling Y as closely as possible,which is exactly equivalent to finding the boundary line that keepsmost of the blue points above it and most of the red points below it.

### 06. 09 Higher Dimensions-eBHunImDmWw.en

Now, you may be wondering what happens if we havemore data columns so not just testing grades,but maybe something else like the ranking of the student in the class.How do we fit three columns of data?Well the only difference is that now,we won't be working in two dimensions,we'll be working in three.So now, we have three axis: x_1 for the test,x_2 for the grades and x_3 for the class ranking.And our data will look like this,like a bunch of blue and red points flying around in 3D.On our equation won't be a line in two dimension,but a plane in three dimensions with a similar equation as before.Now, the equation would be w_1_x_1 plus w_2_x_2 plus w_3_x_3 plus b equals zero,which will separate this space into two regions.This equation can still be abbreviated by Wx plus b equals zero,except our vectors will now have three entries instead of two.And our prediction will still be y head equals one ifWx plus b is greater than or equal to zero,and zero if Wx plus b is less than zero.And what if we have many columns like say n of them?Well, it's the same thing. Now, our data just leaps in n-dimensional space.Now, I have trouble picturing things in more than three dimensions.But if we can imagine that the points are just things with n coordinates called x_1, x_2,x_3 all the way up to x_n with our labels being y,then our boundaries just an n minus one dimensional hyperplane,which is a high dimensional equivalent of a line in 2D or a plane in 3D.And the equation of this n minusone dimensional hyperplane is going to be w_1_x_1 plus w_2_x_2plus all the way to w_n_x_n plus b equals zero,which we can still abbreviate to Wx plus b equals zero,where our vectors now have n entries.And our prediction is still the same as before.It is y head equals one if Wx plus b is greater than or equal tozero and y head equals zero if Wx plus b is less than zero.

### 07. DL 06 Perceptron Definition Fix V2-hImSxZyRiOw.en

So let's recap.We have our data which is all these students.The blue ones have been accepted and the red ones have been rejected.And we have our model which consists of the equation two times test plus grades minus 18,which gives rise to this boundary whichthe point where the score is zero and a prediction.The prediction says that the student gets accepted of the score is positive or zero,and rejected if the score is negative.So now we'll introduce the notion of a preceptron,which is the building block of neural networks,and it's just an encoding of our equation into a small graph.The way we've build it is the following.Here we have our data and our boundary line and we fit it inside a node.And now we add small nodes for the inputs which,in this case, they are the test and the grades.Here we can see an example where test equals seven and grades equals six.And what the perceptron does is it blocks the points seven,six and checks if the point is in the positive or negative area.If the point is in the positive area,then it returns a yes.And if it is in the negative area, it returns and no.So let's recall that our equation is score equals twotimes test plus one times grade minus 18,and that our prediction consists of acceptingthe student if the score is positive or zero,and rejecting them if the score is negative.These weights two, one, and minus 18,are what define the linear equation,and so we'll use them as labels in the graph.The two and the one will label the edges coming from X1 and X 2 respectively,and the bias unit minus 18 will label the node.Thus, when we see a node with these labels,we can think of the linear equation they generate.Another way to grab this node is to consider the bias as part of the input.Now since W1 gets multiplied by X1 and W2 by X2,It's natural to think that B gets multiplied by a one.So we'll have the B labeling and and edge coming from a one.Then what the node does is it multiplies the values coming fromthe incoming nodes by the values and the corresponding edges.Then it adds them and finally,it checks if the result is greater that are equal to zero.If it is, then the node returns a yes or a value of one,and if it isn't then the node returns a no or a value of zero.We'll be using both notations throughoutthis class although the second one will be used more often.In the general case,this is how the nodes look.We will have our node over here then end inputs comingin with values X1 up to Xn and one,and edges with weights W1 up to Wn,and B corresponding to the bias unit.And then the node calculates the linear equation Wx plus B,which is a summation from I equals one to n,of WIXI plus B.This node then checks if the value is zero or bigger, and if it is,then the node returns a value of one for yes and if not,then it returns a value of zero for no.Note that we're using an implicit function,here, which is called a step function.What the step function does is it returns a one if the input is positive or zero,and a zero if the input is negative.So in reality, these perceptrons can be seen as a combination of nodes,where the first node calculates a linear equation and the inputs on the weights,and the second node applies the step function to the result.These can be graphed as follows:the summation sign represents a linear function in the first node,and the drawing represents a step function in the second node.In the future, we will use different step functions.So this is why it's useful to specify it in the node.So as we've seen there are two ways to represent perceptions.The one on the left has a bias unit coming from an input node with a value of one,and the one in the right has the bias inside the node.

### 08. -zAkzOZntK6Y.en

So you may be wondering why are these objects called neural networks.Well, the reason why they're called neural networks isbecause perceptions kind of look like neurons in the brain.In the left we have a perception with four inputs.The number is one, zero,four, and minus two.And what the perception does,it calculates some equations on the input and decides to return a one or a zero.In a similar way neurons in the brain take inputs coming from the dendrites.These inputs are nervous impulses.So what the neuron does is it does something with the nervous impulsesand then it decides if it outputs a nervous impulse or not through the axon.The way we'll create neural networks later in this lessonis by concatenating these perceptions so we'll be mimickingthe way the brain connects neurons by taking the output fromone and turning it into the input for another one.

### 10. 07 Perceptron Algorithm Trick-lif_qPmXvWA.en

Now, let me show you a trick that will make a line go closer to a point.Let's say we have our linear equation for example,3x1 + 4x2 -10.And that linear equation gives us a line which isthe points where the equation is zero and two regions.The positive region drawn in blue where 3x1 + 4x2 - 10 is positive,and the negative region drawn in red with 3x1 + 4x2 - 10 is negative.So here we have our lonely misclassified point, the 0.4,5 which is a red point in the blue area,and the point has to come closer.So how do we get that point to come closer to the line?Well, the idea is we're going to take the four and five and use them to modifythe equation of the line in order to get the line to move closer to the point.So here are parameters of the line 3,4 and -10 and the coordinates of the point are 4 and 5,and let's also add a one here for the bias unit.So what we'll do is subtract these numbers from the parameters of the line to get 3 - 4,4 - 5, and -10 -1.The new line will have parameters -1, -1, -11.And this line will move drastically towards the point,possibly even going over it and placing it in the correct area.Now, since we have a lot of other points,we don't want to make any drastic moves since we mayaccidentally misclassify all our other points.We want the line to make a small move towards that point and for this,we need to take small steps towards the point.So here's where we introduce the learning rate,the learning rate is a small number for example,0.1 and what we'll do is instead of subtracting four,five and one from the coordinates of the line,we'll multiply these numbers by 0.1 and then subtract them from the equation of the line.This means we'll be subtracting 0.4,0.5, and 0.1 from the equation of the line.Obtaining a new equation of 2.6x1 + 3.5x 2 - 10.1 = 0.This new line will actually move closer to the point.In the same way, if we have a blue point in the red area, for example,the point 1,1 is a positively labeled point in the negative area.This point is also misclassified and it says, come closer.So what do we do here is the same thing,except now instead of subtractingthe coordinates to the parameters of the line, we add them.Again, we multiply by the learning rate in order to make small steps.So here we take the coordinates of the point 1,1 and putan extra one for the constant term and now,we multiply them by the learning rates 0.1.Now, we add them to the parameters of the line and we get a new line withequation 3.1x1 + 4.1x2 - 9.9.And magic, this line is closer to the point.So that's the trick we're going to use repeatedly for the Perceptron Algorithm.

### 10. DL 10 S  Perceptron Algorithm-fATmrG2hQzI.en

Well, consider this. If you're in the wrong area,you would like the line to go over you,in order to be in the right area.Thus, the points just come closer!So the line can move towards it and eventually classify it correctly.

### 10. Perceptron Algorithm--zhTROHtscQ.en

So we had a question we're trying to answer and the question is,how do we find this line that separatesthe blue points from the red points in the best possible way?Let's answer this question by first looking ata small example with three blue points and three red points.And we're going to describe an algorithm that will findthe line that splits these points properly.So the computer doesn't know where to start.It might as well start at a random place by picking a random linear equation.This equation will define a line anda positive and negative area given in blue and red respectively.What we're going to do is to look at how badly this lineis doing and then move it around to try to get better and better.Now the question is,how do we find how badly this line is doing?So let's ask all the points.Here we have four points that are correctly classified.They are these two blue points in the blue area and these two red points in the red area.And these points are correctly classified,so they say, "I'm good."And then we have these two points that are incorrectly classified.That's this red point in the blue area and this blue point in the red area.We want to get as much information from them so we want themto tell us something so that we can improve this line.So what is it that they can tell us?So here we have a misclassified point,this red point in the blue area.Now think about this.If you were this point,what would you tell the line to do?Would you like it to come closer to you or farther from you?That's our quiz.Will the misclassified point want the line to come closer to it or farther from it?

### 11. Perceptron Agorithm Pseudocode-p8Q3yu9YqYk.en

Now, we finally have all the tools for describing the perceptron algorithm.We start with the random equation,which will determine some line,and two regions, the positive and the negative region.Now, we'll move this line around to get a better and better fit.So, we ask all the points how they're doing.The four correctly classified points say, "I'm good."And the two incorrectly classified points say, "Come closer."So, let's listen to the point in the right,and apply the trick to make the line closer to this point.So, here it is. Now, this point is good.Now, let's listen to the point in the left.The points says, "Come closer."We apply the trick,and now the line goes closer to it,and it actually goes over it classifying correctly.Now, every point is correctly classified and happy.So, let's actually write the pseudocode for this perceptron algorithm.We start with random weights,w1 up to wn and b.This gives us the question wx plus b,the line, and the positive and negative areas.Now, for every misclassified point with coordinates x1 up to xn,we do the following.If the prediction was zero,which means the point is a positive point in the negative area,then we'll update the weights as follows: for i equals 1 to n,we change wi, to wi plus alpha times xi,where alpha is the learning rate.In this case, we're using 0.1.Sometimes, we use 0.01 etc.It depends. Then we also change the bi as unit to b plus alpha.That moves the line closer to the misclassified point.Now, if the prediction was one,which means a point is a negative point in the positive area,then we'll update the weights in a similar way,except we subtract instead of adding.This means for i equals 1, change wi,to wi minus alpha xi,and change the bi as unit b to b minus alpha.And now, the line moves closer to our misclassified point.And now, we just repeat this step until we get no errors,or until we have a number of error that is small.Or simply we can just say,do the step a thousand times and stop.We'll see what are our options later in the class.

### 12. Non-Linear Regions-B8UrWnHh1Wc.en

Okay, so let's look more carefully at this model for accepting and rejecting students.Let's say we have this student four,who got nine in the test,but only one on the grades.According to our model this student gets accepted since it'splaced over here in the positive region of this line.But let's say we don't want that since we'll say,"If your grades were terrible,no matter what you got on the test, you won't get accepted".So our data should look more like this instead.This model is much more realistic but now we have a problemwhich is the data can no longer be separated by just a line.So what is the next thing after a line?Maybe a circle. A circle would work.Maybe two lines. That could work, too.Or maybe a curve like this.That would also work. So let's go with that.Let's go with the curve.Now, unfortunately, the perceptron algorithm won't work for us this time.We'll have to come up with something more complex and actually the solution will be,we need to redefine our perceptron algorithm fora line in a way that it'll generalize to other types of curves.

### 13. Error Functions-YfUUunxWIJw.en

So the way we'll solve our problems from now on is with the help of an error function.An error function is simply something that tells us how far we are from the solution.For example, if I'm here and my goal is to get to this plant,an error function will just tell me the distance from the plant.My approach would then be to look around myself,check in which direction I can take a step to get closer to the plant,take that step and then repeat.Here the error is simply the distance from the plant.

### 14. Error Functions-jfKShxGAbok.en

Here is obvious realization of the error function.We're standing on top a mountain,Mount Errorest and I want to descendbut it's not that easy because it's cloudy and the mountain is very big,so we can't really see the big picture.What we'll do to go down is we'll look around us and weconsider all the possible directions in which we can walk.Then we pick a direction that makes us descend the most.Let's say it's this one over here.So we take a step in that direction.Thus, we've decreased the height.Once we take the step and we start the process again and again always decreasingthe height until we go all the way down the mountain, minimizing the height.In this case the key metric that we use to solve the problem is the height.We'll call the height the error.The error is what's telling us how badly we're doing atthe moment and how far we are from an ideal solution.And if we constantly take steps to decrease the error thenwe'll eventually solve our problem, descending from Mt.Errorest.Some of you may be thinking,wait, that doesn't necessarily solve the problem.What if I get stuck in a valley,a local minimum, but that's not the bottom of the mountain.This happens a lot in machine learning and we'llsee different ways to solve it later in this Nanodegree.It's also worth noting that many timesa local minimum will give us a pretty good solution to a problem.This method, which we'll study in more detail later,is called gradient descent.So let's try that approach to solve a problem.What would be a good error function here?What would be a good way to tell the computer how badly it's doing?Well, here's our line with our positive and negative area.And the question is how do we tell the computer how far it is from a perfect solution?Well, maybe we can count the number mistakes.There are two mistakes here.So that's our height. That's our error.So just as we did to descend from the mountain,we look around all the directions in which we can movethe line in order to decrease our error.So let's say we move in this direction.We'll decrease the number of errors to one and then if we're moving in that direction,we'll decrease the number of errors to zero.And then we're done, right? Well, almost.There's a small problem with that approach.In our algorithms we'll be taking very small steps and the reason for that is calculus,because our tiny steps will be calculated by derivatives.So what happens if we take very small steps here?We start with two errors and then move a tiny amount and we're still at two errors.Then move a tiny amount again and we're still two errors.Another tiny amount and we're still at two and again and again.So not much we can do here.This is equivalent to using gradient descent to try todescend from an Aztec pyramid with flat steps.If we're standing here in the second floor,for the two errors and we look around ourselves,we'll always see two errors and we'll get confused and not know what to do.On the other hand in Mt.Errorest we can detect very small variations in height and we canfigure out in what direction it can decrease the most.In math terms this means that in order for us to do gradient descentour error function can not be discrete, it should be continuous.Mt. Errorest is continuous sincesmall variations in our position will translate to small variations inthe height but the Aztec pyramid does notsince the high jumps from two to one and then from one to zero.As a matter of fact, our error function needsto be differentiable, but we'll see that later.So, what we need to do here is to constructan error function that is continuous and we'll do this as follows.So here are six points with four of them correctly classified,that's two blue and two red,and two of them incorrectly classified,that is this red point at the very left and this blue point at the very right.The error function is going to assign a large penalty tothe two incorrectly classified points andsmall penalties to the four correctly classified points.Here we are representing the size of the point as the penalty.The penalty is roughly the distance from the boundary when the point ismisclassified and almost zero when the point is correctly classified.We'll learn the formula for the error later in the class.So, now we obtain the total error by adding all the errors from the corresponding points.Here we have a large number so it istwo misclassified points add a large amount to the error.And the idea now is to move the line around in order to decrease these error.But now we can do it because we can make very tiny changes to the parameters ofthe line which will amount to very tiny changes in the error function.So, if you move the line,say, in this direction,we can see that some errors decrease, some slightly increase,but in general when we consider the sum,the sum gets smaller and we can see that because we've nowcorrectly classified the two points that were misclassified before.So once we are able to build an error function with this property,we can now use gradient descent to solve our problem.So here's the full picture.Here we are at the summit of Mt.Errorest. We're quite high up because our error is large.As you can see the error is the height which is the sum of the blue and red areas.We explore around to see what direction brings us down the most, or equivalently,what direction can we move the line to reduce the error the most,and we take a step in that direction.So in the mountain we go down one step and in the graph we've reduced the error abit by correctly classifying one of the points. And now we do it again.We calculate the error,we look around ourselves to see in what direction we descend the most,we take a step in that direction and that brings us down the mountain.So on the left we have reduced the height and successfullydescended from the mountain and on the right we havereduced the error to its minimum possible value and successfully classified our points.Now the question is, how do we define this error function?That's what we'll do next.

### 15. Discrete vs Continuous-rdP-RPDFkl0.en

In the last section we pointed out the difference between a discrete anda continuous error function and discovered that in order forus to use gradient descent we need a continuous error function.In order to do this we also need to movefrom discrete predictions to continuous predictions.Let me show you what I mean by that.

### 15. Discrete vs. Continuous-Rm2KxFaPiJg.en

The prediction is basically the answer we get from the algorithm.A discreet answer will be of the form yes, no.Whereas a continued answer will be a number,normally between zero and one which we'll consider a probability.In the running example,here we have our students where blue is accepted and red is rejected.And the discrete algorithm will tell us if a student is accepted or rejectedby typing a zero for rejected students and a one for accepted students.On the other hand,the farther our point is from the black line,the more drastic these probabilities are.Points that are well into the blue area get very high probabilities,such as this point with an 85% probability of being blue.And points that are well into the red region are given very low probabilities,such as this point on the bottom that is given a 20% probability of being blue.The points over the line are all given a 50% probability of being blue.As you can see the probability is a function of the distance from the line.The way we move from discrete predictions to continuous,is to simply change your activation function from the step function in the left,to the sigmoid function on the right.The sigmoid function is simply a function which forlarge positive numbers will give us values very close to one.For large negative numbers will give us values very close to zero.And for numbers that are close to zero,it'll give you values that are close to point five.The formula is sigmoid effects equals (x) = 1/(1 + exp(-x))So, before our model consisted of a line with a positive region and a negative region.Now it consists of an entire probability space or for each point in the planewe are given the probability that the label of the point is one for the blue points,and zero for the red points.For example, for this point the probability ofbeing blue is 50% and of being red is 50%.For this point, the probabilities are 40% for being blue,and 60% for being red.For this one over here it's 30% for blue,and 70% for red.And for this point all over here is 80% for beingblue and 25 percent for being red.The way we obtain this probability space is very simple.We just combine the linear function WX + b with the sigmoid function.So in the left we have the lines that represent the points for which WX + b iszero, one, two, minus one, minus two, etc.And once we apply the sigmoid function to each of these values in the plane,we then obtain numbers from zero to one for each point.These numbers are just the probabilities of the point being blue.The probability of the point being blue is a prediction ofthe model Y hat to sigmoid of W x plus b.Here we can see the lines for which the prediction is point five,point six, point seven,point four, point three, et cetera.As you can see, as we get more into the blue area,(Wx + b) gets closer and closer to one.And as we move into the red area,(Wx + b) gets closer and closer to zero.When we're over the main line,W x plus b is zero,which means sigmoid of W s plus b is exactly zero point five.So here on the left we have our old perceptron withthe activation function as a step function.And on the right we have our new perceptron,where the activation function is the sigmoid function.What our new perceptron does,it takes the inputs,multiplies them by the weights in the edges and adds the results,then applies the sigmoid function.So instead of returning one and zero like before it returnsvalues between zero and one such as 0.99 or 0.67 etc.Before it used to say the student got accepted or not,and now it says the probability of the student got accepted is this much.

### 16. DL 18 Q Softmax V2-RC_A9Tu99y4.en

Let's switch to a different example for a moment.Let's say we have a model that will predict if you receive a gift or not.So, the model use predictions in the following way.It says, the probability that you get a gift is 0.8,which automatically implies that the probability that you don't receive a gift is 0.2.And what does the model do?What the model does is take some inputs.For example, is it your birthday or have it been good all year?And based on those inputs,it calculates a linear model which would be the score.Then, the probability that you get the gift or notis simply the sigmoid function applied to that score.Now, what if you had more options than just getting a gift or not a gift?Let's say we have a model that just tell us what animal we just saw,and the options are a duck,a beaver and a walrus.We want a model that tells an answer along the lines of,the probability of a duck is 0.67,the probability of a beaver is 0.24,and the probability of a walrus is 0.09.Notice that the probabilities need to add to one.Let's say we have a linear model based on some inputs.The inputs could be, does it have a beak or not?Number of teeth. Number of feathers.Hair, no hair. Does it live in the water? Does it fly?Etc. We calculate linear function based on those inputs,and let's say we get some scores.So, the duck gets a score of two,and the beaver gets a score of one,and the walrus gets a score of zero.And now the question is,how do we turn these scores into probabilities?The first thing we need to satisfy with probabilities is as we said,they need to add to one.So the two, the one,and the zero do not add to one.The second thing we need to satisfy is,since the duck had a higher score than the beaverand the beaver had a higher score than the walrus,then we want the probability of the duck to be higher than the probability of the beaver,and the probability of the beaver to be higher than the probability of the walrus.Here's a simple way of doing it.Let's take each score and divide it by the sum of all the scores.The two becomes two divided by two plus one plus zero,the one becomes one divided by two plus one plus zero,and the zero becomes zero divided by two plus one plus zero.This kind of works because the probabilities we obtain are two thirds for the duck,one third for the beaver,and zero for the walrus.That works but there's a little problem. Let's think about it.What could this problem be?The problem is the following.What happens if our scores are negative?This is completely plausible since the scores arelinear function which could give negative values.What if we had, say, scores of 1, 0 and (-1)?Then, one of the probabilities would turn into one divided byone plus zero plus minus one which is zero,and we know very well that we cannot divide by zero.This unfortunately won't work,but the idea is good.How can we turn this idea into one that works all the time even for negative numbers?Well, it's almost like we need to turn these scores into positive scores.How do we do this?Is there a function that can help us?This is the quiz. Let's look at some options.There's sine, cosine, logarithm, and exponential.Quiz. Which one of these functions will turn every number into a positive number?Enter your answer below.

### 16. DL 18 S Softmax-n8S-v_LCTms.en

So, if you said exponential, you are correct.Because this is a function that returns a positive number for every input.E to the X is always a positive number.So, what we're going to do is exactly what we did before,except, applying it to the X to the scores.So, instead of 2,1, 0,we have E to the 2,E to the 1 and E to the 0.So, that 2 becomes E to the 2 divided by E to the two plus E to the 1 plus E to the 0.And, similarly for 1 and 0.So, the probabilities we obtain now are as 0.67, 0.24 and 0.09.This clearly add to 1.And, also notice that since the exponential function is increasing,then the duck has a higher probability than the beaver.And this one has a higher probability than the walrus.This function is called the Softmax function and it's defined formally like this.Let's say we have N classes and a linear model that gives us the following scores.Z1, Z2, up to ZN.Each score for each of the classes.What we do to turn them into probabilities is to saythe probability that the object is in class I is going to beE to the power of the ZI divided bythe sum of E to the power of Z1 plus all the way to E to the power ZN.That's how we turn scores into probabilities.So, here's a question for you.When we had two classes,we applied the sigmoid function to the scores.Now, that we have more classes we apply the softmax function to the scores.The question is, is the softmax functionfor N equals to the same as the sigmoid function?I'll let you think about it. The answer is actually,yes, but it's not super trivial why.And, it's a nice thing to remember.

### 16. Quiz - Softmax-NNoezNnAMTY.en

So far we have models that give us an answer ofyes/no or the probability of a label being positive or negative.What if we have more classes?What if we want our model to tell us if something is red,blue, yellow or dog, cat, bird?In this video I'll show you what to do.

### 17. One-Hot Encoding-AePvjhyvsBo.en

So, as we've seen so far,all our algorithms are numerical.This means we need to input numbers,such as a score in a test or the grades,but the input data will not always look like numbers.Sometimes it looks like this.Let's say the module receives as an inputthe fact that you got a gift or didn't get a gift.How do we turn that into numbers? Well, that's easy.If you've got a gift, we'll just say that the input variable is 1.And, if you didn't get a gift,we'll just say that the input variable is 0.But, what if we have more classes as before or,let's say, our classes are Duck, Beaver and Walrus?What variable do we input in the algorithm?Maybe, we can input a 0 or 1 and a 2,but that would not work because it would assume dependencies betweenthe classes that we can't have. So, this is what we do.What we do is, we come up with one variable for each of the classes.So, our table becomes like this.That's one variable for Duck,one for Beaver and one for Walrus.And, each one has its corresponding column.Now, if the input is a duck then the variable for duck is1 and the variables for beaver and walrus are 0.Similarly for the beaver and the walrus.We may have more columns of data but at least there are no unnecessary dependencies.This process is called The One-Hot Encodingand it will be used a lot for processing data.

### 18. Maximum Likelihood 1-1yJx-QtlvNI.en

So we're still in our quest for an algorithm that will helpus pick the best model that separates our data.Well, since we're dealing with probabilities then let's use them in our favor.Let's say I'm a student and I have two models.One that tells me that my probability of getting accepted is80% and one that tells me the probability is 55%.Which model looks more accurate?Well, if I got accepted then I'd saythe better model is probably the one that says 80%.What if I didn't get accepted?Then the more accurate model is more likely the one that says 55 percent.But I'm just one person. What if it was me and a friend?Well, the best model would more likely be the one thatgives the higher probabilities to the events that happened to us,whether it's acceptance or rejection.This sounds pretty intuitive.The method is called maximum likelihood.What we do is we pick the model that gives the existing labels the highest probability.Thus, by maximizing the probability,we can pick the best possible model.

### 18. Maximum Likelihood 2-6nUUeQ9AeUA.en

So let me be more specific.Let's look at the following four points: two blue and two redand two models that classify them,the one on the left and the one on the right.Quick. Which model looks better? You are correct.The model on the right is much better since itclassifies the four points correctly whereasthe model in the left gets two points correctly and two points incorrectly.But let's see why the model in the right is better from the probability perspective.And by that, we'll show you that the arrangement in the right ismuch more likely to happen than the one in the left.So let's recall that our prediction is  = (Wx+b) and that thatis precisely the probability of a point being labeled positive which means blue.So for the points in the figure,let's say the model tells you that the probability of being blue are 0.9,0.6, 0.3, and 0.2.Notice that the points in the blue region are much more likely to beblue and the points in the red region are much less likely to be blue.Now obviously, the probability of being red is one minus the probability of being blue.So in this case, the probability of some of the points being red are 0.1,0.4, 0.7 and 0.8.Now what we want to do is we want to calculate the probability ofthe four points are of the colors that they actually are.This means the probability that the two red pointsare red and that the two blue points are blue.Now if we assume that the colors of the points are independent events thenthe probability for the whole arrangement isthe product of the probabilities of the four points.This is equal to 0.1  0.6  0.7  0.2 = 0.0084. This is very small.It's less than 1%.What we mean by this is that if the model is given by these probability spaces,then the probability that the points are of these colors is 0.0084.Now let's do this for both models.As we saw the model on the left tells us that the probabilities ofthese points being of those colors is 0.0084.If we do the same thing for the model on the right.Let's say we get that the probabilities of the two points inthe right being blue are 0.7 and0.9 and of the two points in the left being red are 0.8 and 0.6.When we multiply these we get 0.3024 which is around 30%.This is much higher than 0.0084.Thus, we confirm that the model on the right is better because it makesthe arrangement of the points much more likely to have those colors.So now, what we do is the following?We start from the bad modeling,calculate the probability that the points are those colors,multiply them and we obtain the total probability is 0.0084.Now if we just had a way to maximize this probability we can increaseit all the way to 0.3024.Thus, our new goal becomes precisely that,to maximize this probability.This method, as we stated before,is called maximum likelihood.

### 19. Quiz - Cross 1--xxrisIvD0E.en

Well we're getting somewhere now.We've concluded that the probability is important.And that the better model will give us a better probability.Now the question is,how we maximize the probability.Also, if remember correctly we're talking about an error function and howminimizing this error function will take us to the best possible solution.Could these two things be connected?Could we obtain an error function from the probability?Could it be that maximizing the probability is equivalentto minimizing the error function? Maybe.

### 19. Quiz Cross Entropy-njq6bYrPqSU.en

So a quick recap. We have two models,the bad one on the left and the good one on the right.And the way to tell they're bad or good is to calculatethe probability of each point being the color it is according to the model.Multiply these probabilities in order to obtain the probability ofthe whole arrangement and then check that the model onthe right gives us a much higher probability than the model on the left.Now all we need to do is to maximize this probability.But probability is a product of numbers and products are hard.Maybe this product of four numbers doesn't look so scary.But what if we have thousands of datapoints?That would correspond to a product of thousands of numbers,all of them between zero and one.This product would be very tiny,something like 0.0000 something and we definitely want to stay away from those numbers.Also, if I have a product of thousands of numbers and I change one of them,the product will change drastically.In summary, we really want to stay away from products.And what's better than products?Well, let's ask our friend here.Products are bad, but sums are good. Let's do sums.So let's try to turn these products into sums.We need to find a function that will help us turn products into sums.What would this function be?It sounds like it's time for a quiz.Quiz. Which function will help us out here?Sine, cosine, logarithm or the exponential function?Enter your answer below.

### 20. Cross Entropy 1-iREoPUrpXvE.en

Correct. The answer is logarithm,because logarithm has this very nice identity that says that the logarithm ofthe product A times B is the sum of the logarithms of A and B.So this is what we do.We take our products and we take the logarithms,so now we get a sum of the logarithms of the factors.So the ln(0.6*0.2*0.1*0.7) is equal toln(0.6) + ln(0.2) + ln(0.1) + ln(0.7) etc. Now from now until the end of class,we'll be taking the natural logarithm which is base e instead of 10.Nothing different happens with base 10.Everything works the same as everything gets scaled by the same factor.So it's just more for convention.We can calculate those values and get minus 0.51, minus 1.61,minus 0.23 etc. Notice that they are all negative numbers and that actually makes sense.This is because the logarithm of a number between 0 and 1 is alwaysa negative number since the logarithm of one is zero.So it actually makes sense to think of the negative ofthe logarithm of the probabilities and we'll get positive numbers.So that's what we'll do. We'll take the negative of the logarithm of the probabilities.That sums up negatives of logarithms of the probabilities,we'll call the cross entropy which is a very important concept in the class.If we calculate the cross entropies,we see that the bad model on left has a cross entropy 4.8 which is high.Whereas the good model on the right has a cross entropy of 1.2 which is low.This actually happens all the time.A good model will give usa low cross entropy and a bad model will give us a high cross entropy.The reason for this is simply thata good model gives us a high probability and the negativeof the logarithm of a large number is a small number and vice versa.This method is actually much more powerful than we think.If we calculate the probabilities and pair the points with the corresponding logarithms,we actually get an error for each point.So again, here we have probabilities for both models and the products of them.Now, we take the negative of the logarithms which gives us sum oflogarithms and if we pair each logarithm with the point where it came from,we actually get a value for each point.And if we calculate the values,we get this. Check it out.If we look carefully at the values we can see thatthe points that are mis-classified has likevalues like 2.3 for this point or 1.6 one for this point,whereas the points that are correctly classified have small values.And the reason for this is again is thata correctly classified point will have a probability that as close to 1,which when we take the negative of the logarithm,we'll get a small value.Thus we can think of the negatives of these logarithms as errors at each point.Points that are correctly classified will havesmall errors and points that are mis-classified will have large errors.And now we've concluded that our cross entropy will tell us if a model is good or bad.So now our goal has changed from maximizing a probability to minimizinga cross entropy in order to get from the model in left to the model in the right.And that error function that we're looking for,that was precisely the cross entropy.

### 21. CrossEntropy V1-1BnhC6e0TFw.en

Let's look a bit closer into Cross-Entropy by switching to a different example.Let's say we have three doors.And no this is not the Monty Hall problem.We have the green door, the red door,and the blue door, and behind each door we could have a gift or not have a gift.And the probabilities of there being a gift behind each door is 0.8 for the first one,0.7 for the second one,0.1 for the third one.So for example behind the green doorthere is an 80 percent probability of there being a gift,and a 20 percent probability of there not being a gift.So we can put the information in this table wherethe probabilities of there being a gift are given in the top row,and the probabilities of there not being a gift are given in the bottom row.So let's say we want to make a bet on the outcomes.So we want to try to figure out what is the most likely scenario here.And for that we'll assume they're independent events.In this case, the most likely scenario is justobtained by picking the largest probability in each column.So for the first door is more likely to have a gift than not have a gift.So we'll say there's a gift behind the first door.For the second door, it's also more likely that there's a gift.So we'll say there's a gift behind the second door.And for the third door it's much more likely that there's no gift,so we'll say there's no gift behind the third door.And as the events are independent,the probability for this whole arrangement isthe product of the three probabilities which is 0.8,times 0.7, times 0.9,which ends up being 0.504,which is roughly 50 percent.So let's look at all the possible scenarios in the table.Here's a table with all the possible scenarios for each doorand there are eight scenarios since each door gives us two possibilities each,and there are three doors.So we do as before to obtain the probability ofeach arrangement by multiplying the three independent probabilities to get these numbers.You can check that these numbers add to one.And from last video we learned that the negativeof the logarithm of the probabilities across entropy.So let's go ahead and calculate the cross-entropy.And notice that the events with high probability havelow cross-entropy and the events with low probability have high cross-entropy.For example, the second row which has probability of0.504 gives a small cross-entropy of 0.69,and the second to last row which is very very unlikely has a probability of0.006 gives a cross entropy a 5.12.So let's actually calculate a formula for the cross-entropy.Here we have our three doors,and our sample scenario said that there is a gift behind the first and second doors,and no gift behind the third door.Recall that the probabilities of these events happeningare 0.8 for a gift behind the first door,0.7 for a gift behind the second door,and 0.9 for no gift behind the third door.So when we calculate the cross-entropy,we get the negative of the logarithm of the product,which is a sum of the negatives of the logarithms of the factors,which is negative logarithm of 0.8 minus logarithm of 0.7 minus logarithm 0.9.And in order to drive the formula we'll have some variables.So let's call P1 the probability that there's a gift behind the first door,P2 the probability there's a gift behind the second door,and P3 the probability there's a gift behind the third door.So this 0.8 here is P1,this 0.7 here is P2,and this 0.9 here is one minus P3.So it's a probability of there not beinga gift is one minus the probability of there being a gift.Let's have another variable called Yi,which will be one of there's a present behind the ith door,and zero there's no present.So Yi is technically a number of presents behind the ith door.In this case Y1 equals one,Y2 equals one, and Y3 equals zero.So we can put all this together and derive a formulafor the cross-entropy and it's this sum.Now let's look at the formula inside the summation.Noted that if there is a present behind the ith door,then Yi equals one.So the first term is logarithm of the Pi.And the second term is zero.Likewise, if there is no present behind the ith door,then Yi is zero.So this first term is zero.And this term is precisely logarithm of one minus Pi.Therefore, this formula really encompasses the sums of thenegative of logarithms which is precisely the cross-entropy.So the cross-entropy really tells us when two vectors are similar or different.For example, if you calculate the cross entropy of the pair one one zero,and 0.8, 0.7, 0.1, we get 0.69.And that is low because one one zero is a similar vector to 0.8, 0.7, 0.1.Which means that the arrangement of gifts given by the first set ofnumbers is likely to happen basedon the probabilities given by the second set of numbers.But on the other hand if we calculate the cross-entropy of the pairs zero zero one,and 0.8, 0.7, 0.1,that is 5.12 which is very high.This is because the arrangement of gifts being given by the first set of numbers isvery unlikely to happen from the probabilities given by the second set of numbers.

### 21. Formula For Cross 1-qvr_ego_d6w.en

So this cross entropy, it looks like kind of a big deal.Cross entropy really says the following.If I have a bunch of events and a bunch of probabilities,how likely is it that those events happen based on the probabilities?If it's very likely,then we have a small cross entropy.If it's unlikely, then we have a large cross entropy. Let's elaborate.

### 22. DL 27 Multi-Class Cross Entropy 2 Fix-keDswcqkees.en

Now that was when we had two classes namely receiving a gift or not receiving a gift.What happens if we have more classes? Let's take a look.So we have a similar problem.We still have three doors.And this problem is still not the Monty Hall problem.Behind each door there can be an animal,and the animal can be of three types.It can be a duck, it can be a beaver,or it can be a walrus.So let's look at this table of probabilities.According to the first column on the table,behind the first door,the probability of finding a duck is 0.7,the probability of finding a beaver is 0.2,and the probability of finding a walrus is 0.1.Notice that the numbers in each column need to add toone because there is some animal behind door one.The numbers in the rows do not need to add to one as you can see.It could easly be that we have a duck behind every door and that's okay.So let's look at a sample scenario.Let's say we have our three doors,and behind the first door, there's a duck,behind the second door there's a walrus,and behind the third door there's also a walrus.Recall that the probabilities are again by the table.So a duck behind the first door is 0.7 likely,a walrus behind the second door is 0.3 likely,and a walrus behind the third door is 0.4 likely.So the probability of obtaining this three animals is the product ofthe probabilities of the three events since they are independent events,which in this case it's 0.084.And as we learn,that cross entropy here is given bythe sums of the negatives of the logarithms of the probabilities.So the first one is negative logarithm of 0.7.The second one is negative logarithm of 0.3.And the third one is negative logarithm of 0.4.The Cross entropy's and the sum of these three which is actually 2.48.But we want a formula, so let's put some variables here.So P11 is the probability of finding a duck behind door one.P12 is the probability of finding a duck behind door two etc.And let's have the indicator variables Y1j D1 if there'sa duck behind door J. Y2j B1 if there's a beaver behind door J,and Y3j B1 if there's a walrus behind door J.And these variables are zero otherwise.And so, the formula for the cross entropy issimply the negative of the summation from i_ equals_ one to n,up to summation from y_ equals_ j to m of Yij_ times_ the logarithm of Pij.In this case, m is a number of classes.This formula works because Yij being zero one,makes sure that we're only adding the logarithmsof the probabilities of the events that actually have occurred.And voila, this is the formula for the cross entropy in more classes.Now I'm going to leave this equestion.Given that we have a formula for cross entropy for two classes and one for m classes.These formulas look different but are they the same for m_ equals_ two?Obviously the answer is yes,but it's a cool exercise to actually write them down andconvince yourself that they are actually the same.

### 23. DL 29 Logistic Regression-Minimizing The Error Function-KayqiYijlzc.en

Okay. So now our goal is to minimize the error function and we'll do it as follows.We started some random weights,which will give us the predictions (Wx+b).As we saw, that also gives us a error function given by this formula.Remember that the summands are also error functions for each point.So each point will give us a larger function ifit's mis-classified and a smaller one if it's correctly classified.And the way we're going to minimize this function,is to use gradient decent.So here's Mt. Errorest and this is us,and we're going to try to jiggle the line around tosee how we can decrease the error function.Now, the error function is the height which is E(W,b),where W and b are the weights.Now what we'll do, is we'll use gradient decent in order toget to the bottom of the mountain at a much smaller height,which gives us a smaller error function E of W', b'.This will give rise to new weights,W' and b' which will give us a much better prediction.Namely,  (W'x+b').

### 23. Error Function-V5kkHldUlVU.en

So this is a good time for a quick recap of the last couple of lessons.Here we have two models.The bad model on the left and the good model on the right.And for each one of those we calculate the cross entropy which is the sum ofthe negatives of the logarithms off the probabilities of the points being their colors.And we conclude that the one on the right is betterbecause a cross entropy is much smaller.So let's actually calculate the formula for the error function.Let's split into two cases.The first case being when y=1.So when the point is blue to begin with,the model tells us that the probability of being blue is the prediction y_hat.So for these two points the probabilities are 0.6 and 0.2.As we can see the point in the blue area hasmore probability of being blue than the point in the red area.And our error is simply the negative logarithm of this probability.So it's precisely minus logarithm of y_hat.In the figure it's minus logarithm of 0.6. and minus logarithm of 0.2.Now if y=0, so when the point is red,then we need to calculate the probability of the point being red.The probability of the point being red is one minus the probability of the point beingblue which is precisely 1 minus the prediction y_hat.So the error is precisely the negative logarithm ofthis probability which is negative logarithm of 1 - y_hat.In this case we get negative logarithm 0.1 and negative logarithm 0.7.So we conclude that the error is a negative logarithm of y_hat if the point is blue.And negative logarithm of one - y_hat the point is red.We can summarize these two formulas into this one.Error = - (1-y)(ln( 1- y_hat)) - y ln(y_hat).Why does this formula work?Well because if the point is blue,then y=1 which means 1-y=0 which makes the first term0 and the second term is simply logarithm of y_hat.Similarly, if the point is red then y=0.So the second term of the formula is 0 and the first one is logarithm of 1- y_hat.Now the formula for the error function is simply the sum overall the error functions of points which is precisely the summation here.That's going to be this 4.8 we have over here.Now by convention we'll actually consider the average,not the sum which is where we are dividing by n over here.This will turn the 4.8 into a 1.2.From now on we'll use this formula as our error function.And now since y_hat is given by the sigmoid of the linear function wx + b,then the total formula for the error is actually in termsof w and b which are the weights of the model.And it's simply the summation we see here.In this case y_i is just the label of the point x_superscript_i.So now that we've calculated it our goal is to minimize it.And that's what we'll do next.And just a small aside,what we did is for binary classification problems.If we have a multiclass classification problem thenthe error is now given by the multiclass entropy.This formula is given here where for every data point we take the productof the label times the logarithm of the prediction and then we average all these values.And again it's a nice exercise to convince yourself thatthe two are the same when there are just two classes.

### 24. Gradient Descent-rhVIF-nigrY.en

So let's study gradient descent in more mathematical detail.Our function is a function of the weights and it can be graph like this.It's got a mathematical structure so it's not Mt.Everest anymore, it's more of a mount Math-Er-Horn.So we're standing somewhere in Mount Math-Er-Horn and we need to go down.So now the inputs of the functions are W1 and W2 and the error function is given byE. Then the gradient of E is given bythe vector sum of the partial derivatives of E with respect to W1 and W2.This gradient actually tells us the direction we want tomove if we want to increase the error function the most.Thus, if we take the negative of the gradient,this will tell us how to decrease the error function the most.And this is precisely what we'll do.At the point we're standing,we'll take the negative of the gradient of the error function at that point.Then we take a step in that direction.Once we take a step,we'll be in a lower position.So we do it again, and again,and again, until we are able to get to the bottom of the mountain.So this is how we calculate the gradient.We start with our initial prediction Y had equals sigmoid of W Expo's B.And let's say this prediction is bad becausethe error is large since we're high up in the mountain.The prediction looks like this,Y had equal sigmoid of W 1 x 1 plus all the way to WnXn plus b.Now the error function is given by the formula we saw before.But what matters here is the gradient of the error function.The gradient of the error function is precisely the vector formed bythe partial derivative of the error function with respect to the weights and the bias.Now, we take a step in the direction of the negative of the gradient.As before, we don't want to make any dramatic changes,so we'll introduce a smaller learning rate alpha.For example, 0.1.And we'll multiply the gradient by that number.Now taking the step is exactly the same thing asupdating the weights and the bias as follows.The weight Wi will now become Wi prime.Given by Wi minus alpha times the partial derivative of the error,with respect to Wi.And the bias will now become b prime given by b minusalpha times partial derivative of the error with respect to b.Now this will take us to a prediction with a lower error function.So, we can conclude that the prediction we have now with weights W prime b prime,is better than the one we had before with weights W and b.This is precisely the gradient descent step.

### 25. Gradient Descent Algorithm-snxmBgi_GeU.en

And now we finally have the tools to writethe pseudocode for the grading descent algorithm,and it goes like this.Step one, start with random weights w_one up to w_n and b which will give us a line,and not just a line, but the whole probability function given by sigmoid of w x plus b.Now for every point we'll calculate the error,and as we can see the error is high formisclassified points and small for correctly classified points.Now for every point with coordinates x_one up to x_n,we update w_i by adding the learning ratealpha times the partial derivative of the error function with respect to w_i.We also update b by adding alpha timesthe partial derivative of the error function with respect to be.This gives us new weights,w_i_prime and then new bias b_prime.Now we've already calculated these partial derivatives and weknow that they are y_hat minus y timesx_i for the derivative with respect to w_iand y_hat minus y for the derivative with respect to b.So that's how we'll update the weights.Now repeat this process until the error is small,or we can repeat it a fixed number of times.The number of times is called the epochs and we'll learn them later.Now this looks familiar,have we seen something like that before?Well, we look at the points and what each point is doing isit's adding a multiple of itself into the weights ofthe line in order to get the line to move closer towards it if it's misclassified.That's pretty much what the Perceptron algorithm is doing.So in the next video, we'll look atthe similarities because it's a bit suspicious how similar they are.

### 28. Gradient Descent Vs Perceptron Algorithm-uL5LuRPivTA.en

So let's compare the Perceptron algorithm and the Gradient Descent algorithm.In the Gradient Descent algorithm,we take the weights and change them from Wi toWi_ plus_ alpha_ times_ Y hat_ minus_ Y_ times_ Xi.In the Perceptron algorithm,not every point changes weights,only the misclassified ones.Here, if X is misclassified,we'll change the weights by adding Xi to Wi if the point label is positive,and subtracting if negative.Now the question is, are these two things the same?Well, let's remember that in that Perceptron algorithm,the labels are one and zero.And the predictions Y-hat are also one and zero.So, if the point is correct, classified,then Y_ minus_ Y-hat is zero because Y is equal to Y-hat.Now, if the point is labeled blue,then Y_ equals_ one.And if it's misclassified,then the prediction must be Y-hat_ equals_ zero.So Y-hat_ minus_ Y is minus one.Similarly, with the points labeled red,then Y_ equals_ zero and Y-hat_ equals_ one.So, Y-hat_ minus_ Y_ equals_ one.This may not be super clear right away.But if you stare at the screen for long enough,you'll realize that the right and the left are exactly the same thing.The only difference is that in the left,Y-hat can take any number between zero and one,whereas in the right,Y-hat can take only the values zero or one.It's pretty fascinating, isn't it?But let's study Gradient Descent even more carefully.Both in the Perceptron algorithm and the Gradient Descent algorithm,a point that is misclassified tells a line to come closer because eventually,it wants the line to surpass it so it can be in the correct side.Now, what happens if the point is correctly classified?Well, the Perceptron algorithm says do absolutely nothing.In the Gradient Descent algorithm,you are changing the weights.But what is it doing?Well, if we look carefully,what the point is telling the line,is to go farther away.And this makes sense, right?Because if you're correctly classified,say, if you're a blue point in the blue region,you'd like to be even more into the blue region,so your prediction is even closer to one,and your error is even smaller.Similarly, for a red point in the red region.So it makes sense that the point tells the line to go farther away.And that's precisely what the Gradient Descent algorithm does.The misclassified points asks the line to come closer andthe correctly classified points asks the line to go farther away.The line listens to all the points and takes steps insuch a way that it eventually arrives to a pretty good solution.

### 29. Continuous Perceptrons-07-JJ-aGEfM.en

So, this is just a small recap video that will get us ready for what's coming.Recall that if we have our data in the form of these points overhere and the linear model like this one, for example,with equation 2x1 + 7x2 - 4 = 0,this will give rise to a probability function that looks like this.Where the points on the blue or positive region have more chance of beingblue and the points in the red or negative region have more chance of being red.And this will give rise to this perception where we labelthe edges by the weights and the node by the bias.So, what the perception does,it takes to point (x1, x2),plots it in the graph and then it returns a probability that the point is blue.In this case, it returns a 0.9and this mimics the neurons in the brain because they receive nervous impulses,do something inside and return a nervous impulse.

### 30. Non-Linear Data-F7ZiE8PQiSc.en

Now we've been dealing a lot with data sets that can be separated by a line,like this one over here.But as you can imagine the real world is much more complex than that.This is where neural networks can show their full potential.In the next few videos we'll see how to deal withmore complicated data sets that requirehighly non-linear boundaries such as this one over here.

### 31. Non-Linear Models-HWuBKCZsCo8.en

So, let's go back to this example of where we sawsome data that is not linearly separable.So a line can not divide these red and blue points and we looked at some solutions,and if you remember, the one we considered more seriously was this curve over here.So what I'll teach you now is to find this curve and it's very similar than before.We'll still use grading dissent.In a nutshell, what we're going to do is forthese data which is not separable with a line,we're going to create a probability function where the points in the blue region are morelikely to be blue and the points in the red region are more likely to be red.And this curve here that separates them isa set of points which are equally likely to be blue or red.Everything will be the same as before except this equationwon't be linear and that's where neural networks come into play.

### 32. 29 Neural Network Architecture 2-FWN3Sw5fFoM.en

So in the previous session we learn that we canadd to linear models to obtain a third model.As a matter of fact, we did even more.We can take a linear combination of two models.So, the first model times a constant plus the second model times aconstant plus a bias and that gives us a non-linear model.That looks a lot like perceptrons where we can take a value times a constant plusanother value times a constant plus a bias and get a new value.And that's no coincidence.That's actually the building block of Neural Networks.So, let's look at an example.Let's say, we have this linear model where the linear equation is 5x1 minus 2x2 plus 8.That's represented by this perceptron.And we have another linear model with equations 7x1 minus3x2 minus 1 which is represented by this perceptron over here.Let's draw them nicely in here and let's use another perceptronto combine these two models using the Linear Equation,seven times the first model plus five times the second model minus six.And now the magic happens when we join these together and we get a Neural Network.We clean it up a bit and we obtain this. All the weights are there.The weights on the left,tell us what equations the linear models have.And the weights on the right,tell us what the linear combination is ofthe two models to obtain the curve non-linear model in the right.So, whenever you see a Neural Network like the one on the left,think of what could be the nonlinear boundary defined by the Neural Network.Now, note that this was drawn using the notation that puts a bias inside the node.This can also be drawn using the notation that keeps the bias as a separate node.Here, what we do is, in every layer we havea bias unit coming from a node with a one on it.So for example, the minus eight on the top nodebecomes an edge labelled minus eight coming from the bias node.We can see that this Neural Network usesa Sigmoid Activation Function and the Perceptrons.

### 32. Combinando modelos-Boy3zHVrWB4.en

Now I'm going to show you how to create these nonlinear models.What we're going to do is a very simple trick.We're going to combine two linear models into a nonlinear model as follows.Visually it looks like this.The two models over imposed creating the model on the right.It's almost like we're doing arithmetic on models.It's like saying "This line plus this line equals that curve."Let me show you how to do this mathematically.So a linear model as we know is a whole probability space.This means that for every point it gives us the probability of the point being blue.So, for example, this point over here is inthe blue region so its probability of being blue is 0.7.The same point given by the second probability space isalso in the blue region so it's probability of being blue is 0.8.Now the question is,how do we combine these two?Well, the simplest way to combine two numbers is to add them, right?So 0.8 plus 0.7 is 1.5.But now, this doesn't look like a probability anymore since it's bigger than one.And probabilities need to be between 0 and 1. So what can we do?How do we turn this number that is larger than 1 into something between 0 and 1?Well, we've been in this situation before and we have a pretty good tool thatturns every number into something between 0 and 1.That's just a sigmoid function.So that's what we're going to do.We applied the sigmoid function to 1.5 to get the value0.82 and that's the probability ofthis point being blue in the resulting probability space.So now we've managed to create a probability function forevery single point in the plane and that's how we combined two models.We calculate the probability for one of them,the probability for the other,then add them and then we apply the sigmoid function.Now, what if we wanted to weight this sum?What, if say, we wanted the model in the top to havemore of a saying the resulting probability than the second?So something like this where the resulting model looks a lot more like the one inthe top then like the one in the bottom. Well, we can add weights.For example, we can say "I want seven times the first model plus the second one."Actually, I can add the weights since I want.For example, I can say "Seven times the first one plus five times the second one."And when I do get the combine the model is I take the first probability,multiply it by seven,then take the second one and multiply it by five and I can even add a bias if I want.Say, the bias is minus 6,then we add it to the whole equation.So we'll have seven times this plus five times this minus six,which gives us 2.9.We then apply the sigmoid function and that gives us 0.95.So it's almost like we had before, isn't it?Before we had a line that is a linear combinationof the input values times the weight plus a bias.Now we have that this model is a linear combination ofthe two previous model times the weights plus some bias.So it's almost the same thing.It's almost like this curved model in the right.It's a linear combination of the two linear models beforeor we can even think of it as the line between the two models.This is no coincidence.This is at the heart of how neural networks get built.Of course, we can imagine that we can keep doing this always obtainingmore new complex models out of linear combinations of the existing ones.And this is what we're going to do to build our neural networks.

### 32. Layers-pg99FkXYK0M.en

Neural networks have a certain special architecture with layers.The first layer is called the input layer,which contains the inputs,in this case, x1 and x2.The next layer is called the hidden layer,which is a set of linear models created with this first input layer.And then the final layer is called the output layer,where the linear models get combined to obtain a nonlinear model.You can have different architectures.For example, here's one with a larger hidden layer.Now we're combining three linear models toobtain the triangular boundary in the output layer.Now what happens if the input layer has more nodes?For example, this neural network has three nodes in its input layer.Well, that just means we're not living in two-dimensional space anymore.We're living in three-dimensional space,and now our hidden layer,the one with the linear models,just gives us a bunch of planes in three space,and the output layer bounds a nonlinear region in three space.In general, if we have n nodes in our input layer,then we're thinking of data living in n-dimensional space.Now what if our output layer has more nodes?Then we just have more outputs.In that case, we just have a multiclass classification model.So if our model is telling us if an image is a cat or dog or a bird,then we simply have each node inthe output layer output a score for each one of the classes: one for the cat,one for the dog, and one for the bird.And finally, and here's where things get pretty cool,what if we have more layers?Then we have what's called a deep neural network.Now what happens here is our linear models combine to createnonlinear models and then these combine to create even more nonlinear models.In general, we can do this many times and obtainhighly complex models with lots of hidden layers.This is where the magic of neural networks happens.Many of the models in real life,for self-driving cars or for game-playing agents,have many, many hidden layers.That neural network will just splitthe n-dimensional space with a highly nonlinear boundary,such as maybe the one on the right.

### 32. Multiclass Classification-uNTtvxwfox0.en

We briefly mentioned multi-class classificationin the last video but let me be more specific.It seems that neural networks work really well whenthe problem consist on classifying two classes.For example, if the model predicts a probability of receivinga gift or not then the answer just comes as the output of the neural network.But what happens if we have more classes?Say, we want the model to tell us if an image is a duck,a beaver, or a walrus.Well, one thing we can do is create a neural network to predict if the image is a duck,then another neural network to predict if the image is a beaver,and a third neural network to predict if the image is a walrus.Then we can just use SoftMax or pick the answer that gives us the highest probability.But this seems like overkill, right?The first layers of the neural network should be enough to tell us things aboutthe image and maybe just the last layer should tell us which animal it is.As a matter of fact, as you'll see in the CNN section,this is exactly the case.So what we need here is to add more nodes in the output layer and each one ofthe nodes will give us the probability that the image is each of the animals.Now, we take the scores and apply the SoftMax function that was previouslydefined to obtain well-defined probabilities.This is how we get neural networks to do multi-class classification.

### 33. DL 41 Feedforward FIX V2-hVCuvMGOfyY.en

So now that we have defined what neural networks are,we need to learn how to train them.Training them really means what parameters should theyhave on the edges in order to model our data well.So in order to learn how to train them,we need to look carefully at how they process the input to obtain an output.So let's look at our simplest neural network, a perceptron.This perceptron receives a data point of the form x1,x2 where the label is Y=1.This means that the point is blue.Now the perceptron is defined by a linear equation say w1, x1 plus w2,x2 plus B, where w1 and w2 are the weights in the edges and B is the bias in the note.Here, w1 is bigger than w2,so we'll denote that by drawing the edge labelled w1much thicker than the edge labelled w2.Now, what the perceptron does is it plots the point x1,x2 and it outputs the probability that the point is blue.Here is the point is in the red area and then the output is a small number,since the point is not very likely to be blue.This process is known as feedforward.We can see that this is a bad model because the point is actually blue.Given that the third coordinate,the Y is one.Now if we have a more complicated neural network,then the process is the same.Here, we have thick edges corresponding to large weights andthin edges corresponding to small weights and the neural network plotsthe point in the top graph and also inthe bottom graph and the outputs coming out will be a small number from the top model.The point lies in the red area which means it has a small probability of beingblue and a large number from the second model,since the point lies in the blue area which meansit has a large probability of being blue.Now, as the two models get combined into this nonlinear model andthe output layer just plotsthe point and it tells the probability that the point is blue.As you can see, this is a bad model because itputs the point in the red area and the point is blue.Again, this process called feedforward and we'll look at it more carefully.Here, we have our neural network and the other notations so the bias is in the outside.Now we have a matrix of weights.The matrix w superscript one denoting the first layer and the entries are the weights w1,1 up to w3, 2.Notice that the biases have now been written as w3,1 and w3, 2 this is just for convenience.Now in the next layer,we also have a matrix this one is w superscript two for the second layer.This layer contains the weights that tell us how to combinethe linear models in the first layer to obtain the nonlinear model in the second layer.Now what happens is some math.We have the input in the form x1, x2,1 where the one comes from the bias unit.Now we multiply it by the matrix w1 to get these outputs.Then, we apply the sigmoid function to turn the outputs into values between zero and one.Then the vector format these values gets a one attatched forthe bias unit and multiplied by the second matrix.This returns an output that now gets thrown into a sigmoid function toobtain the final output which is y-hat.Y-hat is the prediction or the probability that the point is labeled blue.So this is what neural networks do.They take the input vector and then applya sequence of linear models and sigmoid functions.These maps when combined become a highly non-linear map.And the final formula is simply y-hat equals sigmoid ofw2 combined with sigmoid of w1 applied to x.Just for redundance, we do this again on a multi-layer perceptron or neural network.To calculate our prediction y-hat,we start with the unit vector x,then we apply the first matrix anda sigmoid function to get the values in the second layer.Then, we apply the second matrix and another sigmoid function to get the values onthe third layer and so on and so forth until we get our final prediction, y-hat.And this is the feedforward process that the neural networksuse to obtain the prediction from the input vector.

### 33. DL 42 Neural Network Error Function (1)-SC1wEW7TtKs.en

So, our goal is to train our neural network.In order to do this,we have to define the error function.So, let's look again at what the error function was for perceptrons.So, here's our perceptron.In the left, we have our input vector withentries x_1 up to x_n, and one for the bias unit.And the edges with weights W_1 up to W_n,and b for the bias unit.Finally, we can see that this perceptor uses a sigmoid function.And the prediction is defined as y-hat equals sigmoid of Wx plus b.And as we saw, this function gives us a measure ofthe error of how badly each point is being classified.Roughly, this is a very small number if the point is correctly classified,and a measure of how far the point is fromthe line and the point is incorrectly classified.So, what are we going to do to define the error function in a multilayer perceptron?Well, as we saw, our prediction is simplya combination of matrix multiplications and sigmoid functions.But the error function can be the exact same thing, right?It can be the exact same formula,except now, y-hat is just a bit more complicated.And still, this function will tell us how badly a point gets misclassified.Except now, it's looking at a more complicated boundary.

### 34. Backpropagation V2-1SmY3TZTyUk.en

So now we're finally ready to get our hands into training a neural network.So let's quickly recall feedforward.We have our perceptron with a point coming in labeled positive.And our equation w1x1 + w2x2 + b,where w1 and w2 are the weights and b is the bias.Now, what the perceptron does is,it plots a point and returns a probability that the point is blue.Which in this case is small since the point is in the red area.Thus, this is a bad perceptron since itpredicts that the point is red when the point is really blue.And now let's recall what we did in the gradient descent algorithm.We did this thing called Backpropagation.We went in the opposite direction.We asked the point, "What do you want the model to do for you?"And the point says, "Well,I am misclassified so I want this boundary to come closer to me."And we saw that the line got closer to it by updating the weights.Namely, in this case,let's say that it tells the weight w1 to go lower and the weight w2 to go higher.And this is just an illustration,it's not meant to be exact.So we obtain new weights,w1' and w2' which define a new line which is now closer to the point.So what we're doing is like descending fromMt. Errorest, right?The height is going to be the error function E(W) and we calculate the gradientof the error function which is exactlylike asking the point what does is it want the model to do.And as we take the step down the direction of the negative of the gradient,we decrease the error to come down the mountain.This gives us a new error,E(W') and a new model W' with a smaller error,which means we get a new line closer to the point.We continue doing this process in order to minimize the error.So that was for a single perceptron.Now, what do we do for multi-layer perceptrons?Well, we still do the same process of reducing the error by descending from the mountain,except now, since the error function is more complicated then it's notMt. Errorest, now it'sMt. Kilimanjerror. But same thing,we calculate the error function and its gradient.We then walk in the direction of the negative of the gradient in order to finda new model W' with a smaller errorE(W') which will give us a better prediction.And we continue doing this process in order to minimize the error.So let's look again at what feedforward does in a multi-layer perceptron.The point comes in with coordinates (x1, x2) and label y = 1.It gets plotted in the linear models corresponding to the hidden layer.And then, as this layer gets combined the point getsplotted in the resulting non-linear model in the output layer.And the probability that the point is blue is obtained bythe position of this point in the final model.Now, pay close attention because this isthe key for training neural networks, it's Backpropagation.We'll do as before, we'll check the error.So this model is not good because it predicts thatthe point will be red when in reality the point is blue.So we'll ask the point,"What do you want this model to do in order for you to be better classified?"And the point says, "I kind of want this blue region to come closer to me."Now, what does it mean for the region to come closer to it?Well, let's look at the two linear models in the hidden layer.Which one of these two models is doing better?Well, it seems like the top one is badly misclassifyingthe point whereas the bottom one is classifying it correctly.So we kind of want to listen to the bottom one more and to the top one less.So what we want to do is to reduce the weight coming fromthe top model and increase the weight coming from the bottom model.So now our final model will look a lotmore like the bottom model than like the top model.But we can do even more.We can actually go to the linear models and ask the point,"What can these models do to classify you better?"And the point will say, "Well,the top model is misclassifying me,so I kind of want this line to move closer to me.And the second model is correctly classifying me,so I want this line to move farther away from me."And so this change in the model will actually update the weights.Let's say, it'll increase these two and decrease these two.So now after we update all the weights we have better predictions atall the models in the hidden layer andalso a better prediction at the model in the output layer.Notice that in this video we intentionally left the bias unit away for clarity.In reality, when you update the weights we're also updating the bias unit.If you're the kind of person who likes formality,don't worry, we'll calculate these gradients in detail soon.

### 34. Calculating The Gradient 1 -tVuZDbUrzzI.en

Okay. So, now we'll do the same thing as we did before,painting our weights in the neural network to better classify our points.But we're going to do it formally,so fasten your seat belts because math is coming.On your left, you have a single perceptron with the input vector,the weights and the bias and the sigmoid function inside the node.And on the right, we have a formula for the prediction,which is the sigmoid function of the linear function of the input.And below, we have a formula for the error,which is the average of all points ofthe blue term for the blue points and the red term for the red points.And in order to descend from Mount Errorest,we calculate the gradient.And the gradient is simply the vector formed by all the partial derivatives ofthe error function with respect to the weights w1 up to wn and and the bias b.They correspond to these edges over here,and what do we do in a multilayer perceptron?Well, this time it's a little more complicated but it's pretty much the same thing.We have our prediction,which is simply a composition of functions namely matrix multiplications and sigmoids.And the error function is pretty much the same,except the  is a bit more complicated.And the gradient is pretty much the same thing,it's just much, much longer.It's a huge vector where each entry isa partial derivative of the error with respect to each of the weights.And these just correspond to all the edges.If we want to write this more formally,we recall that the prediction is a composition of sigmoids and matrix multiplications,where these are the matrices andthe gradient is just going to be formed by all these partial derivatives.Here, it looks like a matrix but in reality,it's just a long vector.And the gradient descent is going to do the following;we take each weight,w_i_j super k and we update it by adding a small number,the learning rate times the partial derivative of E with respect to that same weight.This is the gradient descent step,so it will give us new updated weight w_i_j super k prime.That step is going to give us a whole new modelwith new weights that will classify the point much better.

### 34. Chain Rule-YAhIBOnbt54.en

So before we start calculating derivatives,let's do a refresher on the chain rule whichis the main technique we'll use to calculate them.The chain rule says, if you have a variable x on a function f that youapply to x to get f of x, which we're gonna call A,and then another function g,which you apply to f of x to get g of f of x,which we're gonna call B, the chain rule says,if you want to find the partial derivative of B with respect to x,that's just a partial derivative of B with respect toA times the partial derivative of A with respect to x.So it literally says,when composing functions, that derivatives just multiply,and that's gonna be super useful for us becausefeed forwarding is literally composing a bunch of functions,and back propagation is literally taking the derivative at each piece,and since taking the derivative of a compositionis the same as multiplying the partial derivatives,then all we're gonna do is multiply a bunch ofpartial derivatives to get what we want. Pretty simple, right?

### 34. DL 46 Calculating The Gradient 2 V2 (2)-7lidiTGIlN4.en

So, let us go back to our neural network with our weights and our input.And recall that the weights with superscript 1 belong to the first layer,and the weights with superscript 2 belong to the second layer.Also, recall that the bias is not called b anymore.Now, it is called W31,W32 etc. for convenience,so that we can have everything in matrix notation.And now what happens with the input?So, let us do the feedforward process.In the first layer,we take the input and multiply it by the weights and that gives us h1,which is a linear function of the input and the weights.Same thing with h2,given by this formula over here.Now, in the second layer,we would take this h1 and h2 and the new bias,apply the sigmoid function,and then apply a linear function to them by multiplying them bythe weights and adding them to get a value of h. And finally,in the third layer,we just take a sigmoid function of h to getour prediction or probability between 0 and 1, which is .And we can read this in more condensed notation by saying thatthe matrix corresponding to the first layer is W superscript 1,the matrix corresponding to the second layer is W superscript 2,and then the prediction we had is just going to bethe sigmoid of W superscript 2 combined withthe sigmoid of W superscript 1 applied to the input x and that is feedforward.Now, we are going to develop backpropagation,which is precisely the reverse of feedforward.So, we are going to calculate the derivative ofthis error function with respect to each of the weights inthe labels by using the chain rule.So, let us recall that our error function is this formula over here,which is a function of the prediction .But, since the prediction is a function of all the weights wij,then the error function can be seen as the function on all the wij.Therefore, the gradient is simply the vector formed byall the partial derivatives of the error function E with respect to each of the weights.So, let us calculate one of these derivatives.Let us calculate derivative of E with respect to W11 superscript 1.So, since the prediction is simply a composition of functions and by the chain rule,we know that the derivative with respect to thisis the product of all the partial derivatives.In this case, the derivative E with respectto W11 is the derivative of either respect to  timesthe derivative  with respect to htimes the derivative h with respect to h1 times the derivative h1 with respect to W11.This may seem complicated,but the fact that we can calculate a derivative ofsuch a complicated composition function by justmultiplying 4 partial derivatives is remarkable.Now, we have already calculated the first one,the derivative of E with respect to .And if you remember, we got  minus y.So, let us calculate the other ones.Let us zoom in a bit and look at just one piece of our multi-layer perceptron.The inputs are some values h1 and h2,which are values coming in from before.And once we apply the sigmoid and a linear functionon h1 and h2 and 1 corresponding to the biased unit,we get a result h. So,now what is the derivative of h with respect to h1?Well, h is a sum of three things and only one of them contains h1.So, the second and the third summon just give us a derivative of 0.The first summon gives us W11 superscript 2 because that is a constant,and that times the derivative of the sigmoid function with respect to h1.This is something that we calculated below in the instructor comments,which is that the sigmoid function has a beautiful derivative,namely the derivative of sigmoid of h isprecisely sigmoid of h times 1 minus sigmoid of h. Again,you can see this development underneath in the instructor comments.You also have the chance to code this in the quiz because at the end of the day,we just code these formulas and then use them forever, and that is it.That is how you train a neural network.

### img

## Part 09-Module 01-Lesson 01_Intro to Computer Vision

### 01. Welcome to Computer Vision-GgA3_-MMT_I.en

One of the most fundamental ways we learn aboutour world and environment is through our sense of sight.Our ability to see permeates our very being.It has come to define, not only how we see the physical world,but also how we see the abstract.We speak of a company or an entrepreneur having a vision; we may say,"I see what you mean",to a colleague or send on the,"I see what you did there, Jeff".Our ability to see, our vision,in a very real sense determines how we interact with the world.Computer Vision strives to give a similar ability toa machine and if you want to build an agent that uses vision to extract,analyze and understand useful information then you're in the right place.We have Cezanne Camacho,a resident expert in the subject to help you learnabout Computer Vision and its fascinating applications.Hey everyone. I'm Cezanne and I'm excited toguide you through some important Computer Vision topics.Computer Vision is a fascinating set of techniques, inspired by mathematics,statistics and probability theoryand in this lesson we'll be learning about the basics ofComputer Vision and about the role it plays inartificial intelligence systems. Let's get started.

### 02. 02. What Is Vision-_99V1rUNFa4.en

So, what is vision?At its most basic,vision or visual perception is the act ofobserving patterns and objects through sight or visual input.But vision is so much more than that.It allows us to build a model of the physical world.For example, take a look around you and think about what you see,maybe you see a computer screen or a mobile phone.You may also see a desk or different items in a room.When I look around the studio I see a camera in front of me,and tables and chairs besides me.We use site to learn aboutour physical surroundings and where they are in relation to us.And with this information,we build a physical model of our world.This is how typical human vision works,but different creatures have evolvedvision systems that are tailored to their specific needs.Bees for instance have compound eyes which are many tiny eyes bundled together.Their eyes have really low resolution so they'renot so great at recognizing things from a distance,but they're very sensitive to motion which is essential while flying fast.In essence, there are a variety of vision systems each with their own strengths.This is something to keep in mind when you start building and designingsystems to perform specific visual tasks in future lessons

### 03. 03. Role In AI Render-xm1TXnNe5Pw.en

Some of you are probably wondering how vision fits into artificial intelligence.Why do AI systems need to see?Why do they need vision?Well, the basis of any AI system is that it can: one,perceive its environment and two,take actions based on those perceptions.And as we noted previously vision is a central part of how we perceive the world.One example of an AI system we're working on Udacity is a self-driving car.The car sees the world through cameras and sensorsand then uses that input to safely navigate roads by itself.Computer vision is used to analyze camera images andintelligently identify objects like other cars or pedestrians.It's also used to recognize where the car is onthe road and in the world based on the car's surroundings.So, self-driving cars and many other AI systems need vision to build a physical modelof the world just like humans do to identify physical surroundings and respond to them.

### 04. 04. Computer Vision Applications-aFJKp2NltCY.en

When we think of computer vision's role in AI,we often focus on smart object and behavior recognition.We've already talked about the self-driving car example,in which computer vision is used to recognize cars andpedestrians and also recognize their behavior,like whether they're moving fast or slow or even moving erratically.But there's a plethora of other far-reaching uses thatone wouldn't necessarily think of when talking about computer vision.It's used in state-of-the-art medical technologyto analyze medical images and identify points of interest.An AI system can even learn to recognize cancerous tissue and help withearly detection and diagnostics byanalyzing data about cancer and images of cancer cells.On a more mundane level,many of us have phones that use computer vision on a daily basis.Computer vision is used to identify and categorize personal images and video.My phone recognizes me when I takea selfie and even knows that some of my friends faces look like.Some systems are smart enough to recognizebehavior and images and even caption it correctly.So, AI systems can learn to recognize a multitude of objects andbehaviors using computer vision and by analyzing masses of images or video data.We've already spoken of AI systems using computer vision ina similar manner to the human visual system to recognize visual surroundings,but there's another essential use that humans have for visual data.We use it to gauge the emotional state of our fellow humans.That is, vision not only let's us recognize objects,but when we're communicating with another person,we use vision to look at a person's face and body languageto help us identify their mental and emotional states.As we'll see next, AI systems can learn to do this too.

### 05. 05. Emotional Intelligence-D_LzJsJH5qk.en

Today I'm with Dr. Rana el Kaliouby,Co-founder and CEO at Affectiva,a company that uses computer vision to build systems that are emotionally intelligent.Rana, I've been talking to our students about the role vision plays at a basic level inAI systems by helping to recognize objects andbehavior but your work is in recognizing and responding to human emotion.What do you think makes this such important work?When we think of human intelligence,we tend to think of cognitive intelligence or IQ,but having emotional intelligence,which is the ability to empathize and read emotions,is central to how we all interact with the world.So the question becomes,how can we build systems that are artificially emotionally intelligent?Right. And this is the challenge we work on at Affectiva.We use computer vision and machine learning torecognize emotions as they manifest on your face.Can you tell what I'm feeling right now?Maybe a combination of surprise,a little bit of horror?Yeah, definitely happy surprise.So Affectiva software can recognize emotion just like you can?Yes, by observing your facial features,so how your eyebrows and your mouth and your cheeks move.And once a computer app, you know,or other application can recognize emotions,it can respond to them in real time.And this creates a lot of opportunity for improving andpersonalizing how we and machines interact with each other.

### 06. Vision-based Emotion AI-7nKKWWn1sAc.en

So Rana, at Affectiva you build software that reads emotion from facial expressions.I was wondering if you could tell us a bit more aboutyour background and how you got involved in this work.Sure. So I studied computer science asan undergraduate at the American University in Cairoand I was really interested in howcomputers change how we connect and communicate with one another.And then I got an opportunity to study at Cambridge University formy doctorate degree and that was really my first living abroad experience,and I realized I was spending more hours with my laptop than I did with any other human.Yet it was completely oblivious to my mental and emotional state.So that got me thinking.What if that computer could read and respond to my emotions just the wayan awesome friend would?I think when I think of emotional intelligence,I don't automatically link it with artificial intelligence.So I was also wondering if you could talk aboutsome current applications of this technology.Yeah. Artificial emotional intelligence,or what we're calling emotion AI,is the core AI and it cuts across many many industries.I'll give you two examples.One is social robots.Social robots are expected to build a rapport withits users and understand and respond to social and emotional skills.Autonomous vehicles is another area where emotion AI plays a really critical role.Understanding the sentiment insidethe car and also deciding when to relinquish control back to a driver.And is that driver paying attention?Is that driver even awake?That's exciting to hear.These are problems we're working on teaching at Udacity too.I have one last question for you.What kind of advice would you give to a student whomight be wanting to pursue a career in AI?My advice would be, get hands-on and get building.Great. So let's keep learning.

### 07. 08. Computer Vision Pipeline-64hFcqhnNow.en

Let me walk you through a sequence of steps that youneed to analyze facial expressions and emotions.Other computer vision tasks have different desired outputs and corresponding algorithms,but they use a similar overall pipeline.First off, a computer receives visual input from an imaging device like a camera.This is typically captured as a sequence of images or frames.Each frame is then sent throughsome preprocessing steps that enhance the quality and detail of the image.You may also perform other transformations here such as changing from color to grayscale.Next, to these images are analyzed and our software recognizescertain facial features of interest like the curve of the mouth and shape of the eyes,and then data about these features is fed into a so-called trained model that frompreviously known data can recognize patterns inthese facial expressions and finally identify a certain emotion.It does so with a certain probability that's reported back.Finally, having recognized an emotion,an application can then act on this and interactwith a human in a way that takes their emotional state into account.

### 09. 09. Training a Model-m4GVfwVkj74.en

So, I've just described a computer vision pipeline that takes in a sequence ofimages and through a series of steps canrecognize different facial expressions and emotions.But it still seems kind of mysterious.Can you talk a bit about how exactly a model likethis can be trained to recognize different facial expressions?Sure. The process is similar to the pipeline you just described.We have 45 facial muscles that drive thousands of different expressions on our face.But let's take a specific example.Let's say we are training an algorithm to discriminate between a smile and a smirk.We collect tens of thousands of examples of people smiling  the more diverse,the better  and then tens of thousands of examples of people smirking.We feed those prerecorded images along with their labels to the system.The algorithm then looks for visual differences between the two expressions.For instance, when you smile,your teeth might show,but that's not the case with a smirk.So, you give the model lots of examples of smiles andsmirks and other facial expressions until it learns to recognize them.It sounds like how a baby learns,by lots of examples.Exactly.And similar to how humans learn,at the beginning of the training phase,the model typically performs very badly,but then it monitors the errors it makes and uses those toimprove the performance each time it sees more images.After many iterations, the model converges onthe right set of parameters once the error rate becomes acceptable,and that's when we consider the model to be fully trained.Now, this is a very high-level view of how to train any machine learning model,and the details will vary based on the type ofmodel you use and the training algorithm you choose.For instance, you can use a convolutional neural network trained using gradient descent.Next, let's see how this computer vision pipeline works in a real-time application.

### 10. AffdexMe Demo-dpFtXDqakvY.en

The best way to understand how emotion AI works is by example.Would you like to see a live demo?Sure. Why not?Everyone wants their computers to understand them better.All right, so here's the demo.Basically, what's happening here is that the algorithm islooking for faces and it's detecting your face by,you know, drawing this bounding box around your different facial features.And then it's tracking the movement of your facial features,like your eyebrows, your mouth,your nose over time.That's what these dots are for?Exactly, and it's mapping it into a probability score for each emotion.So let's give this a try.For instance, yeah, let's try smiling.Smile.We can see that the probability score of the joy classifier goes up.Let's try sadness.It's also mapping your most dominant emotion into an emoji,so you can experiment with, you know,eliciting a bunch of different emojis.Yeah, that's interesting.It also detects the presence of glasses and gender.So it's identified us both as female.And looks quite a lot like me too.Well, I'm excited for our students to try out this technology.

### 11. Emotion as a Service-2jAP3rP3USM.en

Thank you for sharing this with us.It sounds like you and your team have put inan incredible amount of effort to build this service.I'm wondering if we can use this to build apps of our own.Absolutely. At Affectiva we've made it really easy for you to incorporateemotion AI into your own applications and projects through a variety of STKs and APIs.Great. This way we can include emotion recognition software in a variety of applications.Next, it will be up to our students to get some practice withcomputer vision by using the software in a project of their own.

### img

## Part 10-Module 01-Lesson 01_Intro to NLP

### 01. Intro Arpan-MW5MWOLj064.en

Our first section will be taught by Arpan Chakraborty.Arpan has a Ph.D. in Computer Science and for several years,has taught at Udacity and at Georgia Tech. Hi Arpan.Hi Luis. Hello everyone.I'm glad to get you started on your journey through Natural Language Processing.Everything in NLP starts with raw text typically,produced by humans like you and me, and Luis here.This text is first processed using some simple transformations such as,splitting it into individual words,reducing verbs to their root form, et cetera.You need to do this before performing any other analysis or training complex models.This stage may sound simple but you have to becareful about how you process your raw text.It may affect the results you obtain further down the line. Ready? Let's do it.

### 02. Welcome to NLP-g-AlFF61p0I.en

Welcome to Natural Language Processing.Language is an important medium for human communication.It allows us to convey information,express our ideas, and give instructions to others.Some philosophers argue that it enablesus to form complex thoughts and reason about them.It may turn out to be a critical component of human intelligence.Now consider the various artificial systems we interact with every day,phones, cars, websites, coffee machines.It's natural to expect them to be able to process and understand human language, right?Yet, computers are still lagging behind.No doubt, we have madesome incredible progress in the field of natural language processing,but there is still a long way to go.And that's what makes this an exciting and dynamic area of study.In this lesson you will not only get to knowmore about the applications and challenges in NLP,you will learn how to design an intelligent application thatuses NLP techniques and deploy it on a scalable platform.Sounds fun? Let's get started.

### 03. Structured Languages-NsmqUIHlk6U.en

What makes it so hard for computers to understand us?One drawback of human languages,or feature depending on how you look at it,is the lack of a precisely defined structure.To understand how that makes things difficult let'sfirst take a look at some languages that are more structured.Mathematics, for instance, uses a structured language.When I write y equals 2x plus 5 there is no ambiguity in what I want to convey.I'm saying that the variable y is related to the variable x as two times x plus five.Formal logic also uses a structure language.For example, consider the expression parent(x,y) and parent(x,z) implies sibling(y, z).This statement is asserting that if x is a parent of y and x is a parent of z,then y and z are siblings.A set of structure languages that may be morefamiliar to you are scripting and programming languages.Consider this SQL statement.SELECT name, email FROM users WHERE name LIKE A%.We are asking the database to return the names ande-mail addresses of all users whose names begin with an A.These languages are designed to be asunambiguous as possible and are suitable for computers to process.

### 04. Grammar-Jw3dA7xmoQ4.en

Structured languages are easy to parse and understand forcomputers because they are defined by a strict set of rules or grammar.There are standard forms of expressing such grammars and algorithms,that can parse properly formed statements to understand exactly what is meant.When a statement doesn't match the prescribed grammar,a typical computer doesn't try to guess the meaning, it simply gives up.Such violations of grammatical rules are reported as syntax errors.

### 05. Unstructured Text-OmwSdaec5vU.en

The languages we use to communicate with each other also have defined grammatical rules.And indeed, in some situations we usesimple structured sentences but forthe most part human discourse is complex and unstructured.Despite that, we seem to be really good at understandingeach other and even ambiguities are welcome to a certain extent.So, what can computers do to make sense of unstructured text?Here are some preliminary ideas.Computers can do some level of processing with words and phrases,trying to identify key words,parts of speech, named entities, dates, quantities, etc.Using this information they can also try to parse sentences,at least ones that are relatively more structured.This can help extract the relevant parts of statements, questions, or instructions.At a higher level computers can analyze documents to find frequent and rare words,assess the overall tone or sentiment being expressed,and even cluster or group similar documents together.You can imagine that building on top of these ideas,computers can do a whole lot withunstructured text even if they cannot understand it like us.

### 07. Context-J-4pfu2w1C0.en

So what is stopping computers from becoming as capableas humans in understanding natural language?Part of the problem lies in the variability and complexity of our sentences.Consider this excerpt from a movie review."I was lured to see this on the promise ofa smart witty slice of old fashioned fun and intrigue. I was conned. "Although it starts withsome potentially positive words it turns out to be a strongly negative review.Sentences like this might be somewhat entertaining forus but computers tend to make mistakes when trying to analyze them.But there is a bigger challenge that makes NLP harder than you think.Take a look at this sentence."The sofa didn't fit through the door because it was too narrow."What does "it" refer to?Clearly "it" refers to the door.Now consider a slight variation of this sentence."The sofa didn't fit through the door because it was too wide."What does "it" refer to in this case?Here it's the sofa. Think about it.To understand the proper meaning or semantics ofthe sentence you implicitly applied your knowledge about the physical world,that wide things don't fit through narrow things.You may have experienced a similar situation before.You can imagine that there are countless other scenarios in which some knowledge orcontext is indispensable for correctly understanding what is being said.

### 08. Natural Language Processing-UQBxJzoCp-I.en

Natural language processing is one of the fastest growing fields in the world.NLP Is making its way into a number of products and services that we use every day.Let's begin with an overview of how to design an end-to-end NLP pipeline.Not that kind of pipeline;a natural language processing pipeline,where you start with raw text,in whatever form it is available, process it,extract relevant features, and build models to accomplish various NLP tasks.Now that I think about it,that is kind of like refining crude oil.Anyways, you'll learn how these different stages in the pipeline depend on each other.You'll also learn how to make design decisions,how to choose existing libraries,and tools to perform each step.

### 09. NLP M1-L1 01 NLP Pipeline-vJx6oKlu_MM.en

Let's look at a common NLP pipeline.It consists of three stages,text processing, feature extraction and modeling.Each stage transforms text in some way and produces a result that the next stage needs.For example, the goal of text processing is to take raw input text,clean it, normalize it,and convert it into a form that is suitable for feature extraction.Similarly, the next stage needs to extract and produce feature representations that areappropriate for that type of model you're planning touse and the NLP task you're trying to accomplish.When you're building such a pipeline,your workflow may not be perfectly linear.Let's say, you spend some time implementing text processing functions,then make some simple feature extractors,and then design a baseline statistical model.But then, maybe you are not happy with the results.So you go back and rethink what features you need,and that in turn,can make you change your processing routines.Keep in mind that this is a very simplified view of natural language processing.Your application may require additional steps.

### 10. Text Processing-pqheVyctkNQ.en

Let's take a closer look at text processing.The first question that comes to mind is,why do we need to process text?Why can we not feed it in directly?To understand that, think about where we get this text to begin with.Websites are a common source of textual information.Here's a portion of a sample web page from Wikipedia and the corresponding HTML markup,which serves as our raw input.For the purpose of natural language processing,you would typically want to get rid of all or most of the HTML tags,and retain only plain text.You can also remove or set aside any URLs or other items not relevant to your task.The Web is probably the most common and fastest growing source of textual content.But you may also need to consume PDFs,Word documents or other file formats.Or your raw input may even come froma speech recognition system or from a book scan using OCR.Some knowledge of the source medium can help you properly handle the input.In the end, your goal is to extract plain text that is free ofany source specific markers or constructs that are not relevant to your task.Once you have obtained plain text,some further processing may be necessary.For instance, capitalization doesn't usually change the meaning of a word.We can convert all the words to the same case so that they're not treated differently.Punctuation marks that we use to indicate pauses, etc.can also be removed.Some common words in a language often help provide structure,but don't add much meaning.For example, a, and,the, of, are, and so on.Sometimes it's best to remove them if that helpsreduce the complexity of the procedures you want to apply later.

### 11. Feature Extraction-UgENzCmfFWE.en

Okay. We now have clean normalized text.Can we feed this into a statistical or a machine learning model?Not quite. Let's see why.Text data is represented on modern computers using an encodingsuch as ASCII or Unicode that maps every character to a number.Computer store and transmit these values as binary, zeros and ones.These numbers also have an implicit ordering.65 is less than 66 which is less than 67.But does that mean A is less than B,and B is less and C?No. In fact, that would be an incorrect assumption tomake and might mislead our natural language processing algorithms.Moreover, individual characters don't carry much meaning at all.It is words that we should be concerned with,but computers don't have a standard representation for words.Yes, internally they are just sequences of ASCII or Unicodevalues but they don't quite capture the meanings or relationships between words.Compare this with how an image is represented in computer memory.Each pixel value contains the relative intensity of light at that spot in the image.For a color image,we keep one value per primary color;red, green, and blue.These values carry relevant information.Two pixels with similar values are perceptually similar.Therefore, it makes sense to directly use pixel values in a numerical model.Yes, some feature engineering may be necessary such as edge detection or filtering,but pixels are a good starting point.So the question is,how do we come up with a similar representation fortext data that we can use as features for modeling?The answer again depends on what kind of model you'reusing and what task you're trying to accomplish.If you want to use a graph based model to extract insights,you may want to represent your words assymbolic nodes with relationships between them like WordNet.For statistical models however,you need some sort of numerical representation.Even then, you have to think about the end goal.If you're trying to perform a document level task,such as spam detection or sentiment analysis,you may want to use a per document representations such as bag-of-words or doc2vec.If you want to work with individual words and phrasessuch as for text generation or machine translation,you'll need a word level representation such as word2vec or glove.There are many ways of representing textual information,and it is only through practice that you can learn what you need for each problem.

### 12. Modeling-P4w_2rkxBvE.en

The final stage in this process is what I like to call modeling.This includes designing a model,usually a statistical or a machine learning model,fitting its parameters to training data using an optimization procedure,and then using it to make predictions about unseen data.The nice thing about working with numerical features is thatit allows you to utilize pretty much any machine learning model.This includes support vector machines, decision trees,neural networks, or any custom model of your choice.You could even combine multiple models to get better performance.How you utilize the model is up to you.You can deploy it as a web-based application,package it up into a handy mobile app,integrate it with other products,services, and so on.The possibilities are endless.

## assets

### css

#### fonts

### img

### js

